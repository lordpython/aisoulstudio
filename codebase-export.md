This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

# File Summary

## Purpose
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: docs/**, plans/**, last-run/**, **/*.test.ts, **/*.test.tsx, project-export.json, pnpm-lock.yaml, story-pipeline-output.md, codebase-export.md, repomix-output.json, public/production_*/**
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)

# Directory Structure
```
packages/frontend/App.tsx
packages/frontend/components/AgentProgress.tsx
packages/frontend/components/AmbientBackground.tsx
packages/frontend/components/auth/AuthModal.tsx
packages/frontend/components/auth/index.ts
packages/frontend/components/auth/UserMenu.tsx
packages/frontend/components/chat/ChatInput.tsx
packages/frontend/components/chat/index.ts
packages/frontend/components/chat/MessageBubble.tsx
packages/frontend/components/chat/QuickActions.tsx
packages/frontend/components/CheckpointApproval.tsx
packages/frontend/components/ErrorBoundary.tsx
packages/frontend/components/FormatSelector.tsx
packages/frontend/components/gradient-generator/GradientControls.tsx
packages/frontend/components/gradient-generator/GradientExport.tsx
packages/frontend/components/gradient-generator/GradientGenerator.tsx
packages/frontend/components/gradient-generator/GradientPresets.tsx
packages/frontend/components/gradient-generator/GradientPreview.tsx
packages/frontend/components/gradient-generator/hooks.ts
packages/frontend/components/gradient-generator/index.ts
packages/frontend/components/gradient-generator/types.ts
packages/frontend/components/gradient-generator/utils.ts
packages/frontend/components/HomeView.tsx
packages/frontend/components/IntroAnimation.css
packages/frontend/components/IntroAnimation.tsx
packages/frontend/components/layout/AppLayout.tsx
packages/frontend/components/layout/AppShell.tsx
packages/frontend/components/layout/DirectionalIcon.tsx
packages/frontend/components/layout/Header.tsx
packages/frontend/components/layout/index.ts
packages/frontend/components/layout/LanguageSwitcher.tsx
packages/frontend/components/layout/ScreenLayout.tsx
packages/frontend/components/layout/Sidebar.tsx
packages/frontend/components/MusicChatModalV2.tsx
packages/frontend/components/MusicGeneratorModal.tsx
packages/frontend/components/PipelineProgress.tsx
packages/frontend/components/projects/ProjectCard.tsx
packages/frontend/components/QualityDashboard.tsx
packages/frontend/components/QuickExport.tsx
packages/frontend/components/QuickUpload.tsx
packages/frontend/components/ReferenceDocumentUpload.tsx
packages/frontend/components/SceneEditor.tsx
packages/frontend/components/SEO/index.ts
packages/frontend/components/SEO/StructuredData.tsx
packages/frontend/components/SettingsModal.tsx
packages/frontend/components/story/BreakdownProgress.tsx
packages/frontend/components/story/CharacterView.tsx
packages/frontend/components/story/ExportOptionsPanel.tsx
packages/frontend/components/story/IdeaView.tsx
packages/frontend/components/story/index.ts
packages/frontend/components/story/LockWarningDialog.tsx
packages/frontend/components/story/SceneCard.tsx
packages/frontend/components/story/ScriptView.tsx
packages/frontend/components/story/ShotEditorModal.tsx
packages/frontend/components/story/StepProgressBar.tsx
packages/frontend/components/story/StoryboardProgress.tsx
packages/frontend/components/story/StoryboardView.tsx
packages/frontend/components/story/StoryWorkspace.tsx
packages/frontend/components/story/StoryWorkspaceErrorBoundary.tsx
packages/frontend/components/story/StyleSelector.tsx
packages/frontend/components/story/TemplatesGallery.tsx
packages/frontend/components/story/VersionHistoryPanel.tsx
packages/frontend/components/TimelineEditor/AudioClip.tsx
packages/frontend/components/TimelineEditor/AudioTimelineEditor.tsx
packages/frontend/components/TimelineEditor/editor/ImageClipComponent.tsx
packages/frontend/components/TimelineEditor/editor/ImportMediaModal.tsx
packages/frontend/components/TimelineEditor/editor/index.ts
packages/frontend/components/TimelineEditor/editor/SubtitleClip.tsx
packages/frontend/components/TimelineEditor/editor/TimelineControls.tsx
packages/frontend/components/TimelineEditor/editor/TimelinePanel.tsx
packages/frontend/components/TimelineEditor/editor/TrackSidebar.tsx
packages/frontend/components/TimelineEditor/editor/VideoClipComponent.tsx
packages/frontend/components/TimelineEditor/editor/VideoPreview.tsx
packages/frontend/components/TimelineEditor/editor/WaveformClip.tsx
packages/frontend/components/TimelineEditor/FooterNav.tsx
packages/frontend/components/TimelineEditor/graphite-timeline-utils.ts
packages/frontend/components/TimelineEditor/graphite-timeline.css
packages/frontend/components/TimelineEditor/GraphiteClip.tsx
packages/frontend/components/TimelineEditor/GraphiteTimeline.tsx
packages/frontend/components/TimelineEditor/index.ts
packages/frontend/components/TimelineEditor/Playhead.tsx
packages/frontend/components/TimelineEditor/timelineAdapter.ts
packages/frontend/components/TimelineEditor/TimeRuler.tsx
packages/frontend/components/TimelineEditor/TrackLabel.tsx
packages/frontend/components/TimelineEditor/TrackLane.tsx
packages/frontend/components/TimelineEditor/TransportBar.tsx
packages/frontend/components/TimelineEditor/useTimelineScroll.ts
packages/frontend/components/TimelinePlayer.tsx
packages/frontend/components/ui/badge.tsx
packages/frontend/components/ui/button.tsx
packages/frontend/components/ui/card.tsx
packages/frontend/components/ui/dialog.tsx
packages/frontend/components/ui/dropdown-menu.tsx
packages/frontend/components/ui/ErrorState.tsx
packages/frontend/components/ui/input.tsx
packages/frontend/components/ui/label.tsx
packages/frontend/components/ui/LoadingState.tsx
packages/frontend/components/ui/MarkdownContent.tsx
packages/frontend/components/ui/progress.tsx
packages/frontend/components/ui/scroll-area.tsx
packages/frontend/components/ui/select.tsx
packages/frontend/components/ui/skeleton.tsx
packages/frontend/components/ui/skip-link.tsx
packages/frontend/components/ui/SlidePanel.tsx
packages/frontend/components/ui/slider.tsx
packages/frontend/components/ui/switch.tsx
packages/frontend/components/ui/tabs.tsx
packages/frontend/components/ui/textarea.tsx
packages/frontend/components/ui/toast.tsx
packages/frontend/components/ui/toaster.tsx
packages/frontend/components/ui/tooltip.tsx
packages/frontend/components/ui/use-toast.tsx
packages/frontend/components/VideoEditor/CanvasPreview.tsx
packages/frontend/components/VideoEditor/clips/AudioTrackClip.tsx
packages/frontend/components/VideoEditor/clips/ImageTrackClip.tsx
packages/frontend/components/VideoEditor/clips/TextClip.tsx
packages/frontend/components/VideoEditor/clips/VideoTrackClip.tsx
packages/frontend/components/VideoEditor/EnhancedTransportBar.tsx
packages/frontend/components/VideoEditor/hooks/useVideoEditorStore.ts
packages/frontend/components/VideoEditor/index.ts
packages/frontend/components/VideoEditor/MultiTrackTimeline.tsx
packages/frontend/components/VideoEditor/ToolPanels.tsx
packages/frontend/components/VideoEditor/TrackLabelPanel.tsx
packages/frontend/components/VideoEditor/TrackRow.tsx
packages/frontend/components/VideoEditor/types/video-editor-types.ts
packages/frontend/components/VideoEditor/video-editor.css
packages/frontend/components/VideoEditor/VideoEditor.tsx
packages/frontend/components/VideoEditor/VideoEditorToolbar.tsx
packages/frontend/components/VideoExportModal.tsx
packages/frontend/components/VideoPreviewCard.tsx
packages/frontend/components/visualizer/AudioUploadForm.tsx
packages/frontend/components/visualizer/index.ts
packages/frontend/components/visualizer/SceneThumbnails.tsx
packages/frontend/components/visualizer/VisualPreview.tsx
packages/frontend/hooks/useAuth.ts
packages/frontend/hooks/useFocusTrap.ts
packages/frontend/hooks/useFormatPipeline.ts
packages/frontend/hooks/useLyricLens.ts
packages/frontend/hooks/useMediaPlayback.ts
packages/frontend/hooks/useModalState.ts
packages/frontend/hooks/useProjectSession.ts
packages/frontend/hooks/useStoryGeneration.ts
packages/frontend/hooks/useStoryProject.ts
packages/frontend/hooks/useSunoMusic.ts
packages/frontend/hooks/useTimelineAdapter.ts
packages/frontend/hooks/useTimelineKeyboard.ts
packages/frontend/hooks/useTimelineSelection.ts
packages/frontend/hooks/useVideoNarration.ts
packages/frontend/hooks/useVideoProductionCore.ts
packages/frontend/hooks/useVideoProductionRefactored.ts
packages/frontend/hooks/useVideoPromptTools.ts
packages/frontend/hooks/useVideoQuality.ts
packages/frontend/hooks/useVideoSFX.ts
packages/frontend/hooks/useVideoVisuals.ts
packages/frontend/i18n/index.ts
packages/frontend/i18n/locales/ar.json
packages/frontend/i18n/locales/en.json
packages/frontend/i18n/useLanguage.ts
packages/frontend/index.tsx
packages/frontend/router/guards/index.ts
packages/frontend/router/guards/UnsavedChangesGuard.tsx
packages/frontend/router/index.tsx
packages/frontend/router/RouteLayout.tsx
packages/frontend/router/routes.ts
packages/frontend/screens/GradientGeneratorScreen.tsx
packages/frontend/screens/HomeScreen.tsx
packages/frontend/screens/index.ts
packages/frontend/screens/NotFoundScreen.tsx
packages/frontend/screens/ProjectsScreen.tsx
packages/frontend/screens/SettingsScreen.tsx
packages/frontend/screens/SignInScreen.tsx
packages/frontend/screens/StudioScreen.tsx
packages/frontend/screens/VisualizerScreen.tsx
packages/server/env.ts
packages/server/index.ts
packages/server/routes/cloud.ts
packages/server/routes/deapi.ts
packages/server/routes/director.ts
packages/server/routes/export.ts
packages/server/routes/gemini.ts
packages/server/routes/health.ts
packages/server/routes/import.ts
packages/server/routes/suno.ts
packages/server/routes/video.ts
packages/server/services/encoding/encoderStrategy.ts
packages/server/services/jobQueue/index.ts
packages/server/services/jobQueue/jobStore.ts
packages/server/services/jobQueue/timeoutManager.ts
packages/server/services/validation/frameValidator.ts
packages/server/services/validation/qualityVerifier.ts
packages/server/types.ts
packages/server/types/renderJob.ts
packages/server/utils/index.ts
packages/server/workers/ffmpegWorker.ts
packages/server/workers/workerPool.ts
packages/shared/src/constants/index.ts
packages/shared/src/constants/languages.ts
packages/shared/src/constants/layout.ts
packages/shared/src/constants/styles.ts
packages/shared/src/constants/video.ts
packages/shared/src/constants/visualStyles.ts
packages/shared/src/lib/cinematicMotion.ts
packages/shared/src/lib/utils.ts
packages/shared/src/services/agent/agentLogger.ts
packages/shared/src/services/agent/agentMetrics.ts
packages/shared/src/services/agent/agentTools.ts
packages/shared/src/services/agent/audioMixingTools.ts
packages/shared/src/services/agent/audioUtils.ts
packages/shared/src/services/agent/cloudStorageTools.ts
packages/shared/src/services/agent/enhancementTools.ts
packages/shared/src/services/agent/errorRecovery.ts
packages/shared/src/services/agent/errors.ts
packages/shared/src/services/agent/exportTools.ts
packages/shared/src/services/agent/importTools.ts
packages/shared/src/services/agent/importUtils.ts
packages/shared/src/services/agent/index.ts
packages/shared/src/services/agent/intentDetection.ts
packages/shared/src/services/agent/schemas/exportSchemas.ts
packages/shared/src/services/agent/schemas/importSchemas.ts
packages/shared/src/services/agent/schemas/index.ts
packages/shared/src/services/agent/subtitleTools.ts
packages/shared/src/services/agent/toolRegistry.ts
packages/shared/src/services/agentDirectorService.ts
packages/shared/src/services/agentOrchestrator.ts
packages/shared/src/services/ai/config.ts
packages/shared/src/services/ai/index.ts
packages/shared/src/services/ai/nlpIntentParser.ts
packages/shared/src/services/ai/production/agentCore.ts
packages/shared/src/services/ai/production/agentExecutor.ts
packages/shared/src/services/ai/production/errorHandler.ts
packages/shared/src/services/ai/production/index.ts
packages/shared/src/services/ai/production/persistence.ts
packages/shared/src/services/ai/production/prompts.ts
packages/shared/src/services/ai/production/REFACTORING_SUMMARY.md
packages/shared/src/services/ai/production/resultCache.ts
packages/shared/src/services/ai/production/store.ts
packages/shared/src/services/ai/production/toolRegistration.ts
packages/shared/src/services/ai/production/tools/contentTools.ts
packages/shared/src/services/ai/production/tools/index.ts
packages/shared/src/services/ai/production/tools/mediaTools.ts
packages/shared/src/services/ai/production/tools/statusTools.ts
packages/shared/src/services/ai/production/tools/storyTools.ts
packages/shared/src/services/ai/production/types.ts
packages/shared/src/services/ai/production/utils.ts
packages/shared/src/services/ai/productionAgent.ts
packages/shared/src/services/ai/rag/documents/bestPractices.ts
packages/shared/src/services/ai/rag/documents/styleGuides.ts
packages/shared/src/services/ai/rag/exampleLibrary.ts
packages/shared/src/services/ai/rag/knowledgeBase.ts
packages/shared/src/services/ai/shotBreakdownAgent.ts
packages/shared/src/services/ai/storyPipeline.ts
packages/shared/src/services/ai/studioAgent.ts
packages/shared/src/services/ai/subagents/contentSubagent.ts
packages/shared/src/services/ai/subagents/enhancementExportSubagent.ts
packages/shared/src/services/ai/subagents/importSubagent.ts
packages/shared/src/services/ai/subagents/index.ts
packages/shared/src/services/ai/subagents/mediaSubagent.ts
packages/shared/src/services/ai/subagents/supervisorAgent.ts
packages/shared/src/services/ai/workflowTriggerService.ts
packages/shared/src/services/aiLogService.ts
packages/shared/src/services/assetCalculatorService.ts
packages/shared/src/services/audioConcatService.ts
packages/shared/src/services/audioMixerService.ts
packages/shared/src/services/characterService.ts
packages/shared/src/services/checkpointSystem.ts
packages/shared/src/services/cloudStorageService.ts
packages/shared/src/services/contentPlannerService.ts
packages/shared/src/services/deapiService.ts
packages/shared/src/services/directorService.ts
packages/shared/src/services/documentParser.ts
packages/shared/src/services/editorService.ts
packages/shared/src/services/errorAggregator.ts
packages/shared/src/services/exportFormatsService.ts
packages/shared/src/services/ffmpeg/assetLoader.ts
packages/shared/src/services/ffmpeg/checksumGenerator.ts
packages/shared/src/services/ffmpeg/envUtils.ts
packages/shared/src/services/ffmpeg/exportConfig.ts
packages/shared/src/services/ffmpeg/exporters.ts
packages/shared/src/services/ffmpeg/exportPresets.ts
packages/shared/src/services/ffmpeg/formatAssembly.ts
packages/shared/src/services/ffmpeg/frameRenderer.ts
packages/shared/src/services/ffmpeg/index.ts
packages/shared/src/services/ffmpeg/sseClient.ts
packages/shared/src/services/ffmpeg/textRenderer.ts
packages/shared/src/services/ffmpeg/transitions.ts
packages/shared/src/services/ffmpeg/videoAudioExtractor.ts
packages/shared/src/services/ffmpeg/visualizer.ts
packages/shared/src/services/firebase/authService.ts
packages/shared/src/services/firebase/config.ts
packages/shared/src/services/firebase/index.ts
packages/shared/src/services/firebase/storySync.ts
packages/shared/src/services/formatRegistry.ts
packages/shared/src/services/formatRouter.ts
packages/shared/src/services/formatValidation.ts
packages/shared/src/services/freesoundService.ts
packages/shared/src/services/geminiService.ts
packages/shared/src/services/imageService.ts
packages/shared/src/services/jsonExtractor.ts
packages/shared/src/services/languageDetector.ts
packages/shared/src/services/logger.ts
packages/shared/src/services/musicProducerAgentV2.ts
packages/shared/src/services/narratorService.ts
packages/shared/src/services/parallelExecutionEngine.ts
packages/shared/src/services/pipelines/advertisement.ts
packages/shared/src/services/pipelines/documentary.ts
packages/shared/src/services/pipelines/educational.ts
packages/shared/src/services/pipelines/movieAnimation.ts
packages/shared/src/services/pipelines/musicVideo.ts
packages/shared/src/services/pipelines/newsPolitics.ts
packages/shared/src/services/pipelines/shorts.ts
packages/shared/src/services/pipelines/youtubeNarrator.ts
packages/shared/src/services/projectService.ts
packages/shared/src/services/projectTemplatesService.ts
packages/shared/src/services/prompt/imageStyleGuide.ts
packages/shared/src/services/prompt/index.ts
packages/shared/src/services/prompt/personaData.ts
packages/shared/src/services/prompt/sceneShotTemplates.ts
packages/shared/src/services/prompt/styleEnhancements.ts
packages/shared/src/services/prompt/templateLoader.ts
packages/shared/src/services/prompt/templates/advertisement/breakdown.txt
packages/shared/src/services/prompt/templates/advertisement/cta-creation.txt
packages/shared/src/services/prompt/templates/advertisement/screenplay.txt
packages/shared/src/services/prompt/templates/advertisement/script-generation.txt
packages/shared/src/services/prompt/templates/documentary/breakdown.txt
packages/shared/src/services/prompt/templates/documentary/chapter-structure.txt
packages/shared/src/services/prompt/templates/documentary/screenplay.txt
packages/shared/src/services/prompt/templates/documentary/script-generation.txt
packages/shared/src/services/prompt/templates/educational/breakdown.txt
packages/shared/src/services/prompt/templates/educational/learning-objectives.txt
packages/shared/src/services/prompt/templates/educational/screenplay.txt
packages/shared/src/services/prompt/templates/educational/script-generation.txt
packages/shared/src/services/prompt/templates/movie-animation/breakdown.txt
packages/shared/src/services/prompt/templates/movie-animation/screenplay.txt
packages/shared/src/services/prompt/templates/movie-animation/script-generation.txt
packages/shared/src/services/prompt/templates/music-video/breakdown.txt
packages/shared/src/services/prompt/templates/music-video/lyrics-generation.txt
packages/shared/src/services/prompt/templates/music-video/screenplay.txt
packages/shared/src/services/prompt/templates/music-video/script-generation.txt
packages/shared/src/services/prompt/templates/news-politics/breakdown.txt
packages/shared/src/services/prompt/templates/news-politics/screenplay.txt
packages/shared/src/services/prompt/templates/news-politics/script-generation.txt
packages/shared/src/services/prompt/templates/news-politics/source-citation.txt
packages/shared/src/services/prompt/templates/shorts/breakdown.txt
packages/shared/src/services/prompt/templates/shorts/hook-creation.txt
packages/shared/src/services/prompt/templates/shorts/screenplay.txt
packages/shared/src/services/prompt/templates/shorts/script-generation.txt
packages/shared/src/services/prompt/templates/youtube-narrator/breakdown.txt
packages/shared/src/services/prompt/templates/youtube-narrator/hook-creation.txt
packages/shared/src/services/prompt/templates/youtube-narrator/screenplay.txt
packages/shared/src/services/prompt/templates/youtube-narrator/script-generation.txt
packages/shared/src/services/prompt/vibeLibrary.ts
packages/shared/src/services/promptFormatService.ts
packages/shared/src/services/promptService.ts
packages/shared/src/services/qualityMonitorService.ts
packages/shared/src/services/researchService.ts
packages/shared/src/services/secureApiClient.ts
packages/shared/src/services/sfxService.ts
packages/shared/src/services/shared/apiClient.ts
packages/shared/src/services/shared/robustUtils.ts
packages/shared/src/services/subtitleService.ts
packages/shared/src/services/subtitleStyleService.ts
packages/shared/src/services/sunoService.ts
packages/shared/src/services/textSanitizer.ts
packages/shared/src/services/tracing/Based on the code you uploaded, I've ana.md
packages/shared/src/services/tracing/index.ts
packages/shared/src/services/tracing/langsmithTracing.ts
packages/shared/src/services/transcriptionService.ts
packages/shared/src/services/translationService.ts
packages/shared/src/services/tripletUtils.ts
packages/shared/src/services/tts/deliveryMarkers.ts
packages/shared/src/services/utils/index.ts
packages/shared/src/services/utils/textProcessing.ts
packages/shared/src/services/versionHistoryService.ts
packages/shared/src/services/videoService.ts
packages/shared/src/services/visualConsistencyService.ts
packages/shared/src/stores/appStore.ts
packages/shared/src/stores/index.ts
packages/shared/src/types.ts
packages/shared/src/types/arabic-reshaper.d.ts
packages/shared/src/types/audio-editor.ts
packages/shared/src/utils/audioAnalysis.ts
packages/shared/src/utils/costEstimator.ts
packages/shared/src/utils/platformUtils.ts
packages/shared/src/utils/srtParser.ts
packages/shared/src/utils/testData.ts
packages/shared/src/utils/timeFormatting.ts
scripts/auto-generate-sitemap.ts
scripts/download_mp3.sh
scripts/generate-sitemap.ts
scripts/test-ffmpeg-export.ts
scripts/test-story-pipeline.ts
vitest.config.ts
```

# Files

## File: packages/frontend/App.tsx
````typescript
/**
 * Main App Component
 * Requirements: 3.1 - Support Arabic and English languages
 * Requirements: 2.1, 2.2 - Use React Router for all navigation
 * Requirements: 1.3 - Remove ProductionView and SleekProductionView
 */

import { useState, Suspense, lazy } from "react";
import { AnimatePresence } from "framer-motion";
import { TooltipProvider } from "@/components/ui/tooltip";
import { Toaster } from "@/components/ui/toaster";
import { ErrorBoundary } from "./components/ErrorBoundary";
import { AppShell } from "./components/layout/AppShell";
import { AppRouter } from "./router";

// Import i18n configuration - this initializes i18next
import "./i18n";

// Lazy load intro animation
const IntroAnimation = lazy(() => import("./components/IntroAnimation").then(m => ({ default: m.IntroAnimation })));

export default function App() {
  // UI-specific state for intro animation
  const [showIntro, setShowIntro] = useState(true);

  return (
    <TooltipProvider>
      <ErrorBoundary>
        <AppShell>
          {/* Intro Animation */}
          <AnimatePresence>
            {showIntro && (
              <Suspense fallback={<div className="fixed inset-0 bg-slate-900 z-50" />}>
                <IntroAnimation onComplete={() => setShowIntro(false)} />
              </Suspense>
            )}
          </AnimatePresence>

          {/* Main App with React Router */}
          <AppRouter />
          
          {/* Toast Notifications */}
          <Toaster />
        </AppShell>
      </ErrorBoundary>
    </TooltipProvider>
  );
}
````

## File: packages/frontend/components/AgentProgress.tsx
````typescript
/**
 * AgentProgress Component
 * 
 * Real-time visualization of the multi-agent production pipeline.
 * Shows progress through: ContentPlanner → Narrator → Editor → Export
 * Includes accessible aria-live regions for screen reader support
 */

import React from "react";
import { motion } from "framer-motion";
import {
    FileText,
    Mic,
    Film,
    CheckCircle2,
    Loader2,
    Sparkles
} from "lucide-react";
import { cn } from "@/lib/utils";
import { ProductionProgress, ProductionStage } from "@/services/agentOrchestrator";

interface AgentProgressProps {
    progress: ProductionProgress | null;
    className?: string;
}

interface StageInfo {
    id: ProductionStage;
    label: string;
    icon: React.ReactNode;
}

const STAGES: StageInfo[] = [
    { id: "content_planning", label: "Planning", icon: <FileText className="w-5 h-5" /> },
    { id: "narrating", label: "Narration", icon: <Mic className="w-5 h-5" /> },
    { id: "generating_visuals", label: "Visuals", icon: <Film className="w-5 h-5" /> },
    { id: "validating", label: "Review", icon: <CheckCircle2 className="w-5 h-5" /> },
];

function getStageStatus(
    stageId: ProductionStage,
    currentStage: ProductionStage | undefined
): "pending" | "active" | "complete" {
    if (!currentStage) return "pending";

    const stageOrder = STAGES.map(s => s.id);
    const currentIndex = stageOrder.indexOf(currentStage);
    const stageIndex = stageOrder.indexOf(stageId);

    if (currentStage === "complete") return "complete";
    if (currentStage === "adjusting") {
        // During adjustment, show validating as active
        return stageId === "validating" ? "active" : stageIndex < 3 ? "complete" : "pending";
    }

    if (stageIndex < currentIndex) return "complete";
    if (stageIndex === currentIndex) return "active";
    return "pending";
}

function getAccessibleStageLabel(stage: ProductionStage | undefined): string {
    if (!stage) return "Not started";
    switch (stage) {
        case "content_planning": return "Content Planning";
        case "narrating": return "Narration";
        case "generating_visuals": return "Generating Visuals";
        case "validating": return "Review and Validation";
        case "adjusting": return "Making Adjustments";
        case "complete": return "Complete";
        default: return stage;
    }
}

export function AgentProgress({ progress, className }: AgentProgressProps) {
    if (!progress) return null;

    // Generate accessible status description
    const getAccessibleStatus = () => {
        const stageLabel = getAccessibleStageLabel(progress.stage);
        const progressText = `${Math.round(progress.progress)}% complete`;
        const sceneText = progress.currentScene && progress.totalScenes
            ? `, scene ${progress.currentScene} of ${progress.totalScenes}`
            : "";
        return `${stageLabel} stage: ${progressText}${sceneText}. ${progress.message}`;
    };

    return (
        <div
            className={cn("rounded-xl bg-slate-900/50 border border-slate-700/50 p-6", className)}
            role="status"
            aria-live="polite"
            aria-label={getAccessibleStatus()}
        >
            {/* Stage indicators */}
            <nav
                className="flex items-center justify-between mb-6"
                aria-label="Production pipeline stages"
            >
                {STAGES.map((stage, index) => {
                    const status = getStageStatus(stage.id, progress.stage);

                    return (
                        <React.Fragment key={stage.id}>
                            {/* Stage node */}
                            <div className="flex flex-col items-center gap-2">
                                <motion.div
                                    className={cn(
                                        "w-12 h-12 rounded-full flex items-center justify-center transition-colors",
                                        status === "complete" && "bg-green-500/20 text-green-400 border-2 border-green-500/50",
                                        status === "active" && "bg-cyan-500/20 text-cyan-400 border-2 border-cyan-500/50",
                                        status === "pending" && "bg-slate-800 text-slate-500 border-2 border-slate-600/50"
                                    )}
                                    animate={status === "active" ? { scale: [1, 1.05, 1] } : {}}
                                    transition={{ duration: 1.5, repeat: Infinity }}
                                    aria-label={`${stage.label}: ${status === "complete" ? "completed" : status === "active" ? "in progress" : "pending"}`}
                                    aria-current={status === "active" ? "step" : undefined}
                                >
                                    {status === "active" ? (
                                        <Loader2 className="w-5 h-5 animate-spin" aria-hidden="true" />
                                    ) : status === "complete" ? (
                                        <CheckCircle2 className="w-5 h-5" aria-hidden="true" />
                                    ) : (
                                        <span aria-hidden="true">{stage.icon}</span>
                                    )}
                                </motion.div>
                                <span className={cn(
                                    "text-xs font-medium",
                                    status === "active" && "text-cyan-400",
                                    status === "complete" && "text-green-400",
                                    status === "pending" && "text-slate-500"
                                )}>
                                    {stage.label}
                                </span>
                            </div>

                            {/* Connector line */}
                            {index < STAGES.length - 1 && (
                                <div
                                    className="flex-1 mx-2 h-0.5 bg-slate-700 relative"
                                    aria-hidden="true"
                                >
                                    <motion.div
                                        className="absolute inset-y-0 left-0 bg-linear-to-r from-cyan-500 to-green-500"
                                        initial={{ width: "0%" }}
                                        animate={{
                                            width: (index + 1 < STAGES.length && getStageStatus(STAGES[index + 1]!.id, progress.stage) !== "pending")
                                                ? "100%"
                                                : getStageStatus(stage.id, progress.stage) === "active"
                                                    ? `${progress.progress}%`
                                                    : "0%"
                                        }}
                                        transition={{ duration: 0.5 }}
                                    />
                                </div>
                            )}
                        </React.Fragment>
                    );
                })}
            </nav>

            {/* Progress bar */}
            <div className="mb-4">
                <div
                    className="h-2 bg-slate-800 rounded-full overflow-hidden"
                    role="progressbar"
                    aria-valuenow={Math.round(progress.progress)}
                    aria-valuemin={0}
                    aria-valuemax={100}
                    aria-label={`Overall progress: ${Math.round(progress.progress)}%`}
                >
                    <motion.div
                        className="h-full bg-linear-to-r from-cyan-500 via-purple-500 to-pink-500"
                        initial={{ width: "0%" }}
                        animate={{ width: `${progress.progress}%` }}
                        transition={{ duration: 0.3 }}
                    />
                </div>
            </div>

            {/* Status message */}
            <div className="flex items-center gap-2 text-sm">
                <Sparkles className="w-4 h-4 text-purple-400" aria-hidden="true" />
                <span className="text-slate-300">{progress.message}</span>
                {progress.currentScene && progress.totalScenes && (
                    <span className="text-slate-500 ml-auto">
                        Scene {progress.currentScene}/{progress.totalScenes}
                    </span>
                )}
            </div>

            {/* Screen reader announcement for completion */}
            <div className="sr-only" aria-live="assertive" aria-atomic="true">
                {progress.stage === "complete" && "Production pipeline complete!"}
            </div>
        </div>
    );
}

/**
 * Compact version for inline display
 */
export function AgentProgressCompact({ progress, className }: AgentProgressProps) {
    if (!progress) return null;

    // Generate accessible status for compact view
    const getCompactAccessibleStatus = () => {
        const stageLabel = getAccessibleStageLabel(progress.stage);
        return `${stageLabel}: ${progress.message}`;
    };

    return (
        <div
            className={cn("flex items-center gap-3", className)}
            role="status"
            aria-live="polite"
            aria-label={getCompactAccessibleStatus()}
        >
            <div className="flex items-center gap-1" role="list" aria-label="Stage indicators">
                {STAGES.map((stage) => {
                    const status = getStageStatus(stage.id, progress.stage);
                    return (
                        <div
                            key={stage.id}
                            role="listitem"
                            aria-label={`${stage.label}: ${status === "complete" ? "completed" : status === "active" ? "in progress" : "pending"}`}
                            className={cn(
                                "w-2 h-2 rounded-full",
                                status === "complete" && "bg-green-500",
                                status === "active" && "bg-cyan-500 animate-pulse",
                                status === "pending" && "bg-slate-600"
                            )}
                        />
                    );
                })}
            </div>
            <span className="text-sm text-slate-400">{progress.message}</span>
        </div>
    );
}

export default AgentProgress;
````

## File: packages/frontend/components/AmbientBackground.tsx
````typescript
import React from "react";
import { cn } from "@/lib/utils";

/**
 * AmbientBackground Component
 *
 * Provides the signature "Deep Space" background for the Invisible Interface.
 * Features animated glowing orbs using the OKLCH design token palette.
 * Fixed position to sit behind all content.
 */
export function AmbientBackground({ className }: { className?: string }) {
    return (
        <div className={cn("fixed inset-0 z-[-1] overflow-hidden pointer-events-none", className)}>
            {/* Deep dark base is handled by body bg-background, but we ensure it here too */}
            <div className="absolute inset-0 bg-background/80" />

            {/* Primary Nebula — uses --primary (cyan/teal) */}
            <div
                className="absolute top-[-20%] left-[-10%] w-[70%] h-[70%] rounded-full blur-[120px] mix-blend-screen animate-pulse-slow"
                style={{
                    backgroundColor: 'oklch(0.70 0.15 190 / 0.12)',
                    animationDuration: '15s',
                }}
            />

            {/* Secondary Nebula — uses --secondary (deep indigo) */}
            <div
                className="absolute bottom-[-20%] right-[-10%] w-[60%] h-[60%] rounded-full blur-[100px] mix-blend-screen animate-pulse-slow"
                style={{
                    backgroundColor: 'oklch(0.30 0.08 240 / 0.10)',
                    animationDuration: '20s',
                    animationDelay: '2s',
                }}
            />

            {/* Accent Glow — uses --accent (supernova orange) */}
            <div
                className="absolute top-[40%] left-[60%] w-[30%] h-[30%] rounded-full blur-[80px] mix-blend-screen animate-pulse-slow"
                style={{
                    backgroundColor: 'oklch(0.65 0.25 30 / 0.06)',
                    animationDuration: '12s',
                    animationDelay: '5s',
                }}
            />
        </div>
    );
}
````

## File: packages/frontend/components/auth/AuthModal.tsx
````typescript
/**
 * AuthModal Component
 *
 * Modal dialog for user authentication (Google and email sign-in).
 * Matches the cinematic design system.
 */
import { useState } from 'react';
import {
  Dialog,
  DialogContent,
  DialogHeader,
  DialogTitle,
  DialogDescription,
} from '@/components/ui/dialog';
import { useAuth } from '@/hooks/useAuth';
import { Loader2, Mail, AlertCircle } from 'lucide-react';

interface AuthModalProps {
  open: boolean;
  onOpenChange: (open: boolean) => void;
}

export function AuthModal({ open, onOpenChange }: AuthModalProps) {
  const { signInWithGoogle, signInWithEmail, createAccount, error, clearError, isLoading } = useAuth();
  const [mode, setMode] = useState<'signin' | 'signup'>('signin');
  const [email, setEmail] = useState('');
  const [password, setPassword] = useState('');

  const handleGoogleSignIn = async () => {
    await signInWithGoogle();
    if (!error) {
      onOpenChange(false);
    }
  };

  const handleEmailSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (mode === 'signin') {
      await signInWithEmail(email, password);
    } else {
      await createAccount(email, password);
    }
    if (!error) {
      onOpenChange(false);
    }
  };

  const switchMode = () => {
    setMode(mode === 'signin' ? 'signup' : 'signin');
    clearError();
  };

  return (
    <Dialog open={open} onOpenChange={onOpenChange}>
      <DialogContent className="bg-[var(--cinema-celluloid)] border-[var(--cinema-silver)]/20 max-w-md">
        <DialogHeader>
          <DialogTitle className="font-display text-2xl text-[var(--cinema-silver)]">
            {mode === 'signin' ? 'Welcome Back' : 'Create Account'}
          </DialogTitle>
          <DialogDescription className="text-[var(--cinema-silver)]/60">
            {mode === 'signin'
              ? 'Sign in to sync your stories across devices'
              : 'Create an account to save your work to the cloud'}
          </DialogDescription>
        </DialogHeader>

        <div className="space-y-4 pt-4">
          {/* Google Sign In */}
          <button
            onClick={handleGoogleSignIn}
            disabled={isLoading}
            className="w-full flex items-center justify-center gap-3 px-4 py-3 bg-white hover:bg-gray-50 text-gray-800 rounded-lg font-medium transition-colors disabled:opacity-50"
          >
            {isLoading ? (
              <Loader2 className="w-5 h-5 animate-spin" />
            ) : (
              <svg className="w-5 h-5" viewBox="0 0 24 24">
                <path
                  fill="currentColor"
                  d="M22.56 12.25c0-.78-.07-1.53-.2-2.25H12v4.26h5.92c-.26 1.37-1.04 2.53-2.21 3.31v2.77h3.57c2.08-1.92 3.28-4.74 3.28-8.09z"
                />
                <path
                  fill="#34A853"
                  d="M12 23c2.97 0 5.46-.98 7.28-2.66l-3.57-2.77c-.98.66-2.23 1.06-3.71 1.06-2.86 0-5.29-1.93-6.16-4.53H2.18v2.84C3.99 20.53 7.7 23 12 23z"
                />
                <path
                  fill="#FBBC05"
                  d="M5.84 14.09c-.22-.66-.35-1.36-.35-2.09s.13-1.43.35-2.09V7.07H2.18C1.43 8.55 1 10.22 1 12s.43 3.45 1.18 4.93l2.85-2.22.81-.62z"
                />
                <path
                  fill="#EA4335"
                  d="M12 5.38c1.62 0 3.06.56 4.21 1.64l3.15-3.15C17.45 2.09 14.97 1 12 1 7.7 1 3.99 3.47 2.18 7.07l3.66 2.84c.87-2.6 3.3-4.53 6.16-4.53z"
                />
              </svg>
            )}
            Continue with Google
          </button>

          <div className="relative">
            <div className="absolute inset-0 flex items-center">
              <div className="w-full border-t border-[var(--cinema-silver)]/20" />
            </div>
            <div className="relative flex justify-center text-xs">
              <span className="bg-[var(--cinema-celluloid)] px-2 text-[var(--cinema-silver)]/50">
                or continue with email
              </span>
            </div>
          </div>

          {/* Email Form */}
          <form onSubmit={handleEmailSubmit} className="space-y-3">
            <div>
              <input
                type="email"
                value={email}
                onChange={(e) => setEmail(e.target.value)}
                placeholder="Email address"
                required
                className="w-full px-4 py-3 bg-[var(--cinema-void)] border border-[var(--cinema-silver)]/20 rounded-lg text-[var(--cinema-silver)] placeholder:text-[var(--cinema-silver)]/40 focus:outline-none focus:border-[var(--cinema-spotlight)]"
              />
            </div>
            <div>
              <input
                type="password"
                value={password}
                onChange={(e) => setPassword(e.target.value)}
                placeholder="Password"
                required
                minLength={6}
                className="w-full px-4 py-3 bg-[var(--cinema-void)] border border-[var(--cinema-silver)]/20 rounded-lg text-[var(--cinema-silver)] placeholder:text-[var(--cinema-silver)]/40 focus:outline-none focus:border-[var(--cinema-spotlight)]"
              />
            </div>

            {error && (
              <div className="flex items-center gap-2 text-[var(--cinema-velvet)] text-sm">
                <AlertCircle className="w-4 h-4 shrink-0" />
                <span>{error}</span>
              </div>
            )}

            <button
              type="submit"
              disabled={isLoading}
              className="w-full flex items-center justify-center gap-2 px-4 py-3 bg-[var(--cinema-spotlight)] hover:bg-[var(--cinema-spotlight)]/90 text-[var(--cinema-void)] rounded-lg font-medium transition-colors disabled:opacity-50"
            >
              {isLoading ? (
                <Loader2 className="w-5 h-5 animate-spin" />
              ) : (
                <Mail className="w-5 h-5" />
              )}
              {mode === 'signin' ? 'Sign In' : 'Create Account'}
            </button>
          </form>

          <div className="text-center text-sm text-[var(--cinema-silver)]/60">
            {mode === 'signin' ? (
              <>
                Don't have an account?{' '}
                <button
                  onClick={switchMode}
                  className="text-[var(--cinema-spotlight)] hover:underline"
                >
                  Sign up
                </button>
              </>
            ) : (
              <>
                Already have an account?{' '}
                <button
                  onClick={switchMode}
                  className="text-[var(--cinema-spotlight)] hover:underline"
                >
                  Sign in
                </button>
              </>
            )}
          </div>
        </div>
      </DialogContent>
    </Dialog>
  );
}

export default AuthModal;
````

## File: packages/frontend/components/auth/index.ts
````typescript
/**
 * Auth Components
 */
export { AuthModal } from './AuthModal';
export { UserMenu } from './UserMenu';
````

## File: packages/frontend/components/auth/UserMenu.tsx
````typescript
/**
 * UserMenu Component
 *
 * Dropdown menu for authenticated users showing profile and sign out.
 * Shows sign-in button when not authenticated.
 */
import { useState } from 'react';
import { useNavigate } from 'react-router-dom';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import { useAuth } from '@/hooks/useAuth';
import { AuthModal } from './AuthModal';
import { User, LogOut, Cloud, CloudOff } from 'lucide-react';

interface UserMenuProps {
  className?: string;
}

export function UserMenu({ className }: UserMenuProps) {
  const navigate = useNavigate();
  const { user, isAuthenticated, isAuthAvailable, signOut, isLoading } = useAuth();
  const [authModalOpen, setAuthModalOpen] = useState(false);

  // Navigate to dedicated sign-in page
  const handleSignInClick = () => {
    navigate('/signin');
  };

  // If Firebase is not configured, don't show anything
  if (!isAuthAvailable) {
    return null;
  }

  // Not authenticated - show sign in button (navigates to dedicated page)
  if (!isAuthenticated) {
    return (
      <>
        <button
          onClick={handleSignInClick}
          disabled={isLoading}
          className={`flex items-center gap-2 px-3 py-1.5 text-sm text-[var(--cinema-silver)]/70 hover:text-[var(--cinema-silver)] border border-[var(--cinema-silver)]/20 hover:border-[var(--cinema-silver)]/40 rounded-lg transition-colors ${className}`}
        >
          <CloudOff className="w-4 h-4" />
          <span className="hidden sm:inline">Sign In</span>
        </button>
        {/* AuthModal kept for fallback use in other contexts */}
        <AuthModal open={authModalOpen} onOpenChange={setAuthModalOpen} />
      </>
    );
  }

  // Authenticated - show user menu
  return (
    <DropdownMenu>
      <DropdownMenuTrigger asChild>
        <button
          className={`flex items-center gap-2 px-2 py-1.5 text-sm text-[var(--cinema-silver)]/80 hover:text-[var(--cinema-silver)] rounded-lg transition-colors ${className}`}
        >
          {user?.photoURL ? (
            <img
              src={user.photoURL}
              alt={user.displayName || 'User'}
              className="w-6 h-6 rounded-full"
            />
          ) : (
            <div className="w-6 h-6 rounded-full bg-[var(--cinema-spotlight)]/20 flex items-center justify-center">
              <User className="w-4 h-4 text-[var(--cinema-spotlight)]" />
            </div>
          )}
          <Cloud className="w-3.5 h-3.5 text-emerald-400" />
        </button>
      </DropdownMenuTrigger>
      <DropdownMenuContent
        align="end"
        className="w-56 bg-[var(--cinema-celluloid)] border-[var(--cinema-silver)]/20"
      >
        <DropdownMenuLabel className="font-normal">
          <div className="flex flex-col space-y-1">
            <p className="text-sm font-medium text-[var(--cinema-silver)]">
              {user?.displayName || 'User'}
            </p>
            <p className="text-xs text-[var(--cinema-silver)]/60">
              {user?.email}
            </p>
          </div>
        </DropdownMenuLabel>
        <DropdownMenuSeparator className="bg-[var(--cinema-silver)]/10" />
        <DropdownMenuItem className="text-[var(--cinema-silver)]/80">
          <Cloud className="w-4 h-4 mr-2 text-emerald-400" />
          <span>Sync enabled</span>
        </DropdownMenuItem>
        <DropdownMenuSeparator className="bg-[var(--cinema-silver)]/10" />
        <DropdownMenuItem
          onClick={signOut}
          className="text-[var(--cinema-velvet)] focus:text-[var(--cinema-velvet)]"
        >
          <LogOut className="w-4 h-4 mr-2" />
          <span>Sign out</span>
        </DropdownMenuItem>
      </DropdownMenuContent>
    </DropdownMenu>
  );
}

export default UserMenu;
````

## File: packages/frontend/components/chat/ChatInput.tsx
````typescript
/**
 * ChatInput - Chat message input component
 *
 * Provides a styled textarea with send button for chat interfaces.
 * Supports RTL layout and keyboard shortcuts.
 */

import React, { useRef, useCallback, KeyboardEvent } from 'react';
import { Send, Loader2 } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { cn } from '@/lib/utils';

export interface ChatInputProps {
  value: string;
  onChange: (value: string) => void;
  onSubmit: () => void;
  placeholder?: string;
  disabled?: boolean;
  isLoading?: boolean;
  isRTL?: boolean;
  hintText?: string;
  className?: string;
  inputId?: string;
}

/**
 * Chat input with auto-resizing textarea and send button
 */
export function ChatInput({
  value,
  onChange,
  onSubmit,
  placeholder = 'Type a message...',
  disabled = false,
  isLoading = false,
  isRTL = false,
  hintText,
  className,
  inputId = 'chat-input',
}: ChatInputProps) {
  const inputRef = useRef<HTMLTextAreaElement>(null);

  const handleKeyDown = useCallback(
    (e: KeyboardEvent<HTMLTextAreaElement>) => {
      if (e.key === 'Enter' && !e.shiftKey) {
        e.preventDefault();
        if (!disabled && !isLoading && value.trim()) {
          onSubmit();
        }
      }
    },
    [disabled, isLoading, value, onSubmit]
  );

  const handleSubmit = useCallback(
    (e: React.FormEvent) => {
      e.preventDefault();
      if (!disabled && !isLoading && value.trim()) {
        onSubmit();
      }
    },
    [disabled, isLoading, value, onSubmit]
  );

  const isSubmitDisabled = !value.trim() || disabled || isLoading;

  return (
    <div className={cn('border-t border-white/5 bg-black/20 backdrop-blur-xl', className)}>
      <div className="max-w-3xl mx-auto px-4 py-4">
        <form
          onSubmit={handleSubmit}
          className="bg-white/5 rounded-2xl border border-white/10 focus-within:border-violet-500/50 focus-within:ring-1 focus-within:ring-violet-500/20 transition-all"
        >
          <label htmlFor={inputId} className="sr-only">
            {placeholder}
          </label>
          <textarea
            id={inputId}
            ref={inputRef}
            value={value}
            onChange={(e) => onChange(e.target.value)}
            onKeyDown={handleKeyDown}
            placeholder={placeholder}
            className={cn(
              'w-full bg-transparent border-0 focus:ring-0 resize-none text-sm text-white placeholder:text-white/30 min-h-[52px] max-h-[200px] px-4 py-3',
              isRTL && 'text-right'
            )}
            rows={1}
            disabled={disabled || isLoading}
            dir={isRTL ? 'rtl' : 'ltr'}
            aria-describedby={hintText ? `${inputId}-hint` : undefined}
          />
          <div
            className={cn(
              'flex justify-between items-center px-3 pb-3',
              isRTL && 'flex-row-reverse'
            )}
          >
            {hintText && (
              <span
                id={`${inputId}-hint`}
                className="text-[10px] text-white/20 uppercase tracking-wider"
              >
                {hintText}
              </span>
            )}
            {!hintText && <span />}
            <Button
              type="submit"
              disabled={isSubmitDisabled}
              size="sm"
              className={cn(
                'rounded-xl transition-all',
                value.trim()
                  ? 'bg-violet-600 hover:bg-violet-500 text-white'
                  : 'bg-white/10 text-white/30'
              )}
              aria-label="Send message"
            >
              {isLoading ? (
                <Loader2 className="w-4 h-4 animate-spin" aria-hidden="true" />
              ) : (
                <Send className="w-4 h-4" aria-hidden="true" />
              )}
            </Button>
          </div>
        </form>
      </div>
    </div>
  );
}

export default ChatInput;
````

## File: packages/frontend/components/chat/index.ts
````typescript
/**
 * Chat Components Index
 *
 * Exports all chat-related components for easy importing.
 */

export { MessageBubble } from './MessageBubble';
export type { MessageBubbleProps, ChatMessage, ChatMessageStatus } from './MessageBubble';

export { ChatInput } from './ChatInput';
export type { ChatInputProps } from './ChatInput';

export { QuickActions } from './QuickActions';
export type { QuickActionsProps, QuickActionItem } from './QuickActions';
````

## File: packages/frontend/components/chat/MessageBubble.tsx
````typescript
/**
 * MessageBubble - Chat message display component
 *
 * Renders individual chat messages with support for:
 * - User and assistant message styling
 * - Quick action buttons
 * - Progress indicators
 * - RTL layout support
 */

import React, { useState } from 'react';
import { motion } from 'framer-motion';
import { Sparkles, Loader2, CheckCircle2 } from 'lucide-react';
import { cn } from '@/lib/utils';
import type { Message, QuickAction } from '@/stores/appStore';

export interface ChatMessageStatus {
  status?: 'thinking' | 'generating' | 'complete' | 'error';
  progress?: number;
  videoReady?: boolean;
}

export interface ChatMessage extends Message, ChatMessageStatus {
  quickActions?: Array<{
    id: string;
    label: string;
    labelAr?: string;
    action: QuickAction['action'];
    variant?: 'primary' | 'secondary';
  }>;
}

export interface FeedbackData {
  helpful: boolean;
  rating: number;
  comment?: string;
}

export interface MessageBubbleProps {
  message: ChatMessage;
  isRTL?: boolean;
  onQuickAction?: (action: QuickAction['action']) => void;
  onFeedback?: (messageId: string, feedback: FeedbackData) => void;
  className?: string;
}

/**
 * Renders a single chat message bubble
 */
export function MessageBubble({
  message,
  isRTL = false,
  onQuickAction,
  onFeedback,
  className,
}: MessageBubbleProps) {
  const [showFeedbackForm, setShowFeedbackForm] = useState(false);
  const [feedbackComment, setFeedbackComment] = useState('');

  const isUser = message.role === 'user';
  const hasQuickActions = !isUser && message.quickActions && message.quickActions.length > 0;

  const handleFeedback = (helpful: boolean) => {
    if (helpful) {
      onFeedback?.(message.id, { helpful: true, rating: 5 });
    } else {
      setShowFeedbackForm(true);
    }
  };

  const submitFeedback = () => {
    onFeedback?.(message.id, {
      helpful: false,
      rating: 2,
      comment: feedbackComment,
    });
    setShowFeedbackForm(false);
    setFeedbackComment('');
  };

  return (
    <motion.div
      initial={{ opacity: 0, y: 10 }}
      animate={{ opacity: 1, y: 0 }}
      className={cn(
        'flex gap-4',
        isUser ? 'justify-end' : 'justify-start',
        isRTL && 'flex-row-reverse',
        className
      )}
    >
      {/* Assistant Avatar */}
      {!isUser && (
        <div
          className="w-8 h-8 rounded-full bg-gradient-to-br from-violet-600 to-fuchsia-600 flex items-center justify-center shrink-0 mt-1"
          aria-hidden="true"
        >
          <Sparkles className="w-4 h-4 text-white" />
        </div>
      )}

      {/* Message Content */}
      <div className={cn('max-w-[80%]', isUser && (isRTL ? 'text-start' : 'text-end'))}>
        <div
          className={cn(
            'inline-block px-4 py-3 text-[15px] leading-relaxed',
            isUser
              ? 'bg-violet-600 text-white rounded-2xl rounded-te-md'
              : 'bg-white/5 text-white/90 rounded-2xl rounded-ts-md border border-white/10'
          )}
        >
          <div className="whitespace-pre-wrap">{message.content}</div>
        </div>

        {/* Quick Action Buttons */}
        {hasQuickActions && (
          <motion.div
            initial={{ opacity: 0, y: 5 }}
            animate={{ opacity: 1, y: 0 }}
            transition={{ delay: 0.2 }}
            className={cn('mt-3 flex flex-wrap gap-2', isRTL && 'flex-row-reverse')}
          >
            {message.quickActions!.map((qa) => (
              <button
                key={qa.id}
                onClick={() => onQuickAction?.(qa.action)}
                className={cn(
                  'px-4 py-2 rounded-xl text-sm font-medium transition-all',
                  'hover:scale-105 active:scale-95',
                  qa.variant === 'primary'
                    ? 'bg-gradient-to-r from-violet-600 to-fuchsia-600 text-white hover:from-violet-500 hover:to-fuchsia-500'
                    : 'bg-white/10 text-white/90 border border-white/20 hover:bg-white/20'
                )}
              >
                {isRTL && qa.labelAr ? qa.labelAr : qa.label}
              </button>
            ))}
          </motion.div>
        )}

        {/* Progress Indicator */}
        {message.status === 'generating' && message.progress !== undefined && (
          <div className={cn('mt-3 flex items-center gap-3', isRTL && 'flex-row-reverse')}>
            <div className="flex-1 h-1.5 bg-white/10 rounded-full overflow-hidden max-w-[200px]">
              <motion.div
                className="h-full bg-gradient-to-r from-violet-500 to-fuchsia-500"
                initial={{ width: 0 }}
                animate={{ width: `${message.progress}%` }}
                transition={{ duration: 0.5 }}
              />
            </div>
            <span className="text-xs text-white/40 tabular-nums">
              {Math.round(message.progress)}%
            </span>
          </div>
        )}

        {/* Thinking Indicator */}
        {message.status === 'thinking' && (
          <div
            className={cn(
              'mt-2 flex items-center gap-2 text-white/40 text-sm',
              isRTL && 'flex-row-reverse'
            )}
          >
            <Loader2 className="w-3.5 h-3.5 animate-spin" aria-hidden="true" />
            <span>...</span>
          </div>
        )}

        {/* Video Ready Badge */}
        {message.videoReady && (
          <motion.div
            initial={{ opacity: 0, y: 5 }}
            animate={{ opacity: 1, y: 0 }}
            className="mt-3 inline-flex items-center gap-1.5 px-3 py-1.5 rounded-full bg-emerald-500/10 border border-emerald-500/20 text-emerald-400 text-xs font-medium"
          >
            <CheckCircle2 className="w-3.5 h-3.5" aria-hidden="true" />
            Ready
          </motion.div>
        )}

        {/* Feedback Buttons (only for assistant messages) */}
        {!isUser && onFeedback && (
          <div className={cn('mt-2 flex gap-2', isRTL && 'flex-row-reverse')}>
            <button
              onClick={() => handleFeedback(true)}
              className="text-xs text-white/40 hover:text-green-400 transition-colors flex items-center gap-1"
              aria-label="Mark as helpful"
            >
              <span>👍</span>
              <span>Helpful</span>
            </button>
            <button
              onClick={() => handleFeedback(false)}
              className="text-xs text-white/40 hover:text-red-400 transition-colors flex items-center gap-1"
              aria-label="Mark as not helpful"
            >
              <span>👎</span>
              <span>Not helpful</span>
            </button>
          </div>
        )}

        {/* Feedback Form */}
        {showFeedbackForm && (
          <motion.div
            initial={{ opacity: 0, height: 0 }}
            animate={{ opacity: 1, height: 'auto' }}
            className="mt-3 p-3 bg-white/5 rounded-lg border border-white/10"
          >
            <label htmlFor={`feedback-${message.id}`} className="text-xs text-white/60 block mb-2">
              What could be improved?
            </label>
            <textarea
              id={`feedback-${message.id}`}
              value={feedbackComment}
              onChange={(e) => setFeedbackComment(e.target.value)}
              placeholder="Your feedback helps us improve..."
              className="w-full bg-white/10 rounded p-2 text-sm text-white placeholder:text-white/30 border border-white/10 focus:border-violet-500 focus:ring-1 focus:ring-violet-500 min-h-[60px]"
              dir={isRTL ? 'rtl' : 'ltr'}
            />
            <div className={cn('mt-2 flex gap-2', isRTL && 'flex-row-reverse')}>
              <button
                onClick={submitFeedback}
                disabled={!feedbackComment.trim()}
                className="px-3 py-1.5 bg-violet-600 hover:bg-violet-500 disabled:bg-white/10 disabled:text-white/30 rounded text-sm transition-colors"
              >
                Submit
              </button>
              <button
                onClick={() => setShowFeedbackForm(false)}
                className="px-3 py-1.5 bg-white/10 hover:bg-white/20 rounded text-sm transition-colors"
              >
                Cancel
              </button>
            </div>
          </motion.div>
        )}
      </div>

      {/* User Avatar */}
      {isUser && (
        <div
          className="w-8 h-8 rounded-full bg-white/10 flex items-center justify-center shrink-0 mt-1 text-white/60 text-sm font-medium"
          aria-hidden="true"
        >
          U
        </div>
      )}
    </motion.div>
  );
}

export default MessageBubble;
````

## File: packages/frontend/components/chat/QuickActions.tsx
````typescript
/**
 * QuickActions - Quick action button strip
 *
 * Displays a row of quick action buttons for common actions
 * like creating videos, generating music, etc.
 */

import React from 'react';
import { LucideIcon } from 'lucide-react';
import { cn } from '@/lib/utils';

export interface QuickActionItem {
  icon: LucideIcon;
  label: string;
  prompt?: string;
  onClick?: () => void;
}

export interface QuickActionsProps {
  /** List of quick actions to display */
  actions: QuickActionItem[];
  /** Callback when an action is selected */
  onSelect?: (action: QuickActionItem) => void;
  /** RTL layout */
  isRTL?: boolean;
  /** Additional class names */
  className?: string;
}

/**
 * Quick action button strip
 *
 * @example
 * ```tsx
 * <QuickActions
 *   actions={[
 *     { icon: Video, label: 'Create Video', prompt: 'Create a...' },
 *     { icon: Music, label: 'Generate Music', prompt: 'Generate...' },
 *   ]}
 *   onSelect={(action) => setInput(action.prompt)}
 * />
 * ```
 */
export function QuickActions({
  actions,
  onSelect,
  isRTL = false,
  className,
}: QuickActionsProps) {
  const handleClick = (action: QuickActionItem) => {
    if (action.onClick) {
      action.onClick();
    } else if (onSelect) {
      onSelect(action);
    }
  };

  return (
    <div className={cn('max-w-3xl mx-auto w-full px-4 pb-4', className)}>
      <div
        className={cn(
          'flex flex-wrap justify-center gap-2',
          isRTL && 'flex-row-reverse'
        )}
      >
        {actions.map((action) => (
          <button
            key={action.label}
            onClick={() => handleClick(action)}
            className="flex items-center gap-2 px-4 py-2.5 rounded-full bg-white/5 hover:bg-white/10 border border-white/10 text-sm text-white/70 hover:text-white transition-all"
          >
            <action.icon className="w-4 h-4 text-violet-400" aria-hidden="true" />
            {action.label}
          </button>
        ))}
      </div>
    </div>
  );
}

export default QuickActions;
````

## File: packages/frontend/components/CheckpointApproval.tsx
````typescript
/**
 * Checkpoint Approval Component
 *
 * Displays a checkpoint gate for human-in-the-loop approval during
 * multi-format pipeline execution. Shows preview content, approve/reject
 * controls, and a countdown timer with timeout warnings.
 *
 * Requirements: 17.1, 17.2, 17.3, 17.5
 */

import React, { useState, useEffect, useCallback, useRef } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import {
  Check,
  X,
  Clock,
  Edit3,
  AlertTriangle,
  Save,
} from 'lucide-react';
import { cn } from '@/lib/utils';

export interface CheckpointApprovalProps {
  checkpointId: string;
  phase: string;
  title: string;
  description?: string;
  previewData?: React.ReactNode;
  timeoutMs?: number;
  onApprove: (checkpointId: string) => void;
  onRequestChanges: (checkpointId: string, changeRequest: string) => void;
  onTimeout?: (checkpointId: string) => void;
  className?: string;
}

const DEFAULT_TIMEOUT_MS = 30 * 60 * 1000;
const WARNING_THRESHOLD_MS = 5 * 60 * 1000;

function formatCountdown(ms: number): string {
  if (ms <= 0) return '0:00';
  const totalSeconds = Math.ceil(ms / 1000);
  const minutes = Math.floor(totalSeconds / 60);
  const seconds = totalSeconds % 60;
  return `${minutes}:${seconds.toString().padStart(2, '0')}`;
}

export function CheckpointApproval({
  checkpointId,
  phase,
  title,
  description,
  previewData,
  timeoutMs = DEFAULT_TIMEOUT_MS,
  onApprove,
  onRequestChanges,
  onTimeout,
  className,
}: CheckpointApprovalProps) {
  const [showChangeInput, setShowChangeInput] = useState(false);
  const [changeRequest, setChangeRequest] = useState('');
  const [remaining, setRemaining] = useState(timeoutMs);
  const [timedOut, setTimedOut] = useState(false);
  const startTimeRef = useRef(Date.now());
  const textareaRef = useRef<HTMLTextAreaElement>(null);

  useEffect(() => {
    startTimeRef.current = Date.now();
    setRemaining(timeoutMs);
    setTimedOut(false);

    const interval = setInterval(() => {
      const elapsed = Date.now() - startTimeRef.current;
      const left = Math.max(0, timeoutMs - elapsed);
      setRemaining(left);

      if (left <= 0) {
        clearInterval(interval);
        setTimedOut(true);
        onTimeout?.(checkpointId);
      }
    }, 1000);

    return () => clearInterval(interval);
  }, [checkpointId, timeoutMs, onTimeout]);

  useEffect(() => {
    if (showChangeInput) {
      textareaRef.current?.focus();
    }
  }, [showChangeInput]);

  const handleApprove = useCallback(() => {
    onApprove(checkpointId);
  }, [checkpointId, onApprove]);

  const handleSubmitChanges = useCallback(() => {
    const trimmed = changeRequest.trim();
    if (!trimmed) return;
    onRequestChanges(checkpointId, trimmed);
  }, [checkpointId, changeRequest, onRequestChanges]);

  const handleToggleChangeInput = useCallback(() => {
    setShowChangeInput((prev) => !prev);
    setChangeRequest('');
  }, []);

  const isWarning = remaining <= WARNING_THRESHOLD_MS && remaining > 0;

  return (
    <motion.div
      initial={{ opacity: 0, y: 8 }}
      animate={{ opacity: 1, y: 0 }}
      exit={{ opacity: 0, y: -8 }}
      transition={{ duration: 0.25, ease: 'easeOut' }}
      className={cn('w-full max-w-2xl mx-auto', className)}
      role="dialog"
      aria-label={`Checkpoint approval: ${title}`}
    >
      {/* Header */}
      <div className="flex items-center justify-between mb-4">
        <div>
          <div className="flex items-center gap-2 mb-1">
            <span className="font-mono text-[10px] font-medium tracking-[0.15em] uppercase text-zinc-500">
              {phase}
            </span>
          </div>
          <h2 className="text-lg font-medium text-zinc-100">{title}</h2>
          {description && (
            <p className="text-[13px] text-zinc-400 mt-1">{description}</p>
          )}
        </div>

        <div
          className={cn(
            'flex items-center gap-1.5 px-2.5 py-1 rounded-sm border font-mono text-xs',
            timedOut
              ? 'border-zinc-700 bg-zinc-900/60 text-zinc-500'
              : isWarning
                ? 'border-amber-500/50 bg-amber-500/10 text-amber-400'
                : 'border-zinc-700 text-zinc-400',
          )}
          aria-label={`Time remaining: ${formatCountdown(remaining)}`}
          aria-live="polite"
        >
          <Clock className="w-3 h-3" />
          <span>{timedOut ? 'Timed out' : formatCountdown(remaining)}</span>
        </div>
      </div>

      {/* Warning banner */}
      <AnimatePresence>
        {isWarning && !timedOut && (
          <motion.div
            initial={{ opacity: 0, height: 0 }}
            animate={{ opacity: 1, height: 'auto' }}
            exit={{ opacity: 0, height: 0 }}
            className="mb-4 overflow-hidden"
          >
            <div className="flex items-center gap-2 px-3 py-2 rounded-sm border border-amber-500/40 bg-amber-500/10">
              <AlertTriangle className="w-3.5 h-3.5 text-amber-400 shrink-0" />
              <span className="text-xs font-mono text-amber-300">
                Checkpoint will auto-save in {formatCountdown(remaining)}
              </span>
            </div>
          </motion.div>
        )}
      </AnimatePresence>

      {/* Auto-saved notification */}
      <AnimatePresence>
        {timedOut && (
          <motion.div
            initial={{ opacity: 0, height: 0 }}
            animate={{ opacity: 1, height: 'auto' }}
            exit={{ opacity: 0, height: 0 }}
            className="mb-4 overflow-hidden"
          >
            <div className="flex items-center gap-2 px-3 py-2 rounded-sm border border-zinc-600 bg-zinc-800/80">
              <Save className="w-3.5 h-3.5 text-zinc-400 shrink-0" />
              <span className="text-xs font-mono text-zinc-300">
                State auto-saved. Approval timed out.
              </span>
            </div>
          </motion.div>
        )}
      </AnimatePresence>

      {/* Preview content */}
      {previewData && (
        <div className="mb-4 px-3 py-3 rounded-sm border border-zinc-700 bg-zinc-900/80">
          <span className="font-mono text-[10px] font-medium tracking-[0.15em] uppercase text-zinc-500 block mb-2">
            Preview
          </span>
          <div className="text-[13px] text-zinc-300">{previewData}</div>
        </div>
      )}

      {/* Action buttons */}
      <div className="flex items-center gap-2">
        <button
          type="button"
          onClick={handleApprove}
          disabled={timedOut}
          className={cn(
            'flex items-center gap-2 px-3 py-1.5 rounded-sm border text-xs font-mono transition-colors duration-200',
            'disabled:opacity-50 disabled:cursor-not-allowed',
            'border-emerald-500/50 bg-emerald-500/10 text-emerald-400',
            'hover:bg-emerald-500/20 hover:border-emerald-500/70',
            'focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-emerald-500',
          )}
          aria-label={`Approve checkpoint`}
        >
          <Check className="w-3 h-3" />
          <span>Approve</span>
        </button>

        <button
          type="button"
          onClick={handleToggleChangeInput}
          disabled={timedOut}
          className={cn(
            'flex items-center gap-2 px-3 py-1.5 rounded-sm border text-xs font-mono transition-colors duration-200',
            'disabled:opacity-50 disabled:cursor-not-allowed',
            showChangeInput
              ? 'border-amber-500/50 bg-amber-500/10 text-amber-400'
              : 'border-zinc-700 text-zinc-400 hover:border-zinc-500 hover:text-zinc-200',
            'focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-zinc-500',
          )}
          aria-label="Request changes"
          aria-expanded={showChangeInput}
        >
          <Edit3 className="w-3 h-3" />
          <span>{showChangeInput ? 'Cancel' : 'Request Changes'}</span>
        </button>
      </div>

      {/* Change request textarea */}
      <AnimatePresence>
        {showChangeInput && (
          <motion.div
            initial={{ opacity: 0, height: 0 }}
            animate={{ opacity: 1, height: 'auto' }}
            exit={{ opacity: 0, height: 0 }}
            transition={{ duration: 0.2 }}
            className="overflow-hidden"
          >
            <div className="mt-3 space-y-2">
              <label
                htmlFor={`change-request-${checkpointId}`}
                className="font-mono text-[10px] font-medium tracking-[0.15em] uppercase text-zinc-500"
              >
                Describe requested changes
              </label>
              <textarea
                ref={textareaRef}
                id={`change-request-${checkpointId}`}
                value={changeRequest}
                onChange={(e) => setChangeRequest(e.target.value)}
                placeholder="Describe what should be changed..."
                rows={3}
                className={cn(
                  'w-full px-3 py-2 rounded-sm border border-zinc-700 bg-zinc-900/80',
                  'text-[13px] text-zinc-200 placeholder:text-zinc-600',
                  'font-mono resize-none',
                  'focus:outline-none focus:border-zinc-500',
                )}
              />
              <button
                type="button"
                onClick={handleSubmitChanges}
                disabled={!changeRequest.trim()}
                className={cn(
                  'flex items-center gap-2 px-3 py-1.5 rounded-sm border text-xs font-mono transition-colors duration-200',
                  'disabled:opacity-50 disabled:cursor-not-allowed',
                  'border-amber-500/50 bg-amber-500/10 text-amber-400',
                  'hover:bg-amber-500/20 hover:border-amber-500/70',
                  'focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-amber-500',
                )}
              >
                <X className="w-3 h-3" />
                <span>Submit Changes</span>
              </button>
            </div>
          </motion.div>
        )}
      </AnimatePresence>
    </motion.div>
  );
}

export default CheckpointApproval;
````

## File: packages/frontend/components/ErrorBoundary.tsx
````typescript
import React from "react";
import { Button } from "./ui/button";

interface ErrorBoundaryProps {
  children: React.ReactNode;
  fallback?: React.ReactNode;
  onError?: (error: Error, errorInfo: React.ErrorInfo) => void;
}

interface ErrorBoundaryState {
  hasError: boolean;
  error: Error | null;
}

/**
 * Error Boundary component that catches JavaScript errors in child components,
 * logs them, and displays a fallback UI instead of crashing the entire app.
 * 
 * Requirements: 7.1, 7.2, 7.3, 7.4
 */
class ErrorBoundary extends React.Component<ErrorBoundaryProps, ErrorBoundaryState> {
  constructor(props: ErrorBoundaryProps) {
    super(props);
    this.state = { hasError: false, error: null };
  }

  static getDerivedStateFromError(error: Error): ErrorBoundaryState {
    // Update state so the next render shows the fallback UI
    return { hasError: true, error };
  }

  componentDidCatch(error: Error, errorInfo: React.ErrorInfo): void {
    // Log error for debugging purposes (Requirement 7.3)
    console.error("[ErrorBoundary] Caught error:", error);
    console.error("[ErrorBoundary] Component stack:", errorInfo.componentStack);

    // Call optional error callback
    this.props.onError?.(error, errorInfo);
  }

  handleRetry = (): void => {
    this.setState({ hasError: false, error: null });
  };

  render(): React.ReactNode {
    if (this.state.hasError) {
      // Render custom fallback if provided, otherwise use default
      if (this.props.fallback) {
        return this.props.fallback;
      }

      return (
        <DefaultErrorFallback
          error={this.state.error}
          onRetry={this.handleRetry}
        />
      );
    }

    return this.props.children;
  }
}

interface DefaultErrorFallbackProps {
  error: Error | null;
  onRetry: () => void;
}

/**
 * Generate a unique error ID for support reference
 */
function generateErrorId(): string {
  return `ERR-${Date.now().toString(36).toUpperCase()}-${Math.random().toString(36).substring(2, 6).toUpperCase()}`;
}

/**
 * Default fallback UI shown when an error is caught.
 * Provides retry functionality (Requirement 7.4)
 */
function DefaultErrorFallback({ error, onRetry }: DefaultErrorFallbackProps): React.ReactElement {
  const errorId = React.useMemo(() => generateErrorId(), []);

  return (
    <div
      role="alert"
      aria-live="assertive"
      aria-atomic="true"
      className="flex flex-col items-center justify-center min-h-[200px] p-8 bg-slate-800/50 rounded-lg border border-slate-700"
    >
      <div className="text-red-400 mb-4">
        <svg
          className="w-12 h-12"
          fill="none"
          stroke="currentColor"
          viewBox="0 0 24 24"
          aria-hidden="true"
          focusable="false"
        >
          <path
            strokeLinecap="round"
            strokeLinejoin="round"
            strokeWidth={2}
            d="M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-3L13.732 4c-.77-1.333-2.694-1.333-3.464 0L3.34 16c-.77 1.333.192 3 1.732 3z"
          />
        </svg>
      </div>

      <h2 className="text-xl font-semibold text-white mb-2">
        Something went wrong
      </h2>

      <p className="text-slate-400 text-center mb-4 max-w-md">
        An unexpected error occurred. You can try again or refresh the page.
      </p>

      {error && (
        <details className="mb-4 text-sm text-slate-500 max-w-md">
          <summary className="cursor-pointer hover:text-slate-400">
            Error details
          </summary>
          <pre className="mt-2 p-2 bg-slate-900 rounded text-xs overflow-auto max-h-32">
            {error.message}
          </pre>
        </details>
      )}

      {/* Error ID for support reference */}
      <p className="text-xs text-slate-600 mb-4" aria-label={`Error reference ID: ${errorId}`}>
        Reference: <code className="font-mono">{errorId}</code>
      </p>

      <div className="flex gap-3">
        <Button
          onClick={onRetry}
          variant="default"
          className="bg-purple-600 hover:bg-purple-700"
        >
          Try Again
        </Button>
        <Button
          onClick={() => window.location.reload()}
          variant="outline"
          className="border-slate-600 text-slate-300 hover:bg-slate-700"
        >
          Refresh Page
        </Button>
      </div>
    </div>
  );
}

export { ErrorBoundary, DefaultErrorFallback };
````

## File: packages/frontend/components/FormatSelector.tsx
````typescript
/**
 * Format Selector Component
 *
 * Displays 8 video format options in a grid layout.
 * Handles format selection, genre filtering, and format-specific placeholder text.
 * Prevents pipeline execution until a format is selected.
 *
 * Requirements: 1.1, 1.3, 1.4, 1.5
 * Properties: 43 (Genre Filtering), 44 (Format-Specific Placeholder), 45 (Execution Prevention)
 */

import React, { useState, useMemo } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import {
  Mic,
  Megaphone,
  Film,
  GraduationCap,
  Smartphone,
  Camera,
  Music,
  Newspaper,
  ChevronRight,
  ArrowRight,
  AlertCircle,
} from 'lucide-react';
import { cn } from '@/lib/utils';
import type { VideoFormat, FormatMetadata } from '@/types';
import { formatRegistry } from '@/services/formatRegistry';

// Map format IDs to lucide icons
const FORMAT_ICONS: Record<VideoFormat, React.ElementType> = {
  'youtube-narrator': Mic,
  advertisement: Megaphone,
  'movie-animation': Film,
  educational: GraduationCap,
  shorts: Smartphone,
  documentary: Camera,
  'music-video': Music,
  'news-politics': Newspaper,
};

// Format-specific placeholder text for the idea input
const FORMAT_PLACEHOLDERS: Record<VideoFormat, string> = {
  'youtube-narrator':
    'Describe a topic you want to narrate about... e.g., "The hidden history of the Silk Road"',
  advertisement:
    'Describe your product or service... e.g., "A new fitness app that uses AI to create personalized workouts"',
  'movie-animation':
    'Describe your story concept... e.g., "A young robot dreams of becoming a painter"',
  educational:
    'Describe what you want to teach... e.g., "How photosynthesis works at the molecular level"',
  shorts:
    'Describe a short, punchy idea... e.g., "3 mind-blowing facts about the ocean"',
  documentary:
    'Describe your documentary subject... e.g., "The rise and fall of a forgotten civilization"',
  'music-video':
    'Describe the song mood and theme... e.g., "An upbeat pop song about chasing your dreams"',
  'news-politics':
    'Describe the news topic... e.g., "The impact of AI regulation on global tech industries"',
};

export interface FormatSelectorProps {
  selectedFormat: VideoFormat | null;
  onFormatSelect: (format: VideoFormat) => void;
  selectedGenre: string | null;
  onGenreSelect: (genre: string) => void;
  idea: string;
  onIdeaChange: (idea: string) => void;
  onExecute: () => void;
  isProcessing?: boolean;
}

export function FormatSelector({
  selectedFormat,
  onFormatSelect,
  selectedGenre,
  onGenreSelect,
  idea,
  onIdeaChange,
  onExecute,
  isProcessing = false,
}: FormatSelectorProps) {
  const [showExecutionError, setShowExecutionError] = useState(false);

  const allFormats = useMemo(() => formatRegistry.getAllFormats(), []);

  const selectedFormatMetadata = useMemo(
    () => (selectedFormat ? formatRegistry.getFormat(selectedFormat) : null),
    [selectedFormat],
  );

  // Property 43: Genre list is exactly the format's applicableGenres
  const applicableGenres = useMemo(
    () => selectedFormatMetadata?.applicableGenres ?? [],
    [selectedFormatMetadata],
  );

  // Property 44: Placeholder updates based on format
  const placeholder = selectedFormat
    ? FORMAT_PLACEHOLDERS[selectedFormat]
    : 'Select a format above to get started...';

  // Property 45: Prevent execution without format
  const handleExecuteClick = () => {
    if (!selectedFormat) {
      setShowExecutionError(true);
      setTimeout(() => setShowExecutionError(false), 3000);
      return;
    }
    onExecute();
  };

  const canExecute = !!selectedFormat && idea.trim().length > 0 && !isProcessing;

  return (
    <div className="flex flex-col items-center min-h-[70vh] px-6 py-12 bg-black">
      <div className="w-full max-w-3xl">
        {/* Header */}
        <div className="mb-10">
          <h1 className="font-sans text-3xl font-medium tracking-tight text-zinc-100">
            What will you create?
          </h1>
          <p className="text-zinc-500 text-sm mt-2 leading-relaxed">
            Choose a format to shape your entire production pipeline
          </p>
        </div>

        {/* Format Grid */}
        <div className="grid grid-cols-2 sm:grid-cols-4 gap-3 mb-10">
          {allFormats.map((format) => {
            const Icon = FORMAT_ICONS[format.id];
            const isSelected = selectedFormat === format.id;

            return (
              <button
                key={format.id}
                type="button"
                onClick={() => onFormatSelect(format.id)}
                disabled={isProcessing}
                className={cn(
                  'group relative flex flex-col items-center gap-2.5 px-4 py-5 rounded-sm border transition-all duration-200',
                  'disabled:opacity-40 disabled:cursor-not-allowed',
                  isSelected
                    ? 'bg-blue-500/10 border-blue-500/50'
                    : 'border-zinc-800 hover:border-zinc-600 bg-zinc-900/50',
                )}
              >
                <Icon
                  className={cn(
                    'w-6 h-6 transition-colors duration-200',
                    isSelected ? 'text-blue-400' : 'text-zinc-500 group-hover:text-zinc-300',
                  )}
                />
                <span
                  className={cn(
                    'text-[13px] font-medium text-center leading-tight transition-colors duration-200',
                    isSelected ? 'text-blue-400' : 'text-zinc-400 group-hover:text-zinc-200',
                  )}
                >
                  {format.name}
                </span>
                <span className="text-[10px] text-zinc-600 text-center leading-snug line-clamp-2">
                  {format.description}
                </span>
              </button>
            );
          })}
        </div>

        {/* Genre Selection - only shown when format is selected */}
        <AnimatePresence>
          {selectedFormat && applicableGenres.length > 0 && (
            <motion.div
              initial={{ opacity: 0, height: 0 }}
              animate={{ opacity: 1, height: 'auto' }}
              exit={{ opacity: 0, height: 0 }}
              transition={{ duration: 0.2, ease: 'easeOut' }}
              className="mb-8"
            >
              <div className="flex items-center justify-between mb-3">
                <span className="font-mono text-[11px] font-medium tracking-[0.15em] uppercase text-zinc-500">
                  Genre
                </span>
                <span className="text-[10px] font-mono text-zinc-600">
                  {applicableGenres.length} available
                </span>
              </div>
              <div className="flex flex-wrap gap-2">
                {applicableGenres.map((genre) => {
                  const isSelected = selectedGenre === genre;
                  return (
                    <button
                      key={genre}
                      type="button"
                      onClick={() => onGenreSelect(genre)}
                      disabled={isProcessing}
                      className={cn(
                        'px-3 py-1.5 rounded-sm border text-[13px] font-medium transition-colors duration-200',
                        'disabled:opacity-40 disabled:cursor-not-allowed',
                        isSelected
                          ? 'bg-blue-500/10 border-blue-500/50 text-blue-400'
                          : 'border-zinc-800 text-zinc-500 hover:border-zinc-600 hover:text-zinc-300',
                      )}
                    >
                      {genre}
                    </button>
                  );
                })}
              </div>
            </motion.div>
          )}
        </AnimatePresence>

        {/* Idea Input */}
        <div className="mb-8">
          <div className="bg-zinc-900 border border-zinc-800 rounded-sm focus-within:border-blue-500/50 transition-colors duration-200">
            <textarea
              value={idea}
              onChange={(e) => onIdeaChange(e.target.value)}
              placeholder={placeholder}
              disabled={isProcessing}
              className="
                w-full min-h-[120px] px-5 py-4
                bg-transparent
                text-[15px] text-zinc-100 leading-relaxed
                placeholder:text-zinc-600
                focus:outline-none
                resize-none
              "
            />
            <div className="flex items-center justify-end px-5 pb-3.5 pt-0">
              <span className="font-mono text-[10px] text-zinc-600 tabular-nums">
                {idea.length}
              </span>
            </div>
          </div>
        </div>

        {/* Execute Button */}
        <div className="relative">
          <button
            type="button"
            onClick={handleExecuteClick}
            disabled={isProcessing}
            className={cn(
              'w-full flex items-center justify-center gap-3 px-8 py-3 rounded-sm font-mono text-sm font-medium transition-colors duration-200',
              canExecute
                ? 'bg-white text-black hover:bg-zinc-200'
                : 'bg-zinc-800 text-zinc-600 cursor-not-allowed',
            )}
          >
            {isProcessing ? (
              <>
                <div className="w-4 h-4 rounded-sm border-2 border-current border-t-transparent animate-spin" />
                <span>Processing...</span>
              </>
            ) : (
              <>
                <span>Start Production</span>
                <ArrowRight className="w-4 h-4" />
              </>
            )}
          </button>

          {/* Execution prevention error */}
          <AnimatePresence>
            {showExecutionError && (
              <motion.div
                initial={{ opacity: 0, y: -4 }}
                animate={{ opacity: 1, y: 0 }}
                exit={{ opacity: 0, y: -4 }}
                className="absolute top-full left-0 right-0 mt-2 flex items-center gap-2 text-xs text-red-400 font-mono"
              >
                <AlertCircle className="w-3.5 h-3.5 shrink-0" />
                <span>Please select a format before starting production</span>
              </motion.div>
            )}
          </AnimatePresence>
        </div>
      </div>
    </div>
  );
}

/**
 * Utility: get applicable genres for a format ID.
 * Used by property tests.
 */
export function getGenresForFormat(formatId: string): string[] {
  const format = formatRegistry.getFormat(formatId);
  return format?.applicableGenres ?? [];
}

/**
 * Utility: get placeholder text for a format ID.
 * Used by property tests.
 */
export function getPlaceholderForFormat(formatId: VideoFormat): string {
  return FORMAT_PLACEHOLDERS[formatId] ?? '';
}

export default FormatSelector;
````

## File: packages/frontend/components/gradient-generator/GradientControls.tsx
````typescript
/**
 * CSS Gradient Generator - Gradient Controls Component
 */

import React, { useState } from 'react';
import { cn } from '@/lib/utils';
import { Button } from '@/components/ui/button';
import { Slider } from '@/components/ui/slider';
import { Label } from '@/components/ui/label';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from '@/components/ui/select';
import { Tabs, TabsList, TabsTrigger } from '@/components/ui/tabs';
import { Plus, Trash2, RotateCcw } from 'lucide-react';
import type { GradientControlsProps } from './types';
import { generateGradientCSS } from './utils';

export function GradientControls({
  config,
  onConfigChange,
  maxColorStops = 10,
  minColorStops = 2,
  className,
}: GradientControlsProps) {
  const [editingColorId, setEditingColorId] = useState<string | null>(null);

  const handleGradientTypeChange = (type: string) => {
    if (type !== 'linear' && type !== 'radial' && type !== 'conic') return;

    let newConfig;

    if (type === 'linear') {
      newConfig = {
        type: 'linear' as const,
        angle: 90,
        colorStops: config.colorStops,
      };
    } else if (type === 'radial') {
      newConfig = {
        type: 'radial' as const,
        shape: 'circle' as const,
        position: { x: 50, y: 50 },
        colorStops: config.colorStops,
      };
    } else {
      newConfig = {
        type: 'conic' as const,
        angle: 0,
        position: { x: 50, y: 50 },
        colorStops: config.colorStops,
      };
    }

    onConfigChange(newConfig);
  };

  const handleColorChange = (id: string, color: string) => {
    const newColorStops = config.colorStops.map((stop) =>
      stop.id === id ? { ...stop, color } : stop
    );
    onConfigChange({ ...config, colorStops: newColorStops });
  };

  const handlePositionChange = (id: string, position: number) => {
    const newColorStops = config.colorStops.map((stop) =>
      stop.id === id ? { ...stop, position } : stop
    ).sort((a, b) => a.position - b.position);
    onConfigChange({ ...config, colorStops: newColorStops });
  };

  const handleAddColorStop = () => {
    if (config.colorStops.length >= maxColorStops) return;

    const newStop = {
      id: `${Date.now()}-${Math.random()}`,
      color: '#ffffff',
      position: 50,
    };

    const newColorStops = [...config.colorStops, newStop].sort((a, b) => a.position - b.position);
    onConfigChange({ ...config, colorStops: newColorStops });
  };

  const handleRemoveColorStop = (id: string) => {
    if (config.colorStops.length <= minColorStops) return;

    const newColorStops = config.colorStops.filter((stop) => stop.id !== id);
    onConfigChange({ ...config, colorStops: newColorStops });
  };

  const handleAngleChange = (angle: number) => {
    if (config.type !== 'linear' && config.type !== 'conic') return;
    onConfigChange({ ...config, angle });
  };

  const handlePositionXYChange = (x: number, y: number) => {
    if (config.type !== 'radial' && config.type !== 'conic') return;
    onConfigChange({ ...config, position: { x, y } });
  };

  const handleShapeChange = (shape: 'circle' | 'ellipse') => {
    if (config.type !== 'radial') return;
    onConfigChange({ ...config, shape });
  };

  const canAddColorStop = config.colorStops.length < maxColorStops;
  const canRemoveColorStop = config.colorStops.length > minColorStops;

  return (
    <div className={cn('gradient-controls space-y-6', className)}>
      {/* Gradient Type Selector */}
      <div className="space-y-3">
        <Label htmlFor="gradient-type">Gradient Type</Label>
        <Tabs value={config.type} onValueChange={handleGradientTypeChange}>
          <TabsList className="w-full">
            <TabsTrigger value="linear" className="flex-1">Linear</TabsTrigger>
            <TabsTrigger value="radial" className="flex-1">Radial</TabsTrigger>
            <TabsTrigger value="conic" className="flex-1">Conic</TabsTrigger>
          </TabsList>
        </Tabs>
      </div>

      {/* Direction/Angle Controls */}
      {(config.type === 'linear' || config.type === 'conic') && (
        <div className="space-y-3">
          <Label htmlFor="angle-slider">
            Angle: {config.angle}°
          </Label>
          <Slider
            id="angle-slider"
            value={[config.angle]}
            onValueChange={([value]) => value !== undefined && handleAngleChange(value)}
            min={0}
            max={360}
            step={1}
            className="w-full"
            aria-label="Gradient angle in degrees"
            aria-valuemin={0}
            aria-valuemax={360}
            aria-valuenow={config.angle}
          />
        </div>
      )}

      {/* Radial Shape Control */}
      {config.type === 'radial' && (
        <div className="space-y-3">
          <Label htmlFor="shape-select">Shape</Label>
          <Select value={config.shape} onValueChange={handleShapeChange}>
            <SelectTrigger id="shape-select">
              <SelectValue />
            </SelectTrigger>
            <SelectContent>
              <SelectItem value="circle">Circle</SelectItem>
              <SelectItem value="ellipse">Ellipse</SelectItem>
            </SelectContent>
          </Select>
        </div>
      )}

      {/* Position Controls for Radial/Conic */}
      {(config.type === 'radial' || config.type === 'conic') && (
        <div className="space-y-3">
          <Label>Position</Label>
          <div className="grid grid-cols-2 gap-4">
            <div className="space-y-2">
              <Label htmlFor="position-x" className="text-sm text-muted-foreground">
                X: {config.position.x}%
              </Label>
              <Slider
                id="position-x"
                value={[config.position.x]}
                onValueChange={([value]) => value !== undefined && handlePositionXYChange(value, config.position.y)}
                min={0}
                max={100}
                step={1}
                className="w-full"
                aria-label="Gradient position X"
                aria-valuemin={0}
                aria-valuemax={100}
                aria-valuenow={config.position.x}
              />
            </div>
            <div className="space-y-2">
              <Label htmlFor="position-y" className="text-sm text-muted-foreground">
                Y: {config.position.y}%
              </Label>
              <Slider
                id="position-y"
                value={[config.position.y]}
                onValueChange={([value]) => value !== undefined && handlePositionXYChange(config.position.x, value)}
                min={0}
                max={100}
                step={1}
                className="w-full"
                aria-label="Gradient position Y"
                aria-valuemin={0}
                aria-valuemax={100}
                aria-valuenow={config.position.y}
              />
            </div>
          </div>
        </div>
      )}

      {/* Color Stops */}
      <div className="space-y-3">
        <div className="flex items-center justify-between">
          <Label>Color Stops</Label>
          <Button
            type="button"
            variant="outline"
            size="sm"
            onClick={handleAddColorStop}
            disabled={!canAddColorStop}
            aria-label="Add color stop"
          >
            <Plus className="w-4 h-4 mr-1" />
            Add
          </Button>
        </div>

        <div className="space-y-3">
          {config.colorStops.map((stop, index) => (
            <div
              key={stop.id}
              className="flex items-center gap-3 p-3 rounded-lg border border-border bg-card"
            >
              {/* Color Picker */}
              <div className="relative">
                <input
                  type="color"
                  value={stop.color}
                  onChange={(e) => handleColorChange(stop.id, e.target.value)}
                  className="w-10 h-10 rounded cursor-pointer border-0 p-0"
                  aria-label={`Color stop ${index + 1} color`}
                />
              </div>

              {/* Position Slider */}
              <div className="flex-1 space-y-1">
                <Label htmlFor={`position-${stop.id}`} className="text-sm text-muted-foreground">
                  Position: {stop.position}%
                </Label>
                <Slider
                  id={`position-${stop.id}`}
                  value={[stop.position]}
                  onValueChange={([value]) => value !== undefined && handlePositionChange(stop.id, value)}
                  min={0}
                  max={100}
                  step={1}
                  className="w-full"
                  aria-label={`Color stop ${index + 1} position`}
                  aria-valuemin={0}
                  aria-valuemax={100}
                  aria-valuenow={stop.position}
                />
              </div>

              {/* Remove Button */}
              <Button
                type="button"
                variant="ghost"
                size="icon"
                onClick={() => handleRemoveColorStop(stop.id)}
                disabled={!canRemoveColorStop}
                aria-label={`Remove color stop ${index + 1}`}
              >
                <Trash2 className="w-4 h-4 text-destructive" />
              </Button>
            </div>
          ))}
        </div>
      </div>

      {/* Reset Button */}
      <Button
        type="button"
        variant="outline"
        className="w-full"
        onClick={() => {
          const defaultConfig = {
            type: 'linear' as const,
            angle: 90,
            colorStops: [
              { id: '1', color: '#6366f1', position: 0 },
              { id: '2', color: '#a855f7', position: 100 },
            ],
          };
          onConfigChange(defaultConfig);
        }}
      >
        <RotateCcw className="w-4 h-4 mr-2" />
        Reset to Default
      </Button>
    </div>
  );
}

export default GradientControls;
````

## File: packages/frontend/components/gradient-generator/GradientExport.tsx
````typescript
/**
 * CSS Gradient Generator - Gradient Export Component
 */

import React, { useState } from 'react';
import { cn } from '@/lib/utils';
import { Button } from '@/components/ui/button';
import { Dialog, DialogContent, DialogHeader, DialogTitle, DialogTrigger } from '@/components/ui/dialog';
import { Textarea } from '@/components/ui/textarea';
import { Label } from '@/components/ui/label';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from '@/components/ui/select';
import { Copy, Download, Code } from 'lucide-react';
import { generateGradientCSS, downloadCSS, copyToClipboard } from './utils';
import type { GradientExportProps } from './types';

export function GradientExport({
  config,
  exportOptions,
  onExportOptionsChange,
  onCopy,
  onDownload,
  className,
}: GradientExportProps) {
  const [copied, setCopied] = useState(false);
  const [open, setOpen] = useState(false);

  const css = generateGradientCSS(config, exportOptions);

  const handleCopy = async () => {
    const success = await copyToClipboard(css);
    if (success) {
      setCopied(true);
      onCopy?.(css);
      setTimeout(() => setCopied(false), 2000);
    }
  };

  const handleDownload = () => {
    downloadCSS(css, 'gradient.css');
    onDownload?.(css, 'gradient.css');
  };

  const handleFormatChange = (format: 'standard' | 'shorthand' | 'legacy') => {
    onExportOptionsChange({ ...exportOptions, format });
  };

  const handleIncludePrefixesChange = (includePrefixes: boolean) => {
    onExportOptionsChange({ ...exportOptions, includePrefixes });
  };

  const handleIncludeCommentsChange = (includeComments: boolean) => {
    onExportOptionsChange({ ...exportOptions, includeComments });
  };

  return (
    <div className={cn('gradient-export space-y-4', className)}>
      {/* Quick Actions */}
      <div className="flex gap-2">
        <Button
          type="button"
          variant="outline"
          className="flex-1"
          onClick={handleCopy}
          aria-label="Copy CSS to clipboard"
        >
          <Copy className="w-4 h-4 mr-2" />
          {copied ? 'Copied!' : 'Copy CSS'}
        </Button>
        <Button
          type="button"
          variant="default"
          className="flex-1"
          onClick={handleDownload}
          aria-label="Download CSS file"
        >
          <Download className="w-4 h-4 mr-2" />
          Download
        </Button>
      </div>

      {/* View Code Dialog */}
      <Dialog open={open} onOpenChange={setOpen}>
        <DialogTrigger asChild>
          <Button type="button" variant="outline" className="w-full">
            <Code className="w-4 h-4 mr-2" />
            View CSS Code
          </Button>
        </DialogTrigger>
        <DialogContent className="max-w-2xl">
          <DialogHeader>
            <DialogTitle>CSS Gradient Code</DialogTitle>
          </DialogHeader>

          <div className="space-y-4">
            {/* Export Options */}
            <div className="space-y-3">
              <Label>Export Options</Label>

              <div className="grid grid-cols-2 gap-4">
                <div className="space-y-2">
                  <Label htmlFor="format-select" className="text-sm text-muted-foreground">
                    Format
                  </Label>
                  <Select value={exportOptions.format} onValueChange={handleFormatChange}>
                    <SelectTrigger id="format-select">
                      <SelectValue />
                    </SelectTrigger>
                    <SelectContent>
                      <SelectItem value="standard">Standard</SelectItem>
                      <SelectItem value="shorthand">Shorthand</SelectItem>
                      <SelectItem value="legacy">Legacy</SelectItem>
                    </SelectContent>
                  </Select>
                </div>
              </div>

              <div className="flex items-center gap-4">
                <label className="flex items-center gap-2 cursor-pointer">
                  <input
                    type="checkbox"
                    checked={exportOptions.includePrefixes}
                    onChange={(e) => handleIncludePrefixesChange(e.target.checked)}
                    className="w-4 h-4 rounded border-input"
                    aria-label="Include vendor prefixes"
                  />
                  <span className="text-sm">Include vendor prefixes</span>
                </label>

                <label className="flex items-center gap-2 cursor-pointer">
                  <input
                    type="checkbox"
                    checked={exportOptions.includeComments}
                    onChange={(e) => handleIncludeCommentsChange(e.target.checked)}
                    className="w-4 h-4 rounded border-input"
                    aria-label="Include comments"
                  />
                  <span className="text-sm">Include comments</span>
                </label>
              </div>
            </div>

            {/* CSS Code Display */}
            <div className="space-y-2">
              <Label htmlFor="css-code">CSS Code</Label>
              <Textarea
                id="css-code"
                value={css}
                readOnly
                className="font-mono text-sm min-h-[200px]"
                aria-label="Generated CSS code"
              />
            </div>

            {/* Dialog Actions */}
            <div className="flex gap-2">
              <Button
                type="button"
                variant="outline"
                className="flex-1"
                onClick={handleCopy}
              >
                <Copy className="w-4 h-4 mr-2" />
                {copied ? 'Copied!' : 'Copy'}
              </Button>
              <Button
                type="button"
                variant="default"
                className="flex-1"
                onClick={handleDownload}
              >
                <Download className="w-4 h-4 mr-2" />
                Download
              </Button>
            </div>
          </div>
        </DialogContent>
      </Dialog>
    </div>
  );
}

export default GradientExport;
````

## File: packages/frontend/components/gradient-generator/GradientGenerator.tsx
````typescript
/**
 * CSS Gradient Generator - Main Component
 */

import React from 'react';
import { cn } from '@/lib/utils';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { ScrollArea } from '@/components/ui/scroll-area';
import { useGradientState, useGradientCSS, useGradientPresets } from './hooks';
import { GradientPreview } from './GradientPreview';
import { GradientControls } from './GradientControls';
import { GradientPresets } from './GradientPresets';
import { GradientExport } from './GradientExport';
import { DEFAULT_PRESETS } from './utils';
import type { GradientGeneratorProps } from './types';

export function GradientGenerator({
  initialConfig,
  onGradientChange,
  presets,
  showPresets = true,
  showExportPanel = true,
  className,
  maxColorStops = 10,
  minColorStops = 2,
  enableAnimation = false,
  animationDuration = 3,
}: GradientGeneratorProps) {
  const gradientState = useGradientState({
    initialConfig,
    maxColorStops,
    minColorStops,
    onChange: onGradientChange,
  });

  const { config } = gradientState;
  const { css } = useGradientCSS(config);

  // Use default presets if none provided
  const presetsToUse = presets || DEFAULT_PRESETS;

  // Export options state
  const [exportOptions, setExportOptions] = React.useState({
    format: 'standard' as 'standard' | 'shorthand' | 'legacy',
    includePrefixes: false,
    includeComments: false,
  });

  const handlePresetSelect = (preset: typeof presetsToUse[0]) => {
    gradientState.setConfig(preset.config);
  };

  return (
    <div className={cn('gradient-generator w-full', className)}>
      <div className="grid gap-6 lg:grid-cols-2">
        {/* Left Column: Preview */}
        <div className="space-y-6">
          <Card>
            <CardHeader>
              <CardTitle>Gradient Preview</CardTitle>
            </CardHeader>
            <CardContent>
              <GradientPreview
                config={config}
                enableAnimation={enableAnimation}
                animationDuration={animationDuration}
              />
            </CardContent>
          </Card>

          {/* Export Panel */}
          {showExportPanel && (
            <Card>
              <CardHeader>
                <CardTitle>Export CSS</CardTitle>
              </CardHeader>
              <CardContent>
                <GradientExport
                  config={config}
                  exportOptions={exportOptions}
                  onExportOptionsChange={setExportOptions}
                />
              </CardContent>
            </Card>
          )}
        </div>

        {/* Right Column: Controls */}
        <div className="space-y-6">
          <Card>
            <CardHeader>
              <CardTitle>Gradient Controls</CardTitle>
            </CardHeader>
            <CardContent>
              <ScrollArea className="h-[500px] pr-4">
                <GradientControls
                  config={config}
                  onConfigChange={gradientState.setConfig}
                  maxColorStops={maxColorStops}
                  minColorStops={minColorStops}
                />
              </ScrollArea>
            </CardContent>
          </Card>

          {/* Presets Panel */}
          {showPresets && (
            <Card>
              <CardHeader>
                <CardTitle>Preset Gradients</CardTitle>
              </CardHeader>
              <CardContent>
                <GradientPresets
                  presets={presetsToUse}
                  onPresetSelect={handlePresetSelect}
                  showCategories={true}
                  columns={3}
                />
              </CardContent>
            </Card>
          )}
        </div>
      </div>
    </div>
  );
}

export default GradientGenerator;
````

## File: packages/frontend/components/gradient-generator/GradientPresets.tsx
````typescript
/**
 * CSS Gradient Generator - Gradient Presets Component
 */

import React, { useState } from 'react';
import { cn } from '@/lib/utils';
import { Button } from '@/components/ui/button';
import { Badge } from '@/components/ui/badge';
import { ScrollArea } from '@/components/ui/scroll-area';
import { generateGradientCSS } from './utils';
import type { GradientPresetsProps } from './types';

export function GradientPresets({
  presets,
  selectedPresetId,
  onPresetSelect,
  showCategories = true,
  columns = 3,
  className,
}: GradientPresetsProps) {
  const [activeCategory, setActiveCategory] = useState<string | null>(null);

  // Get unique categories
  const categories = Array.from(
    new Set(presets.map((p) => p.category).filter(Boolean) as string[])
  );

  // Filter presets by category
  const filteredPresets = activeCategory
    ? presets.filter((p) => p.category === activeCategory)
    : presets;

  const handleCategoryClick = (category: string | null) => {
    setActiveCategory(category);
  };

  const handlePresetClick = (preset: typeof presets[0]) => {
    onPresetSelect(preset);
  };

  return (
    <div className={cn('gradient-presets space-y-4', className)}>
      {/* Category Filters */}
      {showCategories && categories.length > 0 && (
        <div className="flex flex-wrap gap-2">
          <Button
            type="button"
            variant={activeCategory === null ? 'default' : 'outline'}
            size="sm"
            onClick={() => handleCategoryClick(null)}
          >
            All
          </Button>
          {categories.map((category) => (
            <Button
              key={category}
              type="button"
              variant={activeCategory === category ? 'default' : 'outline'}
              size="sm"
              onClick={() => handleCategoryClick(category)}
            >
              {category}
            </Button>
          ))}
        </div>
      )}

      {/* Presets Grid */}
      <ScrollArea className="h-64">
        <div
          className="grid gap-3"
          style={{
            gridTemplateColumns: `repeat(${columns}, minmax(0, 1fr))`,
          }}
        >
          {filteredPresets.map((preset) => {
            const isSelected = selectedPresetId === preset.id;
            const gradientStyle = {
              background: generateGradientCSS(preset.config).replace('background: ', '').replace(';', ''),
            };

            return (
              <button
                key={preset.id}
                type="button"
                className={cn(
                  'group relative rounded-lg border-2 transition-all hover:scale-105 focus:outline-none focus:ring-2 focus:ring-ring',
                  isSelected
                    ? 'border-primary ring-2 ring-primary ring-offset-2'
                    : 'border-border hover:border-primary/50'
                )}
                onClick={() => handlePresetClick(preset)}
                aria-label={`Select ${preset.name} preset: ${preset.description || ''}`}
                aria-pressed={isSelected}
              >
                {/* Gradient Preview */}
                <div
                  className="aspect-square w-full rounded-md"
                  style={gradientStyle}
                  aria-hidden="true"
                />

                {/* Preset Info Overlay */}
                <div className="absolute inset-0 flex flex-col items-center justify-center bg-black/50 opacity-0 transition-opacity group-hover:opacity-100">
                  <span className="text-sm font-medium text-white">{preset.name}</span>
                  {preset.category && (
                    <Badge variant="secondary" className="mt-1 text-xs">
                      {preset.category}
                    </Badge>
                  )}
                </div>

                {/* Selected Indicator */}
                {isSelected && (
                  <div className="absolute top-2 right-2 flex h-6 w-6 items-center justify-center rounded-full bg-primary text-primary-foreground">
                    <svg
                      className="h-4 w-4"
                      fill="none"
                      stroke="currentColor"
                      viewBox="0 0 24 24"
                    >
                      <path
                        strokeLinecap="round"
                        strokeLinejoin="round"
                        strokeWidth={2}
                        d="M5 13l4 4L19 7"
                      />
                    </svg>
                  </div>
                )}
              </button>
            );
          })}
        </div>

        {filteredPresets.length === 0 && (
          <div className="flex h-32 items-center justify-center text-muted-foreground">
            No presets found
          </div>
        )}
      </ScrollArea>
    </div>
  );
}

export default GradientPresets;
````

## File: packages/frontend/components/gradient-generator/GradientPreview.tsx
````typescript
/**
 * CSS Gradient Generator - Gradient Preview Component
 */

import React from 'react';
import { cn } from '@/lib/utils';
import { useGradientCSS } from './hooks';
import type { GradientPreviewProps } from './types';

export function GradientPreview({
  config,
  width = '100%',
  height = '200px',
  showTransparencyGrid = true,
  className,
  enableAnimation = false,
  animationDuration = 3,
}: GradientPreviewProps) {
  const { css } = useGradientCSS(config);

  const previewStyle: React.CSSProperties = {
    width,
    height,
    background: css.replace('background: ', '').replace(';', ''),
    ...(enableAnimation && {
      animation: `gradientShift ${animationDuration}s ease infinite`,
    }),
  };

  return (
    <div className={cn('gradient-preview', className)}>
      <div
        className="relative rounded-lg overflow-hidden border border-border"
        style={{ width, height }}
      >
        {showTransparencyGrid && (
          <div
            className="absolute inset-0 opacity-50"
            style={{
              backgroundImage: `
                linear-gradient(45deg, #ccc 25%, transparent 25%),
                linear-gradient(-45deg, #ccc 25%, transparent 25%),
                linear-gradient(45deg, transparent 75%, #ccc 75%),
                linear-gradient(-45deg, transparent 75%, #ccc 75%)
              `,
              backgroundSize: '20px 20px',
              backgroundPosition: '0 0, 0 10px, 10px -10px, -10px 0px',
            }}
            aria-hidden="true"
          />
        )}
        <div
          className="relative w-full h-full"
          style={previewStyle}
          role="img"
          aria-label={`Gradient preview: ${config.type} gradient`}
        />
      </div>

      {enableAnimation && (
        <style>{`
          @keyframes gradientShift {
            0% { filter: hue-rotate(0deg); }
            100% { filter: hue-rotate(360deg); }
          }
        `}</style>
      )}
    </div>
  );
}

export default GradientPreview;
````

## File: packages/frontend/components/gradient-generator/hooks.ts
````typescript
/**
 * CSS Gradient Generator - Custom Hooks
 */

import { useState, useCallback, useMemo } from 'react';
import type { GradientConfig, ColorStop, GradientType, GradientPreset, ExportOptions } from './types';
import { generateGradientCSS, DEFAULT_PRESETS } from './utils';

/**
 * Hook for managing gradient state
 */
interface UseGradientStateOptions {
  initialConfig?: GradientConfig;
  maxColorStops?: number;
  minColorStops?: number;
  onChange?: (config: GradientConfig) => void;
}

interface UseGradientStateReturn {
  config: GradientConfig;
  gradientType: GradientType;
  colorStops: ColorStop[];

  // Actions
  setGradientType: (type: GradientType) => void;
  setColorStops: (stops: ColorStop[]) => void;
  addColorStop: (color?: string, position?: number) => void;
  removeColorStop: (id: string) => void;
  updateColorStop: (id: string, updates: Partial<ColorStop>) => void;
  setLinearAngle: (angle: number) => void;
  setRadialPosition: (x: number, y: number) => void;
  setRadialShape: (shape: 'circle' | 'ellipse') => void;
  setConicAngle: (angle: number) => void;
  setConicPosition: (x: number, y: number) => void;
  setConfig: (config: GradientConfig) => void;
  resetConfig: () => void;

  // Computed
  canAddColorStop: boolean;
  canRemoveColorStop: boolean;
}

export function useGradientState(options: UseGradientStateOptions = {}): UseGradientStateReturn {
  const {
    initialConfig,
    maxColorStops = 10,
    minColorStops = 2,
    onChange,
  } = options;

  // Default initial configuration
  const defaultConfig: GradientConfig = {
    type: 'linear',
    angle: 90,
    colorStops: [
      { id: '1', color: '#6366f1', position: 0 },
      { id: '2', color: '#a855f7', position: 100 },
    ],
  };

  const [config, setConfig] = useState<GradientConfig>(initialConfig || defaultConfig);

  // Computed values
  const gradientType = config.type;
  const colorStops = config.colorStops;
  const canAddColorStop = colorStops.length < maxColorStops;
  const canRemoveColorStop = colorStops.length > minColorStops;

  // Actions
  const setGradientType = useCallback((type: GradientType) => {
    setConfig((prev) => {
      let newConfig: GradientConfig;

      if (type === 'linear') {
        newConfig = {
          type: 'linear',
          angle: 90,
          colorStops: prev.colorStops,
        };
      } else if (type === 'radial') {
        newConfig = {
          type: 'radial',
          shape: 'circle',
          position: { x: 50, y: 50 },
          colorStops: prev.colorStops,
        };
      } else {
        newConfig = {
          type: 'conic',
          angle: 0,
          position: { x: 50, y: 50 },
          colorStops: prev.colorStops,
        };
      }

      onChange?.(newConfig);
      return newConfig;
    });
  }, [onChange]);

  const setColorStops = useCallback((stops: ColorStop[]) => {
    setConfig((prev) => {
      const newConfig = { ...prev, colorStops: stops };
      onChange?.(newConfig);
      return newConfig;
    });
  }, [onChange]);

  const addColorStop = useCallback((color?: string, position?: number) => {
    setConfig((prev) => {
      if (prev.colorStops.length >= maxColorStops) return prev;

      const newStop: ColorStop = {
        id: `${Date.now()}-${Math.random()}`,
        color: color || '#ffffff',
        position: position ?? 50,
      };

      const newConfig = {
        ...prev,
        colorStops: [...prev.colorStops, newStop].sort((a, b) => a.position - b.position),
      };
      onChange?.(newConfig);
      return newConfig;
    });
  }, [maxColorStops, onChange]);

  const removeColorStop = useCallback((id: string) => {
    setConfig((prev) => {
      if (prev.colorStops.length <= minColorStops) return prev;

      const newConfig = {
        ...prev,
        colorStops: prev.colorStops.filter((stop) => stop.id !== id),
      };
      onChange?.(newConfig);
      return newConfig;
    });
  }, [minColorStops, onChange]);

  const updateColorStop = useCallback((id: string, updates: Partial<ColorStop>) => {
    setConfig((prev) => {
      const newConfig = {
        ...prev,
        colorStops: prev.colorStops.map((stop) =>
          stop.id === id ? { ...stop, ...updates } : stop
        ).sort((a, b) => a.position - b.position),
      };
      onChange?.(newConfig);
      return newConfig;
    });
  }, [onChange]);

  const setLinearAngle = useCallback((angle: number) => {
    setConfig((prev) => {
      if (prev.type !== 'linear') return prev;
      const newConfig = { ...prev, angle };
      onChange?.(newConfig);
      return newConfig;
    });
  }, [onChange]);

  const setRadialPosition = useCallback((x: number, y: number) => {
    setConfig((prev) => {
      if (prev.type !== 'radial') return prev;
      const newConfig = { ...prev, position: { x, y } };
      onChange?.(newConfig);
      return newConfig;
    });
  }, [onChange]);

  const setRadialShape = useCallback((shape: 'circle' | 'ellipse') => {
    setConfig((prev) => {
      if (prev.type !== 'radial') return prev;
      const newConfig = { ...prev, shape };
      onChange?.(newConfig);
      return newConfig;
    });
  }, [onChange]);

  const setConicAngle = useCallback((angle: number) => {
    setConfig((prev) => {
      if (prev.type !== 'conic') return prev;
      const newConfig = { ...prev, angle };
      onChange?.(newConfig);
      return newConfig;
    });
  }, [onChange]);

  const setConicPosition = useCallback((x: number, y: number) => {
    setConfig((prev) => {
      if (prev.type !== 'conic') return prev;
      const newConfig = { ...prev, position: { x, y } };
      onChange?.(newConfig);
      return newConfig;
    });
  }, [onChange]);

  const resetConfig = useCallback(() => {
    setConfig(defaultConfig);
    onChange?.(defaultConfig);
  }, [onChange]);

  return {
    config,
    gradientType,
    colorStops,
    setGradientType,
    setColorStops,
    addColorStop,
    removeColorStop,
    updateColorStop,
    setLinearAngle,
    setRadialPosition,
    setRadialShape,
    setConicAngle,
    setConicPosition,
    setConfig,
    resetConfig,
    canAddColorStop,
    canRemoveColorStop,
  };
}

/**
 * Hook for generating CSS from gradient config
 */
interface UseGradientCSSReturn {
  css: string;
  cssWithPrefixes: string;
  getCSS: (options?: ExportOptions) => string;
}

export function useGradientCSS(config: GradientConfig): UseGradientCSSReturn {
  const css = useMemo(() => {
    return generateGradientCSS(config, { format: 'standard', includePrefixes: false, includeComments: false });
  }, [config]);

  const cssWithPrefixes = useMemo(() => {
    return generateGradientCSS(config, { format: 'standard', includePrefixes: true, includeComments: false });
  }, [config]);

  const getCSS = (options?: ExportOptions) => {
    return generateGradientCSS(config, options);
  };

  return { css, cssWithPrefixes, getCSS };
}

/**
 * Hook for managing gradient presets
 */
interface UseGradientPresetsReturn {
  presets: GradientPreset[];
  selectedPreset: GradientPreset | null;
  selectPreset: (preset: GradientPreset) => void;
  addPreset: (preset: GradientPreset) => void;
  removePreset: (id: string) => void;
  filterByCategory: (category: string | null) => void;
  categories: string[];
  activeCategory: string | null;
}

export function useGradientPresets(customPresets?: GradientPreset[]): UseGradientPresetsReturn {
  const [presets, setPresets] = useState<GradientPreset[]>([...DEFAULT_PRESETS, ...(customPresets || [])]);
  const [selectedPreset, setSelectedPreset] = useState<GradientPreset | null>(null);
  const [activeCategory, setActiveCategory] = useState<string | null>(null);

  const selectPreset = useCallback((preset: GradientPreset) => {
    setSelectedPreset(preset);
  }, []);

  const addPreset = useCallback((preset: GradientPreset) => {
    setPresets((prev) => [...prev, preset]);
  }, []);

  const removePreset = useCallback((id: string) => {
    setPresets((prev) => prev.filter((p) => p.id !== id));
    if (selectedPreset?.id === id) {
      setSelectedPreset(null);
    }
  }, [selectedPreset]);

  const filterByCategory = useCallback((category: string | null) => {
    setActiveCategory(category);
  }, []);

  const categories = useMemo(() => {
    const cats = new Set(presets.map((p) => p.category).filter(Boolean) as string[]);
    return Array.from(cats);
  }, [presets]);

  const filteredPresets = activeCategory
    ? presets.filter((p) => p.category === activeCategory)
    : presets;

  return {
    presets: filteredPresets,
    selectedPreset,
    selectPreset,
    addPreset,
    removePreset,
    filterByCategory,
    categories,
    activeCategory,
  };
}
````

## File: packages/frontend/components/gradient-generator/index.ts
````typescript
/**
 * CSS Gradient Generator - Public API Exports
 */

// Main component
export { GradientGenerator } from './GradientGenerator';

// Sub-components
export { GradientPreview } from './GradientPreview';
export { GradientControls } from './GradientControls';
export { GradientPresets } from './GradientPresets';
export { GradientExport } from './GradientExport';

// Custom hooks
export { useGradientState, useGradientCSS, useGradientPresets } from './hooks';

// Utility functions
export {
  generateGradientCSS,
  downloadCSS,
  copyToClipboard,
  hexToRgb,
  rgbToHex,
  isValidColor,
  getContrastColor,
  randomColor,
  interpolateColor,
  validateGradientConfig,
  gradientConfigToHash,
  generateCSSWithClass,
  generateInlineStyle,
  DEFAULT_PRESETS,
} from './utils';

// Types
export type {
  GradientType,
  ColorStop,
  LinearGradientConfig,
  RadialGradientConfig,
  ConicGradientConfig,
  GradientConfig,
  GradientPreset,
  CSSExportFormat,
  ExportOptions,
  GradientGeneratorProps,
  GradientPreviewProps,
  GradientControlsProps,
  GradientPresetsProps,
  GradientExportProps,
  ColorStopEditorProps,
  DirectionControlsProps,
} from './types';
````

## File: packages/frontend/components/gradient-generator/types.ts
````typescript
/**
 * CSS Gradient Generator - TypeScript Type Definitions
 */

/**
 * Supported gradient types
 */
export type GradientType = 'linear' | 'radial' | 'conic';

/**
 * Color stop definition
 */
export interface ColorStop {
  id: string;
  color: string;        // Hex, RGB, or HSL color value
  position: number;     // 0-100 percentage
}

/**
 * Linear gradient configuration
 */
export interface LinearGradientConfig {
  type: 'linear';
  angle: number;        // 0-360 degrees
  colorStops: ColorStop[];
}

/**
 * Radial gradient configuration
 */
export interface RadialGradientConfig {
  type: 'radial';
  shape: 'circle' | 'ellipse';
  position: {
    x: number;         // 0-100 percentage
    y: number;         // 0-100 percentage
  };
  colorStops: ColorStop[];
}

/**
 * Conic gradient configuration
 */
export interface ConicGradientConfig {
  type: 'conic';
  angle: number;        // 0-360 degrees
  position: {
    x: number;         // 0-100 percentage
    y: number;         // 0-100 percentage
  };
  colorStops: ColorStop[];
}

/**
 * Union type for all gradient configurations
 */
export type GradientConfig =
  | LinearGradientConfig
  | RadialGradientConfig
  | ConicGradientConfig;

/**
 * Gradient preset definition
 */
export interface GradientPreset {
  id: string;
  name: string;
  description?: string;
  config: GradientConfig;
  thumbnail?: string;   // Optional preview image
  category?: string;   // For grouping presets
}

/**
 * CSS export format options
 */
export type CSSExportFormat = 'standard' | 'shorthand' | 'legacy';

/**
 * Export options
 */
export interface ExportOptions {
  format: CSSExportFormat;
  includePrefixes: boolean;  // Include vendor prefixes
  includeComments: boolean;  // Include descriptive comments
}

/**
 * Main GradientGenerator component props
 */
export interface GradientGeneratorProps {
  /** Initial gradient configuration */
  initialConfig?: GradientConfig;
  /** Callback when gradient changes */
  onGradientChange?: (config: GradientConfig) => void;
  /** Available presets to display */
  presets?: GradientPreset[];
  /** Whether to show the presets panel */
  showPresets?: boolean;
  /** Whether to show the CSS export panel */
  showExportPanel?: boolean;
  /** Custom className for styling */
  className?: string;
  /** Maximum number of color stops allowed */
  maxColorStops?: number;
  /** Minimum number of color stops required */
  minColorStops?: number;
  /** Whether to enable animation on the preview */
  enableAnimation?: boolean;
  /** Animation duration in seconds (when enabled) */
  animationDuration?: number;
}

/**
 * GradientPreview component props
 */
export interface GradientPreviewProps {
  config: GradientConfig;
  /** Preview dimensions */
  width?: number | string;
  height?: number | string;
  /** Whether to show a checkerboard pattern for transparency */
  showTransparencyGrid?: boolean;
  /** Custom className */
  className?: string;
  /** Whether to enable animation */
  enableAnimation?: boolean;
  /** Animation duration in seconds */
  animationDuration?: number;
}

/**
 * GradientControls component props
 */
export interface GradientControlsProps {
  config: GradientConfig;
  onConfigChange: (config: GradientConfig) => void;
  /** Maximum number of color stops allowed */
  maxColorStops?: number;
  /** Minimum number of color stops required */
  minColorStops?: number;
  /** Custom className */
  className?: string;
}

/**
 * GradientPresets component props
 */
export interface GradientPresetsProps {
  presets: GradientPreset[];
  selectedPresetId?: string;
  onPresetSelect: (preset: GradientPreset) => void;
  /** Whether to show category filters */
  showCategories?: boolean;
  /** Grid layout columns */
  columns?: number;
  /** Custom className */
  className?: string;
}

/**
 * GradientExport component props
 */
export interface GradientExportProps {
  config: GradientConfig;
  exportOptions: ExportOptions;
  onExportOptionsChange: (options: ExportOptions) => void;
  /** Callback when copy to clipboard is triggered */
  onCopy?: (css: string) => void;
  /** Callback when download is triggered */
  onDownload?: (css: string, filename: string) => void;
  /** Custom className */
  className?: string;
}

/**
 * ColorStopEditor component props
 */
export interface ColorStopEditorProps {
  colorStops: ColorStop[];
  onColorStopsChange: (stops: ColorStop[]) => void;
  maxStops?: number;
  minStops?: number;
  /** Whether to show position sliders */
  showPositionSliders?: boolean;
  /** Custom className */
  className?: string;
}

/**
 * DirectionControls component props
 */
export interface DirectionControlsProps {
  config: GradientConfig;
  onConfigChange: (config: GradientConfig) => void;
  /** Available preset directions for linear gradients */
  presetDirections?: Array<{ label: string; angle: number }>;
  /** Custom className */
  className?: string;
}
````

## File: packages/frontend/components/gradient-generator/utils.ts
````typescript
/**
 * CSS Gradient Generator - Utility Functions
 */

import type { GradientConfig, ExportOptions, GradientPreset } from './types';

/**
 * Generate CSS gradient string from configuration
 */
export function generateGradientCSS(
  config: GradientConfig,
  options: ExportOptions = { format: 'standard', includePrefixes: false, includeComments: false }
): string {
  const { format, includePrefixes, includeComments } = options;

  const colorStops = config.colorStops
    .map((stop) => `${stop.color} ${stop.position}%`)
    .join(', ');

  let gradient = '';

  switch (config.type) {
    case 'linear':
      gradient = `linear-gradient(${config.angle}deg, ${colorStops})`;
      break;
    case 'radial':
      gradient = `radial-gradient(${config.shape} at ${config.position.x}% ${config.position.y}%, ${colorStops})`;
      break;
    case 'conic':
      gradient = `conic-gradient(from ${config.angle}deg at ${config.position.x}% ${config.position.y}%, ${colorStops})`;
      break;
  }

  let css = `background: ${gradient};`;

  if (includePrefixes && config.type === 'linear') {
    css = `-webkit-linear-gradient(${config.angle}deg, ${colorStops});\n` +
          `-moz-linear-gradient(${config.angle}deg, ${colorStops});\n` +
          `-o-linear-gradient(${config.angle}deg, ${colorStops});\n` +
          `background: ${gradient};`;
  }

  if (includeComments) {
    css = `/* ${config.type.charAt(0).toUpperCase() + config.type.slice(1)} Gradient */\n` + css;
  }

  return css;
}

/**
 * Download CSS as a file
 */
export function downloadCSS(css: string, filename: string = 'gradient.css'): void {
  const blob = new Blob([css], { type: 'text/css' });
  const url = URL.createObjectURL(blob);
  const link = document.createElement('a');
  link.href = url;
  link.download = filename;
  document.body.appendChild(link);
  link.click();
  document.body.removeChild(link);
  URL.revokeObjectURL(url);
}

/**
 * Copy text to clipboard
 */
export async function copyToClipboard(text: string): Promise<boolean> {
  try {
    await navigator.clipboard.writeText(text);
    return true;
  } catch (error) {
    console.error('Failed to copy to clipboard:', error);
    return false;
  }
}

/**
 * Convert hex color to RGB
 */
export function hexToRgb(hex: string): { r: number; g: number; b: number } | null {
  const result = /^#?([a-f\d]{2})([a-f\d]{2})([a-f\d]{2})$/i.exec(hex);
  return result
    ? {
        r: parseInt(result[1] || '00', 16),
        g: parseInt(result[2] || '00', 16),
        b: parseInt(result[3] || '00', 16),
      }
    : null;
}

/**
 * Convert RGB to hex
 */
export function rgbToHex(r: number, g: number, b: number): string {
  return '#' + [r, g, b].map((x) => {
    const hex = x.toString(16);
    return hex.length === 1 ? '0' + hex : hex;
  }).join('');
}

/**
 * Check if a color string is valid
 */
export function isValidColor(color: string): boolean {
  const s = new Option().style;
  s.color = color;
  return s.color !== '';
}

/**
 * Get contrasting text color (black or white) for a given background
 */
export function getContrastColor(hex: string): 'black' | 'white' {
  const rgb = hexToRgb(hex);
  if (!rgb) return 'black';

  const luminance = (0.299 * rgb.r + 0.587 * rgb.g + 0.114 * rgb.b) / 255;
  return luminance > 0.5 ? 'black' : 'white';
}

/**
 * Generate a random color
 */
export function randomColor(): string {
  return '#' + Math.floor(Math.random() * 16777215).toString(16).padStart(6, '0');
}

/**
 * Interpolate between two colors
 */
export function interpolateColor(color1: string, color2: string, factor: number): string {
  const rgb1 = hexToRgb(color1);
  const rgb2 = hexToRgb(color2);

  if (!rgb1 || !rgb2) return color1;

  const r = Math.round(rgb1.r + factor * (rgb2.r - rgb1.r));
  const g = Math.round(rgb1.g + factor * (rgb2.g - rgb1.g));
  const b = Math.round(rgb1.b + factor * (rgb2.b - rgb1.b));

  return rgbToHex(r, g, b);
}

/**
 * Validate gradient configuration
 */
export function validateGradientConfig(config: GradientConfig): {
  valid: boolean;
  errors: string[];
} {
  const errors: string[] = [];

  if (config.colorStops.length < 2) {
    errors.push('At least 2 color stops are required');
  }

  if (config.colorStops.length > 10) {
    errors.push('Maximum 10 color stops allowed');
  }

  for (const stop of config.colorStops) {
    if (stop.position < 0 || stop.position > 100) {
      errors.push(`Color stop position must be between 0 and 100`);
    }
    if (!isValidColor(stop.color)) {
      errors.push(`Invalid color: ${stop.color}`);
    }
  }

  if (config.type === 'linear') {
    if (config.angle < 0 || config.angle > 360) {
      errors.push('Linear angle must be between 0 and 360');
    }
  }

  if (config.type === 'radial' || config.type === 'conic') {
    if (config.position.x < 0 || config.position.x > 100) {
      errors.push('Position X must be between 0 and 100');
    }
    if (config.position.y < 0 || config.position.y > 100) {
      errors.push('Position Y must be between 0 and 100');
    }
  }

  return { valid: errors.length === 0, errors };
}

/**
 * Convert gradient config to a unique hash for comparison
 */
export function gradientConfigToHash(config: GradientConfig): string {
  return JSON.stringify(config);
}

/**
 * Generate CSS with class wrapper
 */
export function generateCSSWithClass(
  config: GradientConfig,
  className: string = 'gradient-bg',
  options: ExportOptions = { format: 'standard', includePrefixes: false, includeComments: true }
): string {
  const gradientCSS = generateGradientCSS(config, options);

  return `.${className} {
  ${gradientCSS}
}`;
}

/**
 * Generate CSS with inline style
 */
export function generateInlineStyle(config: GradientConfig): string {
  const gradientCSS = generateGradientCSS(config, {
    format: 'standard',
    includePrefixes: false,
    includeComments: false
  });
  return gradientCSS.replace('background: ', '').replace(';', '');
}

/**
 * Default gradient presets
 */
export const DEFAULT_PRESETS: GradientPreset[] = [
  // Warm presets
  {
    id: 'sunset',
    name: 'Sunset',
    description: 'Warm orange to purple gradient',
    category: 'Warm',
    config: {
      type: 'linear',
      angle: 135,
      colorStops: [
        { id: '1', color: '#ff7e5f', position: 0 },
        { id: '2', color: '#feb47b', position: 100 },
      ],
    },
  },
  {
    id: 'fire',
    name: 'Fire',
    description: 'Intense red and orange gradient',
    category: 'Warm',
    config: {
      type: 'linear',
      angle: 45,
      colorStops: [
        { id: '1', color: '#f12711', position: 0 },
        { id: '2', color: '#f5af19', position: 100 },
      ],
    },
  },
  {
    id: 'peach',
    name: 'Peach',
    description: 'Soft peach gradient',
    category: 'Warm',
    config: {
      type: 'linear',
      angle: 180,
      colorStops: [
        { id: '1', color: '#ffecd2', position: 0 },
        { id: '2', color: '#fcb69f', position: 100 },
      ],
    },
  },
  // Cool presets
  {
    id: 'ocean',
    name: 'Ocean',
    description: 'Deep blue to cyan gradient',
    category: 'Cool',
    config: {
      type: 'linear',
      angle: 180,
      colorStops: [
        { id: '1', color: '#2193b0', position: 0 },
        { id: '2', color: '#6dd5ed', position: 100 },
      ],
    },
  },
  {
    id: 'sky',
    name: 'Sky',
    description: 'Light blue sky gradient',
    category: 'Cool',
    config: {
      type: 'linear',
      angle: 90,
      colorStops: [
        { id: '1', color: '#56ccf2', position: 0 },
        { id: '2', color: '#2f80ed', position: 100 },
      ],
    },
  },
  {
    id: 'ice',
    name: 'Ice',
    description: 'Cool ice blue gradient',
    category: 'Cool',
    config: {
      type: 'linear',
      angle: 135,
      colorStops: [
        { id: '1', color: '#a8edea', position: 0 },
        { id: '2', color: '#fed6e3', position: 100 },
      ],
    },
  },
  // Nature presets
  {
    id: 'forest',
    name: 'Forest',
    description: 'Green nature gradient',
    category: 'Nature',
    config: {
      type: 'linear',
      angle: 45,
      colorStops: [
        { id: '1', color: '#134e5e', position: 0 },
        { id: '2', color: '#71b280', position: 100 },
      ],
    },
  },
  {
    id: 'meadow',
    name: 'Meadow',
    description: 'Fresh meadow gradient',
    category: 'Nature',
    config: {
      type: 'linear',
      angle: 90,
      colorStops: [
        { id: '1', color: '#d4fc79', position: 0 },
        { id: '2', color: '#96e6a1', position: 100 },
      ],
    },
  },
  // Vibrant presets
  {
    id: 'neon',
    name: 'Neon',
    description: 'Vibrant purple to pink gradient',
    category: 'Vibrant',
    config: {
      type: 'linear',
      angle: 90,
      colorStops: [
        { id: '1', color: '#667eea', position: 0 },
        { id: '2', color: '#764ba2', position: 100 },
      ],
    },
  },
  {
    id: 'aurora',
    name: 'Aurora',
    description: 'Northern lights inspired gradient',
    category: 'Vibrant',
    config: {
      type: 'linear',
      angle: 120,
      colorStops: [
        { id: '1', color: '#00c6ff', position: 0 },
        { id: '2', color: '#0072ff', position: 100 },
      ],
    },
  },
  // Dark presets
  {
    id: 'midnight',
    name: 'Midnight',
    description: 'Dark blue to black gradient',
    category: 'Dark',
    config: {
      type: 'linear',
      angle: 180,
      colorStops: [
        { id: '1', color: '#0f2027', position: 0 },
        { id: '2', color: '#203a43', position: 50 },
        { id: '3', color: '#2c5364', position: 100 },
      ],
    },
  },
  {
    id: 'galaxy',
    name: 'Galaxy',
    description: 'Space-inspired purple gradient',
    category: 'Dark',
    config: {
      type: 'radial',
      shape: 'circle',
      position: { x: 50, y: 50 },
      colorStops: [
        { id: '1', color: '#667eea', position: 0 },
        { id: '2', color: '#764ba2', position: 100 },
      ],
    },
  },
];
````

## File: packages/frontend/components/HomeView.tsx
````typescript
/**
 * HomeView - Landing Page Component
 *
 * Provides a clear, guided experience for users to choose what they want to create
 * before entering the AI workspace. This helps the AI agent understand user intent.
 */

import { useState, useMemo } from "react";
import { motion, AnimatePresence } from "framer-motion";
import {
  Video,
  Music,
  AudioWaveform,
  Sparkles,
  Settings2,
  ChevronRight,
  Clock,
  Palette,
  Users,
  Globe,
  ArrowLeft,
} from "lucide-react";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { Label } from "@/components/ui/label";
import { Slider } from "@/components/ui/slider";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select";
import { cn } from "@/lib/utils";
import { ART_STYLES, VIDEO_PURPOSES, type VideoPurpose } from "@/constants";

// Creation mode types
export type CreationMode = "video" | "music" | "visualizer" | null;

// Preset configurations for AI agent
export interface CreationPreset {
  mode: CreationMode;
  // Video presets
  videoPurpose?: VideoPurpose;
  visualStyle?: string;
  duration?: number;
  topic?: string;
  // Music presets
  musicStyle?: string;
  instrumental?: boolean;
  musicPrompt?: string;
  lyrics?: string;
  musicModel?: "V4" | "V4_5" | "V4_5PLUS" | "V4_5ALL" | "V5";
  // Common
  targetAudience?: string;
  language?: string;
}

interface HomeViewProps {
  onStartCreation: (preset: CreationPreset) => void;
  onSwitchToVisualizer: () => void;
}

// Mode card data
const CREATION_MODES = [
  {
    id: "video" as const,
    title: "Create Video",
    description: "Generate AI videos from any topic with narration, visuals, and music",
    icon: Video,
    color: "from-violet-500 to-purple-600",
    features: ["AI Narration", "Visual Generation", "Background Music", "Auto SFX"],
  },
  {
    id: "music" as const,
    title: "Generate Music",
    description: "Create full songs, instrumentals, or background tracks with Suno AI",
    icon: Music,
    color: "from-pink-500 to-rose-600",
    features: ["Full Songs", "Instrumentals", "Custom Lyrics", "Multiple Styles"],
  },
  {
    id: "visualizer" as const,
    title: "Audio Visualizer",
    description: "Upload audio + SRT to create lyric videos with synced visuals",
    icon: AudioWaveform,
    color: "from-cyan-500 to-blue-600",
    features: ["Lyric Sync", "Visual Effects", "Waveform Display", "Custom Timing"],
  },
];

// Quick start presets
const VIDEO_PRESETS = [
  { id: "documentary", label: "Documentary", purpose: "documentary" as VideoPurpose, style: "Cinematic", duration: 60 },
  { id: "social", label: "Social Short", purpose: "social_short" as VideoPurpose, style: "Modern", duration: 30 },
  { id: "educational", label: "Educational", purpose: "educational" as VideoPurpose, style: "Clean", duration: 90 },
  { id: "storytelling", label: "Storytelling", purpose: "storytelling" as VideoPurpose, style: "Cinematic", duration: 120 },
  { id: "travel", label: "Travel", purpose: "travel" as VideoPurpose, style: "Vibrant", duration: 60 },
  { id: "motivational", label: "Motivational", purpose: "motivational" as VideoPurpose, style: "Inspiring", duration: 45 },
];

const MUSIC_PRESETS = [
  { id: "pop", label: "Pop Song", style: "Pop, Catchy, Upbeat", instrumental: false },
  { id: "cinematic", label: "Cinematic Score", style: "Cinematic, Orchestral, Epic", instrumental: true },
  { id: "lofi", label: "Lo-Fi Beats", style: "Lo-Fi, Chill, Relaxing", instrumental: true },
  { id: "rock", label: "Rock Anthem", style: "Rock, Powerful, Electric Guitar", instrumental: false },
  { id: "electronic", label: "Electronic", style: "Electronic, Synthwave, Energetic", instrumental: true },
  { id: "ambient", label: "Ambient", style: "Ambient, Atmospheric, Peaceful", instrumental: true },
];

export function HomeView({ onStartCreation, onSwitchToVisualizer }: HomeViewProps) {
  const [selectedMode, setSelectedMode] = useState<CreationMode>(null);
  const [showConfig, setShowConfig] = useState(false);

  // Video config state
  const [videoPurpose, setVideoPurpose] = useState<VideoPurpose>("documentary");
  const [visualStyle, setVisualStyle] = useState("Cinematic");
  const [duration, setDuration] = useState(60);
  const [topic, setTopic] = useState("");

  // Music config state
  const [musicStyle, setMusicStyle] = useState("Pop, Catchy, Upbeat");
  const [instrumental, setInstrumental] = useState(false);
  const [musicPrompt, setMusicPrompt] = useState("");
  const [lyrics, setLyrics] = useState("");
  const [musicModel, setMusicModel] = useState<"V4" | "V4_5" | "V4_5PLUS" | "V4_5ALL" | "V5">("V5");

  // Character limits based on model - reactive to model changes
  const charLimits = useMemo(() => {
    if (musicModel === "V4") {
      return { prompt: 3000, style: 200, title: 80 };
    } else if (musicModel === "V4_5ALL") {
      return { prompt: 5000, style: 1000, title: 80 };
    } else {
      // V4_5, V4_5PLUS, V5
      return { prompt: 5000, style: 1000, title: 100 };
    }
  }, [musicModel]);

  const handleModeSelect = (mode: CreationMode) => {
    if (mode === "visualizer") {
      onSwitchToVisualizer();
      return;
    }
    setSelectedMode(mode);
    setShowConfig(true);
  };

  const handlePresetSelect = (preset: any) => {
    if (selectedMode === "video") {
      setVideoPurpose(preset.purpose);
      setVisualStyle(preset.style);
      setDuration(preset.duration);
    } else if (selectedMode === "music") {
      setMusicStyle(preset.style);
      setInstrumental(preset.instrumental);
    }
  };

  const handleStartCreation = () => {
    const preset: CreationPreset = {
      mode: selectedMode,
    };

    if (selectedMode === "video") {
      preset.videoPurpose = videoPurpose;
      preset.visualStyle = visualStyle;
      preset.duration = duration;
      preset.topic = topic;
    } else if (selectedMode === "music") {
      preset.musicStyle = musicStyle;
      preset.instrumental = instrumental;
      preset.musicPrompt = musicPrompt;
      preset.musicModel = musicModel;
      if (!instrumental && lyrics) {
        preset.lyrics = lyrics;
      }
    }

    onStartCreation(preset);
  };

  const handleBack = () => {
    setShowConfig(false);
    setSelectedMode(null);
  };

  return (
    <div className="min-h-screen bg-[#0a0a0f] text-white overflow-hidden">
      {/* Background gradient */}
      <div className="fixed inset-0 pointer-events-none">
        <div className="absolute top-0 left-1/4 w-96 h-96 bg-violet-500/10 rounded-full blur-[128px]" />
        <div className="absolute bottom-0 right-1/4 w-96 h-96 bg-pink-500/10 rounded-full blur-[128px]" />
      </div>

      {/* Content */}
      <div className="relative z-10 min-h-screen flex flex-col">
        {/* Header */}
        <header className="p-6 flex items-center justify-between">
          <div className="flex items-center gap-3">
            <div className="w-10 h-10 rounded-xl bg-gradient-to-br from-violet-500 to-purple-600 flex items-center justify-center">
              <Sparkles className="w-5 h-5 text-white" />
            </div>
            <span className="text-xl font-semibold">LyricLens</span>
          </div>

          {showConfig && (
            <Button
              variant="ghost"
              size="sm"
              onClick={handleBack}
              className="text-white/60 hover:text-white"
            >
              <ArrowLeft className="w-4 h-4 mr-2" />
              Back
            </Button>
          )}
        </header>

        {/* Main Content */}
        <main className="flex-1 flex items-center justify-center p-6">
          <AnimatePresence mode="wait">
            {!showConfig ? (
              /* Mode Selection */
              <motion.div
                key="mode-select"
                initial={{ opacity: 0, y: 20 }}
                animate={{ opacity: 1, y: 0 }}
                exit={{ opacity: 0, y: -20 }}
                className="max-w-5xl w-full"
              >
                {/* Title */}
                <div className="text-center mb-12">
                  <h1 className="text-4xl md:text-5xl font-bold mb-4">
                    What would you like to create?
                  </h1>
                  <p className="text-lg text-white/60 max-w-2xl mx-auto">
                    Choose a creation mode to get started. Our AI will guide you through the process.
                  </p>
                </div>

                {/* Mode Cards */}
                <div className="grid md:grid-cols-3 gap-6">
                  {CREATION_MODES.map((mode) => {
                    const Icon = mode.icon;
                    return (
                      <motion.button
                        key={mode.id}
                        onClick={() => handleModeSelect(mode.id)}
                        whileHover={{ scale: 1.02, y: -4 }}
                        whileTap={{ scale: 0.98 }}
                        className={cn(
                          "group relative p-6 rounded-2xl text-left transition-all duration-300",
                          "bg-white/5 hover:bg-white/10 border border-white/10 hover:border-white/20",
                          "backdrop-blur-sm"
                        )}
                      >
                        {/* Icon */}
                        <div className={cn(
                          "w-14 h-14 rounded-xl bg-gradient-to-br flex items-center justify-center mb-4",
                          mode.color
                        )}>
                          <Icon className="w-7 h-7 text-white" />
                        </div>

                        {/* Title & Description */}
                        <h3 className="text-xl font-semibold mb-2">{mode.title}</h3>
                        <p className="text-sm text-white/60 mb-4">{mode.description}</p>

                        {/* Features */}
                        <div className="flex flex-wrap gap-2">
                          {mode.features.map((feature) => (
                            <span
                              key={feature}
                              className="px-2 py-1 text-xs rounded-full bg-white/10 text-white/70"
                            >
                              {feature}
                            </span>
                          ))}
                        </div>

                        {/* Arrow indicator */}
                        <div className="absolute top-6 right-6 opacity-0 group-hover:opacity-100 transition-opacity">
                          <ChevronRight className="w-5 h-5 text-white/60" />
                        </div>
                      </motion.button>
                    );
                  })}
                </div>
              </motion.div>
            ) : (
              /* Configuration Panel */
              <motion.div
                key="config"
                initial={{ opacity: 0, y: 20 }}
                animate={{ opacity: 1, y: 0 }}
                exit={{ opacity: 0, y: -20 }}
                className="max-w-3xl w-full"
              >
                {/* Title */}
                <div className="text-center mb-8">
                  <div className={cn(
                    "w-16 h-16 rounded-2xl bg-gradient-to-br flex items-center justify-center mx-auto mb-4",
                    selectedMode === "video" ? "from-violet-500 to-purple-600" : "from-pink-500 to-rose-600"
                  )}>
                    {selectedMode === "video" ? (
                      <Video className="w-8 h-8 text-white" />
                    ) : (
                      <Music className="w-8 h-8 text-white" />
                    )}
                  </div>
                  <h2 className="text-3xl font-bold mb-2">
                    {selectedMode === "video" ? "Configure Your Video" : "Configure Your Music"}
                  </h2>
                  <p className="text-white/60">
                    {selectedMode === "video"
                      ? "Choose a preset or customize your video settings"
                      : "Choose a preset or customize your music style"
                    }
                  </p>
                </div>

                {/* Quick Presets */}
                <div className="mb-8">
                  <Label className="text-sm text-white/60 mb-3 block">Quick Presets</Label>
                  <div className="grid grid-cols-3 md:grid-cols-6 gap-2">
                    {(selectedMode === "video" ? VIDEO_PRESETS : MUSIC_PRESETS).map((preset) => (
                      <button
                        key={preset.id}
                        onClick={() => handlePresetSelect(preset)}
                        className={cn(
                          "px-3 py-2 rounded-lg text-sm font-medium transition-all",
                          "bg-white/5 hover:bg-white/10 border border-white/10",
                          ((selectedMode === "video" && "purpose" in preset && videoPurpose === preset.purpose) ||
                            (selectedMode === "music" && "style" in preset && musicStyle === preset.style)) &&
                          "bg-violet-500/20 border-violet-500/50"
                        )}
                      >
                        {preset.label}
                      </button>
                    ))}
                  </div>
                </div>

                {/* Configuration Form */}
                <div className="bg-white/5 rounded-2xl p-6 border border-white/10 mb-6">
                  {selectedMode === "video" ? (
                    <div className="space-y-6">
                      {/* Topic Input */}
                      <div>
                        <Label className="text-sm text-white/80 mb-2 block">
                          What's your video about? (optional)
                        </Label>
                        <Input
                          value={topic}
                          onChange={(e) => setTopic(e.target.value)}
                          placeholder="e.g., The history of coffee, A day in Tokyo..."
                          className="bg-white/5 border-white/10 text-white placeholder:text-white/40"
                        />
                        <p className="text-xs text-white/40 mt-1">
                          Leave empty to describe it to the AI later
                        </p>
                      </div>

                      {/* Purpose & Style */}
                      <div className="grid md:grid-cols-2 gap-4">
                        <div>
                          <Label className="text-sm text-white/80 mb-2 block">
                            <Users className="w-4 h-4 inline mr-2" />
                            Video Purpose
                          </Label>
                          <Select value={videoPurpose} onValueChange={(v) => setVideoPurpose(v as VideoPurpose)}>
                            <SelectTrigger className="bg-white/5 border-white/10 text-white">
                              <SelectValue />
                            </SelectTrigger>
                            <SelectContent className="bg-zinc-900 border-white/10">
                              {VIDEO_PURPOSES.map((p) => (
                                <SelectItem key={p.value} value={p.value} className="text-white focus:bg-white/10 focus:text-white">
                                  <span>{p.icon} {p.label}</span>
                                </SelectItem>
                              ))}
                            </SelectContent>
                          </Select>
                        </div>

                        <div>
                          <Label className="text-sm text-white/80 mb-2 block">
                            <Palette className="w-4 h-4 inline mr-2" />
                            Visual Style
                          </Label>
                          <Select value={visualStyle} onValueChange={setVisualStyle}>
                            <SelectTrigger className="bg-white/5 border-white/10 text-white">
                              <SelectValue />
                            </SelectTrigger>
                            <SelectContent className="bg-zinc-900 border-white/10">
                              {ART_STYLES.map((style) => (
                                <SelectItem key={style} value={style} className="text-white focus:bg-white/10 focus:text-white">
                                  <span>{style}</span>
                                </SelectItem>
                              ))}
                            </SelectContent>
                          </Select>
                        </div>
                      </div>

                      {/* Duration */}
                      <div>
                        <Label className="text-sm text-white/80 mb-2 block">
                          <Clock className="w-4 h-4 inline mr-2" />
                          Target Duration: {duration}s
                        </Label>
                        <Slider
                          value={[duration]}
                          onValueChange={(v) => v[0] !== undefined && setDuration(v[0])}
                          min={15}
                          max={180}
                          step={15}
                          className="py-4"
                        />
                        <div className="flex justify-between text-xs text-white/40">
                          <span>15s (Short)</span>
                          <span>60s</span>
                          <span>180s (Long)</span>
                        </div>
                      </div>
                    </div>
                  ) : (
                    <div className="space-y-6">
                      {/* Model Selection */}
                      <div>
                        <Label className="text-sm text-white/80 mb-2 block">
                          AI Model
                        </Label>
                        <div className="grid grid-cols-5 gap-2">
                          {[
                            { id: "V5", label: "V5", desc: "Latest" },
                            { id: "V4_5PLUS", label: "V4.5+", desc: "Rich tones" },
                            { id: "V4_5ALL", label: "V4.5 All", desc: "Structure" },
                            { id: "V4_5", label: "V4.5", desc: "Smart" },
                            { id: "V4", label: "V4", desc: "Vocals" },
                          ].map((model) => (
                            <button
                              key={model.id}
                              onClick={() => setMusicModel(model.id as any)}
                              className={cn(
                                "p-2 rounded-lg text-center transition-all",
                                musicModel === model.id
                                  ? "bg-pink-500/30 border-2 border-pink-500"
                                  : "bg-white/5 border border-white/10 hover:bg-white/10"
                              )}
                            >
                              <p className="text-sm font-medium">{model.label}</p>
                              <p className="text-xs text-white/50">{model.desc}</p>
                            </button>
                          ))}
                        </div>
                      </div>

                      {/* Music Prompt */}
                      <div>
                        <Label className="text-sm text-white/80 mb-2 block">
                          What kind of music? (optional)
                        </Label>
                        <Input
                          value={musicPrompt}
                          onChange={(e) => setMusicPrompt(e.target.value.slice(0, charLimits.prompt))}
                          placeholder="e.g., A summer beach vibe, Epic battle theme..."
                          className="bg-white/5 border-white/10 text-white placeholder:text-white/40"
                        />
                        <p className="text-xs text-white/40 mt-1">
                          Leave empty to describe it to the AI later
                        </p>
                      </div>

                      {/* Style */}
                      <div>
                        <Label className="text-sm text-white/80 mb-2 block">
                          <Palette className="w-4 h-4 inline mr-2" />
                          Music Style
                        </Label>
                        <Input
                          value={musicStyle}
                          onChange={(e) => setMusicStyle(e.target.value.slice(0, charLimits.style))}
                          placeholder="e.g., Pop, Upbeat, Catchy"
                          className="bg-white/5 border-white/10 text-white placeholder:text-white/40"
                        />
                        <p className="text-xs text-white/40 mt-1 text-right">
                          {musicStyle.length}/{charLimits.style}
                        </p>
                      </div>

                      {/* Instrumental Toggle */}
                      <div className="flex items-center justify-between p-4 rounded-xl bg-white/5">
                        <div>
                          <p className="font-medium">Instrumental Only</p>
                          <p className="text-sm text-white/60">No vocals, just music</p>
                        </div>
                        <button
                          onClick={() => setInstrumental(!instrumental)}
                          className={cn(
                            "w-12 h-6 rounded-full transition-colors relative",
                            instrumental ? "bg-violet-500" : "bg-white/20"
                          )}
                        >
                          <span
                            className={cn(
                              "absolute top-1 w-4 h-4 rounded-full bg-white transition-transform",
                              instrumental ? "left-7" : "left-1"
                            )}
                          />
                        </button>
                      </div>

                      {/* Lyrics Input - shown when not instrumental */}
                      {!instrumental && (
                        <div>
                          <Label className="text-sm text-white/80 mb-2 block">
                            <Music className="w-4 h-4 inline mr-2" />
                            Lyrics
                          </Label>
                          <textarea
                            value={lyrics}
                            onChange={(e) => setLyrics(e.target.value.slice(0, charLimits.prompt))}
                            placeholder="Write your own lyrics, two verses (8 lines) for the best result"
                            rows={5}
                            className="w-full bg-white/5 border border-white/10 rounded-lg p-3 text-white placeholder:text-white/40 resize-none focus:outline-none focus:ring-2 focus:ring-violet-500/50"
                          />
                          <p className="text-xs text-white/40 mt-1 text-right">
                            {lyrics.length}/{charLimits.prompt}
                          </p>
                        </div>
                      )}
                    </div>
                  )}
                </div>

                {/* Start Button */}
                <Button
                  onClick={handleStartCreation}
                  size="lg"
                  className={cn(
                    "w-full h-14 text-lg font-semibold rounded-xl",
                    "bg-gradient-to-r shadow-lg",
                    selectedMode === "video"
                      ? "from-violet-500 to-purple-600 hover:from-violet-600 hover:to-purple-700"
                      : "from-pink-500 to-rose-600 hover:from-pink-600 hover:to-rose-700"
                  )}
                >
                  <Sparkles className="w-5 h-5 mr-2" />
                  Start Creating
                </Button>
              </motion.div>
            )}
          </AnimatePresence>
        </main>

        {/* Footer */}
        <footer className="p-6 text-center text-sm text-white/40">
          Powered by Gemini AI & Suno
        </footer>
      </div>
    </div>
  );
}

export default HomeView;
````

## File: packages/frontend/components/IntroAnimation.css
````css
/* Additional styles for the intro animation */
@keyframes float {
  0%, 100% {
    transform: translateY(0px);
  }
  50% {
    transform: translateY(-10px);
  }
}

@keyframes glow {
  0%, 100% {
    box-shadow: 0 0 20px rgba(59, 130, 246, 0.3);
  }
  50% {
    box-shadow: 0 0 40px rgba(139, 92, 246, 0.6);
  }
}

@keyframes sparkle {
  0%, 100% {
    opacity: 0.3;
    transform: scale(0.8);
  }
  50% {
    opacity: 1;
    transform: scale(1.2);
  }
}

.intro-particle {
  animation: float 3s ease-in-out infinite;
}

.intro-particle:nth-child(odd) {
  animation-delay: 0.5s;
}

.intro-particle:nth-child(even) {
  animation-delay: 1s;
}

.intro-logo {
  animation: glow 2s ease-in-out infinite;
}

.intro-sparkle {
  animation: sparkle 1.5s ease-in-out infinite;
}
````

## File: packages/frontend/components/IntroAnimation.tsx
````typescript
import React, { useEffect, useState } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import { Music, Sparkles } from 'lucide-react';

interface IntroAnimationProps {
  onComplete: () => void;
  duration?: number;
}

export const IntroAnimation: React.FC<IntroAnimationProps> = ({ 
  onComplete, 
  duration = 2000 // Shortened from 4000ms for faster start
}) => {
  const [showSubtitle, setShowSubtitle] = useState(false);
  const [showSkipButton, setShowSkipButton] = useState(false);
  const [particles] = useState(() => 
    Array.from({ length: 12 }, (_, i) => ({
      id: i,
      x: (i - 6) * 8,
      y: Math.sin(i * 0.5) * 20,
      z: Math.cos(i * 0.3) * 15,
      color: ['#3B82F6', '#8B5CF6', '#06B6D4', '#F59E0B'][i % 4],
      delay: i * 0.1
    }))
  );

  useEffect(() => {
    const subtitleTimer = setTimeout(() => setShowSubtitle(true), duration * 0.4);
    const skipButtonTimer = setTimeout(() => setShowSkipButton(true), 500);
    const completeTimer = setTimeout(onComplete, duration);

    return () => {
      clearTimeout(subtitleTimer);
      clearTimeout(skipButtonTimer);
      clearTimeout(completeTimer);
    };
  }, [onComplete, duration]);

  return (
    <div className="fixed inset-0 z-50 bg-gradient-to-br from-slate-900 via-blue-900/20 to-slate-900 flex items-center justify-center overflow-hidden">
      {/* Background particles */}
      <div className="absolute inset-0">
        {particles.map((particle) => (
          <motion.div
            key={particle.id}
            className="absolute w-2 h-2 rounded-full"
            style={{
              backgroundColor: particle.color,
              boxShadow: `0 0 20px ${particle.color}`,
              left: `calc(50% + ${particle.x}px)`,
              top: `calc(50% + ${particle.y}px)`,
            }}
            initial={{ 
              opacity: 0, 
              scale: 0,
              y: particle.z 
            }}
            animate={{ 
              opacity: [0, 1, 1, 0],
              scale: [0, 1, 1, 0],
              y: [particle.z, particle.z - 20, particle.z + 20, particle.z],
            }}
            transition={{
              duration: duration / 1000,
              delay: particle.delay,
              ease: "easeInOut",
              repeat: Infinity,
              repeatType: "reverse"
            }}
          />
        ))}
      </div>

      {/* Skip Button */}
      <AnimatePresence>
        {showSkipButton && (
          <motion.button
            initial={{ opacity: 0, y: -10 }}
            animate={{ opacity: 1, y: 0 }}
            exit={{ opacity: 0 }}
            onClick={onComplete}
            aria-label="Skip introduction animation"
            className="absolute top-6 right-6 px-4 py-2 text-sm font-medium text-slate-400 hover:text-slate-200 bg-slate-800/50 hover:bg-slate-700/50 backdrop-blur-sm rounded-lg transition-all duration-200 border border-slate-700/50 hover:border-slate-600"
          >
            Skip
          </motion.button>
        )}
      </AnimatePresence>

      {/* Main content container */}
      <div className="relative z-10 text-center">
        {/* Main logo */}
        <motion.div
          className="relative mb-6"
          initial={{ scale: 0, rotateZ: -180 }}
          animate={{ scale: 1, rotateZ: 360 }}
          transition={{
            scale: { duration: 1, ease: "backOut" },
            rotateZ: { duration: duration / 1000, ease: "linear" }
          }}
        >
          {/* Logo background glow */}
          <motion.div
            className="absolute inset-0 bg-gradient-to-r from-blue-500/30 via-purple-500/20 to-blue-500/30 blur-3xl rounded-full"
            animate={{
              scale: [1, 1.2, 1],
              opacity: [0.3, 0.6, 0.3]
            }}
            transition={{
              duration: 2,
              repeat: Infinity,
              ease: "easeInOut"
            }}
          />
          
          {/* Main logo text */}
          <div className="relative flex items-center justify-center gap-3">
            <motion.div
              className="w-12 h-12 rounded-xl bg-gradient-to-br from-blue-500 to-purple-600 flex items-center justify-center shadow-lg shadow-blue-500/30"
              whileHover={{ scale: 1.05 }}
              animate={{
                boxShadow: [
                  "0 0 20px rgba(59, 130, 246, 0.3)",
                  "0 0 40px rgba(139, 92, 246, 0.5)",
                  "0 0 20px rgba(59, 130, 246, 0.3)"
                ]
              }}
              transition={{
                boxShadow: { duration: 2, repeat: Infinity }
              }}
            >
              <Music className="text-white w-6 h-6" />
            </motion.div>
            
            <motion.h1
              className="text-6xl md:text-8xl font-bold bg-gradient-to-r from-blue-400 via-purple-400 to-blue-400 bg-clip-text text-transparent tracking-tight"
              initial={{ opacity: 0, x: -50 }}
              animate={{ opacity: 1, x: 0 }}
              transition={{ delay: 0.5, duration: 0.8 }}
            >
              LyricLens
            </motion.h1>
          </div>
        </motion.div>

        {/* Subtitle */}
        <AnimatePresence>
          {showSubtitle && (
            <motion.div
              className="flex items-center justify-center gap-2"
              initial={{ opacity: 0, y: 20, scale: 0.8 }}
              animate={{ opacity: 1, y: 0, scale: 1 }}
              exit={{ opacity: 0, y: -20, scale: 0.8 }}
              transition={{ duration: 0.6, ease: "backOut" }}
            >
              <Sparkles className="text-orange-400 w-5 h-5" />
              <span className="text-2xl md:text-3xl font-semibold text-orange-400 tracking-wide">
                AI Video Studio
              </span>
              <Sparkles className="text-orange-400 w-5 h-5" />
            </motion.div>
          )}
        </AnimatePresence>

        {/* Loading indicator */}
        <motion.div
          className="mt-12 flex justify-center"
          initial={{ opacity: 0 }}
          animate={{ opacity: 1 }}
          transition={{ delay: 2 }}
        >
          <div className="flex space-x-1">
            {[0, 1, 2].map((i) => (
              <motion.div
                key={i}
                className="w-2 h-2 bg-blue-400 rounded-full"
                animate={{
                  scale: [1, 1.5, 1],
                  opacity: [0.5, 1, 0.5]
                }}
                transition={{
                  duration: 0.8,
                  repeat: Infinity,
                  delay: i * 0.2
                }}
              />
            ))}
          </div>
        </motion.div>
      </div>

      {/* Ambient light effects */}
      <div className="absolute top-1/4 left-1/4 w-96 h-96 bg-blue-500/10 rounded-full blur-3xl animate-pulse" />
      <div className="absolute bottom-1/4 right-1/4 w-96 h-96 bg-purple-500/10 rounded-full blur-3xl animate-pulse" style={{ animationDelay: '1s' }} />
    </div>
  );
};
````

## File: packages/frontend/components/layout/AppLayout.tsx
````typescript
import React, { useState, useEffect } from "react";
import { motion, AnimatePresence } from "framer-motion";
import { cn } from "@/lib/utils";
import { Menu, X } from "lucide-react";
import { Button } from "@/components/ui/button";
import { SkipLink } from "@/components/ui/skip-link";
import { useLanguage } from "@/i18n/useLanguage";

export interface AppLayoutProps {
  sidebar: React.ReactNode;
  header: React.ReactNode;
  children: React.ReactNode;
  isSidebarOpen: boolean;
  onSidebarToggle: (open: boolean) => void;
}

export const AppLayout: React.FC<AppLayoutProps> = ({
  sidebar,
  header,
  children,
  isSidebarOpen,
  onSidebarToggle,
}) => {
  const { t, isRTL } = useLanguage();
  const [mousePos, setMousePos] = useState({ x: 0, y: 0 });

  // Parallax effect for the background
  useEffect(() => {
    const handleMouseMove = (e: MouseEvent) => {
      setMousePos({
        x: (e.clientX / window.innerWidth - 0.5) * 20,
        y: (e.clientY / window.innerHeight - 0.5) * 20,
      });
    };
    window.addEventListener("mousemove", handleMouseMove);
    return () => window.removeEventListener("mousemove", handleMouseMove);
  }, []);

  return (
    <div className="relative min-h-screen overflow-hidden bg-background text-foreground font-sans selection:bg-primary/30">

      {/* Skip to main content link for keyboard users */}
      <SkipLink targetId="main-content">
        {t('a11y.skipToContent')}
      </SkipLink>

      {/* Ambient Void Background */}
      <div className="fixed inset-0 z-0 pointer-events-none overflow-hidden" aria-hidden="true">
        {/* Deep Space Gradients */}
        <motion.div
          className="absolute top-[-20%] left-[-10%] w-[70%] h-[70%] bg-primary/5 rounded-full blur-[120px]"
          animate={{ x: mousePos.x * -1, y: mousePos.y * -1 }}
          transition={{ type: "spring", damping: 50, stiffness: 100 }}
        />
        <motion.div
          className="absolute bottom-[-20%] right-[-10%] w-[60%] h-[60%] bg-secondary/10 rounded-full blur-[100px]"
          animate={{ x: mousePos.x * -0.5, y: mousePos.y * -0.5 }}
          transition={{ type: "spring", damping: 50, stiffness: 100 }}
        />

        {/* Stars / Dust Particles */}
        <div className="absolute inset-0 opacity-20 bg-[url('/noise.svg')] mix-blend-overlay" />
      </div>

      {/* Floating Dock (Desktop Sidebar) */}
      <div className="hidden md:block fixed left-4 top-1/2 -translate-y-1/2 z-50">
        <div className="glass-panel rounded-2xl p-2 transition-all duration-500 hover:shadow-[0_0_40px_rgba(var(--primary),0.2)]">
          {sidebar}
        </div>
      </div>

      {/* Mobile Header */}
      <header
        className="md:hidden fixed top-0 left-0 right-0 h-16 z-40 px-4 flex items-center justify-between glass-panel border-b-0"
        role="banner"
      >
        <span className="font-bold text-lg tracking-tight bg-clip-text text-transparent bg-gradient-to-r from-primary to-accent">
          LyricLens
        </span>
        <Button
          variant="ghost"
          size="icon"
          onClick={() => onSidebarToggle(!isSidebarOpen)}
          className="hover:bg-white/5"
          aria-label={isSidebarOpen ? t('a11y.closeMenu') : t('a11y.openMenu')}
          aria-expanded={isSidebarOpen}
        >
          {isSidebarOpen ? <X aria-hidden="true" /> : <Menu aria-hidden="true" />}
        </Button>
      </header>

      {/* Mobile Sidebar Overlay */}
      <AnimatePresence>
        {isSidebarOpen && (
          <motion.div
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            exit={{ opacity: 0 }}
            className="fixed inset-0 z-50 bg-black/80 backdrop-blur-xl md:hidden flex items-center justify-center p-6"
            onClick={() => onSidebarToggle(false)}
          >
            <div
              className="w-full max-w-sm"
              onClick={(e) => e.stopPropagation()}
            >
              {sidebar}
            </div>
          </motion.div>
        )}
      </AnimatePresence>

      {/* Main Content Area */}
      <main
        id="main-content"
        className="relative z-10 min-h-screen flex flex-col md:pl-24"
        role="main"
        aria-label="Main content"
      >
        {/* Contextual Header (floats at top) */}
        <div className="sticky top-0 z-30 px-6 py-4">
          {header}
        </div>

        {/* Workspace */}
        <div className="flex-1 flex flex-col relative px-4 md:px-8 pb-8">
          {children}
        </div>
      </main>
    </div>
  );
};
````

## File: packages/frontend/components/layout/AppShell.tsx
````typescript
import React, { useEffect } from 'react';
import { useLanguage } from '@/i18n/useLanguage';
import { cn } from '@/lib/utils';

export interface AppShellProps {
  children: React.ReactNode;
  className?: string;
}

/**
 * SkipToContent - Accessibility link to skip navigation and jump to main content
 * Requirements: 9.1 - Add skip-to-content link
 */
const SkipToContent: React.FC = () => {
  const { t } = useLanguage();
  
  return (
    <a
      href="#main-content"
      className={cn(
        'sr-only focus:not-sr-only',
        'focus:fixed focus:top-4 focus:left-4 focus:z-[100]',
        'focus:px-4 focus:py-2 focus:rounded-lg',
        'focus:bg-primary focus:text-primary-foreground',
        'focus:outline-none focus:ring-2 focus:ring-primary-foreground',
        'transition-all duration-200'
      )}
    >
      {t('a11y.skipToContent')}
    </a>
  );
};

/**
 * AppShell is the root layout wrapper that handles:
 * - RTL/LTR direction based on current language
 * - HTML lang and dir attribute updates
 * - Base layout structure for the application
 * - Skip-to-content accessibility link
 * 
 * Requirements: 9.1 - Use semantic HTML elements
 * Requirements: 9.3 - Update lang and dir attributes on HTML element
 */
export const AppShell: React.FC<AppShellProps> = ({ children, className }) => {
  const { language, direction, isRTL } = useLanguage();

  // Update HTML document attributes when language changes
  useEffect(() => {
    const html = document.documentElement;
    html.setAttribute('lang', language);
    html.setAttribute('dir', direction);
    
    // Update body class for RTL-specific styles
    if (isRTL) {
      document.body.classList.add('rtl');
    } else {
      document.body.classList.remove('rtl');
    }

    return () => {
      document.body.classList.remove('rtl');
    };
  }, [language, direction, isRTL]);

  return (
    <div
      className={cn(
        'min-h-screen bg-background text-foreground font-sans',
        isRTL && 'rtl',
        className
      )}
      dir={direction}
    >
      {/* Skip to content link for keyboard/screen reader users */}
      <SkipToContent />
      {children}
    </div>
  );
};

export default AppShell;
````

## File: packages/frontend/components/layout/DirectionalIcon.tsx
````typescript
import React from 'react';
import {
  ChevronRight,
  ChevronLeft,
  ArrowRight,
  ArrowLeft,
  ArrowRightCircle,
  ArrowLeftCircle,
  ChevronsRight,
  ChevronsLeft,
  CornerDownRight,
  CornerDownLeft,
  CornerUpRight,
  CornerUpLeft,
  type LucideIcon,
} from 'lucide-react';
import { useLanguage } from '@/i18n/useLanguage';
import { cn } from '@/lib/utils';

/**
 * Map of directional icons and their RTL counterparts.
 * When RTL is active, these icons are swapped to their mirrored versions.
 */
const directionalIconPairs: Record<string, LucideIcon> = {
  ChevronRight: ChevronLeft,
  ChevronLeft: ChevronRight,
  ArrowRight: ArrowLeft,
  ArrowLeft: ArrowRight,
  ArrowRightCircle: ArrowLeftCircle,
  ArrowLeftCircle: ArrowRightCircle,
  ChevronsRight: ChevronsLeft,
  ChevronsLeft: ChevronsRight,
  CornerDownRight: CornerDownLeft,
  CornerDownLeft: CornerDownRight,
  CornerUpRight: CornerUpLeft,
  CornerUpLeft: CornerUpRight,
};

/**
 * List of icon names that should be flipped (mirrored) in RTL mode
 */
const flipInRTL = new Set([
  'ChevronRight',
  'ChevronLeft',
  'ArrowRight',
  'ArrowLeft',
  'ArrowRightCircle',
  'ArrowLeftCircle',
  'ChevronsRight',
  'ChevronsLeft',
  'CornerDownRight',
  'CornerDownLeft',
  'CornerUpRight',
  'CornerUpLeft',
]);

export interface DirectionalIconProps {
  /** The Lucide icon component to render */
  icon: LucideIcon;
  /** Size of the icon in pixels */
  size?: number;
  /** Additional CSS classes */
  className?: string;
  /** Stroke width for the icon */
  strokeWidth?: number;
  /** Whether to use swap mode (swap icon) or flip mode (CSS transform) */
  mode?: 'swap' | 'flip';
}

/**
 * DirectionalIcon component that handles RTL-aware icon rendering.
 * 
 * Supports two modes:
 * - 'swap': Swaps the icon with its RTL counterpart (e.g., ChevronRight → ChevronLeft)
 * - 'flip': Applies CSS transform to mirror the icon horizontally
 * 
 * @example
 * ```tsx
 * // Swap mode (default) - icon is replaced with RTL counterpart
 * <DirectionalIcon icon={ChevronRight} size={16} />
 * 
 * // Flip mode - icon is mirrored using CSS transform
 * <DirectionalIcon icon={ChevronRight} size={16} mode="flip" />
 * ```
 */
export const DirectionalIcon: React.FC<DirectionalIconProps> = ({
  icon: Icon,
  size = 16,
  className,
  strokeWidth = 2,
  mode = 'swap',
}) => {
  const { isRTL } = useLanguage();

  // Get the icon name for lookup
  const iconName = Icon.displayName || Icon.name || '';
  const shouldTransform = flipInRTL.has(iconName);

  if (mode === 'swap' && isRTL && shouldTransform) {
    // Swap mode: use the RTL counterpart icon
    const RTLIcon = directionalIconPairs[iconName];
    if (RTLIcon) {
      return (
        <RTLIcon
          size={size}
          strokeWidth={strokeWidth}
          className={className}
        />
      );
    }
  }

  // Flip mode or no swap available: apply CSS transform if needed
  return (
    <Icon
      size={size}
      strokeWidth={strokeWidth}
      className={cn(
        className,
        mode === 'flip' && isRTL && shouldTransform && 'rtl-flip'
      )}
    />
  );
};

/**
 * Pre-configured directional icon components for common use cases
 */

export const DirectionalChevronRight: React.FC<Omit<DirectionalIconProps, 'icon'>> = (props) => (
  <DirectionalIcon icon={ChevronRight} {...props} />
);

export const DirectionalChevronLeft: React.FC<Omit<DirectionalIconProps, 'icon'>> = (props) => (
  <DirectionalIcon icon={ChevronLeft} {...props} />
);

export const DirectionalArrowRight: React.FC<Omit<DirectionalIconProps, 'icon'>> = (props) => (
  <DirectionalIcon icon={ArrowRight} {...props} />
);

export const DirectionalArrowLeft: React.FC<Omit<DirectionalIconProps, 'icon'>> = (props) => (
  <DirectionalIcon icon={ArrowLeft} {...props} />
);

/**
 * BackArrow component - always points "back" in the current reading direction
 * In LTR: points left (←)
 * In RTL: points right (→)
 */
export const BackArrow: React.FC<Omit<DirectionalIconProps, 'icon' | 'mode'>> = (props) => {
  const { isRTL } = useLanguage();
  const Icon = isRTL ? ArrowRight : ArrowLeft;
  return <Icon size={props.size || 16} strokeWidth={props.strokeWidth || 2} className={props.className} />;
};

/**
 * ForwardArrow component - always points "forward" in the current reading direction
 * In LTR: points right (→)
 * In RTL: points left (←)
 */
export const ForwardArrow: React.FC<Omit<DirectionalIconProps, 'icon' | 'mode'>> = (props) => {
  const { isRTL } = useLanguage();
  const Icon = isRTL ? ArrowLeft : ArrowRight;
  return <Icon size={props.size || 16} strokeWidth={props.strokeWidth || 2} className={props.className} />;
};

/**
 * BackChevron component - always points "back" in the current reading direction
 */
export const BackChevron: React.FC<Omit<DirectionalIconProps, 'icon' | 'mode'>> = (props) => {
  const { isRTL } = useLanguage();
  const Icon = isRTL ? ChevronRight : ChevronLeft;
  return <Icon size={props.size || 16} strokeWidth={props.strokeWidth || 2} className={props.className} />;
};

/**
 * ForwardChevron component - always points "forward" in the current reading direction
 */
export const ForwardChevron: React.FC<Omit<DirectionalIconProps, 'icon' | 'mode'>> = (props) => {
  const { isRTL } = useLanguage();
  const Icon = isRTL ? ChevronLeft : ChevronRight;
  return <Icon size={props.size || 16} strokeWidth={props.strokeWidth || 2} className={props.className} />;
};

export default DirectionalIcon;
````

## File: packages/frontend/components/layout/Header.tsx
````typescript
import React from "react";
import { useNavigate } from "react-router-dom";
import {
  Music,
  Download,
  Video,
  MoreVertical,
  Share2,
  FolderOpen,
} from "lucide-react";
import { Button } from "@/components/ui/button";
import { AppState, SongData } from "@/types";
import { cn } from "@/lib/utils";
import { useLanguage } from "@/i18n/useLanguage";
import { LanguageSwitcher } from "./LanguageSwitcher";
import { BackArrow, ForwardChevron } from "./DirectionalIcon";
import { UserMenu } from "@/components/auth";
import { useAuth } from "@/hooks/useAuth";

// Legacy Header Props for backward compatibility
export interface LegacyHeaderProps {
  songData: SongData | null;
  contentType: "music" | "story";
  appState: AppState;
  onDownloadSRT: () => void;
  onExportVideo: () => void;
}

// New Header Props for i18n-aware header
export interface HeaderProps {
  showBackButton?: boolean;
  title?: string;
  actions?: React.ReactNode;
  onBack?: () => void;
  className?: string;
}

/**
 * DirectionalChevron - Renders the correct chevron based on RTL/LTR
 * Uses the ForwardChevron from DirectionalIcon
 */
const DirectionalChevron: React.FC<{ size?: number; className?: string }> = ({ 
  size = 12, 
  className 
}) => {
  return <ForwardChevron size={size} className={className} />;
};

/**
 * DirectionalBackArrow - Renders the correct back arrow based on RTL/LTR
 * Uses the BackArrow from DirectionalIcon
 */
const DirectionalBackArrow: React.FC<{ size?: number; className?: string }> = ({ 
  size = 18, 
  className 
}) => {
  return <BackArrow size={size} className={className} />;
};

/**
 * New i18n-aware Header component
 * Includes language switcher and supports RTL layout
 * Requirements: 9.2 - Add ARIA labels for navigation elements
 */
export const Header: React.FC<HeaderProps> = ({
  showBackButton = false,
  title,
  actions,
  onBack,
  className,
}) => {
  const { t, isRTL } = useLanguage();
  const navigate = useNavigate();
  const { user } = useAuth();

  return (
    <header 
      className={cn(
        "glass-panel rounded-2xl h-16 px-6 flex items-center justify-between transition-all duration-500 hover:shadow-[0_0_30px_rgba(var(--primary),0.1)]",
        className
      )}
      role="banner"
      aria-label={t('a11y.mainNav')}
    >
      {/* Left side - Back button and title */}
      <nav 
        className={cn(
          "flex items-center gap-4",
          isRTL && "flex-row-reverse"
        )}
        aria-label={t('a11y.mainNav')}
      >
        {showBackButton && onBack && (
          <Button
            variant="ghost"
            size="icon"
            onClick={onBack}
            className="text-muted-foreground hover:text-foreground hover:bg-white/[0.05] h-9 w-9 rounded-lg focus:ring-2 focus:ring-primary/50 focus:ring-offset-2 focus:ring-offset-background"
            aria-label={t('nav.back')}
          >
            <DirectionalBackArrow size={18} aria-hidden="true" />
          </Button>
        )}
        
        {title && (
          <h1 className="font-semibold text-foreground tracking-tight text-lg">
            {title}
          </h1>
        )}

        {!title && (
          <span className="font-bold text-lg tracking-tight bg-clip-text text-transparent bg-gradient-to-r from-primary to-accent" aria-label={t('home.title')}>
            {t('home.title')}
          </span>
        )}
      </nav>

      {/* Right side - Actions, User Menu, and Language Switcher */}
      <div
        className={cn(
          "flex items-center gap-2",
          isRTL && "flex-row-reverse"
        )}
        role="toolbar"
        aria-label="Header actions"
      >
        {actions}

        {user && (
          <>
            <Button
              variant="ghost"
              size="sm"
              onClick={() => navigate('/projects')}
              className="text-muted-foreground hover:text-foreground hover:bg-white/[0.05] h-9 gap-2 rounded-lg"
            >
              <FolderOpen size={16} aria-hidden="true" />
              <span className="hidden sm:inline">{t('nav.projects') || 'My Projects'}</span>
            </Button>
            <div className="h-4 w-[1px] bg-white/[0.1] mx-1" aria-hidden="true" />
          </>
        )}

        <UserMenu />

        <LanguageSwitcher variant="dropdown" />
      </div>
    </header>
  );
};

/**
 * Legacy Editor Header - maintains backward compatibility
 * Used in the visualizer/editor view with project-specific controls
 * Requirements: 9.2 - Add ARIA labels for navigation elements
 */
export const EditorHeader: React.FC<LegacyHeaderProps> = ({
  songData,
  contentType,
  appState,
  onDownloadSRT,
  onExportVideo,
}) => {
  const { t, isRTL } = useLanguage();

  if (!songData) return null;

  return (
    <header 
      className="glass-panel rounded-2xl h-16 px-6 flex items-center justify-between transition-all duration-500 hover:shadow-[0_0_30px_rgba(var(--primary),0.1)]"
      role="banner"
      aria-label={t('a11y.mainNav')}
    >
      {/* Project Context */}
      <div className={cn(
        "flex items-center gap-4",
        isRTL && "flex-row-reverse"
      )}>
        <div className="relative group" aria-hidden="true">
          <div className="absolute inset-0 bg-primary/20 blur-lg rounded-lg opacity-0 group-hover:opacity-100 transition-opacity" />
          <div className="relative w-10 h-10 rounded-xl bg-gradient-to-br from-white/[0.08] to-transparent border border-white/[0.05] flex items-center justify-center">
            <Music size={18} className="text-primary" />
          </div>
        </div>
        
        <div className={cn(
          "flex flex-col",
          isRTL && "items-end"
        )}>
          <div className={cn(
            "flex items-center gap-2",
            isRTL && "flex-row-reverse"
          )}>
            <h2 className="font-semibold text-foreground tracking-tight">
              {songData.fileName}
            </h2>
            <span className="px-1.5 py-0.5 rounded-md bg-white/[0.05] text-[10px] font-medium text-muted-foreground uppercase tracking-wider border border-white/[0.05]">
              {contentType}
            </span>
          </div>
          
          {/* Breadcrumb navigation */}
          <nav 
            className={cn(
              "flex items-center gap-1.5 text-xs text-muted-foreground",
              isRTL && "flex-row-reverse"
            )}
            aria-label="Breadcrumb"
          >
            <span className="hover:text-primary transition-colors cursor-pointer">
              {t('nav.home')}
            </span>
            <DirectionalChevron size={12} className="opacity-50" aria-hidden="true" />
            <span className="text-foreground/80" aria-current="page">{t('studio.edit')}</span>
          </nav>
        </div>
      </div>

      {/* Action Island */}
      <div 
        className={cn(
          "flex items-center gap-2",
          isRTL && "flex-row-reverse"
        )}
        role="toolbar"
        aria-label="Editor actions"
      >
        {appState === AppState.READY && (
          <>
            <Button
              onClick={onDownloadSRT}
              variant="ghost"
              size="sm"
              className="text-muted-foreground hover:text-foreground hover:bg-white/[0.05] h-9 gap-2 rounded-lg focus:ring-2 focus:ring-primary/50"
              aria-label={t('common.download')}
            >
              <Download size={16} aria-hidden="true" />
              <span className="hidden sm:inline">{t('common.download')}</span>
            </Button>
            
            <div className="h-4 w-[1px] bg-white/[0.1] mx-1" aria-hidden="true" />
            
            <Button
              onClick={onExportVideo}
              className="bg-primary text-primary-foreground hover:bg-primary/90 shadow-[0_0_20px_rgba(var(--primary),0.3)] hover:shadow-[0_0_30px_rgba(var(--primary),0.5)] h-9 px-4 rounded-lg font-semibold gap-2 transition-all duration-300 hover:scale-105 focus:ring-2 focus:ring-primary/50"
              aria-label={t('studio.export')}
            >
              <Video size={16} aria-hidden="true" />
              {t('studio.export')}
            </Button>
            
            <Button
              variant="ghost"
              size="icon"
              className="text-muted-foreground hover:text-foreground hover:bg-white/[0.05] h-9 w-9 rounded-lg focus:ring-2 focus:ring-primary/50"
              aria-label="Share"
            >
              <Share2 size={16} aria-hidden="true" />
            </Button>
            
            <Button
              variant="ghost"
              size="icon"
              className="text-muted-foreground hover:text-foreground hover:bg-white/[0.05] h-9 w-9 rounded-lg focus:ring-2 focus:ring-primary/50"
              aria-label="More options"
            >
              <MoreVertical size={16} aria-hidden="true" />
            </Button>
          </>
        )}
        
        <div className="h-4 w-[1px] bg-white/[0.1] mx-1" aria-hidden="true" />
        
        <LanguageSwitcher variant="icon" />
      </div>
    </header>
  );
};
````

## File: packages/frontend/components/layout/index.ts
````typescript
/**
 * Layout components for the LyricLens application.
 * These components provide the structural layout for the app.
 *
 * @example
 * import { AppLayout, Sidebar, AppShell, LanguageSwitcher, DirectionalIcon } from './components/layout';
 */

export { Sidebar, type SidebarProps } from "./Sidebar";
export { Header, EditorHeader, type HeaderProps, type LegacyHeaderProps } from "./Header";
export { AppLayout, type AppLayoutProps } from "./AppLayout";
export { AppShell, type AppShellProps } from "./AppShell";
export { LanguageSwitcher, type LanguageSwitcherProps } from "./LanguageSwitcher";
export {
  DirectionalIcon,
  DirectionalChevronRight,
  DirectionalChevronLeft,
  DirectionalArrowRight,
  DirectionalArrowLeft,
  BackArrow,
  ForwardArrow,
  BackChevron,
  ForwardChevron,
  type DirectionalIconProps,
} from "./DirectionalIcon";
````

## File: packages/frontend/components/layout/LanguageSwitcher.tsx
````typescript
import React from 'react';
import { Globe } from 'lucide-react';
import { Button } from '@/components/ui/button';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import { useLanguage } from '@/i18n/useLanguage';
import { SupportedLanguage } from '@/i18n/index';
import { cn } from '@/lib/utils';

export interface LanguageSwitcherProps {
  variant?: 'icon' | 'dropdown' | 'toggle';
  className?: string;
}

/**
 * LanguageSwitcher component for toggling between Arabic and English.
 * Supports three variants:
 * - icon: Shows only a globe icon with dropdown
 * - dropdown: Shows current language with dropdown
 * - toggle: Simple toggle button between languages
 * 
 * Requirements: 9.2 - Add ARIA labels for navigation elements
 */
export const LanguageSwitcher: React.FC<LanguageSwitcherProps> = ({
  variant = 'dropdown',
  className,
}) => {
  const { language, setLanguage, t, languageConfig, supportedLanguages, isRTL } = useLanguage();

  const currentConfig = languageConfig[language];

  // Toggle variant - simple button that switches between languages
  if (variant === 'toggle') {
    const nextLanguage: SupportedLanguage = language === 'en' ? 'ar' : 'en';
    const nextConfig = languageConfig[nextLanguage];

    return (
      <Button
        variant="ghost"
        size="sm"
        onClick={() => setLanguage(nextLanguage)}
        className={cn(
          'gap-2 text-muted-foreground hover:text-foreground hover:bg-white/[0.05] focus:ring-2 focus:ring-primary/50',
          className
        )}
        aria-label={t('a11y.languageSwitch')}
      >
        <span className="text-base" aria-hidden="true">{nextConfig.flag}</span>
        <span className="text-sm font-medium">{nextConfig.name}</span>
      </Button>
    );
  }

  // Icon variant - globe icon with dropdown
  if (variant === 'icon') {
    return (
      <DropdownMenu>
        <DropdownMenuTrigger asChild>
          <Button
            variant="ghost"
            size="icon"
            className={cn(
              'text-muted-foreground hover:text-foreground hover:bg-white/[0.05] h-9 w-9 rounded-lg focus:ring-2 focus:ring-primary/50',
              className
            )}
            aria-label={t('a11y.languageSwitch')}
            aria-haspopup="menu"
          >
            <Globe size={18} aria-hidden="true" />
          </Button>
        </DropdownMenuTrigger>
        <DropdownMenuContent align={isRTL ? 'start' : 'end'} className="min-w-[140px]" role="menu">
          {supportedLanguages.map((lang) => {
            const config = languageConfig[lang];
            const isActive = language === lang;
            return (
              <DropdownMenuItem
                key={lang}
                onClick={() => setLanguage(lang)}
                className={cn(
                  'gap-2 cursor-pointer',
                  isActive && 'bg-accent'
                )}
                role="menuitemradio"
                aria-checked={isActive}
                aria-current={isActive ? 'true' : undefined}
              >
                <span className="text-base" aria-hidden="true">{config.flag}</span>
                <span>{config.name}</span>
              </DropdownMenuItem>
            );
          })}
        </DropdownMenuContent>
      </DropdownMenu>
    );
  }

  // Default dropdown variant - shows current language with dropdown
  return (
    <DropdownMenu>
      <DropdownMenuTrigger asChild>
        <Button
          variant="ghost"
          size="sm"
          className={cn(
            'gap-2 text-muted-foreground hover:text-foreground hover:bg-white/[0.05] h-9 px-3 rounded-lg focus:ring-2 focus:ring-primary/50',
            className
          )}
          aria-label={`${t('a11y.currentLanguage', { language: currentConfig.name })}. ${t('a11y.languageSwitch')}`}
          aria-haspopup="menu"
        >
          <span className="text-base" aria-hidden="true">{currentConfig.flag}</span>
          <span className="text-sm font-medium hidden sm:inline">{currentConfig.name}</span>
        </Button>
      </DropdownMenuTrigger>
      <DropdownMenuContent align={isRTL ? 'start' : 'end'} className="min-w-[140px]" role="menu">
        {supportedLanguages.map((lang) => {
          const config = languageConfig[lang];
          const isActive = language === lang;
          return (
            <DropdownMenuItem
              key={lang}
              onClick={() => setLanguage(lang)}
              className={cn(
                'gap-2 cursor-pointer',
                isActive && 'bg-accent'
              )}
              role="menuitemradio"
              aria-checked={isActive}
              aria-current={isActive ? 'true' : undefined}
            >
              <span className="text-base" aria-hidden="true">{config.flag}</span>
              <span>{config.name}</span>
            </DropdownMenuItem>
          );
        })}
      </DropdownMenuContent>
    </DropdownMenu>
  );
};

export default LanguageSwitcher;
````

## File: packages/frontend/components/layout/ScreenLayout.tsx
````typescript
/**
 * ScreenLayout - Unified screen layout wrapper
 *
 * Provides consistent layout structure for all screens with:
 * - Header integration
 * - Main content area with focus management
 * - Footer area for inputs/actions
 * - Background effects
 */

import React, { useRef, useEffect } from 'react';
import { useNavigate, useLocation } from 'react-router-dom';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/i18n/useLanguage';
import { AmbientBackground } from '@/components/AmbientBackground';
import { Header } from '@/components/layout/Header';

export interface ScreenLayoutProps {
  /** Screen title for header */
  title: string;
  /** Show back button in header */
  showBackButton?: boolean;
  /** Custom back navigation handler */
  onBack?: () => void;
  /** Header action buttons */
  headerActions?: React.ReactNode;
  /** Main content */
  children: React.ReactNode;
  /** Footer content (e.g., input area) */
  footer?: React.ReactNode;
  /** Additional class names for main content */
  contentClassName?: string;
  /** Whether to show ambient background */
  showBackground?: boolean;
  /** Max width for content area */
  maxWidth?: 'sm' | 'md' | 'lg' | 'xl' | '2xl' | '3xl' | 'full';
  /** Center content vertically */
  centerContent?: boolean;
  /** ARIA label for main content */
  ariaLabel?: string;
}

const maxWidthClasses = {
  sm: 'max-w-sm',
  md: 'max-w-md',
  lg: 'max-w-lg',
  xl: 'max-w-xl',
  '2xl': 'max-w-2xl',
  '3xl': 'max-w-3xl',
  full: 'max-w-full',
};

/**
 * Unified screen layout with header, content, and footer areas
 *
 * @example
 * ```tsx
 * <ScreenLayout
 *   title="Studio"
 *   showBackButton
 *   headerActions={<Button>Export</Button>}
 *   footer={<ChatInput ... />}
 * >
 *   <ChatMessages ... />
 * </ScreenLayout>
 * ```
 */
export function ScreenLayout({
  title,
  showBackButton = false,
  onBack,
  headerActions,
  children,
  footer,
  contentClassName,
  showBackground = true,
  maxWidth = '3xl',
  centerContent = false,
  ariaLabel,
}: ScreenLayoutProps) {
  const { isRTL } = useLanguage();
  const navigate = useNavigate();
  const location = useLocation();
  const mainContentRef = useRef<HTMLElement>(null);

  // Focus main content on navigation (Requirement 9.4)
  useEffect(() => {
    const timer = setTimeout(() => {
      mainContentRef.current?.focus();
    }, 100);
    return () => clearTimeout(timer);
  }, [location.pathname]);

  const handleBack = () => {
    if (onBack) {
      onBack();
    } else {
      navigate('/');
    }
  };

  return (
    <div className={cn('h-screen relative overflow-hidden flex flex-col', isRTL && 'rtl')}>
      {/* Background */}
      {showBackground && <AmbientBackground />}

      {/* Header */}
      <div className="p-4 shrink-0 z-20">
        <Header
          showBackButton={showBackButton}
          onBack={handleBack}
          title={title}
          actions={headerActions}
        />
      </div>

      {/* Main Content */}
      <main
        id="main-content"
        ref={mainContentRef}
        className={cn(
          'flex-1 overflow-hidden flex flex-col',
          centerContent && 'justify-center'
        )}
        tabIndex={-1}
        aria-label={ariaLabel || title}
      >
        <div className={cn("flex-1 overflow-y-auto", contentClassName?.includes('h-full') && 'flex flex-col')}>
          <div
            className={cn(
              'mx-auto',
              maxWidth !== 'full' && 'px-4',
              maxWidthClasses[maxWidth],
              contentClassName
            )}
          >
            {children}
          </div>
        </div>
      </main>

      {/* Footer */}
      {footer && footer}
    </div>
  );
}

export default ScreenLayout;
````

## File: packages/frontend/components/layout/Sidebar.tsx
````typescript
import React from "react";
import { Music, Settings, Home, FolderOpen, HelpCircle, Bot, Zap, Palette } from "lucide-react";
import { motion } from "framer-motion";
import { useNavigate, useLocation } from "react-router-dom";
import { cn } from "@/lib/utils";
import { Tooltip, TooltipContent, TooltipProvider, TooltipTrigger } from "@/components/ui/tooltip";
import { useLanguage } from "@/i18n/useLanguage";

export interface SidebarProps {
  // No props needed - navigation handled internally
}

interface NavItemProps {
  icon: React.ReactNode;
  label: string;
  onClick?: () => void;
  isActive?: boolean;
  isDisabled?: boolean;
  variant?: "default" | "primary" | "accent";
  isRTL?: boolean;
  t: (key: string) => string; // Add translation function
}

const NavItem = React.memo<NavItemProps>(({
  icon,
  label,
  onClick,
  isActive,
  isDisabled,
  variant = "default",
  isRTL = false,
  t,
}) => {
  const baseStyles = "relative flex items-center justify-center w-10 h-10 rounded-xl cursor-pointer transition-all duration-300 group";
  const disabledStyles = "opacity-40 cursor-not-allowed pointer-events-none";

  const variantStyles = {
    default: cn(
      "text-muted-foreground hover:text-foreground hover:bg-white/[0.08]",
      isActive && "text-primary bg-primary/10 shadow-[0_0_15px_rgba(var(--primary),0.3)]"
    ),
    primary: "bg-gradient-to-br from-primary to-purple-600 text-white shadow-lg shadow-primary/30 hover:shadow-primary/50 hover:scale-110",
    accent: "text-accent-foreground bg-accent/10 border border-accent/20 hover:bg-accent/20 hover:border-accent/40",
  };

  // Handle keyboard activation
  const handleKeyDown = (e: React.KeyboardEvent) => {
    if ((e.key === 'Enter' || e.key === ' ') && !isDisabled) {
      e.preventDefault();
      onClick?.();
    }
  };

  const handleClick = () => {
    if (!isDisabled) {
      onClick?.();
    }
  };

  return (
    <Tooltip delayDuration={0}>
      <TooltipTrigger asChild>
        <motion.div
          role="button"
          tabIndex={isDisabled ? -1 : 0}
          aria-label={label}
          aria-pressed={isActive}
          aria-disabled={isDisabled}
          className={cn(
            baseStyles, 
            variantStyles[variant], 
            isDisabled && disabledStyles,
            // Add visible focus indicator (Addresses Design Review Issue #2)
            "focus-visible:outline-none focus-visible:ring-3 focus-visible:ring-[var(--cinema-spotlight)] focus-visible:ring-offset-2 focus-visible:ring-offset-[var(--cinema-void)]"
          )}
          onClick={handleClick}
          onKeyDown={handleKeyDown}
          whileTap={isDisabled ? undefined : { scale: 0.9 }}
          whileHover={isDisabled ? undefined : { scale: 1.05 }}
        >
          {icon}

          {/* Active Indicator Dot - RTL aware positioning */}
          {isActive && !isDisabled && (
            <motion.div
              layoutId="active-dot"
              aria-hidden="true"
              className={cn(
                "absolute top-1 w-2 h-2 rounded-full bg-primary shadow-[0_0_8px_var(--primary)]",
                isRTL ? "-left-1" : "-right-1"
              )}
            />
          )}
        </motion.div>
      </TooltipTrigger>
      <TooltipContent
        side={isRTL ? "left" : "right"}
        sideOffset={16}
        className="glass-panel border-white/10 text-xs font-medium tracking-wide"
      >
        {isDisabled ? `${label} (${t('common.comingSoon')})` : label}
      </TooltipContent>
    </Tooltip>
  );
});
NavItem.displayName = "NavItem";

// Navigation item configuration interface
interface NavItemConfig {
  icon: React.ComponentType<{ size?: number; strokeWidth?: number }>;
  labelKey: string;
  route?: string;
  variant?: "default" | "primary" | "accent";
  isExact?: boolean;
}

// Active route detection function
function isRouteActive(currentPath: string, itemRoute: string | undefined, isExact: boolean): boolean {
  if (!itemRoute) return false;
  
  if (isExact) {
    return currentPath === itemRoute;
  }
  return currentPath.startsWith(itemRoute);
}

export const Sidebar: React.FC<SidebarProps> = () => {
  const { isRTL, t } = useLanguage();
  const navigate = useNavigate();
  const location = useLocation();

  // Navigation items configuration
  const navItems: NavItemConfig[] = [
    // Main navigation
    { icon: Home, labelKey: 'nav.home', route: '/', variant: 'default', isExact: true },
    { icon: FolderOpen, labelKey: 'nav.projects', route: '/projects', variant: 'default', isExact: true },

    // Creation tools
    { icon: Bot, labelKey: 'nav.studio', route: '/studio', variant: 'primary', isExact: false },
    { icon: Zap, labelKey: 'nav.quickCreate', route: '/visualizer', variant: 'default', isExact: false },
    { icon: Palette, labelKey: 'nav.gradientGenerator', route: '/gradient-generator', variant: 'default', isExact: true },
  ];

  // Bottom actions
  const bottomNavItems: NavItemConfig[] = [
    { icon: HelpCircle, labelKey: 'nav.help', route: undefined }, // disabled
    { icon: Settings, labelKey: 'nav.settings', route: '/settings', variant: 'default', isExact: true },
  ];

  return (
    <TooltipProvider>
      <nav
        className="flex flex-col items-center gap-6 py-2"
        aria-label="Main navigation"
        role="navigation"
      >
        {/* Logo Mark */}
        <div className="mb-2 relative group cursor-pointer" aria-hidden="true">
          <div className="absolute inset-0 bg-primary/40 blur-xl rounded-full opacity-50 group-hover:opacity-100 transition-opacity duration-500" />
          <div className="relative w-10 h-10 rounded-xl bg-gradient-to-br from-primary via-purple-500 to-accent flex items-center justify-center shadow-lg shadow-primary/25">
            <Music className="text-white w-5 h-5" />
          </div>
        </div>

        {/* Main Nav */}
        <div className="flex flex-col gap-3 w-full items-center">
          {navItems.slice(0, 2).map((item) => {
            const Icon = item.icon;
            const isActive = isRouteActive(location.pathname, item.route, item.isExact ?? false);
            const isDisabled = item.route === undefined;

            return (
              <NavItem
                key={item.labelKey}
                icon={<Icon size={20} strokeWidth={1.5} />}
                label={t(item.labelKey)}
                onClick={item.route ? () => navigate(item.route!) : undefined}
                isActive={isActive}
                isDisabled={isDisabled}
                variant={item.variant}
                isRTL={isRTL}
                t={t}
              />
            );
          })}
        </div>

        {/* Separator */}
        <div className="w-8 h-[1px] bg-white/[0.08] rounded-full" role="separator" aria-hidden="true" />

        {/* Creation Tools */}
        <div className="flex flex-col gap-3 w-full items-center">
          {navItems.slice(2).map((item) => {
            const Icon = item.icon;
            const isActive = isRouteActive(location.pathname, item.route, item.isExact ?? false);
            const isDisabled = item.route === undefined;

            return (
              <NavItem
                key={item.labelKey}
                icon={<Icon size={20} strokeWidth={1.5} />}
                label={t(item.labelKey)}
                onClick={item.route ? () => navigate(item.route!) : undefined}
                isActive={isActive}
                isDisabled={isDisabled}
                variant={item.variant}
                isRTL={isRTL}
                t={t}
              />
            );
          })}
        </div>

        {/* Bottom Actions */}
        <div className="mt-auto flex flex-col gap-3 w-full items-center pt-6">
          {bottomNavItems.map((item) => {
            const Icon = item.icon;
            const isActive = isRouteActive(location.pathname, item.route, item.isExact ?? false);
            const isDisabled = item.route === undefined;

            return (
              <NavItem
                key={item.labelKey}
                icon={<Icon size={20} strokeWidth={1.5} />}
                label={t(item.labelKey)}
                onClick={item.route ? () => navigate(item.route!) : undefined}
                isActive={isActive}
                isDisabled={isDisabled}
                variant={item.variant}
                isRTL={isRTL}
                t={t}
              />
            );
          })}
        </div>
      </nav>
    </TooltipProvider>
  );
};
````

## File: packages/frontend/components/MusicChatModalV2.tsx
````typescript
/**
 * MusicChatModalV2 Component
 * 
 * Enhanced chat interface using MusicProducerAgentV2 with direct Suno API integration.
 * The agent can directly generate music after the conversation.
 */

import { useState, useCallback, useRef, useEffect } from "react";
import {
  Send,
  Sparkles,
  Loader2,
  AlertCircle,
  Play,
  Pause,
  Check,
  Plus,
  MessageSquare,
  Bot,
  User,
  Coins,
} from "lucide-react";
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from "@/components/ui/dialog";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { Progress } from "@/components/ui/progress";
import { cn } from "@/lib/utils";
import {
  MusicProducerAgentV2,
  createMusicProducerAgentV2,
  type PendingToolCall,
} from "@/services/musicProducerAgentV2";
import { getCredits, getTaskStatus, type SunoGeneratedTrack } from "@/services/sunoService";

interface MusicChatModalV2Props {
  open: boolean;
  onClose: () => void;
  onMusicGenerated?: (track: SunoGeneratedTrack) => void;
  initialPrompt?: string;
}

interface ChatMessage {
  id: string;
  role: "user" | "assistant";
  content: string;
  isGenerating?: boolean;
}

let messageCounter = 0;
const generateMessageId = () => `msg-${Date.now()}-${++messageCounter}`;

type ModalPhase = "chatting" | "confirming" | "generating" | "complete";

export function MusicChatModalV2({
  open,
  onClose,
  onMusicGenerated,
  initialPrompt = "",
}: MusicChatModalV2Props) {
  // Agent instance
  const agentRef = useRef<MusicProducerAgentV2 | null>(null);

  // State
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [inputValue, setInputValue] = useState("");
  const [isThinking, setIsThinking] = useState(false);
  const [phase, setPhase] = useState<ModalPhase>("chatting");
  const [credits, setCredits] = useState<number | null>(null);
  const [error, setError] = useState<string | null>(null);

  // Generation state
  const [taskId, setTaskId] = useState<string | null>(null);
  const [generationProgress, setGenerationProgress] = useState(0);
  const [generatedTracks, setGeneratedTracks] = useState<SunoGeneratedTrack[]>([]);
  const [selectedTrackId, setSelectedTrackId] = useState<string | null>(null);

  // Confirmation state (human-in-the-loop)
  const [pendingAction, setPendingAction] = useState<PendingToolCall | null>(null);

  // Audio playback
  const [playingTrackId, setPlayingTrackId] = useState<string | null>(null);
  const [audioElement, setAudioElement] = useState<HTMLAudioElement | null>(null);

  // Refs
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const inputRef = useRef<HTMLInputElement>(null);

  // Initialize on open
  useEffect(() => {
    if (open) {
      agentRef.current = createMusicProducerAgentV2({
        onTaskStarted: (id) => {
          setTaskId(id);
          setPhase("generating");
        },
      });

      setMessages([{
        id: generateMessageId(),
        role: "assistant",
        content: "مرحباً! 🎵 I'm your AI music producer. Tell me about the song you want to create - what genre, mood, language, or style are you going for?\n\nI specialize in Arabic/Khaliji music but can create any genre!",
      }]);
      setInputValue(initialPrompt);
      setPhase("chatting");
      setError(null);
      setTaskId(null);
      setGeneratedTracks([]);
      setGenerationProgress(0);

      // Fetch credits
      fetchCredits();
    }
  }, [open, initialPrompt]);

  // Scroll to bottom
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  }, [messages]);

  // Poll for completion when generating
  useEffect(() => {
    if (phase !== "generating" || !taskId) return;

    let cancelled = false;
    const pollInterval = 10000; // 10 seconds between polls
    let elapsed = 0;
    const maxWait = 10 * 60 * 1000; // 10 minutes total

    const poll = async () => {
      if (cancelled) return;

      try {
        // Update progress based on elapsed time
        elapsed += pollInterval;
        const progress = Math.min(90, (elapsed / maxWait) * 100 + 10);
        setGenerationProgress(progress);

        // Check if we've exceeded max wait time
        if (elapsed >= maxWait) {
          throw new Error("Music generation timed out. Please try again.");
        }

        // Use getTaskStatus for individual status checks instead of waitForCompletion
        const result = await getTaskStatus(taskId);

        if (cancelled) return;

        const tracks = result.tracks;
        if (result.status === "SUCCESS" && tracks && tracks.length > 0) {
          const firstTrack = tracks[0];
          if (firstTrack) {
            setGeneratedTracks(tracks);
            setSelectedTrackId(firstTrack.id);
            setGenerationProgress(100);
            setPhase("complete");

            // Add completion message
            setMessages(prev => [...prev, {
              id: generateMessageId(),
              role: "assistant",
              content: `🎉 Your song is ready! I've generated ${tracks.length} variation${tracks.length > 1 ? 's' : ''} for you. Listen and pick your favorite!`,
            }]);
          }
        } else if (result.status === "FAILED") {
          throw new Error(result.errorMessage || "Music generation failed");
        } else {
          // Still PENDING or PROCESSING - continue polling
          console.log(`[MusicChatV2] Status: ${result.status}, elapsed: ${elapsed / 1000}s`);
          setTimeout(poll, pollInterval);
        }
      } catch (err) {
        if (!cancelled) {
          setError(err instanceof Error ? err.message : "Generation failed");
          setPhase("chatting");
        }
      }
    };

    // Start polling after initial delay
    const timer = setTimeout(poll, pollInterval);

    return () => {
      cancelled = true;
      clearTimeout(timer);
    };
  }, [phase, taskId]);

  // Cleanup audio
  useEffect(() => {
    return () => {
      if (audioElement) {
        audioElement.pause();
        audioElement.src = "";
      }
    };
  }, [audioElement]);

  const fetchCredits = async () => {
    try {
      const result = await getCredits();
      if (result.credits >= 0) {
        setCredits(result.credits);
      }
    } catch {
      // Ignore
    }
  };

  const handleSend = useCallback(async () => {
    if (!inputValue.trim() || isThinking || !agentRef.current) return;

    const userMessage = inputValue.trim();
    setInputValue("");
    setError(null);

    // Add user message
    setMessages(prev => [...prev, { id: generateMessageId(), role: "user", content: userMessage }]);
    setIsThinking(true);

    try {
      const response = await agentRef.current.chat(userMessage);

      if (response.type === "error") {
        setError(response.error || response.message);
        setMessages(prev => [...prev, {
          id: generateMessageId(),
          role: "assistant",
          content: response.message
        }]);
      } else if (response.type === "confirmation_required") {
        // Show confirmation UI
        console.log("[MusicChatV2] Received confirmation_required:", {
          message: response.message,
          pendingAction: response.pendingAction,
        });
        setMessages(prev => [...prev, {
          id: generateMessageId(),
          role: "assistant",
          content: response.message
        }]);
        setPendingAction(response.pendingAction || null);
        setPhase("confirming");
        console.log("[MusicChatV2] Set phase to confirming");
      } else if (response.type === "generating") {
        setMessages(prev => [...prev, {
          id: generateMessageId(),
          role: "assistant",
          content: response.message,
          isGenerating: true,
        }]);
        // Phase change handled by callback
      } else {
        setMessages(prev => [...prev, {
          id: generateMessageId(),
          role: "assistant",
          content: response.message
        }]);
      }
    } catch (err) {
      setError(err instanceof Error ? err.message : "Something went wrong");
    } finally {
      setIsThinking(false);
    }
  }, [inputValue, isThinking]);

  const handlePlayTrack = useCallback((track: SunoGeneratedTrack) => {
    if (playingTrackId === track.id) {
      audioElement?.pause();
      setPlayingTrackId(null);
    } else {
      if (audioElement) audioElement.pause();
      const audio = new Audio(track.audio_url);
      audio.onended = () => setPlayingTrackId(null);
      audio.play();
      setAudioElement(audio);
      setPlayingTrackId(track.id);
    }
  }, [playingTrackId, audioElement]);

  const handleAddToTimeline = useCallback(() => {
    const track = generatedTracks.find(t => t.id === selectedTrackId);
    if (track && onMusicGenerated) {
      onMusicGenerated(track);
    }
    onClose();
  }, [generatedTracks, selectedTrackId, onMusicGenerated, onClose]);

  const handleReset = useCallback(() => {
    agentRef.current = createMusicProducerAgentV2();
    setMessages([{
      id: generateMessageId(),
      role: "assistant",
      content: "Let's create something new! 🎵 What kind of music would you like?",
    }]);
    setPhase("chatting");
    setTaskId(null);
    setGeneratedTracks([]);
    setPendingAction(null);
    setError(null);
  }, []);

  // Handle user confirming the pending action
  const handleConfirm = useCallback(async () => {
    if (!agentRef.current) return;

    setIsThinking(true);
    setError(null);

    try {
      const response = await agentRef.current.confirmAndExecute();

      if (response.type === "error") {
        setError(response.error || response.message);
        setPhase("chatting");
      } else if (response.type === "generating") {
        setMessages(prev => [...prev, {
          id: generateMessageId(),
          role: "assistant",
          content: response.message,
          isGenerating: true,
        }]);
        // Phase change handled by callback (onTaskStarted)
      }

      setPendingAction(null);
    } catch (err) {
      setError(err instanceof Error ? err.message : "Failed to confirm");
      setPhase("chatting");
    } finally {
      setIsThinking(false);
    }
  }, []);

  // Handle user cancelling the pending action
  const handleCancelConfirmation = useCallback(async () => {
    if (!agentRef.current) return;

    agentRef.current.cancelPendingAction();
    setPendingAction(null);
    setPhase("chatting");

    // Get the agent to respond to the cancellation
    setIsThinking(true);
    try {
      const response = await agentRef.current.chat("I want to change something before generating.");
      setMessages(prev => [...prev, {
        id: generateMessageId(),
        role: "assistant",
        content: response.message
      }]);
    } catch {
      // Ignore
    } finally {
      setIsThinking(false);
    }
  }, []);

  return (
    <Dialog open={open} onOpenChange={(isOpen) => !isOpen && phase !== "generating" && phase !== "confirming" && onClose()}>
      <DialogContent className="sm:max-w-xl md:max-w-2xl bg-background border-border text-foreground max-h-[90vh] flex flex-col">
        <DialogHeader className="shrink-0">
          <DialogTitle className="flex items-center gap-2 text-xl">
            <MessageSquare className="w-5 h-5 text-cyan-400" />
            AI Music Producer
            {credits !== null && credits >= 0 && (
              <span className={cn(
                "ml-auto text-sm font-normal flex items-center gap-1.5",
                credits < 10 ? "text-amber-500" : "text-muted-foreground"
              )}>
                <Coins className="w-3.5 h-3.5" />
                {credits} credits
              </span>
            )}
          </DialogTitle>
          <DialogDescription>
            Chat with AI to create your perfect song. I'll handle everything from lyrics to generation.
          </DialogDescription>
        </DialogHeader>

        {/* Chat Messages */}
        <div className="flex-1 overflow-y-auto py-4 space-y-4 min-h-[300px] max-h-[400px]">
          {messages.map((msg) => (
            <div
              key={msg.id}
              className={cn("flex gap-3", msg.role === "user" ? "justify-end" : "justify-start")}
            >
              {msg.role === "assistant" && (
                <div className="w-8 h-8 rounded-full bg-cyan-500/10 flex items-center justify-center shrink-0">
                  <Bot className="w-4 h-4 text-cyan-400" />
                </div>
              )}
              <div
                className={cn(
                  "max-w-[80%] rounded-2xl px-4 py-2.5 text-sm whitespace-pre-wrap",
                  msg.role === "user"
                    ? "bg-primary text-primary-foreground rounded-br-md"
                    : "bg-muted rounded-bl-md"
                )}
              >
                {msg.content}
                {msg.isGenerating && (
                  <div className="mt-2 flex items-center gap-2 text-cyan-400">
                    <Loader2 className="w-3 h-3 animate-spin" />
                    <span className="text-xs">Generating...</span>
                  </div>
                )}
              </div>
              {msg.role === "user" && (
                <div className="w-8 h-8 rounded-full bg-muted flex items-center justify-center shrink-0">
                  <User className="w-4 h-4" />
                </div>
              )}
            </div>
          ))}

          {isThinking && (
            <div className="flex gap-3">
              <div className="w-8 h-8 rounded-full bg-cyan-500/10 flex items-center justify-center shrink-0">
                <Bot className="w-4 h-4 text-cyan-400" />
              </div>
              <div className="bg-muted rounded-2xl rounded-bl-md px-4 py-2.5">
                <Loader2 className="w-4 h-4 animate-spin text-cyan-400" />
              </div>
            </div>
          )}

          <div ref={messagesEndRef} />
        </div>

        {/* Error */}
        {error && (
          <div className="bg-destructive/10 border border-destructive/20 rounded-lg p-3 flex items-start gap-2">
            <AlertCircle className="w-4 h-4 text-destructive shrink-0 mt-0.5" />
            <p className="text-sm text-destructive">{error}</p>
          </div>
        )}

        {/* Confirmation UI (Human-in-the-Loop) */}
        {phase === "confirming" && pendingAction && (
          <div className="bg-cyan-500/10 border border-cyan-500/30 rounded-lg p-4 space-y-3">
            <div className="flex items-start gap-3">
              <div className="w-8 h-8 rounded-full bg-cyan-500/20 flex items-center justify-center shrink-0">
                <Sparkles className="w-4 h-4 text-cyan-400" />
              </div>
              <div className="flex-1 min-w-0">
                <p className="text-sm font-medium text-cyan-400 mb-2">Ready to generate your song!</p>
                <div className="bg-background/50 rounded-md p-3 text-sm whitespace-pre-wrap font-mono">
                  {pendingAction.summary}
                </div>
              </div>
            </div>
            <div className="flex gap-2 justify-end">
              <Button
                variant="ghost"
                size="sm"
                onClick={handleCancelConfirmation}
                disabled={isThinking}
              >
                Modify
              </Button>
              <Button
                size="sm"
                onClick={handleConfirm}
                disabled={isThinking}
                className="bg-cyan-500 hover:bg-cyan-600"
              >
                {isThinking ? (
                  <>
                    <Loader2 className="w-4 h-4 mr-2 animate-spin" />
                    Starting...
                  </>
                ) : (
                  <>
                    <Check className="w-4 h-4 mr-2" />
                    Generate Now
                  </>
                )}
              </Button>
            </div>
          </div>
        )}

        {/* Generation Progress */}
        {phase === "generating" && (
          <div className="space-y-2 py-2">
            <div className="flex justify-between text-xs">
              <span className="text-cyan-400 font-medium flex items-center gap-2">
                <Sparkles className="w-3 h-3 animate-pulse" />
                Creating your song...
              </span>
              <span className="text-muted-foreground">{Math.round(generationProgress)}%</span>
            </div>
            <Progress value={generationProgress} className="h-2" />
            <p className="text-xs text-muted-foreground text-center">
              This usually takes 2-4 minutes. The AI is composing your music!
            </p>
          </div>
        )}

        {/* Generated Tracks */}
        {phase === "complete" && generatedTracks.length > 0 && (
          <div className="space-y-3 py-2">
            <p className="text-sm font-medium text-muted-foreground">Your Generated Tracks</p>
            <div className="space-y-2">
              {generatedTracks.map((track) => (
                <div
                  key={track.id}
                  className={cn(
                    "flex items-center gap-3 p-3 rounded-lg border transition-all cursor-pointer",
                    selectedTrackId === track.id
                      ? "border-cyan-500 bg-cyan-500/10"
                      : "border-border bg-card hover:border-cyan-500/50"
                  )}
                  onClick={() => setSelectedTrackId(track.id)}
                >
                  <Button
                    variant="ghost"
                    size="icon"
                    className="h-10 w-10 shrink-0"
                    onClick={(e) => { e.stopPropagation(); handlePlayTrack(track); }}
                  >
                    {playingTrackId === track.id ? <Pause className="w-5 h-5" /> : <Play className="w-5 h-5" />}
                  </Button>
                  <div className="flex-1 min-w-0">
                    <p className="font-medium truncate">{track.title}</p>
                    <p className="text-xs text-muted-foreground">
                      {track.style && `${track.style} • `}
                      {Math.floor(track.duration / 60)}:{String(Math.floor(track.duration % 60)).padStart(2, "0")}
                    </p>
                  </div>
                  {selectedTrackId === track.id && <Check className="w-5 h-5 text-cyan-400 shrink-0" />}
                </div>
              ))}
            </div>
          </div>
        )}

        {/* Input - hidden during confirmation and generation */}
        {(phase === "chatting") && (
          <div className="shrink-0 pt-2 border-t border-border">
            <div className="flex gap-2">
              <Input
                ref={inputRef}
                value={inputValue}
                onChange={(e) => setInputValue(e.target.value)}
                placeholder="Describe your music..."
                className="flex-1"
                disabled={isThinking}
                onKeyDown={(e) => e.key === "Enter" && !e.shiftKey && handleSend()}
              />
              <Button onClick={handleSend} disabled={!inputValue.trim() || isThinking} className="bg-cyan-500 hover:bg-cyan-600">
                <Send className="w-4 h-4" />
              </Button>
            </div>
          </div>
        )}

        <DialogFooter className="shrink-0 pt-2">
          {phase === "chatting" && (
            <Button variant="ghost" onClick={onClose}>Cancel</Button>
          )}

          {phase === "confirming" && (
            <Button variant="ghost" onClick={onClose} disabled={isThinking}>Cancel</Button>
          )}

          {phase === "generating" && (
            <Button variant="ghost" disabled>
              <Loader2 className="w-4 h-4 mr-2 animate-spin" />
              Generating...
            </Button>
          )}

          {phase === "complete" && (
            <>
              <Button variant="ghost" onClick={handleReset}>Create Another</Button>
              <Button onClick={handleAddToTimeline} disabled={!selectedTrackId} className="bg-cyan-500 hover:bg-cyan-600">
                <Plus className="w-4 h-4 mr-2" />
                Add to Timeline
              </Button>
            </>
          )}
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
}

export default MusicChatModalV2;
````

## File: packages/frontend/components/MusicGeneratorModal.tsx
````typescript
/**
 * MusicGeneratorModal Component
 * 
 * Modal for generating AI music using Suno API.
 * Features topic/prompt input, style selection, vocal mode, and advanced options.
 */

import { useState, useCallback, useEffect } from "react";
import {
  Music,
  Sparkles,
  Mic,
  MicOff,
  Settings2,
  ChevronDown,
  ChevronUp,
  Loader2,
  AlertCircle,
  RefreshCw,
  Play,
  Pause,
  Check,
  Plus,
  Upload,
  Layers,
} from "lucide-react";
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from "@/components/ui/dialog";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { Label } from "@/components/ui/label";
import { Textarea } from "@/components/ui/textarea";
import { Slider } from "@/components/ui/slider";
import { Switch } from "@/components/ui/switch";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select";
import { Progress } from "@/components/ui/progress";
import { cn } from "@/lib/utils";
import type { SunoModel, SunoGeneratedTrack, SunoGenerationConfig, SunoTaskStatus } from "@/services/sunoService";

// Music style/genre options
const MUSIC_STYLES = [
  "Pop",
  "Rock",
  "Electronic",
  "Hip Hop",
  "R&B",
  "Jazz",
  "Classical",
  "Country",
  "Folk",
  "Ambient",
  "Cinematic",
  "Lo-Fi",
  "Indie",
  "Metal",
  "Reggae",
  "Latin",
  "World",
  "Blues",
] as const;

// Suno model versions
const MODEL_VERSIONS: { value: SunoModel; label: string; description: string }[] = [
  { value: "V5", label: "V5 (Latest)", description: "Highest quality, recommended" },
  { value: "V4_5ALL", label: "V4.5 All", description: "All styles supported" },
  { value: "V4_5PLUS", label: "V4.5 Plus", description: "Enhanced quality" },
  { value: "V4_5", label: "V4.5", description: "Standard quality" },
  { value: "V4", label: "V4", description: "Legacy model" },
];

interface MusicGeneratorModalProps {
  open: boolean;
  onClose: () => void;
  onMusicGenerated?: (track: SunoGeneratedTrack) => void;
  initialTopic?: string;
  // Hook integration
  musicState: {
    isGenerating: boolean;
    status: SunoTaskStatus | string | null;
    progress: number;
    generatedTracks: SunoGeneratedTrack[];
    selectedTrackId: string | null;
    lyrics: string | null;
    lyricsTaskId?: string | null;
    credits: number | null;
    error: string | null;
  };
  onGenerateMusic: (config: Partial<SunoGenerationConfig> & { prompt: string }) => Promise<void>;
  onGenerateLyrics: (prompt: string) => Promise<void>;
  onSelectTrack: (trackId: string) => void;
  onAddToTimeline: () => void;
  onRefreshCredits: () => Promise<void>;
  // New props for Extended features (optional)
  onUploadAudio?: (file: File) => Promise<string>;
  onUploadAndCover?: (config: any) => Promise<string>;
  onAddVocals?: (config: any) => Promise<string>;
  onAddInstrumental?: (config: any) => Promise<string>;
  /** Initial mode to start the modal in */
  initialMode?: "generate" | "remix";
}

interface MusicFormState {
  topic: string;
  style: string;
  title: string;
  vocalMode: "vocal-male" | "vocal-female" | "instrumental";
  customLyrics: string;
  useCustomLyrics: boolean;
  // Advanced options
  model: SunoModel;
  styleWeight: number;
  weirdnessConstraint: number;
  negativeTags: string;
}

export function MusicGeneratorModal({
  open,
  onClose,
  onMusicGenerated,
  initialTopic = "",
  musicState,
  onGenerateMusic,
  onGenerateLyrics,
  onSelectTrack,
  onAddToTimeline,
  onRefreshCredits,
  onUploadAudio,
  onUploadAndCover,
  onAddVocals,
  onAddInstrumental,
  initialMode = "generate",
}: MusicGeneratorModalProps) {
  // Mode state - use initialMode prop
  const [mode, setMode] = useState<"generate" | "remix">(initialMode);
  const [uploadedUrl, setUploadedUrl] = useState<string | null>(null);
  const [isUploading, setIsUploading] = useState(false);
  const [remixAction, setRemixAction] = useState<"cover" | "vocals" | "instrumental">("cover");

  // Form state
  const [formState, setFormState] = useState<MusicFormState>({
    topic: initialTopic,
    style: "Cinematic",
    title: "",
    vocalMode: "vocal-male",
    customLyrics: "",
    useCustomLyrics: false,
    model: "V5",
    styleWeight: 0.65,
    weirdnessConstraint: 0.5,
    negativeTags: "",
  });

  // UI state
  const [showAdvanced, setShowAdvanced] = useState(false);
  const [isGeneratingLyrics, setIsGeneratingLyrics] = useState(false);
  const [playingTrackId, setPlayingTrackId] = useState<string | null>(null);
  const [audioElement, setAudioElement] = useState<HTMLAudioElement | null>(null);

  // Update topic when initialTopic changes
  useEffect(() => {
    if (initialTopic) {
      setFormState(prev => ({ ...prev, topic: initialTopic }));
    }
  }, [initialTopic]);

  // Sync mode when initialMode changes (e.g., when modal opens)
  useEffect(() => {
    setMode(initialMode);
  }, [initialMode, open]);

  // Update lyrics from musicState
  useEffect(() => {
    if (musicState.lyrics && !formState.customLyrics) {
      setFormState(prev => ({ ...prev, customLyrics: musicState.lyrics || "" }));
    }
  }, [musicState.lyrics]);

  // Fetch credits on mount
  useEffect(() => {
    if (open && musicState.credits === null) {
      onRefreshCredits();
    }
  }, [open, musicState.credits, onRefreshCredits]);

  // Cleanup audio on unmount
  useEffect(() => {
    return () => {
      if (audioElement) {
        audioElement.pause();
        audioElement.src = "";
      }
    };
  }, [audioElement]);

  // Handle form field changes
  const updateField = useCallback(<K extends keyof MusicFormState>(
    field: K,
    value: MusicFormState[K]
  ) => {
    setFormState(prev => ({ ...prev, [field]: value }));
  }, []);

  // Handle music generation
  const handleGenerate = useCallback(async () => {
    if (!formState.topic.trim()) return;

    // Use lyrics as prompt when not instrumental and lyrics are provided
    const isInstrumental = formState.vocalMode === "instrumental";
    const hasLyrics = formState.customLyrics.trim().length > 0;

    const config: Partial<SunoGenerationConfig> & { prompt: string } = {
      prompt: (!isInstrumental && hasLyrics)
        ? formState.customLyrics
        : formState.topic,
      model: formState.model,
      style: formState.style,
      styleWeight: formState.styleWeight,
      weirdnessConstraint: formState.weirdnessConstraint,
      instrumental: isInstrumental,
      vocalGender: formState.vocalMode === "vocal-male" ? "m" : formState.vocalMode === "vocal-female" ? "f" : undefined,
      customMode: true, // Always custom when using this advanced modal form
    };

    if (formState.title.trim()) {
      config.title = formState.title;
    }

    if (formState.negativeTags.trim()) {
      config.negativeTags = formState.negativeTags;
    }

    await onGenerateMusic(config);
  }, [formState, onGenerateMusic]);

  // Handle lyrics generation
  const handleGenerateLyrics = useCallback(async () => {
    if (!formState.topic.trim()) return;

    setIsGeneratingLyrics(true);
    try {
      await onGenerateLyrics(formState.topic);
    } finally {
      setIsGeneratingLyrics(false);
    }
  }, [formState.topic, onGenerateLyrics]);

  // Handle file upload
  const handleFileUpload = useCallback(async (e: React.ChangeEvent<HTMLInputElement>) => {
    const file = e.target.files?.[0];
    if (!file) return;

    setIsUploading(true);
    try {
      if (!onUploadAudio) throw new Error("Audio upload not supported");
      const url = await onUploadAudio(file);
      setUploadedUrl(url);
    } catch (error) {
      console.error("Upload failed", error);
      // Error state handling is implicit via musicState.error usually, 
      // but here we might want local feedback. 
      // Ideally we'd set a local error or reuse the parent's error mechanism if exposed settable.
      // For now just console.
    } finally {
      setIsUploading(false);
    }
  }, [onUploadAudio]);

  // Handle Remix Actions
  const handleRemix = useCallback(async () => {
    if (!uploadedUrl) return;

    if (remixAction === "cover") {
      // Cover requires: uploadUrl, customMode, instrumental, model, callBackUrl
      // If not instrumental: style, prompt (lyrics), title required
      // If instrumental: style, title required
      const isInstrumental = formState.vocalMode === "instrumental";

      if (!onUploadAndCover) throw new Error("Cover generation not supported");
      await onUploadAndCover({
        uploadUrl: uploadedUrl,
        style: formState.style,
        title: formState.title || "Cover",
        instrumental: isInstrumental,
        // prompt is the lyrics when not instrumental
        prompt: isInstrumental ? undefined : formState.customLyrics,
        model: formState.model || "V4_5ALL",
        styleWeight: formState.styleWeight,
        weirdnessConstraint: formState.weirdnessConstraint,
        negativeTags: formState.negativeTags,
        vocalGender: formState.vocalMode === "vocal-male" ? "m" : formState.vocalMode === "vocal-female" ? "f" : undefined,
      });
    } else if (remixAction === "vocals") {
      if (!onAddVocals) throw new Error("Adding vocals not supported");
      await onAddVocals({
        uploadUrl: uploadedUrl,
        prompt: formState.topic || "Add vocals",
        title: formState.title || "With Vocals",
        model: formState.model || "V4_5PLUS",
        style: formState.style,
      });
    } else if (remixAction === "instrumental") {
      if (!onAddInstrumental) throw new Error("Adding instrumental not supported");
      await onAddInstrumental({
        uploadUrl: uploadedUrl,
        prompt: formState.topic || "Instrumental version",
        title: formState.title || "Instrumental",
        model: formState.model || "V4_5PLUS",
        style: formState.style,
      });
    }
  }, [uploadedUrl, remixAction, onUploadAndCover, onAddVocals, onAddInstrumental, formState]);

  // Handle track playback
  const handlePlayTrack = useCallback((track: SunoGeneratedTrack) => {
    if (playingTrackId === track.id) {
      // Pause current track
      audioElement?.pause();
      setPlayingTrackId(null);
    } else {
      // Play new track
      if (audioElement) {
        audioElement.pause();
      }
      const audio = new Audio(track.audio_url);
      audio.onended = () => setPlayingTrackId(null);
      audio.play();
      setAudioElement(audio);
      setPlayingTrackId(track.id);
    }
  }, [playingTrackId, audioElement]);

  // Handle adding track to timeline
  const handleAddToTimeline = useCallback(() => {
    onAddToTimeline();
    const selectedTrack = musicState.generatedTracks.find(
      t => t.id === musicState.selectedTrackId
    );
    if (selectedTrack && onMusicGenerated) {
      onMusicGenerated(selectedTrack);
    }
    onClose();
  }, [onAddToTimeline, musicState.generatedTracks, musicState.selectedTrackId, onMusicGenerated, onClose]);

  // Calculate lyrics stats
  const lyricsStats = {
    chars: formState.customLyrics.length,
    lines: formState.customLyrics.split("\n").filter(l => l.trim()).length,
  };

  const isGenerating = musicState.isGenerating;
  const hasGeneratedTracks = musicState.generatedTracks.length > 0;
  const canGenerate = formState.topic.trim().length > 0 && !isGenerating;

  return (
    <Dialog open={open} onOpenChange={(isOpen) => !isOpen && !isGenerating && onClose()}>
      <DialogContent className="sm:max-w-xl md:max-w-2xl bg-background border-border text-foreground max-h-[90vh] overflow-y-auto">
        <DialogHeader>
          <DialogTitle className="flex items-center gap-2 text-xl">
            <Music className="w-5 h-5 text-primary" />
            Generate AI Music
            {/* Credit Display */}
            {musicState.credits !== null && musicState.credits >= 0 && (
              <span className={cn(
                "ml-auto text-sm font-normal flex items-center gap-1.5",
                musicState.credits < 10 ? "text-amber-500" : "text-muted-foreground"
              )}>
                {musicState.credits < 10 && (
                  <AlertCircle className="w-3.5 h-3.5" />
                )}
                {musicState.credits} credits
                <Button
                  variant="ghost"
                  size="icon"
                  className="h-6 w-6"
                  onClick={onRefreshCredits}
                >
                  <RefreshCw className="w-3 h-3" />
                </Button>
              </span>
            )}
            {musicState.credits === null && (
              <span className="ml-auto text-sm font-normal text-muted-foreground flex items-center gap-1.5">
                Credits: Unknown
                <Button
                  variant="ghost"
                  size="icon"
                  className="h-6 w-6"
                  onClick={onRefreshCredits}
                >
                  <RefreshCw className="w-3 h-3" />
                </Button>
              </span>
            )}
          </DialogTitle>
          <DialogDescription className="text-muted-foreground">
            Create custom AI-generated music for your video using Suno.
          </DialogDescription>
        </DialogHeader>

        <div className="py-4 space-y-6">
          {/* Error Display */}
          {musicState.error && (
            <div className="bg-destructive/10 border border-destructive/20 rounded-lg p-4 flex items-start gap-3">
              <AlertCircle className="w-5 h-5 text-destructive shrink-0 mt-0.5" />
              <div className="flex-1">
                <p className="text-sm text-destructive">{musicState.error}</p>
                <Button
                  variant="outline"
                  size="sm"
                  className="mt-2"
                  onClick={handleGenerate}
                  disabled={!canGenerate}
                >
                  Retry
                </Button>
              </div>
            </div>
          )}

          {/* Progress Display */}
          {isGenerating && (
            <div className="space-y-3">
              <div className="flex justify-between text-xs uppercase tracking-wider">
                <span className="text-primary font-medium flex items-center gap-2">
                  <Loader2 className="w-3 h-3 animate-spin" />
                  {musicState.status || "Generating..."}
                </span>
                <span className="text-muted-foreground">
                  {Math.round(musicState.progress)}%
                </span>
              </div>
              <Progress value={musicState.progress} className="h-2" />
              <p className="text-xs text-muted-foreground text-center">
                This may take a few minutes. Please wait...
              </p>
            </div>
          )}

          {/* Generated Tracks Preview */}
          {hasGeneratedTracks && !isGenerating && (
            <div className="space-y-3">
              <Label className="text-muted-foreground">Generated Tracks</Label>
              <div className="space-y-2">
                {musicState.generatedTracks.map((track) => (
                  <div
                    key={track.id}
                    className={cn(
                      "flex items-center gap-3 p-3 rounded-lg border transition-all cursor-pointer",
                      musicState.selectedTrackId === track.id
                        ? "border-primary bg-primary/10"
                        : "border-border bg-card hover:border-primary/50"
                    )}
                    onClick={() => onSelectTrack(track.id)}
                  >
                    <Button
                      variant="ghost"
                      size="icon"
                      className="h-10 w-10 shrink-0"
                      onClick={(e) => {
                        e.stopPropagation();
                        handlePlayTrack(track);
                      }}
                    >
                      {playingTrackId === track.id ? (
                        <Pause className="w-5 h-5" />
                      ) : (
                        <Play className="w-5 h-5" />
                      )}
                    </Button>
                    <div className="flex-1 min-w-0">
                      <p className="font-medium truncate">{track.title}</p>
                      <p className="text-xs text-muted-foreground">
                        {track.style && `${track.style} • `}
                        {Math.floor(track.duration / 60)}:{String(Math.floor(track.duration % 60)).padStart(2, "0")}
                      </p>
                    </div>
                    {musicState.selectedTrackId === track.id && (
                      <Check className="w-5 h-5 text-primary shrink-0" />
                    )}
                  </div>
                ))}
              </div>
              <div className="flex gap-2">
                <Button
                  className="flex-1"
                  onClick={handleAddToTimeline}
                  disabled={!musicState.selectedTrackId}
                >
                  <Plus className="w-4 h-4 mr-2" />
                  Add to Timeline
                </Button>
                <Button
                  variant="outline"
                  onClick={handleGenerate}
                  disabled={!canGenerate}
                >
                  <RefreshCw className="w-4 h-4 mr-2" />
                  Regenerate
                </Button>
              </div>
            </div>
          )}

          {/* Mode Switcher */}
          {!hasGeneratedTracks && !isGenerating && (
            <div className="flex bg-muted p-1 rounded-lg mb-4">
              <button
                onClick={() => setMode("generate")}
                className={cn(
                  "flex-1 flex items-center justify-center gap-2 py-2 text-sm font-medium rounded-md transition-all",
                  mode === "generate" ? "bg-background text-foreground shadow-sm" : "text-muted-foreground hover:text-foreground"
                )}
              >
                <Sparkles className="w-4 h-4" />
                Generate New
              </button>
              <button
                onClick={() => setMode("remix")}
                className={cn(
                  "flex-1 flex items-center justify-center gap-2 py-2 text-sm font-medium rounded-md transition-all",
                  mode === "remix" ? "bg-background text-foreground shadow-sm" : "text-muted-foreground hover:text-foreground"
                )}
              >
                <RefreshCw className="w-4 h-4" />
                Upload & Remix
              </button>
            </div>
          )}

          {/* Form Content */}
          {!hasGeneratedTracks && !isGenerating && mode === "generate" && (
            <>
              {/* Topic/Prompt Input */}
              <div className="space-y-2">
                <Label htmlFor="topic" className="text-muted-foreground">
                  Topic / Prompt <span className="text-destructive">*</span>
                </Label>
                <Textarea
                  id="topic"
                  value={formState.topic}
                  onChange={(e) => updateField("topic", e.target.value)}
                  placeholder="Describe the music you want... e.g., 'An upbeat summer anthem about freedom and adventure'"
                  className="min-h-[80px] resize-none"
                />
              </div>

              {/* Style/Genre Selection */}
              <div className="grid grid-cols-2 gap-4">
                <div className="space-y-2">
                  <Label className="text-muted-foreground">Style / Genre</Label>
                  <Select
                    value={formState.style}
                    onValueChange={(value) => updateField("style", value)}
                  >
                    <SelectTrigger>
                      <SelectValue placeholder="Select style" />
                    </SelectTrigger>
                    <SelectContent>
                      {MUSIC_STYLES.map((style) => (
                        <SelectItem key={style} value={style}>
                          {style}
                        </SelectItem>
                      ))}
                    </SelectContent>
                  </Select>
                </div>

                {/* Title Input */}
                <div className="space-y-2">
                  <Label htmlFor="title" className="text-muted-foreground">
                    Title (optional)
                  </Label>
                  <Input
                    id="title"
                    value={formState.title}
                    onChange={(e) => updateField("title", e.target.value)}
                    placeholder="Song title..."
                  />
                </div>
              </div>

              {/* Vocal Mode Selection */}
              <div className="space-y-2">
                <Label className="text-muted-foreground">Vocal Mode</Label>
                <div className="grid grid-cols-3 gap-2">
                  {[
                    { id: "vocal-male", label: "Male Vocal", icon: Mic },
                    { id: "vocal-female", label: "Female Vocal", icon: Mic },
                    { id: "instrumental", label: "Instrumental", icon: MicOff },
                  ].map(({ id, label, icon: Icon }) => (
                    <button
                      key={id}
                      onClick={() => updateField("vocalMode", id as MusicFormState["vocalMode"])}
                      className={cn(
                        "flex flex-col items-center gap-1.5 p-3 rounded-lg border transition-all text-sm",
                        formState.vocalMode === id
                          ? "border-primary bg-primary/10 text-primary"
                          : "border-border bg-card text-muted-foreground hover:border-primary/30"
                      )}
                    >
                      <Icon className="w-5 h-5" />
                      <span>{label}</span>
                    </button>
                  ))}
                </div>
              </div>

              {/* Instrumental Toggle */}
              <div className="flex items-center justify-between py-2">
                <Label htmlFor="instrumental-toggle" className="text-muted-foreground cursor-pointer">
                  Instrumental
                </Label>
                <Switch
                  id="instrumental-toggle"
                  checked={formState.vocalMode === "instrumental"}
                  onCheckedChange={(checked) => {
                    if (checked) {
                      updateField("vocalMode", "instrumental");
                    } else {
                      updateField("vocalMode", "vocal-male");
                    }
                  }}
                />
              </div>

              {/* Lyrics Prompt - Shows when NOT instrumental */}
              {formState.vocalMode !== "instrumental" && (
                <div className="space-y-2">
                  <Label htmlFor="lyrics" className="text-muted-foreground">
                    Lyrics (Prompt) <span className="text-destructive">*</span>
                  </Label>
                  <div className="relative">
                    <Textarea
                      id="lyrics"
                      value={formState.customLyrics}
                      onChange={(e) => updateField("customLyrics", e.target.value)}
                      placeholder="Enter your lyrics here...&#10;&#10;[Verse 1]&#10;Your lyrics go here...&#10;&#10;[Chorus]&#10;The catchy part..."
                      className="min-h-[150px] resize-none pb-10"
                    />
                    <div className="absolute bottom-2 left-3 right-3 flex items-center justify-between text-xs text-muted-foreground">
                      <span>{lyricsStats.chars}/3000</span>
                      <Button
                        variant="ghost"
                        size="sm"
                        className="h-6 text-xs"
                        onClick={handleGenerateLyrics}
                        disabled={isGeneratingLyrics || !formState.topic.trim()}
                      >
                        {isGeneratingLyrics ? (
                          <>
                            <Loader2 className="w-3 h-3 mr-1 animate-spin" />
                            Generating...
                          </>
                        ) : (
                          <>
                            <Sparkles className="w-3 h-3 mr-1" />
                            Generate Lyrics
                          </>
                        )}
                      </Button>
                    </div>
                  </div>
                </div>
              )}

              {/* Legacy Custom Lyrics Section - Hidden, keeping for backwards compatibility */}
              {/* Lyrics Section */}
              <div className="space-y-3 pt-2 border-t border-border/50 hidden">
                <div className="flex items-center justify-between">
                  <Label className="text-muted-foreground flex items-center gap-2">
                    Custom Lyrics
                  </Label>
                  <div className="flex items-center gap-2">
                    <span className="text-xs text-muted-foreground">
                      {formState.vocalMode === "instrumental" ? "Disabled for instrumental" : "Use custom lyrics"}
                    </span>
                    <Switch
                      checked={formState.useCustomLyrics}
                      onCheckedChange={(checked) => updateField("useCustomLyrics", checked)}
                      disabled={formState.vocalMode === "instrumental"}
                    />
                  </div>
                </div>

                {formState.useCustomLyrics && formState.vocalMode !== "instrumental" && (
                  <div className="space-y-2">
                    <div className="relative">
                      <Textarea
                        value={formState.customLyrics}
                        onChange={(e) => updateField("customLyrics", e.target.value)}
                        placeholder="Enter your lyrics here..."
                        className="min-h-[120px] resize-none pb-8"
                      />
                      <div className="absolute bottom-2 left-3 right-3 flex items-center justify-between text-xs text-muted-foreground">
                        <span>{lyricsStats.chars} chars • {lyricsStats.lines} lines</span>
                        <Button
                          variant="ghost"
                          size="sm"
                          className="h-6 text-xs"
                          onClick={handleGenerateLyrics}
                          disabled={isGeneratingLyrics || !formState.topic.trim()}
                        >
                          {isGeneratingLyrics ? (
                            <>
                              <Loader2 className="w-3 h-3 mr-1 animate-spin" />
                              Generating...
                            </>
                          ) : (
                            <>
                              <Sparkles className="w-3 h-3 mr-1" />
                              Generate Lyrics
                            </>
                          )}
                        </Button>
                      </div>
                    </div>
                  </div>
                )}
              </div>
            </>
          )}

          {/* Remix / Upload Mode */}
          {!hasGeneratedTracks && !isGenerating && mode === "remix" && (
            <div className="space-y-5 animate-in fade-in cursor-default">

              {/* File Upload Area */}
              <div className="space-y-2">
                <Label className="text-muted-foreground">1. Upload Audio Reference</Label>
                <div className="flex gap-2 items-center">
                  <Button variant="outline" className="relative overflow-hidden" disabled={isUploading}>
                    {isUploading ? <Loader2 className="w-4 h-4 animate-spin mr-2" /> : <Upload className="w-4 h-4 mr-2" />}
                    {uploadedUrl ? "Change File" : "Upload MP3/WAV"}
                    <input
                      type="file"
                      accept="audio/*"
                      className="absolute inset-0 opacity-0 cursor-pointer"
                      onChange={handleFileUpload}
                    />
                  </Button>
                  {uploadedUrl && <span className="text-sm text-green-500 flex items-center gap-1"><Check className="w-3 h-3" /> Upload Complete</span>}
                </div>
                <p className="text-xs text-muted-foreground">Upload a track to cover, remix, or add vocals to. Supported: mp3, wav.</p>
              </div>

              {/* Action Selector */}
              {uploadedUrl && (
                <div className="space-y-2">
                  <Label className="text-muted-foreground">2. Select Action</Label>
                  <div className="grid grid-cols-3 gap-2">
                    {[
                      { id: "cover", label: "Change Style", desc: "Cover/Remix" },
                      { id: "vocals", label: "Add Vocals", desc: "To Instrumental" },
                      { id: "instrumental", label: "Add Backing", desc: "To Vocals" }
                    ].map((act) => (
                      <button
                        key={act.id}
                        onClick={() => setRemixAction(act.id as any)}
                        className={cn(
                          "flex flex-col items-center gap-1 p-3 rounded-lg border transition-all text-sm",
                          remixAction === act.id
                            ? "border-primary bg-primary/10 text-primary"
                            : "border-border bg-card text-muted-foreground hover:border-primary/30"
                        )}
                      >
                        <span className="font-medium">{act.label}</span>
                        <span className="text-[10px] opacity-70">{act.desc}</span>
                      </button>
                    ))}
                  </div>
                </div>
              )}

              {/* Dynamic Configuration based on Action */}
              {uploadedUrl && (
                <div className="space-y-4 pt-2 border-t border-white/5">
                  <Label className="text-muted-foreground">3. Configuration</Label>

                  {remixAction === "cover" && (
                    <div className="space-y-4">
                      {/* Style Selection */}
                      <div className="space-y-1">
                        <Label className="text-xs">Style of Music <span className="text-destructive">*</span></Label>
                        <Textarea
                          value={formState.style}
                          onChange={(e) => updateField("style", e.target.value)}
                          placeholder="Style: Khaleeji Tarab, Emotional, Maqam, Soulful Male Vocal, Acoustic, Qanun, Nay, Kamanja Takasim, Melancholic, Traditional Arabic Percussion, High Quality, Cinematic."
                          className="min-h-[80px] resize-none"
                        />
                        <p className="text-[10px] text-muted-foreground">{formState.style.length}/1000</p>
                      </div>

                      {/* Instrumental Toggle */}
                      <div className="flex items-center justify-between py-2">
                        <Label htmlFor="cover-instrumental" className="text-xs cursor-pointer">
                          Instrumental
                        </Label>
                        <Switch
                          id="cover-instrumental"
                          checked={formState.vocalMode === "instrumental"}
                          onCheckedChange={(checked) => {
                            if (checked) {
                              updateField("vocalMode", "instrumental");
                            } else {
                              updateField("vocalMode", "vocal-male");
                            }
                          }}
                        />
                      </div>

                      {/* Lyrics Prompt - Shows when NOT instrumental */}
                      {formState.vocalMode !== "instrumental" && (
                        <div className="space-y-1">
                          <Label className="text-xs">Lyrics (Prompt) <span className="text-destructive">*</span></Label>
                          <div className="relative">
                            <Textarea
                              value={formState.customLyrics}
                              onChange={(e) => updateField("customLyrics", e.target.value)}
                              placeholder="[Verse 1]&#10;Your lyrics go here...&#10;&#10;[Chorus]&#10;The catchy part..."
                              className="min-h-[150px] resize-none pb-8"
                            />
                            <div className="absolute bottom-2 left-3 right-3 flex items-center justify-between text-xs text-muted-foreground">
                              <span>{formState.customLyrics.length}/5000</span>
                              <Button
                                variant="ghost"
                                size="sm"
                                className="h-6 text-xs"
                                onClick={handleGenerateLyrics}
                                disabled={isGeneratingLyrics || !formState.style.trim()}
                              >
                                {isGeneratingLyrics ? (
                                  <>
                                    <Loader2 className="w-3 h-3 mr-1 animate-spin" />
                                    Generating...
                                  </>
                                ) : (
                                  <>
                                    <Sparkles className="w-3 h-3 mr-1" />
                                    Generate Lyrics
                                  </>
                                )}
                              </Button>
                            </div>
                          </div>
                        </div>
                      )}

                      {/* Title */}
                      <div className="space-y-1">
                        <Label className="text-xs">Title <span className="text-destructive">*</span></Label>
                        <Input
                          value={formState.title}
                          onChange={(e) => updateField("title", e.target.value)}
                          placeholder="Cover Title"
                        />
                        <p className="text-[10px] text-muted-foreground">{formState.title.length}/100</p>
                      </div>
                    </div>
                  )}

                  {(remixAction === "vocals" || remixAction === "instrumental") && (
                    <div className="space-y-1">
                      <Label className="text-xs">
                        {remixAction === "vocals" ? "Vocal Description / Lyrics" : "Instrumental Description"}
                      </Label>
                      <Textarea
                        value={formState.topic}
                        onChange={(e) => updateField("topic", e.target.value)}
                        placeholder={remixAction === "vocals" ? "Describe the vocals or paste lyrics..." : "Describe the backing track style..."}
                      />
                    </div>
                  )}

                  {remixAction !== "cover" && (
                    <div className="space-y-1">
                      <Label className="text-xs">Title (Optional)</Label>
                      <Input value={formState.title} onChange={(e) => updateField("title", e.target.value)} placeholder="Remix Title" />
                    </div>
                  )}
                </div>
              )}
            </div>
          )}

          {/* Advanced Options (Shared) */}
          {!hasGeneratedTracks && !isGenerating && (
            <div className="space-y-3 pt-2 border-t border-border/50">
              {/* ... existing advanced options content ... */}
              {/* I need to make sure I don't break the layout. The original code had the dropdown here. */}
              {/* Since I am replacing the block, I should re-add the advanced options functionality or leave it flexible. 
                    The existing code block I'm replacing ENDS at the advanced options start usually?
                    Let's check the target replacement block. 
                */}
              <button
                onClick={() => setShowAdvanced(!showAdvanced)}
                className="flex items-center gap-2 text-sm text-muted-foreground hover:text-foreground transition-colors w-full"
              >
                <Settings2 className="w-4 h-4" />
                Advanced Options
                {showAdvanced ? (
                  <ChevronUp className="w-4 h-4 ml-auto" />
                ) : (
                  <ChevronDown className="w-4 h-4 ml-auto" />
                )}
              </button>

              {showAdvanced && (
                <div className="space-y-4 pl-6 animate-in slide-in-from-top-2">
                  {/* Model Version */}
                  <div className="space-y-2">
                    <Label className="text-muted-foreground text-xs uppercase">Model Version</Label>
                    <Select
                      value={formState.model}
                      onValueChange={(value) => updateField("model", value as SunoModel)}
                    >
                      <SelectTrigger className="h-9">
                        <SelectValue />
                      </SelectTrigger>
                      <SelectContent>
                        {MODEL_VERSIONS.map((model) => (
                          <SelectItem key={model.value} value={model.value}>
                            <div className="flex flex-col">
                              <span>{model.label}</span>
                              <span className="text-xs text-muted-foreground">{model.description}</span>
                            </div>
                          </SelectItem>
                        ))}
                      </SelectContent>
                    </Select>
                  </div>

                  {/* Style Weight */}
                  <div className="space-y-2">
                    <div className="flex justify-between">
                      <Label className="text-muted-foreground text-xs uppercase">Style Weight</Label>
                      <span className="text-xs font-mono">{formState.styleWeight.toFixed(2)}</span>
                    </div>
                    <Slider
                      min={0}
                      max={1}
                      step={0.05}
                      value={[formState.styleWeight]}
                      onValueChange={([val]) => val !== undefined && updateField("styleWeight", val)}
                    />
                    <p className="text-[10px] text-muted-foreground">
                      How strongly the style influences the generation
                    </p>
                  </div>

                  {/* Weirdness Constraint */}
                  <div className="space-y-2">
                    <div className="flex justify-between">
                      <Label className="text-muted-foreground text-xs uppercase">Creativity</Label>
                      <span className="text-xs font-mono">{formState.weirdnessConstraint.toFixed(2)}</span>
                    </div>
                    <Slider
                      min={0}
                      max={1}
                      step={0.05}
                      value={[formState.weirdnessConstraint]}
                      onValueChange={([val]) => val !== undefined && updateField("weirdnessConstraint", val)}
                    />
                    <p className="text-[10px] text-muted-foreground">
                      Higher values produce more creative/experimental results
                    </p>
                  </div>

                  {/* Negative Tags */}
                  <div className="space-y-2">
                    <Label className="text-muted-foreground text-xs uppercase">Negative Tags</Label>
                    <Input
                      value={formState.negativeTags}
                      onChange={(e) => updateField("negativeTags", e.target.value)}
                      placeholder="e.g., Heavy Metal, Screaming, Distortion"
                      className="h-9"
                    />
                    <p className="text-[10px] text-muted-foreground">
                      Comma-separated styles to exclude from generation
                    </p>
                  </div>
                </div>
              )}
            </div>
          )}
        </div>

        <DialogFooter>
          <Button
            variant="ghost"
            onClick={onClose}
            disabled={isGenerating}
          >
            Cancel
          </Button>
          {!hasGeneratedTracks && (
            <Button
              onClick={mode === "generate" ? handleGenerate : handleRemix}
              disabled={mode === "generate" ? !canGenerate : (!uploadedUrl || isUploading)}
              className="bg-primary hover:bg-primary/90"
            >
              {isGenerating || isUploading ? (
                <>
                  <Loader2 className="w-4 h-4 mr-2 animate-spin" />
                  Processing...
                </>
              ) : (
                <>
                  <Sparkles className="w-4 h-4 mr-2" />
                  {mode === "generate" ? "Generate Music" : `Create ${remixAction === "cover" ? "Cover" : remixAction === "vocals" ? "Vocals" : "Instrumental"}`}
                </>
              )}
            </Button>
          )}
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
}

export default MusicGeneratorModal;
````

## File: packages/frontend/components/PipelineProgress.tsx
````typescript
/**
 * Pipeline Progress Component
 *
 * Displays real-time progress for multi-format pipeline execution.
 * Shows concurrent task progress, estimated time remaining, and cancellation UI.
 *
 * Requirements: 16.1, 16.2, 16.3, 16.4, 16.5
 */

import React, { useState, useEffect, useCallback } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import {
  Loader2,
  Check,
  X,
  AlertTriangle,
  Clock,
  Layers,
} from 'lucide-react';
import { cn } from '@/lib/utils';
import type { ExecutionProgress } from '@/services/parallelExecutionEngine';

export interface PipelineTask {
  id: string;
  name: string;
  type: 'research' | 'script' | 'visual' | 'audio' | 'assembly';
  status: 'queued' | 'in-progress' | 'completed' | 'failed' | 'cancelled';
  progress?: number; // 0-100 for individual task
}

export interface PipelineProgressProps {
  /** Current execution progress from the parallel engine */
  executionProgress: ExecutionProgress | null;
  /** Individual task details for concurrent display */
  tasks: PipelineTask[];
  /** Current phase name */
  currentPhase: string;
  /** Whether the pipeline is actively running */
  isRunning: boolean;
  /** Called when user clicks cancel */
  onCancel: () => void;
  /** Whether cancellation is in progress */
  isCancelling?: boolean;
  /** When true, only renders the task list and summary stats (no header or progress bar) */
  summaryOnly?: boolean;
  className?: string;
}

const TASK_TYPE_COLORS: Record<PipelineTask['type'], string> = {
  research: 'text-purple-400',
  script: 'text-cyan-400',
  visual: 'text-amber-400',
  audio: 'text-green-400',
  assembly: 'text-blue-400',
};

const TASK_TYPE_BG: Record<PipelineTask['type'], string> = {
  research: 'bg-purple-500/10',
  script: 'bg-cyan-500/10',
  visual: 'bg-amber-500/10',
  audio: 'bg-green-500/10',
  assembly: 'bg-blue-500/10',
};

function formatTimeRemaining(ms: number): string {
  if (ms <= 0) return '';
  const seconds = Math.ceil(ms / 1000);
  if (seconds < 60) return `${seconds}s remaining`;
  const minutes = Math.floor(seconds / 60);
  const remainingSeconds = seconds % 60;
  return `${minutes}m ${remainingSeconds}s remaining`;
}

export function PipelineProgress({
  executionProgress,
  tasks,
  currentPhase,
  isRunning,
  onCancel,
  isCancelling = false,
  summaryOnly = false,
  className,
}: PipelineProgressProps) {
  const [showCancelConfirm, setShowCancelConfirm] = useState(false);

  const overallProgress = executionProgress
    ? executionProgress.totalTasks > 0
      ? Math.round(
          ((executionProgress.completedTasks + executionProgress.failedTasks) /
            executionProgress.totalTasks) *
            100,
        )
      : 0
    : 0;

  const estimatedTime = executionProgress?.estimatedTimeRemaining ?? 0;

  const handleCancel = useCallback(() => {
    if (showCancelConfirm) {
      onCancel();
      setShowCancelConfirm(false);
    } else {
      setShowCancelConfirm(true);
    }
  }, [showCancelConfirm, onCancel]);

  // Auto-dismiss cancel confirmation after 5 seconds
  useEffect(() => {
    if (!showCancelConfirm) return;
    const timer = setTimeout(() => setShowCancelConfirm(false), 5000);
    return () => clearTimeout(timer);
  }, [showCancelConfirm]);

  const inProgressTasks = tasks.filter((t) => t.status === 'in-progress');
  const completedTasks = tasks.filter((t) => t.status === 'completed');
  const failedTasks = tasks.filter((t) => t.status === 'failed');

  return (
    <div
      className={cn('w-full max-w-2xl mx-auto', className)}
      role="status"
      aria-live="polite"
      aria-label={`Pipeline progress: ${overallProgress}% complete`}
    >
      {/* Header */}
      {!summaryOnly && (
        <div className="flex items-center justify-between mb-6">
          <div>
            <h2 className="text-lg font-medium text-zinc-100">
              {currentPhase || 'Pipeline'}
            </h2>
            <div className="flex items-center gap-3 mt-1">
              <span className="font-mono text-xs text-zinc-500">
                {executionProgress
                  ? `${executionProgress.completedTasks}/${executionProgress.totalTasks} tasks`
                  : 'Initializing...'}
              </span>
              {estimatedTime > 0 && (
                <span className="flex items-center gap-1 font-mono text-xs text-zinc-600">
                  <Clock className="w-3 h-3" />
                  {formatTimeRemaining(estimatedTime)}
                </span>
              )}
            </div>
          </div>

          {/* Cancel button */}
          {isRunning && (
            <div className="relative">
              <button
                type="button"
                onClick={handleCancel}
                disabled={isCancelling}
                className={cn(
                  'flex items-center gap-2 px-3 py-1.5 rounded-sm border text-xs font-mono transition-colors duration-200',
                  'disabled:opacity-50 disabled:cursor-not-allowed',
                  showCancelConfirm
                    ? 'bg-red-500/10 border-red-500/50 text-red-400'
                    : 'border-zinc-700 text-zinc-400 hover:border-zinc-500 hover:text-zinc-200',
                )}
              >
                {isCancelling ? (
                  <>
                    <Loader2 className="w-3 h-3 animate-spin" />
                    <span>Cancelling...</span>
                  </>
                ) : showCancelConfirm ? (
                  <>
                    <AlertTriangle className="w-3 h-3" />
                    <span>Confirm cancel?</span>
                  </>
                ) : (
                  <>
                    <X className="w-3 h-3" />
                    <span>Cancel</span>
                  </>
                )}
              </button>
            </div>
          )}
        </div>
      )}

      {/* Overall Progress Bar */}
      {!summaryOnly && (
        <div className="mb-6">
          <div className="flex justify-between text-xs mb-1.5">
            <span className="font-mono text-zinc-500">Overall</span>
            <span className="font-mono text-zinc-400">{overallProgress}%</span>
          </div>
          <div
            className="h-1.5 bg-zinc-900 rounded-sm overflow-hidden"
            role="progressbar"
            aria-valuenow={overallProgress}
            aria-valuemin={0}
            aria-valuemax={100}
          >
            <motion.div
              className="h-full bg-blue-500 rounded-sm"
              initial={{ width: 0 }}
              animate={{ width: `${overallProgress}%` }}
              transition={{ duration: 0.3, ease: 'easeOut' }}
            />
          </div>
        </div>
      )}

      {/* Concurrent Task List */}
      {tasks.length > 0 && (
        <div className="space-y-2">
          <div className="flex items-center gap-2 mb-3">
            <Layers className="w-3.5 h-3.5 text-zinc-500" />
            <span className="font-mono text-[11px] font-medium tracking-[0.15em] uppercase text-zinc-500">
              Tasks
            </span>
          </div>

          <div className="space-y-1.5">
            {tasks.map((task) => (
              <motion.div
                key={task.id}
                layout
                initial={{ opacity: 0, y: 4 }}
                animate={{ opacity: 1, y: 0 }}
                className={cn(
                  'flex items-center gap-3 px-3 py-2 rounded-sm border',
                  task.status === 'in-progress'
                    ? 'border-zinc-700 bg-zinc-900/80'
                    : task.status === 'completed'
                      ? 'border-zinc-800 bg-zinc-900/40'
                      : task.status === 'failed'
                        ? 'border-red-900/50 bg-red-950/20'
                        : 'border-zinc-800/50 bg-zinc-950/50',
                )}
              >
                {/* Status icon */}
                <div className="shrink-0">
                  {task.status === 'in-progress' ? (
                    <Loader2
                      className={cn(
                        'w-3.5 h-3.5 animate-spin',
                        TASK_TYPE_COLORS[task.type],
                      )}
                    />
                  ) : task.status === 'completed' ? (
                    <Check className="w-3.5 h-3.5 text-emerald-400" />
                  ) : task.status === 'failed' ? (
                    <X className="w-3.5 h-3.5 text-red-400" />
                  ) : task.status === 'cancelled' ? (
                    <X className="w-3.5 h-3.5 text-zinc-600" />
                  ) : (
                    <div className="w-3.5 h-3.5 rounded-full border border-zinc-700" />
                  )}
                </div>

                {/* Task info */}
                <div className="flex-1 min-w-0">
                  <div className="flex items-center gap-2">
                    <span
                      className={cn(
                        'text-[13px] font-medium truncate',
                        task.status === 'completed'
                          ? 'text-zinc-400'
                          : task.status === 'failed'
                            ? 'text-red-400'
                            : task.status === 'in-progress'
                              ? 'text-zinc-200'
                              : 'text-zinc-600',
                      )}
                    >
                      {task.name}
                    </span>
                    <span
                      className={cn(
                        'text-[10px] font-mono px-1.5 py-0.5 rounded-sm',
                        TASK_TYPE_BG[task.type],
                        TASK_TYPE_COLORS[task.type],
                      )}
                    >
                      {task.type}
                    </span>
                  </div>
                </div>

                {/* Task progress */}
                {task.status === 'in-progress' && task.progress != null && (
                  <span className="font-mono text-[10px] text-zinc-500 shrink-0">
                    {task.progress}%
                  </span>
                )}
              </motion.div>
            ))}
          </div>
        </div>
      )}

      {/* Summary stats */}
      {(completedTasks.length > 0 || failedTasks.length > 0) && (
        <div className="flex items-center gap-4 mt-4 pt-4 border-t border-zinc-800">
          {completedTasks.length > 0 && (
            <span className="flex items-center gap-1.5 text-xs font-mono text-emerald-400">
              <Check className="w-3 h-3" />
              {completedTasks.length} completed
            </span>
          )}
          {inProgressTasks.length > 0 && (
            <span className="flex items-center gap-1.5 text-xs font-mono text-blue-400">
              <Loader2 className="w-3 h-3 animate-spin" />
              {inProgressTasks.length} running
            </span>
          )}
          {failedTasks.length > 0 && (
            <span className="flex items-center gap-1.5 text-xs font-mono text-red-400">
              <X className="w-3 h-3" />
              {failedTasks.length} failed
            </span>
          )}
        </div>
      )}
    </div>
  );
}

export default PipelineProgress;
````

## File: packages/frontend/components/projects/ProjectCard.tsx
````typescript
/**
 * Project Card Component
 *
 * Displays a project with thumbnail, title, metadata, and quick actions.
 * Uses the cinematic design system tokens for consistent visual language.
 */

import React from 'react';
import { motion } from 'framer-motion';
import {
  Video,
  Music,
  AudioWaveform,
  MoreVertical,
  Trash2,
  Star,
  Edit3,
  Download,
  Clock,
  Image as ImageIcon,
  Mic,
  Film,
} from 'lucide-react';
import { useNavigate } from 'react-router-dom';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/i18n/useLanguage';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuSeparator,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import type { Project, ProjectType } from '@/services/projectService';

interface ProjectCardProps {
  project: Project;
  onDelete: (projectId: string) => void;
  onToggleFavorite: (projectId: string) => void;
  onExport?: (projectId: string) => void;
}

// Type-to-icon mapping
const TYPE_ICONS: Record<ProjectType, typeof Video> = {
  production: Video,
  story: Film,
  visualizer: AudioWaveform,
};

// Type-to-gradient mapping using design tokens
const TYPE_GRADIENTS: Record<ProjectType, string> = {
  production: 'from-primary/60 to-primary/20',
  story: 'from-accent/60 to-accent/20',
  visualizer: 'from-ring/60 to-ring/20',
};

// Type-to-accent color for badges
const TYPE_ACCENT: Record<ProjectType, string> = {
  production: 'text-primary bg-primary/10 border-primary/20',
  story: 'text-accent bg-accent/10 border-accent/20',
  visualizer: 'text-ring bg-ring/10 border-ring/20',
};

// Type-to-route mapping
const TYPE_ROUTES: Record<ProjectType, string> = {
  production: '/studio?mode=video',
  story: '/studio?mode=story',
  visualizer: '/visualizer',
};

function formatDate(date: Date): string {
  const now = new Date();
  const diffMs = now.getTime() - date.getTime();
  const diffDays = Math.floor(diffMs / (1000 * 60 * 60 * 24));

  if (diffDays === 0) {
    const diffHours = Math.floor(diffMs / (1000 * 60 * 60));
    if (diffHours === 0) {
      const diffMins = Math.floor(diffMs / (1000 * 60));
      return diffMins <= 1 ? 'Just now' : `${diffMins}m ago`;
    }
    return `${diffHours}h ago`;
  }

  if (diffDays === 1) return 'Yesterday';
  if (diffDays < 7) return `${diffDays}d ago`;

  return date.toLocaleDateString(undefined, {
    month: 'short',
    day: 'numeric',
    year: date.getFullYear() !== now.getFullYear() ? 'numeric' : undefined,
  });
}

function formatDuration(seconds: number | undefined): string {
  if (!seconds) return '';
  const mins = Math.floor(seconds / 60);
  const secs = Math.floor(seconds % 60);
  return `${mins}:${secs.toString().padStart(2, '0')}`;
}

export function ProjectCard({
  project,
  onDelete,
  onToggleFavorite,
  onExport,
}: ProjectCardProps) {
  const navigate = useNavigate();
  const { t, isRTL } = useLanguage();

  const Icon = TYPE_ICONS[project.type];
  const gradientClass = TYPE_GRADIENTS[project.type];
  const accentClass = TYPE_ACCENT[project.type];

  const handleOpen = () => {
    const route = TYPE_ROUTES[project.type];
    // Append project ID to route
    const separator = route.includes('?') ? '&' : '?';
    navigate(`${route}${separator}projectId=${project.id}`);
  };

  const handleDelete = (e: React.MouseEvent) => {
    e.stopPropagation();
    if (window.confirm(t('projects.confirmDelete') || 'Delete this project?')) {
      onDelete(project.id);
    }
  };

  const handleToggleFavorite = (e: React.MouseEvent) => {
    e.stopPropagation();
    onToggleFavorite(project.id);
  };

  const handleExport = (e: React.MouseEvent) => {
    e.stopPropagation();
    onExport?.(project.id);
  };

  return (
    <motion.div
      layout
      initial={{ opacity: 0, scale: 0.95 }}
      animate={{ opacity: 1, scale: 1 }}
      exit={{ opacity: 0, scale: 0.95 }}
      whileHover={{ scale: 1.02, y: -4 }}
      whileTap={{ scale: 0.98 }}
      onClick={handleOpen}
      className={cn(
        'group relative cursor-pointer rounded-xl overflow-hidden',
        'bg-card hover:bg-card/80 border border-border hover:border-primary/30',
        'transition-all duration-300',
        'focus-within:ring-2 focus-within:ring-primary/50',
        'shadow-lg shadow-black/20 hover:shadow-xl hover:shadow-primary/5',
        isRTL && 'text-right'
      )}
    >
      {/* Hover glow effect */}
      <div className="absolute inset-0 opacity-0 group-hover:opacity-100 transition-opacity duration-500 pointer-events-none">
        <div
          className="absolute top-0 left-1/2 -translate-x-1/2 w-[80%] h-[1px]"
          style={{
            background: 'linear-gradient(90deg, transparent, oklch(0.70 0.15 190 / 0.4), transparent)',
          }}
        />
      </div>

      {/* Thumbnail Area */}
      <div className="relative aspect-video bg-gradient-to-br from-secondary to-muted overflow-hidden">
        {project.thumbnailUrl ? (
          <img
            src={project.thumbnailUrl}
            alt={project.title}
            className="w-full h-full object-cover"
          />
        ) : (
          <div
            className={cn(
              'w-full h-full flex items-center justify-center bg-gradient-to-br',
              gradientClass
            )}
          >
            <Icon className="w-12 h-12 text-foreground/30" />
          </div>
        )}

        {/* Duration Badge */}
        {project.duration && (
          <div className="absolute bottom-2 right-2 px-2 py-1 rounded-md bg-black/70 backdrop-blur-sm text-xs text-foreground font-code">
            {formatDuration(project.duration)}
          </div>
        )}

        {/* Favorite Star */}
        {project.isFavorite && (
          <div className="absolute top-2 right-2">
            <Star className="w-5 h-5 text-accent fill-accent drop-shadow-lg" />
          </div>
        )}

        {/* Type Badge */}
        <div
          className={cn(
            'absolute top-2 left-2 px-2.5 py-1 rounded-full text-[10px] font-code font-medium uppercase tracking-wider',
            'border backdrop-blur-sm capitalize',
            accentClass
          )}
        >
          {project.type}
        </div>
      </div>

      {/* Content Area */}
      <div className="p-4">
        {/* Title */}
        <h3 className="font-editorial font-semibold text-foreground truncate mb-1">{project.title}</h3>

        {/* Topic/Description */}
        {project.topic && (
          <p className="text-sm text-muted-foreground truncate mb-2.5">{project.topic}</p>
        )}

        {/* Metadata Row */}
        <div
          className={cn(
            'flex items-center gap-3 text-xs text-muted-foreground',
            isRTL && 'flex-row-reverse'
          )}
        >
          {/* Last Updated */}
          <span className="flex items-center gap-1 font-code">
            <Clock className="w-3 h-3" />
            {formatDate(project.updatedAt)}
          </span>

          {/* Progress Indicators */}
          <div className="flex items-center gap-1.5">
            {project.hasVisuals && (
              <span title="Has visuals"><ImageIcon className="w-3 h-3 text-primary/70" /></span>
            )}
            {project.hasNarration && (
              <span title="Has narration"><Mic className="w-3 h-3 text-ring/70" /></span>
            )}
            {project.hasMusic && (
              <span title="Has music"><Music className="w-3 h-3 text-accent/70" /></span>
            )}
            {project.hasExport && (
              <span title="Has export"><Download className="w-3 h-3 text-primary/70" /></span>
            )}
          </div>
        </div>

        {/* Status Badge */}
        {project.status !== 'draft' && (
          <div className="mt-2.5">
            <span
              className={cn(
                'px-2 py-0.5 rounded-full text-xs font-code',
                project.status === 'completed' && 'bg-primary/15 text-primary',
                project.status === 'in_progress' && 'bg-ring/15 text-ring',
                project.status === 'archived' && 'bg-muted text-muted-foreground'
              )}
            >
              {project.status.replace('_', ' ')}
            </span>
          </div>
        )}
      </div>

      {/* Actions Menu */}
      <div
        className={cn(
          'absolute top-2 opacity-0 group-hover:opacity-100 transition-opacity',
          isRTL ? 'left-2' : 'right-2'
        )}
        style={{ top: 'calc(100% - 40px)' }}
      >
        <DropdownMenu>
          <DropdownMenuTrigger asChild>
            <button
              onClick={(e) => e.stopPropagation()}
              className="p-1.5 rounded-lg bg-black/50 hover:bg-black/70 backdrop-blur-sm transition-colors"
              aria-label="Project actions"
            >
              <MoreVertical className="w-4 h-4 text-foreground/70" />
            </button>
          </DropdownMenuTrigger>
          <DropdownMenuContent align="end" className="w-48">
            <DropdownMenuItem onClick={handleOpen}>
              <Edit3 className="w-4 h-4 mr-2" />
              {t('projects.open') || 'Open'}
            </DropdownMenuItem>
            <DropdownMenuItem onClick={handleToggleFavorite}>
              <Star
                className={cn(
                  'w-4 h-4 mr-2',
                  project.isFavorite && 'fill-accent text-accent'
                )}
              />
              {project.isFavorite
                ? t('projects.unfavorite') || 'Remove from favorites'
                : t('projects.favorite') || 'Add to favorites'}
            </DropdownMenuItem>
            {project.hasExport && onExport && (
              <DropdownMenuItem onClick={handleExport}>
                <Download className="w-4 h-4 mr-2" />
                {t('projects.export') || 'Export'}
              </DropdownMenuItem>
            )}
            <DropdownMenuSeparator />
            <DropdownMenuItem
              onClick={handleDelete}
              className="text-destructive focus:text-destructive"
            >
              <Trash2 className="w-4 h-4 mr-2" />
              {t('projects.delete') || 'Delete'}
            </DropdownMenuItem>
          </DropdownMenuContent>
        </DropdownMenu>
      </div>
    </motion.div>
  );
}
````

## File: packages/frontend/components/QualityDashboard.tsx
````typescript
/**
 * QualityDashboard Component
 * 
 * Displays detailed quality metrics and insights for video production.
 * Helps users understand and improve AI output quality.
 */

import React, { useMemo } from "react";
import { motion, AnimatePresence } from "framer-motion";
import {
  BarChart3,
  TrendingUp,
  TrendingDown,
  Minus,
  AlertTriangle,
  CheckCircle2,
  Lightbulb,
  Clock,
  FileText,
  Image as ImageIcon,
  Volume2,
  Music,
  ChevronDown,
  ChevronUp,
  X,
} from "lucide-react";
import { cn } from "@/lib/utils";
import { Button } from "@/components/ui/button";
import {
  ProductionQualityReport,
  SceneQualityMetrics,
  getHistoricalAverages,
} from "@/services/qualityMonitorService";

interface QualityDashboardProps {
  report: ProductionQualityReport;
  isOpen: boolean;
  onClose: () => void;
}

// Score color helper
function getScoreColor(score: number): string {
  if (score >= 80) return "text-green-400";
  if (score >= 60) return "text-yellow-400";
  return "text-red-400";
}

function getScoreBg(score: number): string {
  if (score >= 80) return "bg-green-500/20 border-green-500/30";
  if (score >= 60) return "bg-yellow-500/20 border-yellow-500/30";
  return "bg-red-500/20 border-red-500/30";
}

// Quality badge component
const QualityBadge = React.memo(({ quality }: { quality: "poor" | "fair" | "good" | "excellent" }) => {
  const config = {
    poor: { color: "bg-red-500/20 text-red-400", label: "Poor" },
    fair: { color: "bg-yellow-500/20 text-yellow-400", label: "Fair" },
    good: { color: "bg-blue-500/20 text-blue-400", label: "Good" },
    excellent: { color: "bg-green-500/20 text-green-400", label: "Excellent" },
  };

  return (
    <span className={cn("px-2 py-0.5 rounded text-xs font-medium", config[quality].color)}>
      {config[quality].label}
    </span>
  );
});
QualityBadge.displayName = "QualityBadge";

// Score ring component
const ScoreRing = React.memo(({ score, label, size = "md" }: { score: number; label: string; size?: "sm" | "md" | "lg" }) => {
  const sizeConfig = {
    sm: { ring: 40, stroke: 4, text: "text-sm", label: "text-xs" },
    md: { ring: 60, stroke: 5, text: "text-xl", label: "text-xs" },
    lg: { ring: 80, stroke: 6, text: "text-2xl", label: "text-sm" },
  };

  const config = sizeConfig[size];
  const radius = (config.ring - config.stroke) / 2;
  const circumference = radius * 2 * Math.PI;
  const offset = circumference - (score / 100) * circumference;

  return (
    <div className="flex flex-col items-center gap-1">
      <div className="relative" style={{ width: config.ring, height: config.ring }}>
        <svg className="transform -rotate-90" width={config.ring} height={config.ring}>
          <circle
            cx={config.ring / 2}
            cy={config.ring / 2}
            r={radius}
            stroke="currentColor"
            strokeWidth={config.stroke}
            fill="none"
            className="text-white/10"
          />
          <circle
            cx={config.ring / 2}
            cy={config.ring / 2}
            r={radius}
            stroke="currentColor"
            strokeWidth={config.stroke}
            fill="none"
            strokeDasharray={circumference}
            strokeDashoffset={offset}
            strokeLinecap="round"
            className={getScoreColor(score)}
          />
        </svg>
        <div className="absolute inset-0 flex items-center justify-center">
          <span className={cn("font-bold", config.text, getScoreColor(score))}>{score}</span>
        </div>
      </div>
      <span className={cn("text-slate-400", config.label)}>{label}</span>
    </div>
  );
});
ScoreRing.displayName = "ScoreRing";

export function QualityDashboard({ report, isOpen, onClose }: QualityDashboardProps) {
  const [expandedScene, setExpandedScene] = React.useState<string | null>(null);
  const historicalAverages = useMemo(() => getHistoricalAverages(), []);
  
  if (!isOpen) return null;
  
  return (
    <AnimatePresence>
      <motion.div
        initial={{ opacity: 0 }}
        animate={{ opacity: 1 }}
        exit={{ opacity: 0 }}
        className="fixed inset-0 bg-black/80 backdrop-blur-sm z-50 flex items-center justify-center p-4"
        onClick={onClose}
      >
        <motion.div
          initial={{ scale: 0.95, opacity: 0 }}
          animate={{ scale: 1, opacity: 1 }}
          exit={{ scale: 0.95, opacity: 0 }}
          className="bg-[#12121a] border border-white/10 rounded-xl max-w-4xl w-full max-h-[90vh] overflow-hidden"
          onClick={(e) => e.stopPropagation()}
        >
          {/* Header */}
          <div className="p-4 border-b border-white/10 flex items-center justify-between">
            <div className="flex items-center gap-3">
              <BarChart3 className="w-5 h-5 text-cyan-400" />
              <div>
                <h2 className="font-semibold">Quality Report</h2>
                <p className="text-xs text-slate-400">{report.title}</p>
              </div>
            </div>
            <Button variant="ghost" size="sm" onClick={onClose}>
              <X className="w-4 h-4" />
            </Button>
          </div>
          
          <div className="overflow-y-auto max-h-[calc(90vh-60px)] p-4 space-y-6">
            {/* Overall Scores */}
            <div className="grid grid-cols-5 gap-4">
              <div className="col-span-1 flex justify-center">
                <ScoreRing score={report.overallScore} label="Overall" size="lg" />
              </div>
              <div className="col-span-4 grid grid-cols-4 gap-4">
                <ScoreRing score={report.contentScore} label="Content" />
                <ScoreRing score={report.timingScore} label="Timing" />
                <ScoreRing score={report.visualScore} label="Visual" />
                <ScoreRing score={report.audioScore} label="Audio" />
              </div>
            </div>
            
            {/* Historical Trend */}
            {historicalAverages && (
              <div className={cn(
                "p-3 rounded-lg border flex items-center justify-between",
                historicalAverages.trend === "improving" ? "bg-green-500/10 border-green-500/20" :
                historicalAverages.trend === "declining" ? "bg-red-500/10 border-red-500/20" :
                "bg-slate-500/10 border-slate-500/20"
              )}>
                <div className="flex items-center gap-2">
                  {historicalAverages.trend === "improving" && <TrendingUp className="w-4 h-4 text-green-400" />}
                  {historicalAverages.trend === "declining" && <TrendingDown className="w-4 h-4 text-red-400" />}
                  {historicalAverages.trend === "stable" && <Minus className="w-4 h-4 text-slate-400" />}
                  <span className="text-sm">
                    Quality trend: <span className="font-medium capitalize">{historicalAverages.trend}</span>
                  </span>
                </div>
                <span className="text-xs text-slate-400">
                  Avg: {Math.round(historicalAverages.avgOverall)}/100
                </span>
              </div>
            )}
            
            {/* Strengths & Weaknesses */}
            <div className="grid grid-cols-2 gap-4">
              {/* Strengths */}
              <div className="p-4 rounded-lg bg-green-500/10 border border-green-500/20">
                <div className="flex items-center gap-2 mb-3">
                  <CheckCircle2 className="w-4 h-4 text-green-400" />
                  <span className="font-medium text-green-400">Strengths</span>
                </div>
                {report.strengths.length > 0 ? (
                  <ul className="space-y-1">
                    {report.strengths.map((s) => (
                      <li key={s} className="text-sm text-slate-300 flex items-start gap-2">
                        <span className="text-green-400 mt-1">•</span>
                        {s}
                      </li>
                    ))}
                  </ul>
                ) : (
                  <p className="text-sm text-slate-400">No notable strengths identified</p>
                )}
              </div>
              
              {/* Weaknesses */}
              <div className="p-4 rounded-lg bg-red-500/10 border border-red-500/20">
                <div className="flex items-center gap-2 mb-3">
                  <AlertTriangle className="w-4 h-4 text-red-400" />
                  <span className="font-medium text-red-400">Areas to Improve</span>
                </div>
                {report.weaknesses.length > 0 ? (
                  <ul className="space-y-1">
                    {report.weaknesses.map((w) => (
                      <li key={w} className="text-sm text-slate-300 flex items-start gap-2">
                        <span className="text-red-400 mt-1">•</span>
                        {w}
                      </li>
                    ))}
                  </ul>
                ) : (
                  <p className="text-sm text-slate-400">No major issues found</p>
                )}
              </div>
            </div>
            
            {/* Actionable Improvements */}
            {report.actionableImprovements.length > 0 && (
              <div className="p-4 rounded-lg bg-cyan-500/10 border border-cyan-500/20">
                <div className="flex items-center gap-2 mb-3">
                  <Lightbulb className="w-4 h-4 text-cyan-400" />
                  <span className="font-medium text-cyan-400">Suggestions to Improve</span>
                </div>
                <ul className="space-y-2">
                  {report.actionableImprovements.map((imp, i) => (
                    <li key={imp} className="text-sm text-slate-300 flex items-start gap-2">
                      <span className="text-cyan-400 font-bold">{i + 1}.</span>
                      {imp}
                    </li>
                  ))}
                </ul>
              </div>
            )}
            
            {/* Technical Metrics */}
            <div className="grid grid-cols-4 gap-3">
              <div className="p-3 rounded-lg bg-white/5 border border-white/10 text-center">
                <Clock className="w-4 h-4 mx-auto mb-1 text-slate-400" />
                <div className="text-lg font-bold">{report.avgWordsPerSecond.toFixed(1)}</div>
                <div className="text-xs text-slate-400">Words/sec</div>
              </div>
              <div className="p-3 rounded-lg bg-white/5 border border-white/10 text-center">
                <FileText className="w-4 h-4 mx-auto mb-1 text-slate-400" />
                <div className="text-lg font-bold">{report.avgSceneDuration.toFixed(0)}s</div>
                <div className="text-xs text-slate-400">Avg Scene</div>
              </div>
              <div className="p-3 rounded-lg bg-white/5 border border-white/10 text-center">
                <Volume2 className="w-4 h-4 mx-auto mb-1 text-slate-400" />
                <div className="text-lg font-bold">{Math.round(report.audioCoverage)}%</div>
                <div className="text-xs text-slate-400">Audio Coverage</div>
              </div>
              <div className="p-3 rounded-lg bg-white/5 border border-white/10 text-center">
                <Music className="w-4 h-4 mx-auto mb-1 text-slate-400" />
                <div className="text-lg font-bold">{Math.round(report.sfxCoverage)}%</div>
                <div className="text-xs text-slate-400">SFX Coverage</div>
              </div>
            </div>
            
            {/* Per-Scene Breakdown */}
            <div>
              <h3 className="font-medium mb-3 flex items-center gap-2">
                <ImageIcon className="w-4 h-4 text-slate-400" />
                Scene-by-Scene Analysis
              </h3>
              <div className="space-y-2">
                {report.sceneMetrics.map((scene, index) => (
                  <div
                    key={scene.sceneId}
                    className="rounded-lg border border-white/10 bg-white/5 overflow-hidden"
                  >
                    <button
                      onClick={() => setExpandedScene(expandedScene === scene.sceneId ? null : scene.sceneId)}
                      className="w-full p-3 flex items-center justify-between hover:bg-white/5 transition-colors"
                    >
                      <div className="flex items-center gap-3">
                        <span className="text-xs text-slate-500 w-6">{index + 1}</span>
                        <span className="font-medium text-sm">{scene.sceneName}</span>
                        <QualityBadge quality={scene.visualDescriptionQuality} />
                        {scene.issues.length > 0 && (
                          <span className="text-xs text-yellow-400 flex items-center gap-1">
                            <AlertTriangle className="w-3 h-3" />
                            {scene.issues.length}
                          </span>
                        )}
                      </div>
                      <div className="flex items-center gap-4">
                        <span className="text-xs text-slate-400">{scene.duration}s</span>
                        <span className={cn("text-sm font-medium", getScoreColor(scene.timingSync))}>
                          {scene.timingSync}%
                        </span>
                        {expandedScene === scene.sceneId ? (
                          <ChevronUp className="w-4 h-4 text-slate-400" />
                        ) : (
                          <ChevronDown className="w-4 h-4 text-slate-400" />
                        )}
                      </div>
                    </button>
                    
                    <AnimatePresence>
                      {expandedScene === scene.sceneId && (
                        <motion.div
                          initial={{ height: 0, opacity: 0 }}
                          animate={{ height: "auto", opacity: 1 }}
                          exit={{ height: 0, opacity: 0 }}
                          className="border-t border-white/10"
                        >
                          <div className="p-3 space-y-3">
                            {/* Scene metrics */}
                            <div className="grid grid-cols-4 gap-2 text-xs">
                              <div className="p-2 rounded bg-white/5">
                                <div className="text-slate-400">Visual</div>
                                <div className="font-medium">{scene.visualDescriptionLength} chars</div>
                              </div>
                              <div className="p-2 rounded bg-white/5">
                                <div className="text-slate-400">Narration</div>
                                <div className="font-medium">{scene.narrationWordCount} words</div>
                              </div>
                              <div className="p-2 rounded bg-white/5">
                                <div className="text-slate-400">Pacing</div>
                                <div className="font-medium">{scene.wordsPerSecond.toFixed(1)} w/s</div>
                              </div>
                              <div className="p-2 rounded bg-white/5">
                                <div className="text-slate-400">SFX</div>
                                <div className="font-medium flex items-center gap-1">
                                  {scene.hasSfx ? (
                                    <>
                                      <CheckCircle2 className="w-3 h-3 text-green-400" />
                                      {scene.hasAudioUrl ? "Loaded" : "Pending"}
                                    </>
                                  ) : (
                                    <span className="text-slate-500">None</span>
                                  )}
                                </div>
                              </div>
                            </div>
                            
                            {/* Issues */}
                            {scene.issues.length > 0 && (
                              <div className="p-2 rounded bg-yellow-500/10 border border-yellow-500/20">
                                <div className="text-xs text-yellow-400 font-medium mb-1">Issues:</div>
                                <ul className="text-xs text-slate-300 space-y-1">
                                  {scene.issues.map((issue) => (
                                    <li key={issue}>• {issue}</li>
                                  ))}
                                </ul>
                              </div>
                            )}
                          </div>
                        </motion.div>
                      )}
                    </AnimatePresence>
                  </div>
                ))}
              </div>
            </div>
            
            {/* AI Performance */}
            <div className="p-4 rounded-lg bg-purple-500/10 border border-purple-500/20">
              <h3 className="font-medium mb-2 text-purple-400">AI Performance</h3>
              <div className="grid grid-cols-2 gap-4 text-sm">
                <div>
                  <span className="text-slate-400">Content Creativity:</span>
                  <span className={cn(
                    "ml-2 font-medium capitalize",
                    report.contentPlannerCreativity === "high" ? "text-green-400" :
                    report.contentPlannerCreativity === "medium" ? "text-yellow-400" : "text-red-400"
                  )}>
                    {report.contentPlannerCreativity}
                  </span>
                </div>
                <div>
                  <span className="text-slate-400">SFX Suggestion Accuracy:</span>
                  <span className={cn(
                    "ml-2 font-medium",
                    report.aiSfxAccuracy >= 80 ? "text-green-400" :
                    report.aiSfxAccuracy >= 50 ? "text-yellow-400" : "text-red-400"
                  )}>
                    {Math.round(report.aiSfxAccuracy)}%
                  </span>
                </div>
              </div>
            </div>
          </div>
        </motion.div>
      </motion.div>
    </AnimatePresence>
  );
}

export default QualityDashboard;
````

## File: packages/frontend/components/QuickExport.tsx
````typescript
/**
 * QuickExport Component
 * 
 * One-click export with visual presets instead of technical options.
 * Users pick a platform, we handle the codec details.
 */

import React, { useState } from "react";
import { motion, AnimatePresence } from "framer-motion";
import {
  Download,
  Smartphone,
  Monitor,
  Youtube,
  Instagram,
  Film,
  Check,
  Loader2,
  X,
  Sparkles,
} from "lucide-react";
import { Button } from "@/components/ui/button";
import { cn } from "@/lib/utils";

interface ExportPreset {
  id: string;
  name: string;
  description: string;
  icon: React.ReactNode;
  resolution: string;
  format: string;
  color: string;
  // Export config
  width: number;
  height: number;
  orientation: "landscape" | "portrait";
  quality: "draft" | "standard" | "high";
}

const EXPORT_PRESETS: ExportPreset[] = [
  {
    id: "social",
    name: "Social Media",
    description: "TikTok, Reels, Shorts",
    icon: <Smartphone className="w-5 h-5 sm:w-6 sm:h-6" />,
    resolution: "1080p",
    format: "MP4 (H.264)",
    color: "from-pink-500 to-rose-500",
    width: 1080,
    height: 1920,
    orientation: "portrait",
    quality: "standard",
  },
  {
    id: "youtube",
    name: "YouTube",
    description: "Full HD, optimized",
    icon: <Youtube className="w-5 h-5 sm:w-6 sm:h-6" />,
    resolution: "1080p",
    format: "MP4 (H.264)",
    color: "from-red-500 to-red-600",
    width: 1920,
    height: 1080,
    orientation: "landscape",
    quality: "high",
  },
  {
    id: "4k",
    name: "4K Master",
    description: "Highest quality",
    icon: <Monitor className="w-5 h-5 sm:w-6 sm:h-6" />,
    resolution: "4K",
    format: "MP4 (H.265)",
    color: "from-purple-500 to-violet-500",
    width: 3840,
    height: 2160,
    orientation: "landscape",
    quality: "high",
  },
  {
    id: "fast",
    name: "Quick Export",
    description: "Fast rendering",
    icon: <Film className="w-5 h-5 sm:w-6 sm:h-6" />,
    resolution: "720p",
    format: "MP4",
    color: "from-slate-500 to-slate-600",
    width: 1280,
    height: 720,
    orientation: "landscape",
    quality: "draft",
  },
];

/** Export configuration returned to parent */
export interface QuickExportConfig {
  presetId: string;
  width: number;
  height: number;
  orientation: "landscape" | "portrait";
  quality: "draft" | "standard" | "high";
}

interface QuickExportProps {
  isOpen: boolean;
  onClose: () => void;
  onExport: (config: QuickExportConfig, onProgress: (percent: number) => void) => Promise<void>;
  videoTitle?: string;
  duration?: number;
  className?: string;
}

export const QuickExport: React.FC<QuickExportProps> = ({
  isOpen,
  onClose,
  onExport,
  videoTitle = "Your Video",
  duration = 60,
  className,
}) => {
  const [selectedPreset, setSelectedPreset] = useState<string>("youtube");
  const [isExporting, setIsExporting] = useState(false);
  const [exportProgress, setExportProgress] = useState(0);
  const [exportComplete, setExportComplete] = useState(false);
  const [exportError, setExportError] = useState<string | null>(null);

  const handleExport = async () => {
    const preset = EXPORT_PRESETS.find(p => p.id === selectedPreset);
    if (!preset) return;

    setIsExporting(true);
    setExportProgress(0);
    setExportError(null);

    // Progress callback for real-time updates from the export process
    const handleProgress = (percent: number) => {
      setExportProgress(Math.min(99, percent)); // Cap at 99 until complete
    };

    try {
      await onExport(
        {
          presetId: preset.id,
          width: preset.width,
          height: preset.height,
          orientation: preset.orientation,
          quality: preset.quality,
        },
        handleProgress
      );

      setExportProgress(100);
      setExportComplete(true);
    } catch (error: any) {
      console.error("Export failed:", error);
      setExportError(error.message || "Export failed. Please try again.");
      setIsExporting(false);
    }
  };

  const handleReset = () => {
    setIsExporting(false);
    setExportProgress(0);
    setExportComplete(false);
    setExportError(null);
  };

  const formatDuration = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return `${mins}:${secs.toString().padStart(2, "0")}`;
  };

  if (!isOpen) return null;

  return (
    <AnimatePresence>
      <motion.div
        initial={{ opacity: 0 }}
        animate={{ opacity: 1 }}
        exit={{ opacity: 0 }}
        className="fixed inset-0 z-50 flex items-end sm:items-center justify-center p-0 sm:p-4 bg-black/60 backdrop-blur-sm"
        onClick={onClose}
      >
        <motion.div
          initial={{ scale: 0.95, opacity: 0, y: 100 }}
          animate={{ scale: 1, opacity: 1, y: 0 }}
          exit={{ scale: 0.95, opacity: 0, y: 100 }}
          onClick={(e) => e.stopPropagation()}
          className={cn(
            "w-full sm:max-w-lg bg-card border border-border/50 rounded-t-3xl sm:rounded-3xl shadow-2xl overflow-hidden max-h-[90vh] overflow-y-auto",
            className
          )}
        >
          {/* Header */}
          <div className="p-4 sm:p-6 border-b border-border/30 sticky top-0 bg-card z-10">
            <div className="flex items-center justify-between">
              <div>
                <h2 className="text-lg sm:text-xl font-bold">Export Video</h2>
                <p className="text-xs sm:text-sm text-muted-foreground mt-1">
                  {videoTitle} • {formatDuration(duration)}
                </p>
              </div>
              <button
                onClick={onClose}
                className="p-2 hover:bg-muted/30 rounded-xl transition-colors"
              >
                <X className="w-5 h-5 text-muted-foreground" />
              </button>
            </div>
          </div>

          {/* Content */}
          <div className="p-4 sm:p-6">
            {exportError ? (
              /* Error State */
              <div className="py-6 sm:py-8 text-center">
                <div className="w-14 h-14 sm:w-16 sm:h-16 mx-auto mb-4 sm:mb-6 rounded-full bg-red-500/20 flex items-center justify-center">
                  <X className="w-7 h-7 sm:w-8 sm:h-8 text-red-500" />
                </div>

                <h3 className="text-base sm:text-lg font-semibold mb-2">Export Failed</h3>
                <p className="text-xs sm:text-sm text-muted-foreground mb-4 sm:mb-6 px-4">
                  {exportError}
                </p>

                <div className="flex gap-2 justify-center">
                  <Button
                    onClick={handleReset}
                    variant="outline"
                    className="rounded-xl"
                  >
                    Try Again
                  </Button>
                  <Button
                    onClick={onClose}
                    variant="ghost"
                    className="rounded-xl"
                  >
                    Close
                  </Button>
                </div>
              </div>
            ) : !isExporting && !exportComplete ? (
              <>
                {/* Preset Grid */}
                <div className="grid grid-cols-2 gap-2 sm:gap-3 mb-4 sm:mb-6">
                  {EXPORT_PRESETS.map((preset) => (
                    <button
                      key={preset.id}
                      onClick={() => setSelectedPreset(preset.id)}
                      className={cn(
                        "relative p-3 sm:p-4 rounded-xl sm:rounded-2xl border-2 transition-all text-left group",
                        selectedPreset === preset.id
                          ? "border-primary bg-primary/5"
                          : "border-border/30 hover:border-border/50 hover:bg-muted/20"
                      )}
                    >
                      {/* Gradient Background */}
                      <div
                        className={cn(
                          "absolute inset-0 rounded-xl sm:rounded-2xl opacity-0 transition-opacity bg-gradient-to-br",
                          preset.color,
                          selectedPreset === preset.id && "opacity-10"
                        )}
                      />

                      <div className="relative">
                        <div
                          className={cn(
                            "w-10 h-10 sm:w-12 sm:h-12 rounded-lg sm:rounded-xl flex items-center justify-center mb-2 sm:mb-3 transition-colors",
                            selectedPreset === preset.id
                              ? "bg-primary/20 text-primary"
                              : "bg-muted/30 text-muted-foreground group-hover:text-foreground"
                          )}
                        >
                          {preset.icon}
                        </div>

                        <h3 className="font-semibold text-sm sm:text-base mb-0.5 sm:mb-1">{preset.name}</h3>
                        <p className="text-[10px] sm:text-xs text-muted-foreground">
                          {preset.description}
                        </p>

                        <div className="mt-2 sm:mt-3 flex items-center gap-1 sm:gap-2 text-[10px] sm:text-xs">
                          <span className="px-1.5 sm:px-2 py-0.5 rounded-full bg-muted/30">
                            {preset.resolution}
                          </span>
                          <span className="text-muted-foreground hidden sm:inline">
                            {preset.format}
                          </span>
                        </div>
                      </div>

                      {/* Selected Check */}
                      {selectedPreset === preset.id && (
                        <div className="absolute top-2 sm:top-3 right-2 sm:right-3 w-5 h-5 sm:w-6 sm:h-6 rounded-full bg-primary flex items-center justify-center">
                          <Check className="w-3 h-3 sm:w-4 sm:h-4 text-primary-foreground" />
                        </div>
                      )}
                    </button>
                  ))}
                </div>

                {/* Export Button */}
                <Button
                  onClick={handleExport}
                  size="lg"
                  className="w-full h-12 sm:h-14 rounded-xl sm:rounded-2xl font-bold text-base sm:text-lg bg-gradient-to-r from-primary to-purple-500 hover:from-primary/90 hover:to-purple-500/90"
                >
                  <Download className="w-4 h-4 sm:w-5 sm:h-5 mr-2" />
                  Export Now
                </Button>
              </>
            ) : isExporting && !exportComplete ? (
              /* Exporting State */
              <div className="py-6 sm:py-8 text-center">
                <motion.div
                  animate={{ rotate: 360 }}
                  transition={{ duration: 2, repeat: Infinity, ease: "linear" }}
                  className="w-14 h-14 sm:w-16 sm:h-16 mx-auto mb-4 sm:mb-6 rounded-full border-4 border-primary/20 border-t-primary"
                />

                <h3 className="text-base sm:text-lg font-semibold mb-2">Exporting...</h3>
                <p className="text-xs sm:text-sm text-muted-foreground mb-4 sm:mb-6">
                  This may take a few minutes
                </p>

                {/* Progress Bar */}
                <div className="h-2 bg-muted/30 rounded-full overflow-hidden mb-2">
                  <motion.div
                    className="h-full bg-gradient-to-r from-primary to-purple-500"
                    initial={{ width: 0 }}
                    animate={{ width: `${exportProgress}%` }}
                  />
                </div>
                <span className="text-xs sm:text-sm font-mono text-muted-foreground">
                  {Math.round(exportProgress)}%
                </span>
              </div>
            ) : (
              /* Complete State */
              <div className="py-6 sm:py-8 text-center">
                <motion.div
                  initial={{ scale: 0 }}
                  animate={{ scale: 1 }}
                  className="w-14 h-14 sm:w-16 sm:h-16 mx-auto mb-4 sm:mb-6 rounded-full bg-green-500/20 flex items-center justify-center"
                >
                  <Check className="w-7 h-7 sm:w-8 sm:h-8 text-green-500" />
                </motion.div>

                <h3 className="text-base sm:text-lg font-semibold mb-2">Export Complete!</h3>
                <p className="text-xs sm:text-sm text-muted-foreground mb-4 sm:mb-6">
                  Your video has been downloaded
                </p>

                <div className="flex gap-2 justify-center">
                  <Button
                    onClick={() => {
                      handleReset();
                    }}
                    variant="outline"
                    className="rounded-xl"
                  >
                    Export Another
                  </Button>
                  <Button
                    onClick={onClose}
                    className="rounded-xl"
                  >
                    Done
                  </Button>
                </div>
              </div>
            )}
          </div>
        </motion.div>
      </motion.div>
    </AnimatePresence>
  );
};

export default QuickExport;
````

## File: packages/frontend/components/QuickUpload.tsx
````typescript
import React, { useState, useCallback } from "react";
import { motion, AnimatePresence } from "framer-motion";
import { Upload, Music, Monitor, Smartphone, Sparkles, ChevronRight, Youtube, Loader2, FileVideo } from "lucide-react";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { cn } from "@/lib/utils";
import { SERVER_URL } from "@/services/ffmpeg";
import { MusicGeneratorModal } from "./MusicGeneratorModal";
import { MusicChatModalV2 } from "./MusicChatModalV2";
import { isSunoConfigured } from "@/services/sunoService";
import type { SunoGeneratedTrack, SunoGenerationConfig, SunoTaskStatus } from "@/services/sunoService";
import {
  generateMusic as sunoGenerateMusic,
  waitForCompletion,
  generateLyrics as sunoGenerateLyrics,
  getLyricsStatus,
  getCredits,
} from "@/services/sunoService";

interface QuickUploadProps {
  onFileSelect: (file: File, aspectRatio: string) => void;
  onLoadDemo: (aspectRatio: string) => void;
  onSwitchToProduction?: () => void;
  disabled?: boolean;
}

export const QuickUpload: React.FC<QuickUploadProps> = ({
  onFileSelect,
  onLoadDemo,
  onSwitchToProduction,
  disabled = false,
}) => {
  const [aspectRatio, setAspectRatio] = useState<"16:9" | "9:16">("16:9");
  const [activeTab, setActiveTab] = useState<"upload" | "youtube" | "production" | "music">("upload");
  const [isDragging, setIsDragging] = useState(false);
  const [youtubeUrl, setYoutubeUrl] = useState("");
  const [isImporting, setIsImporting] = useState(false);
  const [showMusicModal, setShowMusicModal] = useState(false);
  const [showMusicChat, setShowMusicChat] = useState(false);

  // Music generation state
  const [musicState, setMusicState] = useState<{
    isGenerating: boolean;
    taskId: string | null;
    status: SunoTaskStatus | null;
    progress: number;
    generatedTracks: SunoGeneratedTrack[];
    selectedTrackId: string | null;
    lyrics: string | null;
    lyricsTaskId: string | null;
    credits: number | null;
    error: string | null;
  }>({
    isGenerating: false,
    taskId: null,
    status: null,
    progress: 0,
    generatedTracks: [],
    selectedTrackId: null,
    lyrics: null,
    lyricsTaskId: null,
    credits: null,
    error: null,
  });

  // Music generation handlers
  const handleGenerateMusic = useCallback(async (config: Partial<SunoGenerationConfig> & { prompt: string }) => {
    if (!isSunoConfigured()) {
      setMusicState(prev => ({
        ...prev,
        error: "Suno API key not configured. Add VITE_SUNO_API_KEY to .env.local",
      }));
      return;
    }

    setMusicState(prev => ({
      ...prev,
      isGenerating: true,
      taskId: null,
      status: "PENDING",
      progress: 0,
      generatedTracks: [],
      error: null,
    }));

    try {
      const taskId = await sunoGenerateMusic(config);
      setMusicState(prev => ({ ...prev, taskId, status: "PROCESSING" as SunoTaskStatus, progress: 25 }));
      const tracks = await waitForCompletion(taskId);
      setMusicState(prev => ({
        ...prev,
        isGenerating: false,
        status: "SUCCESS" as SunoTaskStatus,
        progress: 100,
        generatedTracks: tracks,
        selectedTrackId: tracks[0]?.id ?? null,
      }));
    } catch (err) {
      setMusicState(prev => ({
        ...prev,
        isGenerating: false,
        status: "FAILED",
        progress: 0,
        error: err instanceof Error ? err.message : String(err),
      }));
    }
  }, []);

  const handleGenerateLyrics = useCallback(async (prompt: string) => {
    if (!isSunoConfigured()) return;

    setMusicState(prev => ({ ...prev, lyricsTaskId: null, lyrics: null, error: null }));

    try {
      const taskId = await sunoGenerateLyrics(prompt);
      setMusicState(prev => ({ ...prev, lyricsTaskId: taskId }));

      const pollIntervalMs = 5000;
      const maxWaitMs = 2 * 60 * 1000;
      const startTime = Date.now();

      while (Date.now() - startTime < maxWaitMs) {
        const result = await getLyricsStatus(taskId);
        if (result.status === "SUCCESS" && result.text) {
          setMusicState(prev => ({ ...prev, lyrics: result.text || null }));
          return;
        }
        if (result.status === "FAILED") {
          throw new Error(result.errorMessage || "Lyrics generation failed");
        }
        await new Promise(resolve => setTimeout(resolve, pollIntervalMs));
      }
      throw new Error("Lyrics generation timed out");
    } catch (err) {
      setMusicState(prev => ({
        ...prev,
        error: err instanceof Error ? err.message : String(err),
      }));
    }
  }, []);

  const handleSelectTrack = useCallback((trackId: string) => {
    setMusicState(prev => ({ ...prev, selectedTrackId: trackId }));
  }, []);

  const handleAddToTimeline = useCallback(() => {
    const selectedTrack = musicState.generatedTracks.find(t => t.id === musicState.selectedTrackId);
    if (selectedTrack) {
      // Download the track or copy URL to clipboard
      window.open(selectedTrack.audio_url, '_blank');
    }
    setShowMusicModal(false);
  }, [musicState.generatedTracks, musicState.selectedTrackId]);

  const handleRefreshCredits = useCallback(async () => {
    if (!isSunoConfigured()) return;
    try {
      const result = await getCredits();
      // Only update if we got a valid credits value (not -1 for unknown)
      if (result.credits >= 0) {
        setMusicState(prev => ({ ...prev, credits: result.credits }));
      }
    } catch {
      // Silently fail - credits display is optional
      console.warn("[QuickUpload] Could not fetch Suno credits");
    }
  }, []);

  const handleDrop = useCallback(
    (e: React.DragEvent) => {
      e.preventDefault();
      setIsDragging(false);
      if (disabled) return;
      const file = e.dataTransfer.files[0];
      if (file && file.type.startsWith("audio/")) {
        onFileSelect(file, aspectRatio);
      }
    },
    [disabled, onFileSelect, aspectRatio]
  );

  const handleFileInput = useCallback(
    (e: React.ChangeEvent<HTMLInputElement>) => {
      const file = e.target.files?.[0];
      if (file) {
        onFileSelect(file, aspectRatio);
      }
    },
    [onFileSelect, aspectRatio]
  );

  const handleYoutubeImport = async () => {
    if (!youtubeUrl.trim() || isImporting) return;

    setIsImporting(true);
    try {
      const response = await fetch(`${SERVER_URL}/api/import/youtube`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ url: youtubeUrl }),
      });

      if (!response.ok) {
        throw new Error("Failed to import from YouTube");
      }

      const blob = await response.blob();
      const file = new File([blob], "youtube_audio.mp3", {
        type: "audio/mpeg",
      });
      onFileSelect(file, aspectRatio);
    } catch (error) {
      console.error(error);
      alert(
        "Failed to import YouTube video. Make sure the backend server is running (npm run server) and yt-dlp is installed.",
      );
    } finally {
      setIsImporting(false);
      setYoutubeUrl("");
    }
  };

  return (
    <motion.div
      initial={{ opacity: 0, y: 30 }}
      animate={{ opacity: 1, y: 0 }}
      transition={{ duration: 0.6, ease: "easeOut" }}
      className="max-w-2xl mx-auto mt-12 flex flex-col items-center px-4 relative z-10"
    >
      {/* Background Glows */}
      <div className="absolute top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 w-full h-full max-w-[600px] max-h-[600px] -z-10 pointer-events-none" aria-hidden="true">
        <div className="absolute top-0 right-0 w-64 h-64 bg-primary/20 rounded-full blur-[100px] opacity-50 animate-pulse-slow" />
        <div className="absolute bottom-0 left-0 w-64 h-64 bg-accent/20 rounded-full blur-[100px] opacity-50 animate-pulse-slow delay-1000" />
      </div>

      {/* Header */}
      <div className="text-center mb-12">
        <motion.div
          initial={{ scale: 0.9, opacity: 0 }}
          animate={{ scale: 1, opacity: 1 }}
          transition={{ delay: 0.2 }}
          className="inline-flex items-center justify-center p-3 mb-6 rounded-2xl bg-background/50 backdrop-blur-md border border-white/10 shadow-xl shadow-primary/5"
        >
          <Sparkles className="w-8 h-8 text-primary" aria-hidden="true" />
        </motion.div>
        <h1 className="text-5xl md:text-6xl font-bold mb-4 tracking-tight bg-clip-text text-transparent bg-linear-to-b from-foreground to-foreground/70">
          Create Your <span className="text-transparent bg-clip-text bg-linear-to-r from-primary via-purple-500 to-accent">Video</span>
        </h1>
        <p className="text-xl text-muted-foreground/80 max-w-lg mx-auto leading-relaxed">
          Transform your audio into cinematic visual storytelling with the power of AI.
        </p>
      </div>

      {/* Main Card */}
      <div className="w-full bg-card/40 backdrop-blur-xl border border-white/10 rounded-3xl shadow-2xl overflow-hidden ring-1 ring-white/5">

        {/* Navigation Tabs - Segmented Control */}
        <nav
          className="p-2 bg-black/20 m-2 rounded-2xl flex relative"
          role="tablist"
          aria-label="Video creation options"
        >
          <div className="absolute inset-0 rounded-2xl border border-white/5 pointer-events-none" aria-hidden="true" />
          {[
            { id: "upload", icon: Upload, label: "Upload File", color: "text-foreground" },
            { id: "youtube", icon: Youtube, label: "YouTube", color: "text-red-500" },
            { id: "music", icon: Music, label: "Song Generator", color: "text-cyan-400" },
            { id: "production", icon: FileVideo, label: "Studio Mode", color: "text-purple-400" }
          ].map((tab) => (
            <button
              key={tab.id}
              role="tab"
              aria-selected={activeTab === tab.id}
              aria-controls={`${tab.id}-panel`}
              id={`${tab.id}-tab`}
              onClick={() => setActiveTab(tab.id as any)}
              className={cn(
                "flex-1 py-3 text-sm font-semibold transition-all relative z-10 rounded-xl flex items-center justify-center gap-2",
                activeTab === tab.id ? "text-foreground shadow-sm" : "text-muted-foreground hover:text-foreground/80"
              )}
            >
              {activeTab === tab.id && (
                <motion.div
                  layoutId="active-tab"
                  className="absolute inset-0 bg-background/80 shadow-[0_0_20px_rgba(0,0,0,0.1)] rounded-xl -z-10"
                  transition={{ type: "spring", bounce: 0.2, duration: 0.6 }}
                />
              )}
              <tab.icon size={16} className={cn(activeTab === tab.id ? tab.color : "opacity-70")} aria-hidden="true" />
              <span>{tab.label}</span>
            </button>
          ))}
        </nav>

        {/* Content Area */}
        <div className="p-8 min-h-[340px] flex flex-col">

          {/* Aspect Ratio Config (Hidden in Studio Mode and Music) */}
          {activeTab !== "production" && activeTab !== "music" && (
            <div className="flex justify-center mb-10">
              <div
                className="inline-flex bg-muted/30 p-1.5 rounded-full border border-white/5"
                role="radiogroup"
                aria-label="Video aspect ratio"
              >
                {[
                  { id: "16:9", icon: Monitor, label: "Landscape" },
                  { id: "9:16", icon: Smartphone, label: "Portrait" }
                ].map((ratio) => (
                  <button
                    key={ratio.id}
                    role="radio"
                    aria-checked={aspectRatio === ratio.id}
                    aria-label={`${ratio.label} (${ratio.id})`}
                    onClick={() => setAspectRatio(ratio.id as any)}
                    className={cn(
                      "px-6 py-2 rounded-full text-xs font-semibold transition-all flex items-center gap-2",
                      aspectRatio === ratio.id
                        ? "bg-primary text-primary-foreground shadow-lg shadow-primary/25"
                        : "text-muted-foreground hover:text-foreground/80 hover:bg-white/5"
                    )}
                  >
                    <ratio.icon size={14} aria-hidden="true" /> {ratio.label}
                  </button>
                ))}
              </div>
            </div>
          )}

          <div className="flex-1 flex flex-col justify-center">
            <AnimatePresence mode="wait">
              {activeTab === "upload" && (
                <motion.div
                  key="upload"
                  id="upload-panel"
                  role="tabpanel"
                  aria-labelledby="upload-tab"
                  initial={{ opacity: 0, x: -20 }}
                  animate={{ opacity: 1, x: 0 }}
                  exit={{ opacity: 0, x: 20 }}
                  transition={{ duration: 0.3 }}
                  className="flex flex-col items-center"
                >
                  <button
                    type="button"
                    onDragOver={(e) => {
                      e.preventDefault();
                      setIsDragging(true);
                    }}
                    onDragLeave={() => setIsDragging(false)}
                    onDrop={handleDrop}
                    onClick={() => !disabled && document.getElementById("audio-input")?.click()}
                    disabled={disabled}
                    aria-label="Click to upload audio file or drag and drop. Supports MP3, WAV, M4A formats."
                    className={cn(
                      "w-full h-56 border-2 border-dashed rounded-2xl flex flex-col items-center justify-center cursor-pointer transition-all duration-300 group relative overflow-hidden",
                      isDragging
                        ? "border-primary bg-primary/10 shadow-[0_0_40px_rgba(var(--primary),0.2)]"
                        : "border-border/40 hover:border-primary/50 hover:bg-primary/5 hover:shadow-lg",
                      disabled && "opacity-50 cursor-not-allowed"
                    )}
                  >
                    <div className="absolute inset-0 bg-linear-to-br from-primary/5 to-transparent opacity-0 group-hover:opacity-100 transition-opacity duration-500" aria-hidden="true" />

                    <input
                      id="audio-input"
                      type="file"
                      accept="audio/*"
                      onChange={handleFileInput}
                      className="hidden"
                      disabled={disabled}
                      aria-label="Select audio file"
                    />

                    <div className={cn(
                      "w-20 h-20 rounded-full flex items-center justify-center mb-6 transition-all duration-300 shadow-md",
                      isDragging ? "bg-primary text-primary-foreground scale-110" : "bg-muted/50 text-muted-foreground group-hover:bg-primary/20 group-hover:text-primary group-hover:scale-110"
                    )}>
                      <Upload size={32} aria-hidden="true" />
                    </div>

                    <h3 className="font-semibold text-lg mb-2 text-foreground/90">Click to upload or drag audio</h3>
                    <p className="text-sm text-muted-foreground/70">
                      Supports MP3, WAV, M4A
                    </p>
                  </button>
                </motion.div>
              )}

              {activeTab === "youtube" && (
                <motion.div
                  key="youtube"
                  id="youtube-panel"
                  role="tabpanel"
                  aria-labelledby="youtube-tab"
                  initial={{ opacity: 0, x: -20 }}
                  animate={{ opacity: 1, x: 0 }}
                  exit={{ opacity: 0, x: 20 }}
                  className="space-y-6 max-w-md mx-auto w-full py-4"
                >
                  <div className="relative group">
                    <div className="absolute inset-y-0 left-4 flex items-center pointer-events-none">
                      <Youtube className={cn("w-5 h-5 transition-colors", youtubeUrl ? "text-red-500" : "text-muted-foreground")} aria-hidden="true" />
                    </div>
                    <Input
                      type="text"
                      value={youtubeUrl}
                      onChange={(e) => setYoutubeUrl(e.target.value)}
                      placeholder="https://youtube.com/watch?v=..."
                      aria-label="YouTube video URL"
                      className="pl-12 h-14 bg-background/30 border-white/10 focus:border-red-500/50 focus:ring-red-500/20 rounded-xl text-lg shadow-inner"
                      disabled={isImporting || disabled}
                      onKeyDown={(e) => e.key === "Enter" && handleYoutubeImport()}
                    />
                  </div>

                  <Button
                    onClick={handleYoutubeImport}
                    disabled={!youtubeUrl || isImporting}
                    aria-label={isImporting ? "Importing from YouTube..." : "Import audio from YouTube"}
                    className={cn(
                      "w-full h-14 rounded-xl text-white font-semibold text-lg shadow-lg transition-all",
                      "bg-linear-to-r from-red-600 to-red-500 hover:from-red-500 hover:to-red-400 hover:shadow-red-500/25 hover:scale-[1.02] active:scale-[0.98]"
                    )}
                  >
                    {isImporting ? <Loader2 className="animate-spin w-5 h-5" aria-hidden="true" /> : "Import from YouTube"}
                  </Button>

                  <div className="flex items-center justify-center gap-2 text-xs text-muted-foreground/60">
                    <div className="w-1.5 h-1.5 rounded-full bg-green-500/50" aria-hidden="true" />
                    <span>Requires backend server (npm run server)</span>
                  </div>
                </motion.div>
              )}

              {activeTab === "production" && (
                <motion.div
                  key="production"
                  id="production-panel"
                  role="tabpanel"
                  aria-labelledby="production-tab"
                  initial={{ opacity: 0, scale: 0.95 }}
                  animate={{ opacity: 1, scale: 1 }}
                  exit={{ opacity: 0, scale: 0.95 }}
                  className="flex flex-col items-center py-2"
                >
                  <div className="relative mb-6" aria-hidden="true">
                    <div className="absolute inset-0 bg-purple-500 blur-2xl opacity-20" />
                    <div className="relative w-24 h-24 rounded-3xl bg-linear-to-br from-purple-500/10 to-blue-500/10 border border-purple-500/20 flex items-center justify-center shadow-inner">
                      <FileVideo size={40} className="text-purple-400" />
                    </div>
                  </div>

                  <h3 className="text-2xl font-bold mb-3 bg-clip-text text-transparent bg-linear-to-r from-purple-400 to-blue-400">
                    Professional Studio
                  </h3>

                  <p className="text-muted-foreground text-center mb-8 max-w-sm leading-relaxed">
                    Access advanced tools for multi-agent orchestration, detailed scene control, and granular editing.
                  </p>

                  <Button
                    onClick={() => onSwitchToProduction?.()}
                    aria-label="Enter Professional Studio mode"
                    className="w-full max-w-sm h-14 rounded-xl font-semibold text-lg bg-white/5 border border-white/10 hover:bg-white/10 hover:border-purple-500/30 text-foreground shadow-xl shadow-purple-900/10 hover:shadow-purple-500/10 transition-all group"
                  >
                    Enter Studio <ChevronRight className="ml-2 w-5 h-5 group-hover:translate-x-1 transition-transform text-purple-400" aria-hidden="true" />
                  </Button>
                </motion.div>
              )}

              {activeTab === "music" && (
                <motion.div
                  key="music"
                  id="music-panel"
                  role="tabpanel"
                  aria-labelledby="music-tab"
                  initial={{ opacity: 0, scale: 0.95 }}
                  animate={{ opacity: 1, scale: 1 }}
                  exit={{ opacity: 0, scale: 0.95 }}
                  className="flex flex-col items-center py-2"
                >
                  <div className="relative mb-6" aria-hidden="true">
                    <div className="absolute inset-0 bg-cyan-500 blur-2xl opacity-20" />
                    <div className="relative w-24 h-24 rounded-3xl bg-linear-to-br from-cyan-500/10 to-blue-500/10 border border-cyan-500/20 flex items-center justify-center shadow-inner">
                      <Music size={40} className="text-cyan-400" />
                    </div>
                  </div>

                  <h3 className="text-2xl font-bold mb-3 bg-clip-text text-transparent bg-linear-to-r from-cyan-400 to-blue-400">
                    AI Song Generator
                  </h3>

                  <p className="text-muted-foreground text-center mb-6 max-w-sm leading-relaxed">
                    Create custom AI-generated music from topics, lyrics, and style preferences using Suno AI.
                  </p>

                  <div className="w-full max-w-sm space-y-3">
                    {/* Chat-based generator (recommended) */}
                    <Button
                      onClick={() => setShowMusicChat(true)}
                      aria-label="Open AI Music Producer chat interface (recommended)"
                      className="w-full h-14 rounded-xl font-semibold text-lg bg-linear-to-r from-cyan-500/20 to-blue-500/20 border border-cyan-500/30 hover:from-cyan-500/30 hover:to-blue-500/30 text-foreground shadow-xl shadow-cyan-900/10 hover:shadow-cyan-500/10 transition-all group"
                    >
                      <Sparkles className="mr-2 w-5 h-5 text-cyan-400" aria-hidden="true" />
                      Chat with AI Producer
                      <span className="ml-2 text-xs bg-cyan-500/20 px-2 py-0.5 rounded-full text-cyan-300">Recommended</span>
                    </Button>

                    {/* Quick generator */}
                    <Button
                      onClick={() => setShowMusicModal(true)}
                      variant="outline"
                      aria-label="Open quick music generation form"
                      className="w-full h-12 rounded-xl font-medium bg-white/5 border border-white/10 hover:bg-white/10 hover:border-cyan-500/20 text-muted-foreground hover:text-foreground transition-all"
                    >
                      Quick Generate (Form)
                    </Button>
                  </div>

                  {!isSunoConfigured() && (
                    <p className="mt-4 text-xs text-amber-400/80 text-center" role="alert">
                      ⚠️ Add VITE_SUNO_API_KEY to .env.local to enable
                    </p>
                  )}
                </motion.div>
              )}
            </AnimatePresence>
          </div>
        </div>
      </div>

      {/* Footer Demo Link */}
      <motion.div
        initial={{ opacity: 0 }}
        animate={{ opacity: 1 }}
        transition={{ delay: 0.5 }}
        className="mt-16"
      >
        <button
          onClick={() => onLoadDemo(aspectRatio)}
          aria-label="Load demo project for development testing"
          className="group flex items-center gap-3 px-6 py-3 rounded-full bg-accent/5 hover:bg-accent/10 border border-accent/10 hover:border-accent/20 transition-all duration-300"
        >
          <Sparkles size={14} className="text-accent group-hover:scale-110 transition-transform" aria-hidden="true" />
          <span className="text-xs font-medium uppercase tracking-widest text-muted-foreground group-hover:text-accent transition-colors">Load Demo Project (Dev)</span>
        </button>
      </motion.div>

      {/* Music Generator Modal */}
      <MusicGeneratorModal
        open={showMusicModal}
        onClose={() => setShowMusicModal(false)}
        onMusicGenerated={(track) => {
          console.log("[QuickUpload] Music generated:", track.title);
        }}
        musicState={musicState}
        onGenerateMusic={handleGenerateMusic}
        onGenerateLyrics={handleGenerateLyrics}
        onSelectTrack={handleSelectTrack}
        onAddToTimeline={handleAddToTimeline}
        onRefreshCredits={handleRefreshCredits}
      />

      {/* Music Chat Modal V2 (AI Producer with direct Suno API) */}
      <MusicChatModalV2
        open={showMusicChat}
        onClose={() => setShowMusicChat(false)}
        onMusicGenerated={(track) => {
          console.log("[QuickUpload] Music generated via chat:", track.title);
        }}
      />
    </motion.div>
  );
};
````

## File: packages/frontend/components/ReferenceDocumentUpload.tsx
````typescript
/**
 * Reference Document Upload Component
 *
 * Drag-and-drop upload for PDF, TXT, DOCX reference documents.
 * Parses documents into indexed chunks for the Research Service.
 *
 * Requirements: 22.4
 */

import React, { useState, useCallback, useRef } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import {
  Upload,
  FileText,
  X,
  AlertCircle,
  Check,
  Loader2,
} from 'lucide-react';
import { cn } from '@/lib/utils';
import {
  parseDocument,
  getSupportedTypes,
  DocumentParseError,
  type IndexedDocument,
} from '@/services/documentParser';

export interface ReferenceDocumentUploadProps {
  documents: IndexedDocument[];
  onDocumentsChange: (docs: IndexedDocument[]) => void;
  maxFiles?: number;
  maxSizeMB?: number;
  className?: string;
}

interface UploadStatus {
  filename: string;
  status: 'parsing' | 'done' | 'error';
  error?: string;
}

const ACCEPT = '.pdf,.txt,.docx';

export function ReferenceDocumentUpload({
  documents,
  onDocumentsChange,
  maxFiles = 5,
  maxSizeMB = 10,
  className,
}: ReferenceDocumentUploadProps) {
  const [dragOver, setDragOver] = useState(false);
  const [uploadStatuses, setUploadStatuses] = useState<UploadStatus[]>([]);
  const inputRef = useRef<HTMLInputElement>(null);

  const maxSizeBytes = maxSizeMB * 1024 * 1024;

  const handleFiles = useCallback(
    async (files: FileList | File[]) => {
      const fileArray = Array.from(files);
      const remaining = maxFiles - documents.length;

      if (remaining <= 0) {
        setUploadStatuses([
          { filename: '', status: 'error', error: `Maximum ${maxFiles} files allowed` },
        ]);
        return;
      }

      const toProcess = fileArray.slice(0, remaining);
      const newStatuses: UploadStatus[] = toProcess.map((f) => ({
        filename: f.name,
        status: 'parsing' as const,
      }));

      setUploadStatuses(newStatuses);

      const results: IndexedDocument[] = [];

      for (let i = 0; i < toProcess.length; i++) {
        const file = toProcess[i]!;

        // Size check
        if (file.size > maxSizeBytes) {
          newStatuses[i] = {
            filename: file.name,
            status: 'error',
            error: `File exceeds ${maxSizeMB}MB limit`,
          };
          setUploadStatuses([...newStatuses]);
          continue;
        }

        try {
          const doc = await parseDocument(file);
          results.push(doc);
          newStatuses[i] = { filename: file.name, status: 'done' };
        } catch (err) {
          const msg =
            err instanceof DocumentParseError
              ? err.message
              : err instanceof Error
                ? err.message
                : 'Unknown parsing error';
          newStatuses[i] = { filename: file.name, status: 'error', error: msg };
        }

        setUploadStatuses([...newStatuses]);
      }

      if (results.length > 0) {
        onDocumentsChange([...documents, ...results]);
      }

      // Clear statuses after a delay
      setTimeout(() => setUploadStatuses([]), 3000);
    },
    [documents, onDocumentsChange, maxFiles, maxSizeBytes, maxSizeMB],
  );

  const handleDrop = useCallback(
    (e: React.DragEvent) => {
      e.preventDefault();
      setDragOver(false);
      if (e.dataTransfer.files.length > 0) {
        handleFiles(e.dataTransfer.files);
      }
    },
    [handleFiles],
  );

  const handleRemove = useCallback(
    (docId: string) => {
      onDocumentsChange(documents.filter((d) => d.id !== docId));
    },
    [documents, onDocumentsChange],
  );

  return (
    <div className={cn('w-full', className)}>
      {/* Drop zone */}
      <div
        onDragOver={(e) => {
          e.preventDefault();
          setDragOver(true);
        }}
        onDragLeave={() => setDragOver(false)}
        onDrop={handleDrop}
        onClick={() => inputRef.current?.click()}
        className={cn(
          'flex flex-col items-center justify-center gap-2 px-4 py-6 rounded-sm border-2 border-dashed cursor-pointer transition-colors duration-200',
          dragOver
            ? 'border-blue-500/50 bg-blue-500/10'
            : 'border-zinc-700 bg-zinc-900/40 hover:border-zinc-500',
        )}
        role="button"
        aria-label="Upload reference documents"
        tabIndex={0}
        onKeyDown={(e) => {
          if (e.key === 'Enter' || e.key === ' ') {
            e.preventDefault();
            inputRef.current?.click();
          }
        }}
      >
        <Upload
          className={cn(
            'w-5 h-5',
            dragOver ? 'text-blue-400' : 'text-zinc-500',
          )}
        />
        <div className="text-center">
          <span className="text-[13px] text-zinc-300">
            Drop files here or click to browse
          </span>
          <span className="block font-mono text-[10px] text-zinc-500 mt-1">
            PDF, TXT, DOCX — max {maxSizeMB}MB, {maxFiles} files
          </span>
        </div>
        <input
          ref={inputRef}
          type="file"
          accept={ACCEPT}
          multiple
          className="hidden"
          onChange={(e) => {
            if (e.target.files?.length) {
              handleFiles(e.target.files);
              e.target.value = '';
            }
          }}
        />
      </div>

      {/* Upload statuses */}
      <AnimatePresence>
        {uploadStatuses.length > 0 && (
          <motion.div
            initial={{ opacity: 0, height: 0 }}
            animate={{ opacity: 1, height: 'auto' }}
            exit={{ opacity: 0, height: 0 }}
            className="mt-2 space-y-1 overflow-hidden"
          >
            {uploadStatuses.map((s, i) => (
              <div
                key={`${s.filename}-${i}`}
                className={cn(
                  'flex items-center gap-2 px-3 py-1.5 rounded-sm border text-xs font-mono',
                  s.status === 'parsing'
                    ? 'border-zinc-700 text-zinc-400'
                    : s.status === 'done'
                      ? 'border-emerald-500/30 text-emerald-400'
                      : 'border-red-500/30 text-red-400',
                )}
              >
                {s.status === 'parsing' && <Loader2 className="w-3 h-3 animate-spin" />}
                {s.status === 'done' && <Check className="w-3 h-3" />}
                {s.status === 'error' && <AlertCircle className="w-3 h-3" />}
                <span className="truncate">{s.filename || s.error}</span>
                {s.error && s.filename && (
                  <span className="text-red-500 truncate ml-auto">{s.error}</span>
                )}
              </div>
            ))}
          </motion.div>
        )}
      </AnimatePresence>

      {/* Document list */}
      {documents.length > 0 && (
        <div className="mt-3 space-y-1.5">
          <span className="font-mono text-[10px] font-medium tracking-[0.15em] uppercase text-zinc-500">
            Reference Documents ({documents.length}/{maxFiles})
          </span>
          {documents.map((doc) => (
            <motion.div
              key={doc.id}
              layout
              initial={{ opacity: 0, y: 4 }}
              animate={{ opacity: 1, y: 0 }}
              exit={{ opacity: 0, y: -4 }}
              className="flex items-center gap-3 px-3 py-2 rounded-sm border border-zinc-700 bg-zinc-900/60"
            >
              <FileText className="w-3.5 h-3.5 text-zinc-400 shrink-0" />
              <div className="flex-1 min-w-0">
                <span className="text-[13px] text-zinc-200 truncate block">
                  {doc.filename}
                </span>
                <span className="font-mono text-[10px] text-zinc-500">
                  {doc.chunks.length} chunks, {doc.metadata.wordCount ?? '?'} words
                </span>
              </div>
              <button
                type="button"
                onClick={() => handleRemove(doc.id)}
                className={cn(
                  'shrink-0 p-1 rounded-sm border border-transparent',
                  'text-zinc-500 hover:text-red-400 hover:border-red-500/30',
                  'transition-colors duration-150',
                  'focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-red-500',
                )}
                aria-label={`Remove ${doc.filename}`}
              >
                <X className="w-3 h-3" />
              </button>
            </motion.div>
          ))}
        </div>
      )}
    </div>
  );
}

export default ReferenceDocumentUpload;
````

## File: packages/frontend/components/SceneEditor.tsx
````typescript
/**
 * SceneEditor Component
 * 
 * Editable scene cards for reviewing and modifying content plans.
 * Allows editing visual descriptions, narration scripts, and timing.
 */

import React, { useState, useCallback } from "react";
import { motion, AnimatePresence, Reorder } from "framer-motion";
import {
    GripVertical,
    Trash2,
    Plus,
    Play,
    Pause,
    Clock,
    Image,
    Mic,
    ChevronDown,
    ChevronUp,
    Eye,
    Download,
    RefreshCw,
    Loader2
} from "lucide-react";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { Textarea } from "@/components/ui/textarea";
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select";
import { cn, isRTL, getTextDirection } from "@/lib/utils";
import { Scene, EmotionalTone, TransitionType, InstructionTriplet } from "@/types";
import { getEffectiveLegacyTone } from "@/services/tripletUtils";

interface SceneEditorProps {
    scenes: Scene[];
    onChange: (scenes: Scene[]) => void;
    onPlayNarration?: (sceneId: string) => void;
    onRegenerateNarration?: (sceneId: string) => Promise<void>;
    playingSceneId?: string | null;
    regeneratingSceneId?: string | null;
    visuals?: Record<string, string>; // sceneId -> imageUrl
    narrationUrls?: Record<string, string>; // sceneId -> audioUrl
    className?: string;
}

interface SceneCardProps {
    scene: Scene;
    index: number;
    isExpanded: boolean;
    isPlaying: boolean;
    isRegenerating: boolean;
    imageUrl?: string;
    audioUrl?: string;
    onToggleExpand: () => void;
    onChange: (updates: Partial<Scene>) => void;
    onDelete: () => void;
    onPlayNarration?: () => void;
    onRegenerateNarration?: () => void;
    onDownloadAudio?: () => void;
}

const EMOTIONAL_TONES: EmotionalTone[] = ["professional", "dramatic", "friendly", "urgent", "calm"];
const TRANSITIONS: TransitionType[] = ["none", "fade", "dissolve", "zoom", "slide"];

function SceneCard({
    scene,
    index,
    isExpanded,
    isPlaying,
    isRegenerating,
    imageUrl,
    audioUrl,
    onToggleExpand,
    onChange,
    onDelete,
    onPlayNarration,
    onRegenerateNarration,
    onDownloadAudio,
}: SceneCardProps) {
    const handleDownloadAudio = (e: React.MouseEvent) => {
        e.stopPropagation();
        if (audioUrl) {
            const a = document.createElement('a');
            a.href = audioUrl;
            a.download = `${scene.name.replace(/\s+/g, '_')}_narration.wav`;
            a.click();
        }
    };

    return (
        <Reorder.Item
            value={scene}
            className="glass-panel rounded-xl overflow-hidden mb-3"
        >
            {/* Header */}
            <div
                className="flex items-center gap-3 p-4 cursor-pointer hover:bg-white/5 transition-colors"
                onClick={onToggleExpand}
            >
                <GripVertical className="w-4 h-4 text-slate-500 cursor-grab" />

                <div className="w-8 h-8 rounded-full bg-linear-to-br from-cyan-500 to-purple-500 flex items-center justify-center text-sm font-bold">
                    {index + 1}
                </div>

                <div className="flex-1 min-w-0">
                    <Input
                        value={scene.name}
                        onChange={(e) => onChange({ name: e.target.value })}
                        onClick={(e) => e.stopPropagation()}
                        className={cn(
                            "bg-transparent border-none p-0 h-auto text-base font-medium focus:ring-0",
                            isRTL(scene.name) && "text-right"
                        )}
                        dir={getTextDirection(scene.name)}
                        placeholder="Scene name... | اسم المشهد..."
                    />
                </div>

                <div className="flex items-center gap-2 text-sm text-slate-400">
                    <Clock className="w-4 h-4" />
                    <Input
                        type="number"
                        value={scene.duration}
                        onChange={(e) => onChange({ duration: parseInt(e.target.value) || 0 })}
                        onClick={(e) => e.stopPropagation()}
                        className="w-16 bg-white/5 border-white/10 h-7 text-center rounded-md"
                        min={1}
                        max={120}
                    />
                    <span>sec</span>
                </div>

                <div className="flex items-center gap-1">
                    {onPlayNarration && (
                        <Button
                            size="sm"
                            variant="ghost"
                            onClick={(e) => { e.stopPropagation(); onPlayNarration(); }}
                            className="h-8 w-8 p-0"
                        >
                            {isPlaying ? (
                                <Pause className="w-4 h-4 text-cyan-400" />
                            ) : (
                                <Play className="w-4 h-4" />
                            )}
                        </Button>
                    )}
                    <Button
                        size="sm"
                        variant="ghost"
                        onClick={(e) => { e.stopPropagation(); onDelete(); }}
                        className="h-8 w-8 p-0 hover:text-red-400"
                    >
                        <Trash2 className="w-4 h-4" />
                    </Button>
                    {isExpanded ? (
                        <ChevronUp className="w-4 h-4 text-slate-400" />
                    ) : (
                        <ChevronDown className="w-4 h-4 text-slate-400" />
                    )}
                </div>
            </div>

            {/* Expanded content */}
            <AnimatePresence>
                {isExpanded && (
                    <motion.div
                        initial={{ height: 0, opacity: 0 }}
                        animate={{ height: "auto", opacity: 1 }}
                        exit={{ height: 0, opacity: 0 }}
                        transition={{ duration: 0.2 }}
                        className="overflow-hidden"
                    >
                        <div className="p-4 pt-0 space-y-4 border-t border-white/10">
                            {/* Generated Image Preview */}
                            {imageUrl && (
                                <div className="mb-4">
                                    <label className="flex items-center gap-2 text-sm text-slate-400 mb-2">
                                        <Eye className="w-4 h-4" />
                                        Generated Image
                                    </label>
                                    <div className="relative aspect-video rounded-lg overflow-hidden bg-slate-900">
                                        <img
                                            src={imageUrl}
                                            alt={scene.name}
                                            className="w-full h-full object-cover"
                                        />
                                    </div>
                                </div>
                            )}

                            {/* Audio Preview & Download */}
                            {audioUrl && (
                                <div className="mb-4">
                                    <label className="flex items-center gap-2 text-sm text-slate-400 mb-2">
                                        <Mic className="w-4 h-4" />
                                        Narration Audio
                                    </label>
                                    <div className="flex items-center gap-3 p-3 rounded-lg bg-slate-700/30">
                                        <audio
                                            src={audioUrl}
                                            controls
                                            className="flex-1 h-8"
                                        />
                                        <Button
                                            size="sm"
                                            variant="glass"
                                            onClick={handleDownloadAudio}
                                            className="gap-2"
                                        >
                                            <Download className="w-4 h-4" />
                                            Download
                                        </Button>
                                    </div>
                                </div>
                            )}

                            {/* Visual Description */}
                            <div>
                                <label className="flex items-center gap-2 text-sm text-slate-400 mb-2">
                                    <Image className="w-4 h-4" />
                                    Visual Description
                                </label>
                                <Textarea
                                    value={scene.visualDescription}
                                    onChange={(e) => onChange({ visualDescription: e.target.value })}
                                    placeholder="Describe what should be shown visually... | صف ما يجب عرضه بصريًا..."
                                    className={cn(
                                        "min-h-[80px] bg-white/5 border-white/10 focus:border-primary/50 rounded-xl",
                                        isRTL(scene.visualDescription) && "text-right"
                                    )}
                                    dir={getTextDirection(scene.visualDescription)}
                                />
                                <p className={cn(
                                    "text-xs text-slate-500 mt-1",
                                    isRTL(scene.visualDescription) && "text-right"
                                )}>
                                    {scene.visualDescription.length}/200 characters
                                </p>
                            </div>

                            {/* Narration Script */}
                            <div>
                                <label className="flex items-center gap-2 text-sm text-slate-400 mb-2">
                                    <Mic className="w-4 h-4" />
                                    Narration Script
                                </label>
                                <Textarea
                                    value={scene.narrationScript}
                                    onChange={(e) => onChange({ narrationScript: e.target.value })}
                                    placeholder="Write what should be spoken... | اكتب ما يجب قوله..."
                                    className={cn(
                                        "min-h-[100px] bg-white/5 border-white/10 focus:border-primary/50 rounded-xl",
                                        isRTL(scene.narrationScript) && "text-right"
                                    )}
                                    dir={getTextDirection(scene.narrationScript)}
                                />
                                <div className={cn(
                                    "flex items-center justify-between mt-2",
                                    isRTL(scene.narrationScript) && "flex-row-reverse"
                                )}>
                                    <p className="text-xs text-slate-500">
                                        ~{Math.ceil(scene.narrationScript.split(/\s+/).length / 2.5)} seconds of speech
                                    </p>
                                    {onRegenerateNarration && (
                                        <Button
                                            size="sm"
                                            variant="glass"
                                            onClick={onRegenerateNarration}
                                            disabled={isRegenerating}
                                            className="gap-2 h-8"
                                        >
                                            {isRegenerating ? (
                                                <>
                                                    <Loader2 className="w-3 h-3 animate-spin" />
                                                    Generating...
                                                </>
                                            ) : (
                                                <>
                                                    <RefreshCw className="w-3 h-3" />
                                                    Regenerate TTS
                                                </>
                                            )}
                                        </Button>
                                    )}
                                </div>
                            </div>

                            {/* Tone and Transition */}
                            <div className="flex gap-4">
                                <div className="flex-1">
                                    <label className="text-sm text-slate-400 mb-2 block">Emotional Tone</label>
                                    <Select
                                        value={getEffectiveLegacyTone(scene)}
                                        onValueChange={(v) => onChange({ emotionalTone: v as EmotionalTone })}
                                    >
                                        <SelectTrigger className="glass-button w-full border-white/10">
                                            <SelectValue />
                                        </SelectTrigger>
                                        <SelectContent>
                                            {EMOTIONAL_TONES.map((tone) => (
                                                <SelectItem key={tone} value={tone}>
                                                    {tone.charAt(0).toUpperCase() + tone.slice(1)}
                                                </SelectItem>
                                            ))}
                                        </SelectContent>
                                    </Select>
                                </div>

                                <div className="flex-1">
                                    <label className="text-sm text-slate-400 mb-2 block">Transition</label>
                                    <Select
                                        value={scene.transitionTo || "dissolve"}
                                        onValueChange={(v) => onChange({ transitionTo: v as TransitionType })}
                                    >
                                        <SelectTrigger className="glass-button w-full border-white/10">
                                            <SelectValue />
                                        </SelectTrigger>
                                        <SelectContent>
                                            {TRANSITIONS.map((t) => (
                                                <SelectItem key={t} value={t}>
                                                    {t.charAt(0).toUpperCase() + t.slice(1)}
                                                </SelectItem>
                                            ))}
                                        </SelectContent>
                                    </Select>
                                </div>
                            </div>

                            {/* Instruction Triplet */}
                            {scene.instructionTriplet && (
                                <div className="space-y-2 mt-3 p-3 rounded-lg bg-white/5 border border-white/10">
                                    <label className="text-xs text-cyan-400 font-medium block">Instruction Triplet</label>
                                    <div className="grid grid-cols-3 gap-2">
                                        <div>
                                            <label className="text-[10px] text-slate-500 block mb-1">Emotion</label>
                                            <Input
                                                value={scene.instructionTriplet.primaryEmotion}
                                                onChange={(e) => onChange({
                                                    instructionTriplet: { ...scene.instructionTriplet!, primaryEmotion: e.target.value }
                                                })}
                                                className="glass-button text-xs h-8 border-white/10"
                                                placeholder="e.g. visceral-dread"
                                            />
                                        </div>
                                        <div>
                                            <label className="text-[10px] text-slate-500 block mb-1">Cinematic</label>
                                            <Input
                                                value={scene.instructionTriplet.cinematicDirection}
                                                onChange={(e) => onChange({
                                                    instructionTriplet: { ...scene.instructionTriplet!, cinematicDirection: e.target.value }
                                                })}
                                                className="glass-button text-xs h-8 border-white/10"
                                                placeholder="e.g. slow-push-in"
                                            />
                                        </div>
                                        <div>
                                            <label className="text-[10px] text-slate-500 block mb-1">Atmosphere</label>
                                            <Input
                                                value={scene.instructionTriplet.environmentalAtmosphere}
                                                onChange={(e) => onChange({
                                                    instructionTriplet: { ...scene.instructionTriplet!, environmentalAtmosphere: e.target.value }
                                                })}
                                                className="glass-button text-xs h-8 border-white/10"
                                                placeholder="e.g. foggy-ruins"
                                            />
                                        </div>
                                    </div>
                                </div>
                            )}
                        </div>
                    </motion.div>
                )}
            </AnimatePresence>
        </Reorder.Item>
    );
}

export function SceneEditor({
    scenes,
    onChange,
    onPlayNarration,
    onRegenerateNarration,
    playingSceneId,
    regeneratingSceneId,
    visuals = {},
    narrationUrls = {},
    className,
}: SceneEditorProps) {
    const [expandedId, setExpandedId] = useState<string | null>(null);

    const handleSceneChange = useCallback((id: string, updates: Partial<Scene>) => {
        onChange(scenes.map(s => s.id === id ? { ...s, ...updates } : s));
    }, [scenes, onChange]);

    const handleDelete = useCallback((id: string) => {
        onChange(scenes.filter(s => s.id !== id));
    }, [scenes, onChange]);

    const handleAddScene = useCallback(() => {
        const newScene: Scene = {
            id: `scene-${Date.now()}`,
            name: `Scene ${scenes.length + 1}`,
            duration: 15,
            visualDescription: "",
            narrationScript: "",
            emotionalTone: "friendly",
            instructionTriplet: {
                primaryEmotion: "warm-welcome",
                cinematicDirection: "medium",
                environmentalAtmosphere: "soft-daylight",
            },
            transitionTo: "dissolve",
        };
        onChange([...scenes, newScene]);
        setExpandedId(newScene.id);
    }, [scenes, onChange]);

    const totalDuration = scenes.reduce((sum, s) => sum + s.duration, 0);

    return (
        <div className={cn("space-y-4", className)}>
            {/* Header */}
            <div className="flex items-center justify-between">
                <div>
                    <h3 className="text-lg font-semibold">Scenes</h3>
                    <p className="text-sm text-slate-400">
                        {scenes.length} scenes • {Math.floor(totalDuration / 60)}:{String(totalDuration % 60).padStart(2, '0')} total
                    </p>
                </div>
                <Button onClick={handleAddScene} size="sm" className="gap-2">
                    <Plus className="w-4 h-4" />
                    Add Scene
                </Button>
            </div>

            {/* Scene list */}
            <Reorder.Group
                axis="y"
                values={scenes}
                onReorder={onChange}
                className="space-y-2"
            >
                {scenes.map((scene, index) => (
                    <SceneCard
                        key={scene.id}
                        scene={scene}
                        index={index}
                        isExpanded={expandedId === scene.id}
                        isPlaying={playingSceneId === scene.id}
                        isRegenerating={regeneratingSceneId === scene.id}
                        imageUrl={visuals[scene.id]}
                        audioUrl={narrationUrls[scene.id]}
                        onToggleExpand={() => setExpandedId(expandedId === scene.id ? null : scene.id)}
                        onChange={(updates) => handleSceneChange(scene.id, updates)}
                        onDelete={() => handleDelete(scene.id)}
                        onPlayNarration={onPlayNarration ? () => onPlayNarration(scene.id) : undefined}
                        onRegenerateNarration={onRegenerateNarration ? () => onRegenerateNarration(scene.id) : undefined}
                    />
                ))}
            </Reorder.Group>

            {scenes.length === 0 && (
                <div className="text-center py-12 text-slate-500">
                    <Eye className="w-12 h-12 mx-auto mb-4 opacity-50" />
                    <p>No scenes yet. Click "Add Scene" to get started.</p>
                </div>
            )}
        </div>
    );
}

export default SceneEditor;
````

## File: packages/frontend/components/SEO/index.ts
````typescript
export { StructuredData, BreadcrumbStructuredData } from './StructuredData';
````

## File: packages/frontend/components/SEO/StructuredData.tsx
````typescript
/**
 * Structured Data (JSON-LD) component for SEO
 * Add this to your pages for better search engine understanding
 */

interface StructuredDataProps {
  type?: 'WebApplication' | 'WebSite' | 'Organization';
  name?: string;
  description?: string;
  url?: string;
}

export function StructuredData({
  type = 'WebApplication',
  name = 'LyricLens',
  description = 'AI-powered lyric video generator - Create stunning music videos with AI',
  url = 'https://yourdomain.com',
}: StructuredDataProps) {
  const structuredData = {
    '@context': 'https://schema.org',
    '@type': type,
    name,
    description,
    url,
    applicationCategory: 'MultimediaApplication',
    operatingSystem: 'Web Browser',
    offers: {
      '@type': 'Offer',
      price: '0',
      priceCurrency: 'USD',
    },
    featureList: [
      'AI-powered video generation',
      'Lyric synchronization',
      'Audio visualization',
      'Multi-language support',
    ],
  };

  return (
    <script
      type="application/ld+json"
      dangerouslySetInnerHTML={{ __html: JSON.stringify(structuredData) }}
    />
  );
}

/**
 * Breadcrumb structured data for navigation
 */
interface BreadcrumbItem {
  name: string;
  url: string;
}

interface BreadcrumbProps {
  items: BreadcrumbItem[];
}

export function BreadcrumbStructuredData({ items }: BreadcrumbProps) {
  const structuredData = {
    '@context': 'https://schema.org',
    '@type': 'BreadcrumbList',
    itemListElement: items.map((item, index) => ({
      '@type': 'ListItem',
      position: index + 1,
      name: item.name,
      item: item.url,
    })),
  };

  return (
    <script
      type="application/ld+json"
      dangerouslySetInnerHTML={{ __html: JSON.stringify(structuredData) }}
    />
  );
}
````

## File: packages/frontend/components/SettingsModal.tsx
````typescript
import React from "react";
import {
  Music,
  Speech,
  Smartphone,
  Monitor,
  Sparkles,
  Film,
  Zap,
  Target,
  User,
  X,
  Settings,
  Video,
} from "lucide-react";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select";
import {
  Dialog,
  DialogContent,
  DialogTitle,
} from "@/components/ui/dialog";
import { motion, AnimatePresence } from "framer-motion";
import { ART_STYLES, VIDEO_PURPOSES, type VideoPurpose } from "@/constants";
// cn utility removed - not used in this component

export interface SettingsModalProps {
  isOpen: boolean;
  onClose: () => void;
  contentType: "music" | "story";
  videoPurpose: VideoPurpose;
  generationMode: "image" | "video";
  videoProvider: "veo" | "deapi";
  veoVideoCount: number;
  aspectRatio: string;
  selectedStyle: string;
  globalSubject: string;
  onContentTypeChange: (type: "music" | "story") => void;
  onVideoPurposeChange: (purpose: VideoPurpose) => void;
  onGenerationModeChange: (mode: "image" | "video") => void;
  onVideoProviderChange: (provider: "veo" | "deapi") => void;
  onVeoVideoCountChange: (count: number) => void;
  onAspectRatioChange: (ratio: string) => void;
  onStyleChange: (style: string) => void;
  onGlobalSubjectChange: (subject: string) => void;
  targetAudience?: string;
  onTargetAudienceChange?: (audience: string) => void;
}

const SettingRow = ({ icon: Icon, label, children }: { icon: React.ElementType, label: string, children: React.ReactNode }) => (
  <div className="group relative">
    <div className="absolute -inset-2 bg-white/5 rounded-xl opacity-0 group-hover:opacity-100 transition-opacity" />
    <div className="relative flex items-center justify-between gap-4 p-1">
      <div className="flex items-center gap-3 text-muted-foreground group-hover:text-foreground transition-colors">
        <div className="w-8 h-8 rounded-lg bg-white/5 flex items-center justify-center">
          <Icon size={16} />
        </div>
        <span className="text-sm font-medium">{label}</span>
      </div>
      <div className="w-[200px]">
        {children}
      </div>
    </div>
  </div>
);

export const SettingsModal: React.FC<SettingsModalProps> = ({
  isOpen,
  onClose,
  contentType,
  videoPurpose,
  generationMode,
  videoProvider,
  veoVideoCount,
  aspectRatio,
  selectedStyle,
  globalSubject,
  onContentTypeChange,
  onVideoPurposeChange,
  onGenerationModeChange,
  onVideoProviderChange,
  onVeoVideoCountChange,
  onAspectRatioChange,
  onStyleChange,
  targetAudience, // Added prop
  onTargetAudienceChange, // Added prop
  onGlobalSubjectChange,
}) => {
  return (
    <Dialog open={isOpen} onOpenChange={(open) => !open && onClose()}>
      <DialogContent className="sm:max-w-2xl bg-background/80 backdrop-blur-2xl border-white/10 p-0 overflow-hidden gap-0 shadow-2xl shadow-black/50">
        {/* Header */}
        <div className="h-16 border-b border-white/10 flex items-center justify-between px-6 bg-white/5">
          <div className="flex items-center gap-3">
            <div className="w-8 h-8 rounded-full bg-primary/20 flex items-center justify-center">
              <Settings size={16} className="text-primary" />
            </div>
            <DialogTitle className="text-lg font-semibold tracking-tight">Studio Configuration</DialogTitle>
          </div>
          <button onClick={onClose} className="p-2 hover:bg-white/10 rounded-full transition-colors">
            <X size={18} className="text-muted-foreground" />
          </button>
        </div>

        <div className="p-6 space-y-8 max-h-[70vh] overflow-y-auto no-scrollbar">

          {/* Section: Project Basics */}
          <section className="space-y-4">
            <h3 className="text-xs font-bold text-muted-foreground/50 uppercase tracking-widest px-1">Core Settings</h3>

            <SettingRow icon={contentType === "music" ? Music : Speech} label="Content Mode">
              <Select value={contentType} onValueChange={onContentTypeChange}>
                <SelectTrigger className="h-9 bg-black/20 border-white/10">
                  <SelectValue />
                </SelectTrigger>
                <SelectContent>
                  <SelectItem value="music">Music Video</SelectItem>
                  <SelectItem value="story">Story / Speech</SelectItem>
                </SelectContent>
              </Select>
            </SettingRow>

            <SettingRow icon={aspectRatio === "16:9" ? Monitor : Smartphone} label="Aspect Ratio">
              <Select value={aspectRatio} onValueChange={onAspectRatioChange}>
                <SelectTrigger className="h-9 bg-black/20 border-white/10">
                  <SelectValue />
                </SelectTrigger>
                <SelectContent>
                  <SelectItem value="16:9">16:9 Landscape</SelectItem>
                  <SelectItem value="9:16">9:16 Portrait</SelectItem>
                </SelectContent>
              </Select>
            </SettingRow>

            <SettingRow icon={Target} label="Video Purpose">
              <Select value={videoPurpose} onValueChange={onVideoPurposeChange}>
                <SelectTrigger className="h-9 bg-black/20 border-white/10">
                  <SelectValue />
                </SelectTrigger>
                <SelectContent>
                  {VIDEO_PURPOSES.map((purpose) => (
                    <SelectItem key={purpose.value} value={purpose.value}>
                      {purpose.label}
                    </SelectItem>
                  ))}
                </SelectContent>
              </Select>
            </SettingRow>
            <SettingRow icon={User} label="Target Audience">
              <Input
                value={targetAudience || ""}
                onChange={(e) => onTargetAudienceChange?.(e.target.value)}
                placeholder="e.g. Children, Professionals, General"
                className="h-9 bg-black/20 border-white/10"
              />
            </SettingRow>
          </section>

          {/* Section: Output Pipeline */}
          <section className="space-y-4">
            <h3 className="text-xs font-bold text-muted-foreground/50 uppercase tracking-widest px-1">Rendering Engine</h3>

            <SettingRow icon={Film} label="Output Format">
              <Select value={generationMode} onValueChange={onGenerationModeChange}>
                <SelectTrigger className="h-9 bg-black/20 border-white/10">
                  <SelectValue />
                </SelectTrigger>
                <SelectContent>
                  <SelectItem value="image">Static Images</SelectItem>
                  <SelectItem value="video">Motion Loops</SelectItem>
                </SelectContent>
              </Select>
            </SettingRow>

            <AnimatePresence>
              {generationMode === "video" && (
                <motion.div
                  initial={{ opacity: 0, height: 0 }}
                  animate={{ opacity: 1, height: "auto" }}
                  exit={{ opacity: 0, height: 0 }}
                >
                  <SettingRow icon={Zap} label="Video Engine">
                    <Select value={videoProvider} onValueChange={onVideoProviderChange}>
                      <SelectTrigger className="h-9 bg-black/20 border-white/10">
                        <SelectValue />
                      </SelectTrigger>
                      <SelectContent>
                        <SelectItem value="veo">Google Veo (Premium)</SelectItem>
                        <SelectItem value="deapi">DeAPI (Fast)</SelectItem>
                      </SelectContent>
                    </Select>
                  </SettingRow>

                  {videoProvider === "veo" && (
                    <SettingRow icon={Video} label="Pro Video Scenes">
                      <div className="flex items-center gap-3">
                        <input
                          type="range"
                          min="0"
                          max="5"
                          value={veoVideoCount}
                          onChange={(e) => onVeoVideoCountChange(Number(e.target.value))}
                          className="w-24 h-2 bg-white/10 rounded-full appearance-none cursor-pointer [&::-webkit-slider-thumb]:appearance-none [&::-webkit-slider-thumb]:w-4 [&::-webkit-slider-thumb]:h-4 [&::-webkit-slider-thumb]:rounded-full [&::-webkit-slider-thumb]:bg-primary"
                        />
                        <span className="text-sm font-mono w-6 text-center">{veoVideoCount}</span>
                      </div>
                    </SettingRow>
                  )}
                </motion.div>
              )}
            </AnimatePresence>
          </section>

          {/* Section: Aesthetics */}
          <section className="space-y-4">
            <h3 className="text-xs font-bold text-muted-foreground/50 uppercase tracking-widest px-1">Art Direction</h3>

            <SettingRow icon={Sparkles} label="Visual Style">
              <Select value={selectedStyle} onValueChange={onStyleChange}>
                <SelectTrigger className="h-9 bg-black/20 border-white/10">
                  <SelectValue />
                </SelectTrigger>
                <SelectContent className="max-h-[200px]">
                  {ART_STYLES.map((style) => (
                    <SelectItem key={style} value={style}>{style}</SelectItem>
                  ))}
                </SelectContent>
              </Select>
            </SettingRow>

            <SettingRow icon={User} label="Main Subject">
              <Input
                value={globalSubject}
                onChange={(e) => onGlobalSubjectChange(e.target.value)}
                placeholder="e.g. A red robot"
                className="h-9 bg-black/20 border-white/10"
              />
            </SettingRow>
          </section>

        </div>

        {/* Footer */}
        <div className="p-6 border-t border-white/10 bg-white/5 flex justify-end">
          <Button onClick={onClose} className="bg-primary text-primary-foreground hover:bg-primary/90">
            Save Changes
          </Button>
        </div>
      </DialogContent>
    </Dialog>
  );
};
````

## File: packages/frontend/components/story/BreakdownProgress.tsx
````typescript
/**
 * BreakdownProgress.tsx
 * Loading screen for story breakdown generation with stage tracking.
 */

import React from 'react';
import { motion } from 'framer-motion';
import { CheckCircle2, Circle, Loader2 } from 'lucide-react';
import { useLanguage } from '@/i18n/useLanguage';

export type BreakdownStage =
    | 'reading'
    | 'aligning'
    | 'identifying'
    | 'creating';

interface BreakdownProgressProps {
    currentStage: BreakdownStage;
    isComplete?: boolean;
    genre?: string;
}

interface StageConfig {
    id: BreakdownStage;
    label: string;
}

const STAGE_KEYS: Record<BreakdownStage, string> = {
    reading: 'story.breakdown_progress.readingIdea',
    aligning: 'story.breakdown_progress.aligningGenre',
    identifying: 'story.breakdown_progress.identifyingCharacters',
    creating: 'story.breakdown_progress.creatingBreakdown',
};

const STAGES: StageConfig[] = [
    { id: 'reading', label: 'Reading your story idea' },
    { id: 'aligning', label: 'Aligning with genre' },
    { id: 'identifying', label: 'Identifying characters' },
    { id: 'creating', label: 'Creating scene breakdown' },
];

const getStageIndex = (stage: BreakdownStage): number => {
    return STAGES.findIndex(s => s.id === stage);
};

export const BreakdownProgress: React.FC<BreakdownProgressProps> = ({
    currentStage,
    isComplete = false,
    genre = 'your genre',
}) => {
    const { t } = useLanguage();
    const currentIndex = getStageIndex(currentStage);

    const getStageStatus = (index: number): 'complete' | 'processing' | 'pending' => {
        if (isComplete) return 'complete';
        if (index < currentIndex) return 'complete';
        if (index === currentIndex) return 'processing';
        return 'pending';
    };

    const getStageLabel = (stage: StageConfig) => {
        if (stage.id === 'aligning') {
            return t('story.breakdown_progress.aligningWith', { genre });
        }
        return t(STAGE_KEYS[stage.id]);
    };

    return (
        <div className="flex flex-col items-center justify-center min-h-[70vh] p-8">
            <motion.div
                initial={{ opacity: 0, y: 20 }}
                animate={{ opacity: 1, y: 0 }}
                transition={{ duration: 0.2, ease: 'easeOut' }}
                className="w-full max-w-lg"
            >
                {/* Header */}
                <div className="text-center mb-12">
                    <div className="flex items-center justify-center mb-6">
                        <Loader2 className="w-8 h-8 text-blue-500 animate-spin" />
                    </div>
                    <h2 className="font-sans text-2xl font-medium tracking-tight text-zinc-100 mb-3">
                        {t('story.breakdown_progress.developing')}
                    </h2>
                    <p className="text-zinc-500 text-sm">
                        {t('story.breakdown_progress.craftedFrameByFrame')}
                    </p>
                </div>

                {/* Stage List */}
                <div className="space-y-0">
                    {STAGES.map((stage, index) => {
                        const status = getStageStatus(index);
                        return (
                            <motion.div
                                key={stage.id}
                                initial={{ opacity: 0, x: -10 }}
                                animate={{ opacity: 1, x: 0 }}
                                transition={{ duration: 0.2, ease: 'easeOut', delay: index * 0.08 }}
                                className={`
                                    relative pl-8 py-4 border-l transition-all duration-200 ease-out
                                    ${status === 'complete'
                                        ? 'border-blue-500'
                                        : status === 'processing'
                                            ? 'border-zinc-400'
                                            : 'border-zinc-800'
                                    }
                                `}
                            >
                                {/* Frame Number */}
                                <span className="absolute left-3 top-4 font-mono text-[10px] text-zinc-700">
                                    {String(index + 1).padStart(2, '0')}
                                </span>

                                {/* Stage Content */}
                                <div className="flex items-center justify-between">
                                    <div className="flex items-center gap-3">
                                        {status === 'complete' && (
                                            <CheckCircle2 className="w-5 h-5 text-blue-400" />
                                        )}
                                        {status === 'processing' && (
                                            <Loader2 className="w-5 h-5 text-zinc-400 animate-spin" />
                                        )}
                                        {status === 'pending' && (
                                            <Circle className="w-5 h-5 text-zinc-700" />
                                        )}

                                        <span className={`
                                            font-sans text-sm transition-colors duration-200 ease-out
                                            ${status === 'complete' || status === 'processing'
                                                ? 'text-zinc-300'
                                                : 'text-zinc-600'
                                            }
                                        `}>
                                            {getStageLabel(stage)}
                                        </span>
                                    </div>

                                    {status === 'processing' && (
                                        <motion.span
                                            animate={{ opacity: [0.3, 1, 0.3] }}
                                            transition={{ duration: 1.5, repeat: Infinity }}
                                            className="font-mono text-[10px] text-zinc-500 uppercase tracking-widest"
                                        >
                                            {t('story.breakdown_progress.processing')}
                                        </motion.span>
                                    )}
                                </div>
                            </motion.div>
                        );
                    })}
                </div>

                {/* Footer */}
                <motion.div
                    initial={{ opacity: 0 }}
                    animate={{ opacity: 1 }}
                    transition={{ delay: 0.4 }}
                    className="mt-12 text-center"
                >
                    <span className="font-mono text-[10px] text-zinc-700 tracking-widest">
                        PROCESSING
                    </span>
                </motion.div>
            </motion.div>
        </div>
    );
};

export default BreakdownProgress;
````

## File: packages/frontend/components/story/CharacterView.tsx
````typescript
import React from 'react';
import type { CharacterProfile, ConsistencyReport } from '@/types';
import { ShieldCheck, AlertCircle, UserPlus, Trash2, RotateCcw, ImagePlus, User, Sparkles } from 'lucide-react';
import { useLanguage } from '@/i18n/useLanguage';

interface CharacterViewProps {
    characters: CharacterProfile[];
    reports?: Record<string, ConsistencyReport>;
    onVerify?: (name: string) => void;
    isProcessing?: boolean;
    onAdd?: () => void;
    onEdit?: (character: CharacterProfile) => void;
    onDelete?: (characterId: string) => void;
    onGenerateImage?: (characterId: string) => void;
}

export const CharacterView: React.FC<CharacterViewProps> = ({
    characters,
    reports = {},
    onVerify,
    isProcessing = false,
    onAdd,
    onEdit,
    onDelete,
    onGenerateImage
}) => {
    const { t } = useLanguage();

    return (
        <div className="flex flex-col h-full bg-black">
            {/* Header */}
            <div className="flex justify-between items-center bg-zinc-950 border-b border-zinc-800 px-8 py-5">
                <div className="flex items-center gap-6">
                    <div>
                        <h2 className="font-sans text-xl font-medium tracking-tight text-zinc-100">
                            Characters
                        </h2>
                        <p className="text-zinc-500 text-xs mt-1">
                            Story ensemble
                        </p>
                    </div>
                    <div className="bg-zinc-900 border border-zinc-800 rounded-sm px-2 py-1">
                        <span className="font-mono text-xs text-blue-400">
                            {characters.length} MEMBER{characters.length !== 1 ? 'S' : ''}
                        </span>
                    </div>
                </div>
            </div>

            {/* Grid Content */}
            <div className="flex-1 p-8 overflow-y-auto">
                <div className="grid grid-cols-1 sm:grid-cols-2 md:grid-cols-3 lg:grid-cols-4 gap-6">
                    {characters.map((char) => {
                        const report = reports[char.name];
                        return (
                            <div
                                key={char.id}
                                className="group relative bg-zinc-900 border border-zinc-800 rounded-sm overflow-hidden hover:-translate-y-0.5 transition-transform duration-200 ease-out"
                            >
                                {/* Portrait Area - 2:3 Aspect */}
                                <div className="aspect-[2/3] w-full relative overflow-hidden bg-zinc-950">
                                    {char.referenceImageUrl ? (
                                        <img
                                            src={char.referenceImageUrl}
                                            alt={char.name}
                                            className="w-full h-full object-cover"
                                        />
                                    ) : (
                                        <div className="w-full h-full flex flex-col items-center justify-center gap-3">
                                            <User className="w-16 h-16 text-zinc-800" />
                                            <span className="font-mono text-[10px] text-zinc-700 uppercase tracking-widest">
                                                No Portrait
                                            </span>
                                        </div>
                                    )}

                                    {/* Name Plate */}
                                    <div className="absolute bottom-0 left-0 right-0 bg-zinc-950/90 p-3 border-t border-zinc-800">
                                        <h3 className="font-sans text-base font-medium text-zinc-100 leading-tight" dir="auto">
                                            {char.name}
                                        </h3>
                                        <p className="font-mono text-xs text-blue-400 mt-0.5" dir="auto">
                                            {char.role || 'Character'}
                                        </p>
                                    </div>

                                    {/* Consistency Score Badge */}
                                    {report && (
                                        <div className="absolute top-3 left-3">
                                            <div className={`
                                                px-2 py-1 rounded-sm
                                                font-mono text-[10px] uppercase tracking-wider
                                                flex items-center gap-1.5
                                                backdrop-blur-md border
                                                ${report.isConsistent
                                                    ? 'bg-emerald-500/10 border-emerald-500/30 text-emerald-400'
                                                    : 'bg-orange-500/10 border-orange-500/30 text-orange-400'}
                                            `}>
                                                {report.isConsistent
                                                    ? <ShieldCheck className="w-3 h-3" />
                                                    : <AlertCircle className="w-3 h-3" />
                                                }
                                                <span>{Math.round(report.score)}%</span>
                                            </div>
                                        </div>
                                    )}

                                    {/* Hover Actions Overlay */}
                                    <div className="
                                        absolute inset-0
                                        bg-black/80
                                        opacity-0 group-hover:opacity-100
                                        transition-opacity duration-200 ease-out
                                        flex items-center justify-center gap-3
                                    ">
                                        {onVerify && (
                                            <button
                                                onClick={() => onVerify(char.name)}
                                                disabled={isProcessing}
                                                className="rounded-sm bg-zinc-900 border border-zinc-800 p-2.5 text-zinc-400 hover:text-blue-400 transition-colors duration-200"
                                                title={t('story.verifyContinuity')}
                                            >
                                                {isProcessing
                                                    ? <div className="w-5 h-5 rounded-sm border-2 border-zinc-700 border-t-blue-400 animate-spin" />
                                                    : <RotateCcw className="w-5 h-5" />
                                                }
                                            </button>
                                        )}
                                        {onGenerateImage && (
                                            <button
                                                onClick={() => onGenerateImage(char.id)}
                                                disabled={isProcessing}
                                                className="bg-blue-500 text-white rounded-sm p-2.5 transition-colors duration-200"
                                                title={char.referenceImageUrl ? "Regenerate Portrait" : "Generate Portrait"}
                                            >
                                                {isProcessing
                                                    ? <div className="w-5 h-5 rounded-sm border-2 border-white/30 border-t-white animate-spin" />
                                                    : <ImagePlus className="w-5 h-5" />
                                                }
                                            </button>
                                        )}
                                        {onEdit && (
                                            <button
                                                onClick={() => onEdit(char)}
                                                className="rounded-sm bg-zinc-900 border border-zinc-800 p-2.5 text-zinc-400 hover:text-blue-400 transition-colors duration-200"
                                            >
                                                <Sparkles className="w-5 h-5" />
                                            </button>
                                        )}
                                        {onDelete && (
                                            <button
                                                onClick={() => onDelete(char.id)}
                                                className="rounded-sm p-2.5 bg-red-500/10 border border-red-500/30 text-red-400 hover:bg-red-500/20 transition-colors duration-200"
                                            >
                                                <Trash2 className="w-5 h-5" />
                                            </button>
                                        )}
                                    </div>
                                </div>
                            </div>
                        );
                    })}

                    {/* Add New Card */}
                    {onAdd && (
                        <button
                            onClick={onAdd}
                            className="
                                group relative
                                flex flex-col items-center justify-center
                                aspect-[2/3]
                                bg-zinc-950
                                rounded-sm
                                border border-dashed border-zinc-800
                                hover:border-blue-500/50
                                hover:-translate-y-0.5
                                transition-all duration-200 ease-out
                            "
                        >
                            <div className="
                                w-16 h-16 rounded-sm
                                bg-zinc-900 border border-zinc-800
                                flex items-center justify-center
                                mb-4
                            ">
                                <UserPlus className="w-7 h-7 text-zinc-500 group-hover:text-zinc-300 transition-colors duration-200" />
                            </div>
                            <span className="font-sans text-sm text-zinc-500 group-hover:text-zinc-300 transition-colors duration-200">
                                Add Character
                            </span>
                        </button>
                    )}
                </div>
            </div>
        </div>
    );
};

export default CharacterView;
````

## File: packages/frontend/components/story/ExportOptionsPanel.tsx
````typescript
/**
 * ExportOptionsPanel - Export options for Story Mode projects.
 */

import React, { useState, useRef } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import {
  Download,
  FileVideo,
  FileJson,
  Subtitles,
  Upload,
  Check,
  AlertCircle,
  Loader2,
  X,
  Globe,
  Wifi,
} from 'lucide-react';
import { cn } from '@/lib/utils';
import type { StoryState } from '@/types';
import { useLanguage } from '@/i18n/useLanguage';
import { isNative } from '@/utils/platformUtils';
import {
  downloadSubtitles,
  downloadProjectJSON,
  importProjectFromJSON,
  downloadAsWebM,
} from '@/services/exportFormatsService';

interface ExportOptionsPanelProps {
  storyState: StoryState;
  videoBlob?: Blob | null;
  onImportProject?: (state: StoryState) => void;
  onExportVideo?: () => Promise<Blob | null | undefined>;
  className?: string;
}

type ExportStatus = 'idle' | 'loading' | 'success' | 'error';

interface ExportOption {
  id: string;
  name: string;
  description: string;
  icon: React.ReactNode;
  category: 'video' | 'subtitle' | 'project';
  action: () => Promise<void> | void;
  disabled?: boolean;
  disabledReason?: string;
}

export function ExportOptionsPanel({
  storyState,
  videoBlob,
  onImportProject,
  onExportVideo,
  className,
}: ExportOptionsPanelProps) {
  const { t } = useLanguage();
  const [exportStatus, setExportStatus] = useState<Record<string, ExportStatus>>({});
  const [error, setError] = useState<string | null>(null);
  const [showImportDialog, setShowImportDialog] = useState(false);
  const fileInputRef = useRef<HTMLInputElement>(null);

  const hasShots = (storyState.shots?.length ?? 0) > 0;
  const hasVideo = videoBlob !== null && videoBlob !== undefined;

  const setStatus = (id: string, status: ExportStatus) => {
    setExportStatus(prev => ({ ...prev, [id]: status }));
  };

  const handleExportSRT = async () => {
    setStatus('srt', 'loading');
    try {
      downloadSubtitles(storyState, 'srt', 'shots');
      setStatus('srt', 'success');
      setTimeout(() => setStatus('srt', 'idle'), 2000);
    } catch {
      setStatus('srt', 'error');
      setError('Failed to export SRT subtitles');
    }
  };

  const handleExportVTT = async () => {
    setStatus('vtt', 'loading');
    try {
      downloadSubtitles(storyState, 'vtt', 'shots');
      setStatus('vtt', 'success');
      setTimeout(() => setStatus('vtt', 'idle'), 2000);
    } catch {
      setStatus('vtt', 'error');
      setError('Failed to export VTT subtitles');
    }
  };

  const handleExportJSON = async () => {
    setStatus('json', 'loading');
    try {
      downloadProjectJSON(storyState);
      setStatus('json', 'success');
      setTimeout(() => setStatus('json', 'idle'), 2000);
    } catch {
      setStatus('json', 'error');
      setError('Failed to export project');
    }
  };

  const handleExportWebM = async () => {
    if (!videoBlob) {
      setError('No video to export. Generate a video first.');
      return;
    }
    setStatus('webm', 'loading');
    try {
      const filename = storyState.script?.title || 'story';
      await downloadAsWebM(videoBlob, filename);
      setStatus('webm', 'success');
      setTimeout(() => setStatus('webm', 'idle'), 2000);
    } catch {
      setStatus('webm', 'error');
      setError('Failed to export WebM video');
    }
  };

  const handleExportMP4 = async () => {
    if (!onExportVideo) {
      setError('Video export not available');
      return;
    }
    setStatus('mp4', 'loading');
    try {
      const blob = await onExportVideo();
      if (blob) {
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `${storyState.script?.title || 'story'}.mp4`;
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
        URL.revokeObjectURL(url);
        setStatus('mp4', 'success');
        setTimeout(() => setStatus('mp4', 'idle'), 2000);
      } else {
        throw new Error('No video generated');
      }
    } catch (err) {
      setStatus('mp4', 'error');
      const detail = err instanceof Error ? err.message : String(err);
      setError(`Failed to export MP4 video: ${detail}`);
    }
  };

  const handleImportFile = async (e: React.ChangeEvent<HTMLInputElement>) => {
    const file = e.target.files?.[0];
    if (!file) return;

    setStatus('import', 'loading');
    try {
      const result = await importProjectFromJSON(file);
      if (result.success && result.state) {
        onImportProject?.(result.state);
        setStatus('import', 'success');
        setShowImportDialog(false);
        setTimeout(() => setStatus('import', 'idle'), 2000);
      } else {
        throw new Error(result.error || 'Failed to import project');
      }
    } catch (err) {
      setStatus('import', 'error');
      setError(err instanceof Error ? err.message : 'Failed to import project');
    }

    if (fileInputRef.current) {
      fileInputRef.current.value = '';
    }
  };

  const exportOptions: ExportOption[] = [
    {
      id: 'mp4',
      name: t('story.export_panel.mp4Video'),
      description: t('story.export_panel.mp4Desc'),
      icon: <FileVideo className="w-5 h-5" />,
      category: 'video',
      action: handleExportMP4,
      disabled: !onExportVideo,
      disabledReason: t('story.export_panel.generateVideoFirst'),
    },
    {
      id: 'webm',
      name: t('story.export_panel.webmVideo'),
      description: t('story.export_panel.webmDesc'),
      icon: <Globe className="w-5 h-5" />,
      category: 'video',
      action: handleExportWebM,
      disabled: !hasVideo,
      disabledReason: t('story.export_panel.generateVideoFirst'),
    },
    {
      id: 'srt',
      name: t('story.export_panel.srtSubtitles'),
      description: t('story.export_panel.srtDesc'),
      icon: <Subtitles className="w-5 h-5" />,
      category: 'subtitle',
      action: handleExportSRT,
      disabled: !hasShots,
      disabledReason: t('story.export_panel.generateShotsFirst'),
    },
    {
      id: 'vtt',
      name: t('story.export_panel.webvttSubtitles'),
      description: t('story.export_panel.webvttDesc'),
      icon: <Subtitles className="w-5 h-5" />,
      category: 'subtitle',
      action: handleExportVTT,
      disabled: !hasShots,
      disabledReason: t('story.export_panel.generateShotsFirst'),
    },
    {
      id: 'json',
      name: t('story.export_panel.projectFile'),
      description: t('story.export_panel.projectFileDesc'),
      icon: <FileJson className="w-5 h-5" />,
      category: 'project',
      action: handleExportJSON,
    },
  ];

  const getStatusIcon = (id: string) => {
    const status = exportStatus[id];
    switch (status) {
      case 'loading':
        return <Loader2 className="w-4 h-4 animate-spin" />;
      case 'success':
        return <Check className="w-4 h-4 text-emerald-400" />;
      case 'error':
        return <AlertCircle className="w-4 h-4 text-red-400" />;
      default:
        return <Download className="w-4 h-4" />;
    }
  };

  const videoOptions = exportOptions.filter(o => o.category === 'video');
  const subtitleOptions = exportOptions.filter(o => o.category === 'subtitle');
  const projectOptions = exportOptions.filter(o => o.category === 'project');

  const renderOptionGrid = (options: ExportOption[], accentClass: string) => (
    <div className="grid grid-cols-2 gap-2">
      {options.map((option) => (
        <button
          key={option.id}
          onClick={() => option.action()}
          disabled={option.disabled || exportStatus[option.id] === 'loading'}
          className={cn(
            'p-3 rounded-sm border text-left transition-all duration-200',
            option.disabled
              ? 'border-zinc-800/50 bg-zinc-900/50 opacity-50 cursor-not-allowed'
              : 'border-zinc-800 bg-zinc-900 hover:border-blue-500/40 hover:bg-blue-500/5'
          )}
        >
          <div className="flex items-center justify-between mb-1">
            <span className={accentClass}>{option.icon}</span>
            {getStatusIcon(option.id)}
          </div>
          <p className="text-sm font-medium text-zinc-100">{option.name}</p>
          <p className="text-xs text-zinc-600">
            {option.disabled ? option.disabledReason : option.description}
          </p>
        </button>
      ))}
    </div>
  );

  return (
    <div className={cn('bg-zinc-950 rounded-sm border border-zinc-800 p-4', className)}>
      <div className="flex items-center justify-between mb-4">
        <div className="flex items-center gap-2">
          <Download className="w-5 h-5 text-blue-400" />
          <h3 className="font-sans font-medium text-zinc-100">{t('story.export_panel.exportOptions')}</h3>
        </div>
        {onImportProject && (
          <button
            onClick={() => setShowImportDialog(true)}
            className="flex items-center gap-1.5 px-3 py-1.5 text-sm text-zinc-500 hover:text-zinc-100 hover:bg-zinc-800 rounded-sm transition-colors duration-200"
          >
            <Upload className="w-4 h-4" />
            {t('common.import')}
          </button>
        )}
      </div>

      {/* Error Display */}
      <AnimatePresence>
        {error && (
          <motion.div
            initial={{ opacity: 0, height: 0 }}
            animate={{ opacity: 1, height: 'auto' }}
            exit={{ opacity: 0, height: 0 }}
            transition={{ duration: 0.15 }}
            className="mb-4 p-3 bg-red-500/10 border border-red-500/30 rounded-sm flex items-start gap-2"
          >
            <AlertCircle className="w-4 h-4 text-red-400 flex-shrink-0 mt-0.5" />
            <p className="flex-1 text-sm text-red-300">{error}</p>
            <button onClick={() => setError(null)} className="text-red-400 hover:text-red-300">
              <X className="w-4 h-4" />
            </button>
          </motion.div>
        )}
      </AnimatePresence>

      {/* Native / mobile server URL warning */}
      {isNative() && !import.meta.env.VITE_SERVER_URL && (
        <div className="mb-4 p-3 bg-amber-500/10 border border-amber-500/30 rounded-sm flex items-start gap-2">
          <Wifi className="w-4 h-4 text-amber-400 flex-shrink-0 mt-0.5" />
          <p className="text-xs text-amber-300 leading-relaxed">
            <span className="font-semibold">Mobile export requires a LAN connection.</span>{' '}
            Add <code className="font-mono bg-amber-500/10 px-1 rounded">VITE_SERVER_URL=http://&lt;your-pc-ip&gt;:3001</code> to{' '}
            <code className="font-mono">.env.local</code> and rebuild the app so the device can reach the export server.
          </p>
        </div>
      )}

      {/* Video Exports */}
      <div className="mb-4">
        <h4 className="font-mono text-[10px] text-zinc-600 uppercase tracking-widest mb-2">
          {t('story.export_panel.videoFormats')}
        </h4>
        {renderOptionGrid(videoOptions, 'text-blue-400')}
      </div>

      {/* Subtitle Exports */}
      <div className="mb-4">
        <h4 className="font-mono text-[10px] text-zinc-600 uppercase tracking-widest mb-2">
          {t('story.export_panel.subtitles')}
        </h4>
        {renderOptionGrid(subtitleOptions, 'text-blue-400')}
      </div>

      {/* Project Exports */}
      <div>
        <h4 className="font-mono text-[10px] text-zinc-600 uppercase tracking-widest mb-2">
          {t('story.export_panel.project')}
        </h4>
        {renderOptionGrid(projectOptions, 'text-orange-400')}
      </div>

      {/* Import Dialog */}
      <AnimatePresence>
        {showImportDialog && (
          <motion.div
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            exit={{ opacity: 0 }}
            transition={{ duration: 0.15 }}
            className="fixed inset-0 bg-black/80 backdrop-blur-sm flex items-center justify-center p-4 z-50"
            onClick={() => setShowImportDialog(false)}
          >
            <motion.div
              initial={{ scale: 0.97, opacity: 0 }}
              animate={{ scale: 1, opacity: 1 }}
              exit={{ scale: 0.97, opacity: 0 }}
              transition={{ duration: 0.15 }}
              className="w-full max-w-md bg-zinc-900 rounded-sm border border-zinc-800 p-6"
              onClick={(e) => e.stopPropagation()}
            >
              <h4 className="font-sans text-lg font-medium text-zinc-100 mb-2">{t('story.export_panel.importProject')}</h4>
              <p className="text-sm text-zinc-500 mb-4">
                {t('story.export_panel.importProjectDesc')}
              </p>

              <input
                ref={fileInputRef}
                type="file"
                accept=".json"
                onChange={handleImportFile}
                className="hidden"
              />

              <div
                onClick={() => fileInputRef.current?.click()}
                className="border-2 border-dashed border-zinc-700 rounded-sm p-8 text-center cursor-pointer hover:border-blue-500/40 hover:bg-blue-500/5 transition-all duration-200"
              >
                <Upload className="w-10 h-10 mx-auto mb-3 text-zinc-600" />
                <p className="text-zinc-500">{t('story.export_panel.clickToSelect')}</p>
                <p className="text-xs text-zinc-700 mt-1">{t('story.export_panel.orDragDrop')}</p>
              </div>

              <div className="flex items-center justify-end gap-3 mt-6">
                <button
                  onClick={() => setShowImportDialog(false)}
                  className="px-4 py-2 text-zinc-500 hover:text-zinc-100 transition-colors duration-200"
                >
                  {t('common.cancel')}
                </button>
              </div>
            </motion.div>
          </motion.div>
        )}
      </AnimatePresence>
    </div>
  );
}

export default ExportOptionsPanel;
````

## File: packages/frontend/components/story/IdeaView.tsx
````typescript
import React, { useState, useRef, useEffect } from 'react';
import { AnimatePresence, motion } from 'framer-motion';
import { Sparkles, Heart, Laugh, Skull, Rocket, Search, Sword, Ghost, Crown, Baby, BookOpen, Lightbulb, Wand2, ChevronRight, Layout, ArrowRight } from 'lucide-react';
import { TemplatesGallery } from './TemplatesGallery';
import type { StoryState } from '@/types';
import { useLanguage } from '@/i18n/useLanguage';

interface IdeaViewProps {
    initialTopic?: string;
    onGenerate: (topic: string, genre: string) => void;
    onApplyTemplate?: (state: Partial<StoryState>) => void;
    isProcessing?: boolean;
}

const GENRES = [
    { id: 'Drama', icon: Heart },
    { id: 'Comedy', icon: Laugh },
    { id: 'Thriller', icon: Skull },
    { id: 'Sci-Fi', icon: Rocket },
    { id: 'Mystery', icon: Search },
    { id: 'Action', icon: Sword },
    { id: 'Horror', icon: Ghost },
    { id: 'Fantasy', icon: Wand2 },
    { id: 'Romance', icon: Heart },
    { id: 'Historical', icon: Crown },
    { id: 'Documentary', icon: BookOpen },
    { id: 'Animation', icon: Baby },
];

const STORY_TEMPLATES = [
    { genre: 'Drama', prompt: 'A family reunites after 20 years to confront a secret that tore them apart...' },
    { genre: 'Thriller', prompt: 'A detective discovers that the murder they\'re investigating was committed by their own future self...' },
    { genre: 'Sci-Fi', prompt: 'In 2150, humanity discovers that Earth\'s moon is actually an ancient alien spacecraft...' },
    { genre: 'Mystery', prompt: 'A small town librarian finds coded messages hidden in returned books, leading to a decades-old conspiracy...' },
    { genre: 'Comedy', prompt: 'A wedding planner must organize the perfect ceremony for their ex and their new partner...' },
    { genre: 'Action', prompt: 'A retired spy is pulled back into the game when their grandchild is kidnapped by an old enemy...' },
    { genre: 'Horror', prompt: 'A family moves into their dream home, only to discover the previous owners never actually left...' },
    { genre: 'Fantasy', prompt: 'A young mapmaker discovers their drawings have the power to reshape reality...' },
    { genre: 'Romance', prompt: 'Two rival food truck owners compete for the same corner, but find themselves falling for each other...' },
    { genre: 'Historical', prompt: 'The untold story of the women codebreakers who helped win World War II...' },
    { genre: 'Documentary', prompt: 'An exploration of how a small village in Japan became the world\'s longest-living community...' },
    { genre: 'Animation', prompt: 'A young robot dreams of becoming a painter in a world where machines aren\'t supposed to create art...' },
];

export const IdeaView: React.FC<IdeaViewProps> = ({
    initialTopic = '',
    onGenerate,
    onApplyTemplate,
    isProcessing = false
}) => {
    const { t } = useLanguage();
    const [topic, setTopic] = useState(initialTopic);
    const [genre, setGenre] = useState('Drama');
    const [showAllGenres, setShowAllGenres] = useState(false);
    const [showTemplatesGallery, setShowTemplatesGallery] = useState(false);
    const [expandMessage, setExpandMessage] = useState<string | null>(null);
    const textareaRef = useRef<HTMLTextAreaElement>(null);

    const currentTemplate = STORY_TEMPLATES.find(t => t.genre === genre);
    const visibleGenres = showAllGenres ? GENRES : GENRES.slice(0, 6);

    const handleUseTemplate = () => {
        if (currentTemplate) {
            setTopic(currentTemplate.prompt);
            textareaRef.current?.focus();
        }
    };

    /** Smart Expand: enrich short prompts with narrative scaffolding */
    const handleSmartExpand = () => {
        if (!topic.trim()) return;
        if (topic.length < 50) {
            setTopic(
                'Expand this idea into a rich narrative: ' +
                topic +
                '... (include vivid settings, character motivations, and a surprising twist)'
            );
            textareaRef.current?.focus();
        } else {
            setExpandMessage('Prompt is already detailed enough');
            setTimeout(() => setExpandMessage(null), 2000);
        }
    };

    const handleSubmit = (e: React.FormEvent) => {
        e.preventDefault();
        if (topic.trim() && !isProcessing) {
            onGenerate(topic.trim(), genre);
        }
    };

    // Auto-resize textarea
    useEffect(() => {
        if (textareaRef.current) {
            textareaRef.current.style.height = 'auto';
            textareaRef.current.style.height = Math.max(120, textareaRef.current.scrollHeight) + 'px';
        }
    }, [topic]);

    return (
        <div className="flex flex-col items-center min-h-[70vh] px-6 py-12 bg-black">
            <div className="w-full max-w-2xl">
                {/* Header */}
                <div className="mb-10">
                    <h1 className="font-sans text-3xl font-medium tracking-tight text-zinc-100">
                        {t('story.whatsYourStory')}
                    </h1>
                    <p className="text-zinc-500 text-sm mt-2 leading-relaxed">
                        {t('story.describeYourConcept')}
                    </p>
                </div>

                <form onSubmit={handleSubmit} className="space-y-8">
                    {/* Textarea */}
                    <div>
                        <div className="bg-zinc-900 border border-zinc-800 rounded-sm focus-within:border-blue-500/50 transition-colors duration-200">
                            <textarea
                                ref={textareaRef}
                                id="topic-input"
                                value={topic}
                                onChange={(e) => setTopic(e.target.value)}
                                placeholder={t('story.placeholderStory')}
                                className="
                                    w-full min-h-[120px] px-5 py-4
                                    bg-transparent
                                    text-[15px] text-zinc-100 leading-relaxed
                                    placeholder:text-zinc-600
                                    focus:outline-none
                                    resize-none
                                "
                                disabled={isProcessing}
                                autoFocus
                            />

                            {/* Bottom bar */}
                            <div className="flex items-center justify-between px-5 pb-3.5 pt-0">
                                <div className="flex items-center gap-3">
                                    <button
                                        type="button"
                                        onClick={handleUseTemplate}
                                        disabled={isProcessing || !currentTemplate}
                                        className="flex items-center gap-1.5 text-xs font-mono text-zinc-500 hover:text-zinc-300 transition-colors duration-200 disabled:opacity-30 disabled:hover:text-zinc-500"
                                    >
                                        <Lightbulb className="w-3.5 h-3.5" />
                                        <span>{t('story.tryTemplate')}</span>
                                    </button>
                                    {onApplyTemplate && (
                                        <>
                                            <div className="w-px h-3 bg-zinc-800" />
                                            <button
                                                type="button"
                                                onClick={() => setShowTemplatesGallery(true)}
                                                disabled={isProcessing}
                                                className="flex items-center gap-1.5 text-xs font-mono text-zinc-500 hover:text-zinc-300 transition-colors duration-200 disabled:opacity-30"
                                            >
                                                <Layout className="w-3.5 h-3.5" />
                                                <span>{t('story.browseAll')}</span>
                                            </button>
                                        </>
                                    )}
                                </div>
                                <span className="font-mono text-[10px] text-zinc-600 tabular-nums">
                                    {topic.length}
                                </span>
                            </div>
                        </div>
                    </div>

                    {/* Smart Expand Button */}
                    <div className="relative">
                        <button
                            type="button"
                            onClick={handleSmartExpand}
                            disabled={isProcessing || !topic.trim()}
                            className="bg-zinc-900 border border-zinc-800 rounded-sm text-zinc-400 hover:text-blue-400 hover:border-blue-500/50 transition-colors duration-200 px-3 py-1.5 text-xs font-mono flex items-center gap-1.5 disabled:opacity-30 disabled:cursor-not-allowed"
                        >
                            <Sparkles className="w-3.5 h-3.5" />
                            <span>Smart Expand</span>
                        </button>
                        {expandMessage && (
                            <span className="absolute left-0 top-full mt-1.5 text-xs font-mono text-zinc-500 animate-pulse">
                                {expandMessage}
                            </span>
                        )}
                    </div>

                    {/* Genre Selection */}
                    <div>
                        <div className="flex items-center justify-between mb-3">
                            <span className="font-mono text-[11px] font-medium tracking-[0.15em] uppercase text-zinc-500">
                                {t('story.genre')}
                            </span>
                            {!showAllGenres && GENRES.length > 6 && (
                                <button
                                    type="button"
                                    onClick={() => setShowAllGenres(true)}
                                    className="flex items-center gap-1 text-xs font-mono text-zinc-500 hover:text-zinc-300 transition-colors duration-200"
                                >
                                    <span>{t('story.allCount', { count: GENRES.length })}</span>
                                    <ChevronRight className="w-3 h-3" />
                                </button>
                            )}
                        </div>

                        <div className="flex flex-wrap gap-2">
                            {visibleGenres.map((g) => {
                                const Icon = g.icon;
                                const isSelected = genre === g.id;
                                return (
                                    <button
                                        key={g.id}
                                        type="button"
                                        onClick={() => setGenre(g.id)}
                                        disabled={isProcessing}
                                        className={`
                                            flex items-center gap-2 px-3 py-1.5 rounded-sm
                                            border transition-colors duration-200
                                            disabled:opacity-40 disabled:cursor-not-allowed
                                            ${isSelected
                                                ? 'bg-blue-500/10 border-blue-500/50'
                                                : 'border-zinc-800 hover:border-zinc-600'
                                            }
                                        `}
                                    >
                                        <Icon
                                            className={`w-3.5 h-3.5 transition-colors duration-200 ${
                                                isSelected ? 'text-blue-400' : 'text-zinc-600'
                                            }`}
                                        />
                                        <span
                                            className={`text-[13px] font-medium transition-colors duration-200 ${
                                                isSelected ? 'text-blue-400' : 'text-zinc-500'
                                            }`}
                                        >
                                            {t(`story.genres.${g.id}`)}
                                        </span>
                                    </button>
                                );
                            })}
                        </div>
                    </div>

                    {/* Submit */}
                    <div>
                        <button
                            type="submit"
                            disabled={!topic.trim() || isProcessing}
                            className={`
                                w-full flex items-center justify-center gap-3
                                px-8 py-3 rounded-sm
                                font-mono text-sm font-medium
                                transition-colors duration-200
                                ${topic.trim() && !isProcessing
                                    ? 'bg-white text-black hover:bg-zinc-200'
                                    : 'bg-zinc-800 text-zinc-600 cursor-not-allowed'
                                }
                            `}
                        >
                            {isProcessing ? (
                                <>
                                    <div className="w-4 h-4 rounded-sm border-2 border-current border-t-transparent animate-spin" />
                                    <span>{t('story.buildingStory')}</span>
                                </>
                            ) : (
                                <>
                                    <span>{t('story.beginStory')}</span>
                                    <ArrowRight className="w-4 h-4" />
                                </>
                            )}
                        </button>
                    </div>
                </form>
            </div>

            {/* Templates Gallery Modal */}
            <AnimatePresence>
                {showTemplatesGallery && onApplyTemplate && (
                    <motion.div
                        initial={{ opacity: 0 }}
                        animate={{ opacity: 1 }}
                        exit={{ opacity: 0 }}
                        transition={{ duration: 0.15 }}
                        className="fixed inset-0 bg-black/80 backdrop-blur-sm flex items-center justify-center p-4 z-50"
                        onClick={() => setShowTemplatesGallery(false)}
                    >
                        <motion.div
                            initial={{ scale: 0.97, opacity: 0 }}
                            animate={{ scale: 1, opacity: 1 }}
                            exit={{ scale: 0.97, opacity: 0 }}
                            transition={{ duration: 0.2, ease: 'easeOut' }}
                            className="w-full max-w-5xl h-[80vh]"
                            onClick={(e) => e.stopPropagation()}
                        >
                            <TemplatesGallery
                                onApplyTemplate={(state) => {
                                    onApplyTemplate(state);
                                    setShowTemplatesGallery(false);
                                }}
                                onClose={() => setShowTemplatesGallery(false)}
                            />
                        </motion.div>
                    </motion.div>
                )}
            </AnimatePresence>
        </div>
    );
};

export default IdeaView;
````

## File: packages/frontend/components/story/index.ts
````typescript
export * from './IdeaView';
export * from './ShotEditorModal';
export * from './ScriptView';
export * from './CharacterView';
export * from './StoryboardView';
export * from './StoryWorkspace';
export * from './LockWarningDialog';
export * from './BreakdownProgress';
export * from './StyleSelector';
export * from './TemplatesGallery';
export * from './VersionHistoryPanel';
export * from './ExportOptionsPanel';
````

## File: packages/frontend/components/story/LockWarningDialog.tsx
````typescript
/**
 * LockWarningDialog.tsx
 * Warning dialog for screenplay locking with cost breakdown.
 */

import React from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import { Lock, Film, X, Info } from 'lucide-react';
import { useLanguage } from '@/i18n/useLanguage';

interface LockWarningDialogProps {
    isOpen: boolean;
    onClose: () => void;
    onConfirmLock: () => void;
    estimatedCost: number;
    sceneCount?: number;
    estimatedShots?: number;
}

export const LockWarningDialog: React.FC<LockWarningDialogProps> = ({
    isOpen,
    onClose,
    onConfirmLock,
    estimatedCost,
    sceneCount = 0,
    estimatedShots = 0,
}) => {
    const { t } = useLanguage();

    return (
        <AnimatePresence>
            {isOpen && (
                <div className="fixed inset-0 z-50 flex items-center justify-center">
                    {/* Backdrop */}
                    <motion.div
                        initial={{ opacity: 0 }}
                        animate={{ opacity: 1 }}
                        exit={{ opacity: 0 }}
                        transition={{ duration: 0.15 }}
                        className="absolute inset-0 bg-black/80 backdrop-blur-sm"
                        onClick={onClose}
                    />

                    {/* Modal */}
                    <motion.div
                        initial={{ opacity: 0, scale: 0.97 }}
                        animate={{ opacity: 1, scale: 1 }}
                        exit={{ opacity: 0, scale: 0.97 }}
                        transition={{ duration: 0.2, ease: 'easeOut' }}
                        className="relative w-full max-w-md mx-4 bg-zinc-900 border border-zinc-800 rounded-sm overflow-hidden"
                    >
                        {/* Close button */}
                        <button
                            onClick={onClose}
                            className="absolute top-4 right-4 p-2 text-zinc-600 hover:text-zinc-300 transition-colors duration-200 z-10"
                        >
                            <X className="w-5 h-5" />
                        </button>

                        <div className="relative p-8">
                            {/* Icon */}
                            <div className="flex justify-center mb-6">
                                <div className="relative">
                                    <div className="w-16 h-16 rounded-sm bg-orange-500/10 border border-orange-500/30 flex items-center justify-center">
                                        <Film className="w-8 h-8 text-orange-400" />
                                    </div>
                                    {/* Lock badge */}
                                    <div className="absolute -bottom-1 -right-1 w-7 h-7 rounded-sm bg-blue-500 flex items-center justify-center">
                                        <Lock className="w-3.5 h-3.5 text-white" />
                                    </div>
                                </div>
                            </div>

                            {/* Title */}
                            <h2 className="font-sans text-2xl font-medium tracking-tight text-zinc-100 text-center mb-3">
                                {t('story.lock_dialog.lockTheScript')}
                            </h2>

                            {/* Warning Message */}
                            <p className="text-zinc-500 text-sm text-center mb-8">
                                {t('story.lock_dialog.scriptFinalized')}
                            </p>

                            {/* Cost Breakdown */}
                            <div className="bg-zinc-950 border border-zinc-800 rounded-sm p-5 mb-6">
                                <div className="flex items-center gap-3 mb-4">
                                    <span className="font-mono text-[10px] text-zinc-600 uppercase tracking-widest">
                                        {t('story.lock_dialog.productionEstimate')}
                                    </span>
                                    <div className="flex-1 h-px bg-zinc-800" />
                                </div>

                                <div className="space-y-3">
                                    {sceneCount > 0 && (
                                        <div className="flex justify-between items-center">
                                            <span className="text-zinc-500 text-sm">{t('story.scenes')}</span>
                                            <span className="font-mono text-zinc-200">{sceneCount}</span>
                                        </div>
                                    )}
                                    {estimatedShots > 0 && (
                                        <div className="flex justify-between items-center">
                                            <span className="text-zinc-500 text-sm">{t('story.lock_dialog.estimatedShots')}</span>
                                            <span className="font-mono text-zinc-200">~{estimatedShots}</span>
                                        </div>
                                    )}
                                    <div className="h-px bg-zinc-800 my-3" />
                                    <div className="flex justify-between items-center">
                                        <span className="text-zinc-500 text-sm">{t('story.lock_dialog.totalBudget')}</span>
                                        <span className="font-sans text-xl text-blue-400">
                                            ${estimatedCost.toFixed(2)}
                                        </span>
                                    </div>
                                </div>
                            </div>

                            {/* Info Note */}
                            <div className="flex items-start gap-3 mb-8 p-4 bg-blue-500/5 border border-blue-500/20 rounded-sm">
                                <Info className="w-4 h-4 text-blue-400 mt-0.5 shrink-0" />
                                <p className="text-zinc-500 text-sm">
                                    {t('story.lock_dialog.shotBreakdownInfo')}
                                </p>
                            </div>

                            {/* Action Buttons */}
                            <div className="flex gap-4">
                                <button
                                    onClick={onClose}
                                    className="flex-1 px-6 py-3.5 rounded-sm text-zinc-300 bg-zinc-950 border border-zinc-800 hover:border-zinc-600 transition-colors duration-200"
                                >
                                    {t('story.lock_dialog.backToEdit')}
                                </button>
                                <button
                                    onClick={onConfirmLock}
                                    className="flex-1 flex items-center justify-center gap-2 px-6 py-3.5 rounded-sm bg-blue-500 hover:bg-blue-600 text-white font-mono text-sm font-medium transition-colors duration-200"
                                >
                                    <Lock className="w-4 h-4" />
                                    {t('story.lock_dialog.lockAndBegin')}
                                </button>
                            </div>
                        </div>
                    </motion.div>
                </div>
            )}
        </AnimatePresence>
    );
};

export default LockWarningDialog;
````

## File: packages/frontend/components/story/SceneCard.tsx
````typescript
import React from 'react';
import { RefreshCcw } from 'lucide-react';
import { MarkdownContent } from '@/components/ui/MarkdownContent';
import { cn } from '@/lib/utils';

interface SceneCardProps {
  sceneNumber: number;
  heading: string;
  content: string;
  onRegenerate?: (sceneNumber: number, feedback: string) => void;
  isProcessing?: boolean;
  children?: React.ReactNode;
  className?: string;
}

export function SceneCard({
  sceneNumber,
  heading,
  content,
  onRegenerate,
  isProcessing = false,
  children,
  className,
}: SceneCardProps) {
  const handleRegenerate = () => {
    const feedback = window.prompt(
      `How should we redo Scene ${sceneNumber}? (Optional)`,
      ''
    );
    if (feedback !== null) onRegenerate?.(sceneNumber, feedback);
  };

  return (
    <div className={cn('bg-zinc-900 border border-zinc-800 rounded-sm p-5', className)}>
      {/* Scene header */}
      <div className="flex justify-between items-start mb-4">
        <div className="flex items-center gap-3">
          <span className="font-mono text-xs text-blue-400">
            SCENE {String(sceneNumber).padStart(2, '0')}
          </span>
          <div className="w-6 h-px bg-zinc-700" />
        </div>
        {onRegenerate && (
          <button
            onClick={handleRegenerate}
            className="p-2 text-zinc-600 hover:text-blue-400 rounded-sm transition-colors duration-200 ease-out"
            disabled={isProcessing}
            aria-label={`Regenerate scene ${sceneNumber}`}
          >
            <RefreshCcw className="w-4 h-4" />
          </button>
        )}
      </div>

      {/* Scene heading */}
      <h3 className="font-sans text-base font-medium text-zinc-100 tracking-tight mb-3" dir="auto">
        {heading}
      </h3>

      {/* Scene content with markdown rendering */}
      <MarkdownContent content={content} className="text-zinc-400 text-sm" />

      {/* Slot for additional content (shots, audio, etc.) */}
      {children && <div className="mt-4 pt-4 border-t border-zinc-800">{children}</div>}
    </div>
  );
}

export default SceneCard;
````

## File: packages/frontend/components/story/ScriptView.tsx
````typescript
import React from 'react';
import type { ScreenplayScene } from '@/types';
import { FileText, GripVertical } from 'lucide-react';

interface ScriptViewProps {
    script: { title: string; scenes: ScreenplayScene[] } | null;
    onUpdate?: (script: any) => void;
}

export const ScriptView: React.FC<ScriptViewProps> = ({ script, onUpdate }) => {
    if (!script) {
        return (
            <div className="flex flex-col items-center justify-center min-h-[50vh] p-12">
                <FileText className="w-12 h-12 text-zinc-700 mb-4" />
                <p className="text-zinc-500 text-sm">
                    No script generated yet. Proceed to create your screenplay.
                </p>
            </div>
        );
    }

    // Detect if content is RTL (Arabic, Hebrew, etc.)
    const isRTL = script.scenes.some(s =>
        /[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF]/.test(
            s.heading + s.action + s.dialogue.map(d => d.text).join('')
        )
    );

    return (
        <div className="py-8 px-4">
            {/* Screenplay Container */}
            <div
                className="max-w-3xl mx-auto bg-zinc-900 border border-zinc-800 rounded-sm overflow-hidden"
                dir={isRTL ? 'rtl' : 'ltr'}
            >
                {/* Title Page Header */}
                <div className="p-8 border-b border-zinc-800 text-center">
                    <div className="flex items-center justify-center gap-3 mb-4">
                        <div className="w-12 h-px bg-zinc-800" />
                        <span className="font-mono text-[10px] text-zinc-600 tracking-[0.3em]">
                            SCREENPLAY
                        </span>
                        <div className="w-12 h-px bg-zinc-800" />
                    </div>
                    <h1
                        className="font-sans text-3xl font-medium tracking-tight text-zinc-100"
                        dir="auto"
                    >
                        {script.title}
                    </h1>
                </div>

                {/* Screenplay Content */}
                <div className="p-8 space-y-8">
                    {script.scenes.map((scene, sceneIdx) => (
                        <div
                            key={scene.id}
                            className="group relative"
                            data-scene-id={scene.id}
                            data-scene-index={sceneIdx}
                        >
                            {/* Scene Heading */}
                            <div
                                className={`
                                    relative mb-4 py-2 flex items-start gap-2
                                    ${isRTL
                                        ? 'border-r-2 pr-4 border-blue-500'
                                        : 'border-l-2 pl-4 border-blue-500'
                                    }
                                `}
                            >
                                {/* Drag Handle */}
                                <div className="text-zinc-700 hover:text-zinc-400 cursor-grab active:cursor-grabbing transition-colors duration-200 mt-0.5 shrink-0 opacity-0 group-hover:opacity-100">
                                    <GripVertical className="w-4 h-4" />
                                </div>

                                <div>
                                    <span className="font-mono text-xs text-blue-400 tracking-widest mb-1 block">
                                        SCENE {String(scene.sceneNumber).padStart(2, '0')}
                                    </span>
                                    <h2
                                        className="font-sans text-lg font-medium text-zinc-100 uppercase tracking-wide"
                                        dir="auto"
                                    >
                                        {scene.heading}
                                    </h2>
                                </div>
                            </div>

                            {/* Action / Description */}
                            <div
                                className="text-zinc-400 text-sm leading-relaxed mb-6 px-4"
                                dir="auto"
                            >
                                {scene.action}
                            </div>

                            {/* Dialogue Block */}
                            {scene.dialogue.length > 0 && (
                                <div className="space-y-6 my-6">
                                    {scene.dialogue.map((line, idx) => (
                                        <div
                                            key={`${scene.id}-dialogue-${idx}`}
                                            className="flex flex-col items-center"
                                        >
                                            <div
                                                className="font-mono text-xs text-blue-400 uppercase tracking-widest mb-2"
                                                dir="auto"
                                            >
                                                {line.speaker}
                                            </div>
                                            <div
                                                className="text-zinc-300 text-sm text-center max-w-md leading-relaxed"
                                                dir="auto"
                                            >
                                                &ldquo;{line.text}&rdquo;
                                            </div>
                                        </div>
                                    ))}
                                </div>
                            )}

                            {/* Characters Present Tags */}
                            {scene.charactersPresent.length > 0 && (
                                <div
                                    className="flex items-center gap-2 mt-6 px-4"
                                    dir="ltr"
                                >
                                    <span className="font-mono text-[10px] text-zinc-700 uppercase tracking-wider">
                                        Present:
                                    </span>
                                    <div className="flex flex-wrap gap-1">
                                        {scene.charactersPresent.map((char, i) => (
                                            <span
                                                key={i}
                                                className="bg-zinc-950 rounded-sm px-2 py-0.5 text-[10px] font-mono text-zinc-600 border border-zinc-800"
                                            >
                                                {char}
                                            </span>
                                        ))}
                                    </div>
                                </div>
                            )}

                            {/* Scene Divider */}
                            {sceneIdx < script.scenes.length - 1 && (
                                <div className="mt-8 pt-8 border-t border-zinc-800" />
                            )}
                        </div>
                    ))}
                </div>

                {/* Footer */}
                <div className="p-6 border-t border-zinc-800 text-center">
                    <span className="font-mono text-[10px] text-zinc-700 tracking-widest">
                        {script.scenes.length} SCENE{script.scenes.length !== 1 ? 'S' : ''}
                    </span>
                </div>
            </div>
        </div>
    );
};

export default ScriptView;
````

## File: packages/frontend/components/story/ShotEditorModal.tsx
````typescript
/**
 * ShotEditorModal.tsx
 * Full-featured per-shot editor dialog matching the Storyboarder.ai design.
 * Opens from the StoryboardView floating panel or shot table rows.
 */

import React, { useState, useEffect, useCallback } from 'react';
import { motion } from 'framer-motion';
import {
    ChevronLeft, ChevronRight, Loader2, RefreshCw, Save, X
} from 'lucide-react';
import {
    Dialog,
    DialogContent,
    DialogHeader,
    DialogTitle,
} from '@/components/ui/dialog';
import type { ShotlistEntry } from '@/types';

export interface ShotEditorModalProps {
    shot: ShotlistEntry | null;
    sceneNumber: number;
    sceneHeading: string;
    sceneLighting?: string;
    shotIndexInScene: number;
    totalShotsInScene: number;
    onClose: () => void;
    onSave: (shotId: string, updates: Partial<ShotlistEntry>) => void;
    onRetry: (shotId: string) => void;
    onNavigate: (direction: 'prev' | 'next') => void;
    isProcessing?: boolean;
}

const SHOT_TYPE_OPTIONS = [
    'Wide',
    'Medium',
    'Close-up',
    'Extreme Close-up',
    'POV',
    'Over-the-shoulder',
];

const CAMERA_ANGLE_OPTIONS = [
    'Eye-level',
    'High',
    'Low',
    'Dutch',
    "Bird's-eye",
    "Worm's-eye",
];

const MOVEMENT_OPTIONS = [
    'Static',
    'Pan',
    'Tilt',
    'Zoom',
    'Dolly',
    'Tracking',
    'Handheld',
];

const ASPECT_RATIO_OPTIONS = ['16:9', '9:16', '1:1', '4:3'];

// Local editable state mirrors the shot fields
interface LocalEdits {
    description: string;
    dialogue: string;
    durationEst: number;
    shotType: string;
    cameraAngle: string;
    movement: string;
    equipment: string;
    focalLength: string;
    aspectRatio: string;
    notes: string;
}

function shotToLocal(shot: ShotlistEntry): LocalEdits {
    return {
        description: shot.description || '',
        dialogue: shot.dialogue || '',
        durationEst: shot.durationEst ?? 5,
        shotType: shot.shotType || '',
        cameraAngle: shot.cameraAngle || '',
        movement: shot.movement || '',
        equipment: shot.equipment || '',
        focalLength: shot.focalLength || '',
        aspectRatio: shot.aspectRatio || '16:9',
        notes: shot.notes || '',
    };
}

function LabelCell({ label }: { label: string }) {
    return (
        <td className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-500 whitespace-nowrap align-top pt-3 w-28">
            {label}
        </td>
    );
}

export const ShotEditorModal: React.FC<ShotEditorModalProps> = ({
    shot,
    sceneNumber,
    sceneHeading,
    sceneLighting,
    shotIndexInScene,
    totalShotsInScene,
    onClose,
    onSave,
    onRetry,
    onNavigate,
    isProcessing = false,
}) => {
    const [edits, setEdits] = useState<LocalEdits>(() =>
        shot ? shotToLocal(shot) : shotToLocal({} as ShotlistEntry)
    );
    const [dialogueOpen, setDialogueOpen] = useState(false);

    // Re-sync local state whenever the shot changes (navigation)
    useEffect(() => {
        if (shot) setEdits(shotToLocal(shot));
    }, [shot?.id]);

    const set = useCallback(<K extends keyof LocalEdits>(key: K, value: LocalEdits[K]) => {
        setEdits(prev => ({ ...prev, [key]: value }));
    }, []);

    const handleSave = () => {
        if (!shot) return;
        onSave(shot.id, {
            description: edits.description,
            dialogue: edits.dialogue || undefined,
            durationEst: edits.durationEst,
            shotType: edits.shotType || undefined,
            cameraAngle: edits.cameraAngle || undefined,
            movement: edits.movement || undefined,
            equipment: edits.equipment || undefined,
            focalLength: edits.focalLength || undefined,
            aspectRatio: edits.aspectRatio || undefined,
            notes: edits.notes || undefined,
        });
        onClose();
    };

    const handleRetry = () => {
        if (!shot) return;
        onRetry(shot.id);
    };

    const inputCls =
        'w-full bg-zinc-900 border border-zinc-700 rounded-sm px-2 py-1.5 text-xs text-zinc-200 focus:outline-none focus:border-zinc-500 transition-colors placeholder:text-zinc-600';
    const selectCls =
        'w-full bg-zinc-900 border border-zinc-700 rounded-sm px-2 py-1.5 text-xs text-zinc-200 focus:outline-none focus:border-zinc-500 transition-colors';

    return (
        <Dialog open={!!shot} onOpenChange={(open) => { if (!open) onClose(); }}>
            <DialogContent
                showCloseButton={false}
                className="max-w-5xl w-full bg-zinc-950 border border-zinc-800 text-zinc-100 p-0 gap-0 rounded-xl overflow-hidden"
            >
                {/* Header */}
                <div className="flex items-center justify-between px-6 py-4 border-b border-zinc-800">
                    <DialogTitle className="text-base font-semibold text-zinc-100">
                        Edit Your Shot
                    </DialogTitle>
                    <button
                        onClick={onClose}
                        className="p-1.5 rounded-sm text-zinc-500 hover:text-zinc-200 hover:bg-zinc-800 transition-colors"
                        aria-label="Close"
                    >
                        <X className="w-4 h-4" />
                    </button>
                </div>

                {/* Body */}
                <div className="flex gap-0 min-h-0">
                    {/* Left: Image Preview + Navigation */}
                    <div className="w-72 shrink-0 flex flex-col bg-zinc-900/50 border-r border-zinc-800">
                        {/* Image */}
                        <div className="relative aspect-video bg-zinc-950 overflow-hidden">
                            {shot?.imageUrl ? (
                                <motion.img
                                    key={shot.id}
                                    initial={{ opacity: 0 }}
                                    animate={{ opacity: 1 }}
                                    src={shot.imageUrl}
                                    alt={shot.description}
                                    className="w-full h-full object-cover"
                                />
                            ) : (
                                <div className="w-full h-full flex items-center justify-center">
                                    {isProcessing ? (
                                        <Loader2 className="w-8 h-8 text-zinc-700 animate-spin" />
                                    ) : (
                                        <div className="text-zinc-700 text-xs font-mono text-center">
                                            No visual generated
                                        </div>
                                    )}
                                </div>
                            )}
                        </div>

                        {/* Navigation */}
                        <div className="flex items-center justify-between px-4 py-2 border-t border-zinc-800 bg-zinc-900/80">
                            <button
                                onClick={() => onNavigate('prev')}
                                disabled={shotIndexInScene === 0}
                                className="p-1 text-zinc-500 hover:text-zinc-200 disabled:text-zinc-700 transition-colors"
                                aria-label="Previous shot"
                            >
                                <ChevronLeft className="w-4 h-4" />
                            </button>
                            <span className="font-mono text-xs text-zinc-500">
                                {shotIndexInScene + 1} / {totalShotsInScene}
                            </span>
                            <button
                                onClick={() => onNavigate('next')}
                                disabled={shotIndexInScene >= totalShotsInScene - 1}
                                className="p-1 text-zinc-500 hover:text-zinc-200 disabled:text-zinc-700 transition-colors"
                                aria-label="Next shot"
                            >
                                <ChevronRight className="w-4 h-4" />
                            </button>
                        </div>

                        {/* Description textarea */}
                        <div className="p-4 flex-1 flex flex-col gap-2">
                            <label className="font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-500">
                                Description
                            </label>
                            <textarea
                                value={edits.description}
                                onChange={e => set('description', e.target.value)}
                                rows={4}
                                className={`${inputCls} resize-none flex-1`}
                                placeholder="Shot description..."
                            />
                        </div>
                    </div>

                    {/* Right: Scene info + shot metadata table */}
                    <div className="flex-1 flex flex-col min-w-0 overflow-y-auto">
                        {/* Scene heading */}
                        <div className="px-6 py-4 border-b border-zinc-800 bg-zinc-900/30">
                            <div className="flex items-center gap-3 mb-1">
                                <span className="font-mono text-[10px] font-bold text-blue-400 bg-blue-500/10 px-2 py-0.5 rounded-sm border border-blue-500/20 shrink-0">
                                    SCENE {sceneNumber}
                                </span>
                                <span className="font-mono text-sm font-medium text-zinc-100 truncate" dir="auto">
                                    {sceneHeading}
                                </span>
                            </div>
                            {sceneLighting && (
                                <div className="flex items-center gap-1.5 text-[11px] text-zinc-500">
                                    <span className="w-1.5 h-1.5 rounded-full bg-amber-400/70 shrink-0" />
                                    {sceneLighting}
                                </div>
                            )}
                        </div>

                        {/* Metadata table */}
                        <div className="px-4 py-4 overflow-x-auto">
                            <table className="w-full border-collapse">
                                <tbody>
                                    {/* Row 1: ERT + SIZE */}
                                    <tr>
                                        <LabelCell label="ERT (sec)" />
                                        <td className="py-2 px-3 align-top pt-3">
                                            <input
                                                type="number"
                                                value={edits.durationEst}
                                                onChange={e => set('durationEst', parseFloat(e.target.value) || 0)}
                                                className={`${inputCls} w-20 text-center`}
                                                min={1}
                                                max={120}
                                                step={0.5}
                                            />
                                        </td>
                                        <LabelCell label="Size" />
                                        <td className="py-2 px-3 align-top pt-3">
                                            <select
                                                value={edits.shotType}
                                                onChange={e => set('shotType', e.target.value)}
                                                className={selectCls}
                                            >
                                                <option value="">—</option>
                                                {SHOT_TYPE_OPTIONS.map(o => (
                                                    <option key={o} value={o}>{o}</option>
                                                ))}
                                            </select>
                                        </td>
                                    </tr>

                                    {/* Row 2: PERSPECTIVE + MOVEMENT */}
                                    <tr>
                                        <LabelCell label="Perspective" />
                                        <td className="py-2 px-3 align-top pt-3">
                                            <select
                                                value={edits.cameraAngle}
                                                onChange={e => set('cameraAngle', e.target.value)}
                                                className={selectCls}
                                            >
                                                <option value="">—</option>
                                                {CAMERA_ANGLE_OPTIONS.map(o => (
                                                    <option key={o} value={o}>{o}</option>
                                                ))}
                                            </select>
                                        </td>
                                        <LabelCell label="Movement" />
                                        <td className="py-2 px-3 align-top pt-3">
                                            <select
                                                value={edits.movement}
                                                onChange={e => set('movement', e.target.value)}
                                                className={selectCls}
                                            >
                                                <option value="">—</option>
                                                {MOVEMENT_OPTIONS.map(o => (
                                                    <option key={o} value={o}>{o}</option>
                                                ))}
                                            </select>
                                        </td>
                                    </tr>

                                    {/* Row 3: EQUIPMENT + FOCAL LENGTH */}
                                    <tr>
                                        <LabelCell label="Equipment" />
                                        <td className="py-2 px-3 align-top pt-3">
                                            <input
                                                type="text"
                                                value={edits.equipment}
                                                onChange={e => set('equipment', e.target.value)}
                                                className={inputCls}
                                                placeholder="e.g. Tripod"
                                            />
                                        </td>
                                        <LabelCell label="Focal Length" />
                                        <td className="py-2 px-3 align-top pt-3">
                                            <input
                                                type="text"
                                                value={edits.focalLength}
                                                onChange={e => set('focalLength', e.target.value)}
                                                className={inputCls}
                                                placeholder="e.g. 35mm"
                                            />
                                        </td>
                                    </tr>

                                    {/* Row 4: ASPECT RATIO + NOTES */}
                                    <tr>
                                        <LabelCell label="Aspect Ratio" />
                                        <td className="py-2 px-3 align-top pt-3">
                                            <select
                                                value={edits.aspectRatio}
                                                onChange={e => set('aspectRatio', e.target.value)}
                                                className={selectCls}
                                            >
                                                {ASPECT_RATIO_OPTIONS.map(o => (
                                                    <option key={o} value={o}>{o}</option>
                                                ))}
                                            </select>
                                        </td>
                                        <LabelCell label="Notes" />
                                        <td className="py-2 px-3 align-top pt-3">
                                            <input
                                                type="text"
                                                value={edits.notes}
                                                onChange={e => set('notes', e.target.value)}
                                                className={inputCls}
                                                placeholder="Production notes..."
                                            />
                                        </td>
                                    </tr>

                                    {/* Row 5: DIALOGUE (collapsible) */}
                                    <tr>
                                        <td colSpan={4} className="py-2 px-3 pt-3">
                                            <button
                                                type="button"
                                                onClick={() => setDialogueOpen(prev => !prev)}
                                                className="flex items-center gap-2 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-500 hover:text-zinc-300 transition-colors"
                                            >
                                                <ChevronRight
                                                    className={`w-3 h-3 transition-transform ${dialogueOpen ? 'rotate-90' : ''}`}
                                                />
                                                Dialogue
                                            </button>
                                            {dialogueOpen && (
                                                <textarea
                                                    value={edits.dialogue}
                                                    onChange={e => set('dialogue', e.target.value)}
                                                    rows={3}
                                                    className={`${inputCls} mt-2 resize-none w-full`}
                                                    placeholder="Character dialogue or voice-over..."
                                                />
                                            )}
                                        </td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </div>

                {/* Footer */}
                <div className="flex items-center justify-end gap-3 px-6 py-4 border-t border-zinc-800 bg-zinc-950">
                    <button
                        type="button"
                        onClick={handleRetry}
                        disabled={isProcessing}
                        className="flex items-center gap-2 px-4 py-2 rounded-sm border border-zinc-700 text-zinc-400 hover:text-zinc-200 hover:border-zinc-500 text-sm font-medium transition-colors disabled:opacity-40"
                    >
                        {isProcessing ? (
                            <Loader2 className="w-4 h-4 animate-spin" />
                        ) : (
                            <RefreshCw className="w-4 h-4" />
                        )}
                        Retry
                    </button>
                    <button
                        type="button"
                        onClick={handleSave}
                        disabled={isProcessing}
                        className="flex items-center gap-2 px-5 py-2 rounded-sm bg-blue-500 hover:bg-blue-600 text-white text-sm font-semibold transition-colors disabled:opacity-40"
                    >
                        <Save className="w-4 h-4" />
                        Save
                    </button>
                </div>
            </DialogContent>
        </Dialog>
    );
};

export default ShotEditorModal;
````

## File: packages/frontend/components/story/StepProgressBar.tsx
````typescript
import React from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import { Loader2, Check } from 'lucide-react';

type StepStatus = 'completed' | 'active' | 'pending' | 'processing';

interface StepTab {
  id: string;
  label: string;
}

interface StepProgressBarProps {
  tabs: StepTab[];
  currentTabId: string;
  onTabClick: (tabId: string) => void;
  getStepStatus: (tabId: string) => StepStatus;
  isProcessing: boolean;
  progress: { message: string; percent: number };
}

export function StepProgressBar({
  tabs,
  currentTabId,
  onTabClick,
  getStepStatus,
  isProcessing,
  progress,
}: StepProgressBarProps) {
  const currentIndex = tabs.findIndex((t) => t.id === currentTabId);

  return (
    <div className="w-full bg-zinc-950 border-b border-zinc-800">
      <div className="max-w-5xl mx-auto px-6 py-5">
        {/* Step track */}
        <div className="relative flex items-center justify-between">
          {/* Background track line */}
          <div className="absolute top-[14px] left-0 right-0 h-px bg-zinc-800" />

          {/* Completed portion of track */}
          <motion.div
            className="absolute top-[14px] left-0 h-px bg-emerald-500"
            initial={false}
            animate={{
              width: `${(currentIndex / Math.max(tabs.length - 1, 1)) * 100}%`,
            }}
            transition={{ duration: 0.2, ease: 'easeOut' }}
          />

          {tabs.map((tab, index) => {
            const status = getStepStatus(tab.id);
            const isActive = tab.id === currentTabId;
            const isCompleted = status === 'completed';
            const isProcessingStep = status === 'processing';
            const isAccessible = index <= currentIndex || isCompleted;

            return (
              <div key={tab.id} className="relative flex flex-col items-center z-10">
                <button
                  onClick={() => isAccessible && onTabClick(tab.id)}
                  disabled={!isAccessible}
                  className={[
                    'relative w-7 h-7 rounded-sm flex items-center justify-center font-mono text-xs font-medium',
                    'transition-colors duration-200 focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-blue-500',
                    isActive
                      ? 'bg-blue-500 text-white border border-blue-500'
                      : isCompleted
                        ? 'bg-emerald-500/20 text-emerald-400 border border-emerald-500/30'
                        : 'bg-zinc-900 text-zinc-600 border border-zinc-800',
                    isAccessible && !isActive ? 'cursor-pointer hover:border-zinc-600' : '',
                    !isAccessible ? 'cursor-not-allowed opacity-40' : '',
                  ].join(' ')}
                >
                  <AnimatePresence mode="wait">
                    {isProcessingStep ? (
                      <motion.div
                        key="processing"
                        initial={{ opacity: 0 }}
                        animate={{ opacity: 1 }}
                        exit={{ opacity: 0 }}
                        transition={{ duration: 0.15 }}
                      >
                        <Loader2 className="w-3.5 h-3.5 animate-spin" />
                      </motion.div>
                    ) : isCompleted ? (
                      <motion.div
                        key="completed"
                        initial={{ opacity: 0, scale: 0.5 }}
                        animate={{ opacity: 1, scale: 1 }}
                        exit={{ opacity: 0, scale: 0.5 }}
                        transition={{ duration: 0.15 }}
                      >
                        <Check className="w-3.5 h-3.5 stroke-[2.5]" />
                      </motion.div>
                    ) : (
                      <motion.span
                        key="number"
                        initial={{ opacity: 0 }}
                        animate={{ opacity: 1 }}
                        exit={{ opacity: 0 }}
                        transition={{ duration: 0.15 }}
                      >
                        {index + 1}
                      </motion.span>
                    )}
                  </AnimatePresence>
                </button>

                <span
                  className={[
                    'mt-2 text-xs font-medium tracking-tight whitespace-nowrap transition-colors duration-200',
                    isActive
                      ? 'text-white'
                      : isCompleted
                        ? 'text-emerald-400'
                        : 'text-zinc-600',
                  ].join(' ')}
                >
                  {tab.label}
                </span>
              </div>
            );
          })}
        </div>

        {/* Processing indicator */}
        <AnimatePresence>
          {isProcessing && (
            <motion.div
              initial={{ opacity: 0, height: 0 }}
              animate={{ opacity: 1, height: 'auto' }}
              exit={{ opacity: 0, height: 0 }}
              transition={{ duration: 0.2, ease: 'easeOut' }}
              className="mt-4 pt-3 border-t border-zinc-800"
            >
              <div className="flex items-center gap-3">
                <Loader2 className="w-3.5 h-3.5 text-blue-500 animate-spin shrink-0" />
                <span className="font-mono text-xs text-zinc-500 truncate">
                  {progress.message || 'Processing...'}
                </span>
                <div className="flex-1 h-1 bg-zinc-900 rounded-sm overflow-hidden">
                  <motion.div
                    className="h-full bg-blue-500"
                    initial={{ width: 0 }}
                    animate={{ width: `${progress.percent}%` }}
                    transition={{ duration: 0.3, ease: 'easeOut' }}
                  />
                </div>
                <span className="font-mono text-xs text-zinc-500 min-w-[3rem] text-right shrink-0">
                  {progress.percent}%
                </span>
              </div>
            </motion.div>
          )}
        </AnimatePresence>
      </div>
    </div>
  );
}

export default StepProgressBar;
````

## File: packages/frontend/components/story/StoryboardProgress.tsx
````typescript
/**
 * StoryboardProgress.tsx
 * Loading screen for storyboard generation with stage tracking.
 */

import React from 'react';
import { motion } from 'framer-motion';
import { CheckCircle2, Circle, Loader2 } from 'lucide-react';
import { useLanguage } from '@/i18n/useLanguage';

export type StoryboardStage =
    | 'shotlist'
    | 'characters'
    | 'storyboard';

interface StoryboardProgressProps {
    currentStage: StoryboardStage;
    isComplete?: boolean;
}

interface StageConfig {
    id: StoryboardStage;
    label: string;
    description: string;
}

const STAGE_KEYS: Record<StoryboardStage, { label: string; description: string }> = {
    shotlist: { label: 'story.storyboard_progress.generatingShotList', description: 'story.storyboard_progress.breakingScenes' },
    characters: { label: 'story.storyboard_progress.preparingCast', description: 'story.storyboard_progress.loadingProfiles' },
    storyboard: { label: 'story.storyboard_progress.renderingStoryboard', description: 'story.storyboard_progress.creatingFrames' },
};

const STAGES: StageConfig[] = [
    { id: 'shotlist', label: 'Generating Shot List', description: 'Breaking down scenes into shots' },
    { id: 'characters', label: 'Preparing Cast', description: 'Loading character profiles' },
    { id: 'storyboard', label: 'Rendering Storyboard', description: 'Creating visual frames' },
];

const getStageIndex = (stage: StoryboardStage): number => {
    return STAGES.findIndex(s => s.id === stage);
};

export const StoryboardProgress: React.FC<StoryboardProgressProps> = ({
    currentStage,
    isComplete = false,
}) => {
    const { t } = useLanguage();
    const currentIndex = getStageIndex(currentStage);

    const getStageStatus = (index: number): 'complete' | 'processing' | 'pending' => {
        if (isComplete) return 'complete';
        if (index < currentIndex) return 'complete';
        if (index === currentIndex) return 'processing';
        return 'pending';
    };

    return (
        <div className="flex flex-col items-center justify-center min-h-[70vh] p-8">
            <motion.div
                initial={{ opacity: 0, y: 20 }}
                animate={{ opacity: 1, y: 0 }}
                transition={{ duration: 0.2, ease: 'easeOut' }}
                className="w-full max-w-lg"
            >
                {/* Header */}
                <div className="text-center mb-12">
                    <div className="flex items-center justify-center mb-6">
                        <Loader2 className="w-8 h-8 text-blue-500 animate-spin" />
                    </div>
                    <h2 className="font-sans text-2xl font-medium tracking-tight text-zinc-100 mb-3">
                        {t('story.storyboard_progress.developingVision')}
                    </h2>
                    <p className="text-zinc-500 text-sm">
                        {t('story.storyboard_progress.renderedFrameByFrame')}
                    </p>
                </div>

                {/* Stage List */}
                <div className="space-y-6">
                    {STAGES.map((stage, index) => {
                        const status = getStageStatus(index);
                        return (
                            <motion.div
                                key={stage.id}
                                initial={{ opacity: 0, x: -10 }}
                                animate={{ opacity: 1, x: 0 }}
                                transition={{ duration: 0.2, ease: 'easeOut', delay: index * 0.08 }}
                                className="flex items-start gap-4"
                            >
                                {/* Status Icon */}
                                <div className="shrink-0 mt-1">
                                    {status === 'complete' && (
                                        <CheckCircle2 className="w-6 h-6 text-blue-400" />
                                    )}
                                    {status === 'processing' && (
                                        <Loader2 className="w-6 h-6 text-zinc-400 animate-spin" />
                                    )}
                                    {status === 'pending' && (
                                        <Circle className="w-6 h-6 text-zinc-700" />
                                    )}
                                </div>

                                {/* Content */}
                                <div className="flex-1">
                                    <span className={`
                                        font-sans text-base block mb-1 transition-colors duration-200 ease-out
                                        ${status === 'complete' || status === 'processing'
                                            ? 'text-zinc-300'
                                            : 'text-zinc-600'
                                        }
                                    `}>
                                        {t(STAGE_KEYS[stage.id].label)}
                                    </span>
                                    <span className={`
                                        text-xs transition-colors duration-200 ease-out
                                        ${status === 'pending' ? 'text-zinc-700' : 'text-zinc-600'}
                                    `}>
                                        {t(STAGE_KEYS[stage.id].description)}
                                    </span>
                                </div>
                            </motion.div>
                        );
                    })}
                </div>
            </motion.div>
        </div>
    );
};

export default StoryboardProgress;
````

## File: packages/frontend/components/story/StoryboardView.tsx
````typescript
/**
 * StoryboardView.tsx
 * Cinematic storyboard viewer — full-bleed preview with floating info panel
 * and scene-grouped thumbnail strip.
 */

import React, { useState, useEffect, useMemo, useRef } from 'react';
import { AnimatePresence, motion } from 'framer-motion';
import type { ShotlistEntry, ScreenplayScene } from '@/types';
import {
    Play, SkipBack, SkipForward,
    ChevronLeft, ChevronRight,
    Wand2, Video, Loader2, ImageIcon, Maximize2, Pencil
} from 'lucide-react';

interface StoryboardViewProps {
    shots: ShotlistEntry[];
    scenes?: ScreenplayScene[];
    scenesWithVisuals?: string[];
    onGenerateVisuals?: (sceneIndex?: number) => void;
    onGenerateVideo?: (shotId: string) => void;
    onUpdateDuration?: (shotId: string, duration: number) => void;
    onEditShot?: (shotId: string) => void;
    isProcessing?: boolean;
}

interface SceneGroup {
    scene: ScreenplayScene;
    shots: ShotlistEntry[];
}

export const StoryboardView: React.FC<StoryboardViewProps> = ({
    shots,
    scenes = [],
    onGenerateVisuals,
    onGenerateVideo,
    onUpdateDuration,
    onEditShot,
    isProcessing = false,
}) => {
    const [selectedShotIndex, setSelectedShotIndex] = useState(0);
    const [localDuration, setLocalDuration] = useState<number>(0);
    const [isFullscreen, setIsFullscreen] = useState(false);
    const containerRef = useRef<HTMLDivElement>(null);
    const thumbnailStripRef = useRef<HTMLDivElement>(null);

    if (!shots || shots.length === 0) {
        return (
            <div className="flex flex-col items-center justify-center min-h-[60vh] gap-6">
                <div className="w-20 h-20 rounded-2xl bg-linear-to-br from-primary/10 to-accent/10 border border-border flex items-center justify-center">
                    <ImageIcon className="w-9 h-9 text-muted-foreground" />
                </div>
                <div className="text-center">
                    <p className="font-editorial text-lg text-foreground/70 mb-2">No Shots Available</p>
                    <p className="text-muted-foreground text-sm">
                        Generate a shot list to begin storyboarding
                    </p>
                </div>
            </div>
        );
    }

    const currentShot = shots[selectedShotIndex];
    const currentScene = currentShot ? scenes.find(s => s.id === currentShot.sceneId) : undefined;

    // Group shots by scene for the timeline strip
    const sceneGroups = useMemo<SceneGroup[]>(() => {
        if (scenes.length === 0) return [];
        return scenes
            .map(scene => ({
                scene,
                shots: shots.filter(s => s.sceneId === scene.id),
            }))
            .filter(g => g.shots.length > 0);
    }, [scenes, shots]);

    useEffect(() => {
        if (currentShot) {
            setLocalDuration(currentShot.durationEst || 5);
        }
    }, [currentShot?.id, currentShot?.durationEst]);

    // Scroll selected thumbnail into view
    useEffect(() => {
        if (thumbnailStripRef.current) {
            const selected = thumbnailStripRef.current.querySelector('[data-selected="true"]');
            if (selected) {
                selected.scrollIntoView({ behavior: 'smooth', block: 'nearest', inline: 'center' });
            }
        }
    }, [selectedShotIndex]);

    const handleNext = () => {
        if (selectedShotIndex < shots.length - 1) {
            setSelectedShotIndex(prev => prev + 1);
        }
    };

    const handlePrev = () => {
        if (selectedShotIndex > 0) {
            setSelectedShotIndex(prev => prev - 1);
        }
    };

    // Keyboard navigation
    useEffect(() => {
        const handleKeyDown = (e: KeyboardEvent): void => {
            if (e.key === 'ArrowLeft') {
                e.preventDefault();
                handlePrev();
            } else if (e.key === 'ArrowRight') {
                e.preventDefault();
                handleNext();
            } else if (e.key === 'f' || e.key === 'F') {
                setIsFullscreen(prev => !prev);
            } else if ((e.key === 'e' || e.key === 'E') && !e.ctrlKey && !e.metaKey) {
                const target = e.target as HTMLElement;
                const isEditing = ['INPUT', 'TEXTAREA', 'SELECT'].includes(target.tagName);
                if (!isEditing && onEditShot && currentShot) {
                    e.preventDefault();
                    onEditShot(currentShot.id);
                }
            }
        };
        window.addEventListener('keydown', handleKeyDown);
        return () => window.removeEventListener('keydown', handleKeyDown);
    }, [selectedShotIndex, shots.length]);

    const handleDurationChange = (e: React.ChangeEvent<HTMLInputElement>) => {
        const val = parseFloat(e.target.value);
        if (!isNaN(val) && val > 0) {
            setLocalDuration(val);
        }
    };

    const handleSaveDuration = () => {
        if (onUpdateDuration && currentShot) {
            onUpdateDuration(currentShot.id, localDuration);
        }
    };

    const toggleFullscreen = () => {
        if (!document.fullscreenElement && containerRef.current) {
            containerRef.current.requestFullscreen?.();
            setIsFullscreen(true);
        } else if (document.fullscreenElement) {
            document.exitFullscreen?.();
            setIsFullscreen(false);
        }
    };

    return (
        <div ref={containerRef} className="flex flex-col h-full bg-background relative">
            {/* Main Preview Area */}
            <div className="flex-1 relative overflow-hidden min-h-0">
                {/* Full-bleed Image/Video */}
                <AnimatePresence mode="wait">
                    <motion.div
                        key={currentShot?.id || selectedShotIndex}
                        initial={{ opacity: 0 }}
                        animate={{ opacity: 1 }}
                        exit={{ opacity: 0 }}
                        transition={{ duration: 0.2 }}
                        className="absolute inset-0"
                    >
                        {currentShot?.imageUrl ? (
                            <img
                                src={currentShot.imageUrl}
                                alt={currentShot.description}
                                className="w-full h-full object-cover"
                            />
                        ) : (
                            <div className="w-full h-full flex flex-col items-center justify-center gap-6 bg-background">
                                <div className="w-20 h-20 rounded-2xl bg-linear-to-br from-primary/10 to-accent/10 border border-border flex items-center justify-center">
                                    <Wand2 className="w-9 h-9 text-muted-foreground" />
                                </div>
                                <p className="font-code text-sm text-muted-foreground uppercase tracking-widest">
                                    No Visual Generated
                                </p>
                                {onGenerateVisuals && (
                                    <button
                                        onClick={() => onGenerateVisuals()}
                                        className="px-6 py-3 rounded-lg bg-primary hover:bg-primary/90 text-primary-foreground font-semibold text-sm transition-colors duration-200 shadow-lg shadow-primary/20"
                                    >
                                        Generate All Visuals
                                    </button>
                                )}
                            </div>
                        )}
                    </motion.div>
                </AnimatePresence>

                {/* Center Generate Video overlay button */}
                {currentShot?.imageUrl && onGenerateVideo && (
                    <div className="absolute inset-0 flex items-center justify-center z-10 pointer-events-none">
                        <button
                            onClick={() => onGenerateVideo(currentShot.id)}
                            disabled={isProcessing}
                            className="pointer-events-auto px-6 py-3 bg-accent hover:bg-accent/90 text-accent-foreground font-bold text-sm rounded-lg flex items-center gap-2 shadow-2xl disabled:opacity-40 disabled:cursor-not-allowed transition-colors"
                        >
                            {isProcessing ? <Loader2 className="w-4 h-4 animate-spin" /> : <Video className="w-4 h-4" />}
                            Generate Video
                        </button>
                    </div>
                )}

                {/* Floating Info Panel (bottom-left) */}
                {currentShot?.imageUrl && (
                    <motion.div
                        initial={{ x: -20, opacity: 0 }}
                        animate={{ x: 0, opacity: 1 }}
                        transition={{ delay: 0.1, duration: 0.3 }}
                        className="absolute bottom-4 left-4 z-20 w-[340px] max-h-[calc(100%-2rem)]"
                    >
                        <div className="bg-black/85 backdrop-blur-md border border-border rounded-xl overflow-hidden flex flex-col shadow-2xl">
                            {/* Mini thumbnail with navigation */}
                            <div className="relative aspect-video bg-secondary overflow-hidden border-b border-border">
                                <img
                                    src={currentShot.imageUrl}
                                    alt=""
                                    className="w-full h-full object-cover"
                                />
                                {/* Shot navigation overlay */}
                                <div className="absolute inset-x-0 bottom-0 flex items-center justify-center gap-3 py-2 bg-linear-to-t from-black/80 to-transparent">
                                    <button
                                        onClick={handlePrev}
                                        disabled={selectedShotIndex === 0}
                                        className="p-1 text-foreground/80 hover:text-foreground disabled:text-foreground/20 transition-colors"
                                    >
                                        <ChevronLeft className="w-5 h-5" />
                                    </button>
                                    <span className="font-code text-xs text-foreground/70">
                                        {selectedShotIndex + 1} / {shots.length}
                                    </span>
                                    <button
                                        onClick={handleNext}
                                        disabled={selectedShotIndex === shots.length - 1}
                                        className="p-1 text-foreground/80 hover:text-foreground disabled:text-foreground/20 transition-colors"
                                    >
                                        <ChevronRight className="w-5 h-5" />
                                    </button>
                                </div>
                            </div>

                            {/* Shot Details */}
                            <div className="p-4 space-y-3 overflow-y-auto max-h-[280px]">
                                {/* Scene / Shot label */}
                                <h3 className="font-editorial font-semibold text-foreground text-sm">
                                    Scene {currentScene?.sceneNumber || '?'} | Shot {currentShot.shotNumber}
                                </h3>

                                {/* Description */}
                                <div>
                                    <p className="text-[11px] font-code font-semibold text-muted-foreground uppercase tracking-wider mb-1">Description</p>
                                    <p className="text-foreground/80 text-sm leading-relaxed">
                                        {currentShot.description || 'No description'}
                                    </p>
                                </div>

                                {/* Dialogue */}
                                {currentShot.dialogue && (
                                    <div>
                                        <p className="text-[11px] font-code font-semibold text-muted-foreground uppercase tracking-wider mb-1">Dialogue</p>
                                        <p className="text-foreground/80 text-sm leading-relaxed italic">
                                            {currentShot.dialogue}
                                        </p>
                                    </div>
                                )}

                                {/* Duration */}
                                <div className="flex items-center gap-3">
                                    <span className="text-[11px] font-code font-semibold text-muted-foreground uppercase tracking-wider">
                                        Duration (ERT):
                                    </span>
                                    <input
                                        type="number"
                                        value={localDuration}
                                        onChange={handleDurationChange}
                                        onBlur={handleSaveDuration}
                                        className="w-14 px-2 py-1 bg-secondary border border-border rounded-md text-foreground text-sm text-center font-code focus:outline-none focus:border-primary"
                                        min={1}
                                        max={60}
                                    />
                                </div>

                                {/* Update ERT button */}
                                <button
                                    onClick={handleSaveDuration}
                                    className="w-full py-2 bg-secondary hover:bg-muted text-foreground/80 text-sm font-medium rounded-lg border border-border transition-colors"
                                >
                                    Update ERT
                                </button>

                                {/* Edit Shot button */}
                                {onEditShot && (
                                    <button
                                        onClick={() => currentShot && onEditShot(currentShot.id)}
                                        className="w-full py-2 bg-secondary hover:bg-muted text-foreground/80 text-sm font-medium rounded-lg border border-border flex items-center justify-center gap-2 transition-colors"
                                    >
                                        <Pencil className="w-3.5 h-3.5" />
                                        Edit Shot
                                    </button>
                                )}

                                {/* Generate Video button */}
                                <button
                                    onClick={() => currentShot && onGenerateVideo?.(currentShot.id)}
                                    disabled={isProcessing || !currentShot?.imageUrl}
                                    className="w-full py-2.5 bg-accent hover:bg-accent/90 text-accent-foreground font-semibold text-sm rounded-lg flex items-center justify-center gap-2 disabled:opacity-40 disabled:cursor-not-allowed transition-colors shadow-lg shadow-accent/20"
                                >
                                    {isProcessing ? (
                                        <Loader2 className="w-4 h-4 animate-spin" />
                                    ) : (
                                        <Video className="w-4 h-4" />
                                    )}
                                    Generate Video
                                </button>

                                <p className="text-[10px] text-muted-foreground text-center">
                                    Video length is limited to up to 12 seconds
                                </p>
                            </div>
                        </div>
                    </motion.div>
                )}

                {/* Navigation Arrows */}
                <button
                    onClick={handlePrev}
                    disabled={selectedShotIndex === 0}
                    className="absolute left-0 bottom-0 z-10 p-3 text-foreground/60 hover:text-foreground disabled:text-foreground/10 transition-colors"
                    style={{ bottom: '8px', left: currentShot?.imageUrl ? '360px' : '16px' }}
                    aria-label="Previous shot"
                >
                    <SkipBack className="w-6 h-6" />
                </button>

                <button
                    onClick={() => {/* play/pause — placeholder */ }}
                    className="absolute z-10 p-3 text-foreground/60 hover:text-foreground transition-colors"
                    style={{ bottom: '8px', left: currentShot?.imageUrl ? '396px' : '52px' }}
                    aria-label="Play"
                >
                    <Play className="w-6 h-6" />
                </button>

                <button
                    onClick={handleNext}
                    disabled={selectedShotIndex === shots.length - 1}
                    className="absolute z-10 p-3 text-foreground/60 hover:text-foreground disabled:text-foreground/10 transition-colors"
                    style={{ bottom: '8px', left: currentShot?.imageUrl ? '432px' : '88px' }}
                    aria-label="Next shot"
                >
                    <SkipForward className="w-6 h-6" />
                </button>

                {/* Fullscreen toggle */}
                <button
                    onClick={toggleFullscreen}
                    className="absolute bottom-3 right-3 z-10 p-2 text-foreground/40 hover:text-foreground transition-colors"
                    aria-label="Toggle fullscreen"
                >
                    <Maximize2 className="w-5 h-5" />
                </button>
            </div>

            {/* Thumbnail Strip */}
            <div
                ref={thumbnailStripRef}
                className="h-24 shrink-0 bg-card/95 backdrop-blur border-t border-border flex items-center gap-1 px-3 overflow-x-auto no-scrollbar"
            >
                {sceneGroups.length > 0 ? (
                    sceneGroups.map((group) => (
                        <React.Fragment key={group.scene.id}>
                            {group.shots.map((shot) => {
                                const idx = shots.indexOf(shot);
                                const isSelected = idx === selectedShotIndex;
                                return (
                                    <button
                                        key={shot.id}
                                        data-selected={isSelected}
                                        onClick={() => setSelectedShotIndex(idx)}
                                        className={`
                                            relative flex-none h-16 w-24 rounded-md overflow-hidden border-2 transition-all duration-150
                                            ${isSelected
                                                ? 'border-primary ring-1 ring-primary/30 scale-105 z-10'
                                                : 'border-transparent opacity-60 hover:opacity-100 hover:border-border'
                                            }
                                        `}
                                    >
                                        {shot.imageUrl ? (
                                            <img
                                                src={shot.imageUrl}
                                                alt=""
                                                className="w-full h-full object-cover"
                                            />
                                        ) : (
                                            <div className="w-full h-full bg-secondary flex items-center justify-center">
                                                <ImageIcon className="w-4 h-4 text-muted-foreground" />
                                            </div>
                                        )}
                                    </button>
                                );
                            })}
                        </React.Fragment>
                    ))
                ) : (
                    shots.map((shot, idx) => {
                        const isSelected = idx === selectedShotIndex;
                        return (
                            <button
                                key={shot.id}
                                data-selected={isSelected}
                                onClick={() => setSelectedShotIndex(idx)}
                                className={`
                                    relative flex-none h-16 w-24 rounded-md overflow-hidden border-2 transition-all duration-150
                                    ${isSelected
                                        ? 'border-primary ring-1 ring-primary/30 scale-105 z-10'
                                        : 'border-transparent opacity-60 hover:opacity-100 hover:border-border'
                                    }
                                `}
                            >
                                {shot.imageUrl ? (
                                    <img
                                        src={shot.imageUrl}
                                        alt=""
                                        className="w-full h-full object-cover"
                                    />
                                ) : (
                                    <div className="w-full h-full bg-secondary flex items-center justify-center">
                                        <ImageIcon className="w-4 h-4 text-muted-foreground" />
                                    </div>
                                )}
                            </button>
                        );
                    })
                )}
            </div>
        </div>
    );
};

export default StoryboardView;
````

## File: packages/frontend/components/story/StoryWorkspace.tsx
````typescript
/**
 * StoryWorkspace.tsx
 * Main orchestrator for the story mode pipeline.
 */

import React, { useState, useEffect, useMemo, useRef } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import { IdeaView } from './IdeaView';
import { ScriptView } from './ScriptView';
import { CharacterView } from './CharacterView';
import { StoryboardView } from './StoryboardView';
import { ShotEditorModal } from './ShotEditorModal';
import { LockWarningDialog } from './LockWarningDialog';
import { StyleSelector } from './StyleSelector';
import { BreakdownProgress } from './BreakdownProgress';
import { StoryboardProgress } from './StoryboardProgress';
import { SceneCard } from './SceneCard';
import { StepProgressBar } from './StepProgressBar';
import type { StoryState, StoryStep, CharacterProfile, ShotlistEntry } from '@/types';
import type { VisualStyleKey, AspectRatioId } from '@/constants/visualStyles';
import { estimateProjectCost } from '@/utils/costEstimator';
import { Download, RefreshCcw, Undo2, Redo2, Lock, CheckCircle2, Circle, Loader2, AlertCircle, X, Film, Mic, Video, Play, Check, History, ImageIcon, MessageCircle, GripVertical, Pencil } from 'lucide-react';
import { VersionHistoryPanel } from './VersionHistoryPanel';
import { ExportOptionsPanel } from './ExportOptionsPanel';
import { useLanguage } from '@/i18n/useLanguage';
import { FormatSelector } from '@/components/FormatSelector';
import { PipelineProgress } from '@/components/PipelineProgress';
import { CheckpointApproval } from '@/components/CheckpointApproval';
import { ReferenceDocumentUpload } from '@/components/ReferenceDocumentUpload';
import { formatRegistry } from '@/services/formatRegistry';
import type { UseFormatPipelineReturn } from '@/hooks/useFormatPipeline';

interface StageProgress {
    totalScenes: number;
    scenesWithShots: number;
    scenesWithVisuals: number;
    shotsComplete: boolean;
    visualsComplete: boolean;
}

interface StoryWorkspaceProps {
    storyState: StoryState;
    onNextStep: () => void;
    onGenerateIdea?: (topic: string, genre: string) => void;
    initialTopic?: string;
    onExportScript?: () => void;
    onRegenerateScene?: (sceneNumber: number, feedback: string) => void;
    onVerifyConsistency?: (characterName: string) => void;
    onGenerateScreenplay?: () => void;
    onGenerateCharacters?: () => void;
    onUndo?: () => void;
    onRedo?: () => void;
    canUndo?: boolean;
    canRedo?: boolean;
    isProcessing: boolean;
    progress: { message: string; percent: number };
    onLockStory?: () => void;
    onUpdateVisualStyle?: (style: string) => void;
    onUpdateAspectRatio?: (ratio: string) => void;
    onUpdateImageProvider?: (provider: 'gemini' | 'deapi') => void;
    onGenerateShots?: (sceneIndex?: number) => void;
    onGenerateVisuals?: (sceneIndex?: number) => void;
    stageProgress?: StageProgress;
    error?: string | null;
    onClearError?: () => void;
    onRetry?: () => void;
    onAddCharacter?: () => void;
    onEditCharacter?: (character: CharacterProfile) => void;
    onDeleteCharacter?: (characterId: string) => void;
    onGenerateCharacterImage?: (characterId: string) => void;
    onGenerateVideo?: (shotId: string) => void;
    onUpdateShotDuration?: (shotId: string, duration: number) => void;
    onUpdateShot?: (shotId: string, updates: Partial<ShotlistEntry>) => void;
    onGenerateNarration?: () => void;
    onAnimateShots?: (shotIndex?: number) => void;
    onExportFinalVideo?: () => Promise<Blob | null | undefined>;
    onDownloadVideo?: () => void;
    allScenesHaveNarration?: () => boolean;
    allShotsHaveAnimation?: () => boolean;
    onApplyTemplate?: (state: Partial<StoryState>) => void;
    onImportProject?: (state: StoryState) => void;
    projectId?: string;
    /** Format pipeline hook for multi-format support */
    formatPipelineHook?: UseFormatPipelineReturn;
    /** Called when user clicks "Start Production" in FormatSelector */
    onFormatExecute?: () => void;
    /** Transition to the advanced Video Editor */
    onOpenInEditor?: () => void;
}

type MainStep = 'idea' | 'breakdown' | 'storyboard';

const quickFade = {
    initial: { opacity: 0 },
    animate: { opacity: 1 },
    exit: { opacity: 0 },
    transition: { duration: 0.15 },
};

function deriveEquipment(movement: string): string {
    const m = movement.toLowerCase();
    if (m === 'static') return 'Tripod';
    if (m.includes('dolly')) return 'Dolly';
    if (m.includes('track')) return 'Steady cam';
    if (m.includes('handheld')) return 'Handheld';
    if (m.includes('pan') || m.includes('tilt')) return 'Tripod';
    return 'Steady cam';
}

function deriveFocalLength(shotType: string): string {
    const s = shotType.toLowerCase();
    if (s.includes('extreme')) return '85mm';
    if (s.includes('close')) return '50mm';
    if (s.includes('medium')) return '35mm';
    if (s.includes('wide') || s.includes('long')) return '24mm';
    if (s.includes('pov') || s.includes('shoulder')) return '40mm';
    return '35mm';
}

export const StoryWorkspace: React.FC<StoryWorkspaceProps> = ({
    storyState,
    onNextStep,
    onGenerateIdea,
    initialTopic,
    onExportScript,
    onRegenerateScene,
    onVerifyConsistency,
    onGenerateScreenplay,
    onGenerateCharacters,
    onUndo,
    onRedo,
    canUndo,
    canRedo,
    isProcessing,
    progress,
    onLockStory,
    onUpdateVisualStyle,
    onUpdateAspectRatio,
    onUpdateImageProvider,
    onGenerateShots,
    onGenerateVisuals,
    stageProgress,
    error,
    onClearError,
    onRetry,
    onAddCharacter,
    onEditCharacter,
    onDeleteCharacter,
    onGenerateCharacterImage,
    onGenerateVideo,
    onUpdateShotDuration,
    onUpdateShot,
    onGenerateNarration,
    onAnimateShots,
    onExportFinalVideo,
    onDownloadVideo,
    allScenesHaveNarration,
    allShotsHaveAnimation,
    onApplyTemplate,
    onImportProject,
    projectId,
    formatPipelineHook,
    onFormatExecute,
    onOpenInEditor,
}) => {
    const { t } = useLanguage();

    const getHighLevelStep = (step: StoryStep): MainStep => {
        if (step === 'idea') return 'idea';
        if (['breakdown', 'script', 'characters'].includes(step)) return 'breakdown';
        return 'storyboard';
    };

    const [activeMainTab, setActiveMainTab] = useState<MainStep>(getHighLevelStep(storyState.currentStep));
    const [subTab, setSubTab] = useState<StoryStep>(storyState.currentStep);
    const [showLockDialog, setShowLockDialog] = useState(false);
    const [showVersionHistory, setShowVersionHistory] = useState(false);
    const [editingShot, setEditingShot] = useState<ShotlistEntry | null>(null);
    // Fix #2: Keep a ref to the current editingShot so onNavigate never closes over a stale value.
    const editingShotRef = useRef(editingShot);
    useEffect(() => { editingShotRef.current = editingShot; }, [editingShot]);
    // Track which individual shots are being animated (by shot.id)
    const [animatingShotIds, setAnimatingShotIds] = useState<Set<string>>(new Set());

    // Clear per-shot animation trackers when global processing finishes
    useEffect(() => {
        if (!isProcessing) setAnimatingShotIds(new Set());
    }, [isProcessing]);

    // Fix #1: Memoize blob URLs from format-pipeline narration results.
    // URL.createObjectURL must never be called in render — it creates a new URL
    // on every render that is never revoked, causing a memory leak.
    const narrationBlobUrlMap = useMemo<Map<string, string>>(() => {
        if (!formatPipelineHook?.result?.success) return new Map();
        const pr = formatPipelineHook.result.partialResults ?? {};
        const narrations = (pr.narrationSegments ?? []) as { sceneId: string; audioBlob: Blob }[];
        return new Map(narrations.map(n => [n.sceneId, URL.createObjectURL(n.audioBlob)]));
    }, [formatPipelineHook?.result]);
    // Revoke old blob URLs when the map changes or component unmounts
    useEffect(() => {
        return () => { narrationBlobUrlMap.forEach(url => URL.revokeObjectURL(url)); };
    }, [narrationBlobUrlMap]);

    // Fix #4: Pre-compute animatedShots lookup map to avoid O(n²) in the animation grid.
    const animatedShotsMap = useMemo(() => {
        const m = new Map<string, NonNullable<typeof storyState.animatedShots>[number]>();
        storyState.animatedShots?.forEach(a => m.set(a.shotId, a));
        return m;
    }, [storyState.animatedShots]);

    useEffect(() => {
        const newMain = getHighLevelStep(storyState.currentStep);
        setActiveMainTab(newMain);
        setSubTab(storyState.currentStep);
    }, [storyState.currentStep]);

    useEffect(() => {
        const handleKeyDown = (e: KeyboardEvent) => {
            if (e.ctrlKey || e.metaKey) {
                if (e.key === 'z') {
                    if (e.shiftKey) {
                        if (canRedo && onRedo) { e.preventDefault(); onRedo(); }
                    } else {
                        if (canUndo && onUndo) { e.preventDefault(); onUndo(); }
                    }
                } else if (e.key === 'y') {
                    if (canRedo && onRedo) { e.preventDefault(); onRedo(); }
                }
            }
        };
        window.addEventListener('keydown', handleKeyDown);
        return () => window.removeEventListener('keydown', handleKeyDown);
    }, [onUndo, onRedo, canUndo, canRedo]);

    const mainTabs: { id: MainStep; label: string; number: string }[] = [
        { id: 'idea', label: t('story.storyIdea'), number: '1' },
        { id: 'breakdown', label: t('story.breakdown'), number: '2' },
        { id: 'storyboard', label: t('story.storyboard'), number: '3' },
    ];

    const handleProceed = () => {
        if (subTab === 'script' && !storyState.isLocked) {
            setShowLockDialog(true);
        } else {
            onNextStep();
        }
    };

    const handleTabNavigation = (tabId: StoryStep) => {
        if (tabId === 'script' && !storyState.script && !isProcessing) {
            onGenerateScreenplay?.();
        } else if (tabId === 'characters' && storyState.characters.length === 0 && !isProcessing) {
            onGenerateCharacters?.();
        }
        setSubTab(tabId);
    };

    const isBreakdownProcessing = isProcessing && activeMainTab === 'breakdown' && storyState.breakdown.length === 0;
    const isStoryboardProcessing = isProcessing && activeMainTab === 'storyboard' && (!storyState.shots || storyState.shots.length === 0);

    const getStepCompletionStatus = (stepId: StoryStep): 'completed' | 'active' | 'pending' | 'processing' => {
        const storyboardOrder: StoryStep[] = ['shots', 'style', 'storyboard', 'narration', 'animation', 'export'];
        const breakdownOrder: StoryStep[] = ['breakdown', 'script', 'characters'];
        const currentOrder = activeMainTab === 'storyboard' ? storyboardOrder : breakdownOrder;
        const currentIndex = currentOrder.indexOf(subTab);
        const stepIndex = currentOrder.indexOf(stepId);

        if (stepId === subTab) return isProcessing ? 'processing' : 'active';

        if (activeMainTab === 'storyboard') {
            switch (stepId) {
                case 'shots': return (storyState.shots?.length ?? 0) > 0 ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
                case 'style': return storyState.visualStyle ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
                case 'storyboard': return (storyState.scenesWithVisuals?.length ?? 0) > 0 ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
                case 'narration': return (storyState.narrationSegments?.length ?? 0) > 0 ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
                case 'animation': return (storyState.animatedShots?.length ?? 0) > 0 ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
                case 'export': return storyState.finalVideoUrl ? 'completed' : 'pending';
            }
        } else {
            switch (stepId) {
                case 'breakdown': return storyState.breakdown.length > 0 ? 'completed' : 'pending';
                case 'script': return storyState.script ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
                case 'characters': return storyState.characters.length > 0 ? 'completed' : 'pending';
            }
        }
        return stepIndex < currentIndex ? 'completed' : 'pending';
    };

    const renderSubNav = (tabs: { id: StoryStep; label: string }[]) => (
        <StepProgressBar
            tabs={tabs}
            currentTabId={subTab}
            onTabClick={(tabId) => handleTabNavigation(tabId as StoryStep)}
            getStepStatus={(tabId) => getStepCompletionStatus(tabId as StoryStep)}
            isProcessing={isProcessing}
            progress={progress}
        />
    );

    const renderMainContent = () => {
        if (activeMainTab === 'idea') {
            // If format pipeline hook is provided, use the multi-format flow
            if (formatPipelineHook) {
                const fpHook = formatPipelineHook;
                const isMovieAnimation = fpHook.selectedFormat === 'movie-animation';

                // State 1: Pipeline is running (non-movie format) → PipelineProgress
                if (fpHook.isRunning && !isMovieAnimation) {
                    return (
                        <motion.div key="pipeline-progress" {...quickFade} className="h-full flex items-center justify-center p-8">
                            <PipelineProgress
                                executionProgress={fpHook.executionProgress}
                                tasks={fpHook.tasks}
                                currentPhase={fpHook.currentPhase}
                                isRunning={fpHook.isRunning}
                                onCancel={fpHook.cancel}
                                isCancelling={fpHook.isCancelling}
                            />
                        </motion.div>
                    );
                }

                // State 2: Pipeline completed (success) → result view with asset preview
                if (!fpHook.isRunning && fpHook.result?.success) {
                    const pr = fpHook.result.partialResults ?? {};
                    const screenplay = (pr.screenplay ?? []) as { id: string; heading: string; action: string; dialogue?: { speaker: string; text: string }[] }[];
                    const visuals = (pr.visuals ?? []) as { sceneId: string; imageUrl: string }[];
                    const narrations = (pr.narrationSegments ?? []) as { sceneId: string; audioBlob: Blob; audioDuration: number; transcript: string }[];
                    const totalDuration = pr.totalDuration as number | undefined;
                    const research = pr.research as { sources?: { title: string }[]; citations?: { text: string }[] } | undefined;
                    const formatName = fpHook.selectedFormat ? formatRegistry.getFormat(fpHook.selectedFormat)?.name : 'video';

                    // Build a visual/narration lookup by sceneId for quick access
                    const visualMap = new Map(visuals.map(v => [v.sceneId, v.imageUrl]));
                    const narrationMap = new Map(narrations.map(n => [n.sceneId, n]));

                    return (
                        <motion.div key="pipeline-complete" {...quickFade} className="h-full overflow-y-auto">
                            <div className="w-full max-w-4xl mx-auto px-6 py-8">
                                {/* Header */}
                                <div className="text-center mb-8">
                                    <div className="w-14 h-14 mx-auto mb-4 rounded-full bg-emerald-500/10 border border-emerald-500/30 flex items-center justify-center">
                                        <Check className="w-7 h-7 text-emerald-400" />
                                    </div>
                                    <h2 className="text-2xl font-medium text-zinc-100 mb-1">Production Complete</h2>
                                    <p className="text-zinc-500 text-sm">
                                        {formatName} — {screenplay.length} scene{screenplay.length !== 1 ? 's' : ''}
                                        {totalDuration != null && ` — ${Math.round(totalDuration)}s`}
                                        {narrations.length > 0 && ` — ${narrations.length} narration${narrations.length !== 1 ? 's' : ''}`}
                                    </p>
                                </div>

                                {/* Stats bar */}
                                <div className="flex flex-wrap justify-center gap-3 mb-8">
                                    <span className="px-3 py-1.5 bg-zinc-800/80 border border-zinc-700/50 rounded-sm text-xs font-mono text-zinc-300 flex items-center gap-1.5">
                                        <Film className="w-3.5 h-3.5 text-blue-400" />
                                        {screenplay.length} scenes
                                    </span>
                                    <span className="px-3 py-1.5 bg-zinc-800/80 border border-zinc-700/50 rounded-sm text-xs font-mono text-zinc-300 flex items-center gap-1.5">
                                        <ImageIcon className="w-3.5 h-3.5 text-purple-400" />
                                        {visuals.length} visuals
                                    </span>
                                    <span className="px-3 py-1.5 bg-zinc-800/80 border border-zinc-700/50 rounded-sm text-xs font-mono text-zinc-300 flex items-center gap-1.5">
                                        <Mic className="w-3.5 h-3.5 text-amber-400" />
                                        {narrations.length} narrations
                                    </span>
                                    {totalDuration != null && (
                                        <span className="px-3 py-1.5 bg-zinc-800/80 border border-zinc-700/50 rounded-sm text-xs font-mono text-zinc-300 flex items-center gap-1.5">
                                            <Play className="w-3.5 h-3.5 text-emerald-400" />
                                            {Math.floor(totalDuration / 60)}:{String(Math.round(totalDuration % 60)).padStart(2, '0')}
                                        </span>
                                    )}
                                    {research?.sources && (
                                        <span className="px-3 py-1.5 bg-zinc-800/80 border border-zinc-700/50 rounded-sm text-xs font-mono text-zinc-300">
                                            {research.sources.length} sources
                                        </span>
                                    )}
                                </div>

                                {/* Scene cards with visuals and audio */}
                                {screenplay.length > 0 && (
                                    <div className="space-y-4 mb-8">
                                        {screenplay.map((scene, i) => {
                                            const imageUrl = visualMap.get(scene.id);
                                            const narration = narrationMap.get(scene.id);
                                            return (
                                                <div key={scene.id} className="bg-zinc-900/60 border border-zinc-800 rounded-sm overflow-hidden">
                                                    <div className="flex flex-col sm:flex-row">
                                                        {/* Visual thumbnail */}
                                                        {imageUrl && (
                                                            <div className="sm:w-48 sm:shrink-0 aspect-video sm:aspect-auto sm:h-auto bg-zinc-950">
                                                                <img
                                                                    src={imageUrl}
                                                                    alt={`Scene ${i + 1}`}
                                                                    className="w-full h-full object-cover"
                                                                />
                                                            </div>
                                                        )}
                                                        {/* Scene content */}
                                                        <div className="flex-1 p-4 min-w-0">
                                                            <div className="flex items-start gap-2 mb-1.5">
                                                                <span className="font-mono text-[10px] text-blue-400 bg-blue-500/10 px-1.5 py-0.5 rounded shrink-0 mt-0.5">
                                                                    {String(i + 1).padStart(2, '0')}
                                                                </span>
                                                                <h4 className="text-sm font-medium text-zinc-200 leading-tight">{scene.heading}</h4>
                                                            </div>
                                                            <p className="text-xs text-zinc-400 line-clamp-3 mb-2" dir="auto">{scene.action}</p>

                                                            {/* Dialogue preview */}
                                                            {scene.dialogue && scene.dialogue.length > 0 && (
                                                                <div className="mb-2">
                                                                    {scene.dialogue.slice(0, 2).map((d, di) => (
                                                                        <p key={di} className="text-[11px] text-zinc-500 truncate">
                                                                            <span className="text-zinc-400 font-medium">{d.speaker}:</span> {d.text}
                                                                        </p>
                                                                    ))}
                                                                    {scene.dialogue.length > 2 && (
                                                                        <p className="text-[10px] text-zinc-600">+{scene.dialogue.length - 2} more</p>
                                                                    )}
                                                                </div>
                                                            )}

                                                            {/* Audio player for narration */}
                                                            {narration && narrationBlobUrlMap.get(scene.id) && (
                                                                <div className="flex items-center gap-2 mt-2">
                                                                    <Mic className="w-3 h-3 text-amber-400 shrink-0" />
                                                                    <audio
                                                                        controls
                                                                        className="h-7 w-full max-w-xs [&::-webkit-media-controls-panel]:bg-zinc-800 [&::-webkit-media-controls-panel]:rounded-sm"
                                                                        src={narrationBlobUrlMap.get(scene.id)}
                                                                    />
                                                                    <span className="text-[10px] text-zinc-500 font-mono shrink-0">{narration.audioDuration.toFixed(1)}s</span>
                                                                </div>
                                                            )}
                                                        </div>
                                                    </div>
                                                </div>
                                            );
                                        })}
                                    </div>
                                )}

                                {/* Completed task summary (collapsed) */}
                                {fpHook.tasks.length > 0 && (
                                    <details className="mb-8 group">
                                        <summary className="text-xs text-zinc-500 cursor-pointer hover:text-zinc-400 transition-colors font-mono text-center">
                                            Pipeline tasks
                                        </summary>
                                        <div className="mt-3">
                                            <PipelineProgress
                                                executionProgress={fpHook.executionProgress}
                                                tasks={fpHook.tasks}
                                                currentPhase="Complete"
                                                isRunning={false}
                                                onCancel={() => { }}
                                                summaryOnly
                                            />
                                        </div>
                                    </details>
                                )}

                                {/* Actions */}
                                <div className="flex items-center justify-center gap-3">
                                    {/* Download all visuals */}
                                    {visuals.length > 0 && (
                                        <button
                                            type="button"
                                            onClick={() => {
                                                visuals.forEach((v, i) => {
                                                    const a = document.createElement('a');
                                                    a.href = v.imageUrl;
                                                    a.download = `scene_${i + 1}.png`;
                                                    a.click();
                                                });
                                            }}
                                            className="inline-flex items-center gap-2 px-5 py-2.5 rounded-sm font-mono text-sm font-medium border border-zinc-700 text-zinc-300 hover:border-zinc-500 hover:text-zinc-100 transition-colors duration-200"
                                        >
                                            <Download className="w-4 h-4" />
                                            Download Images
                                        </button>
                                    )}
                                    {onOpenInEditor && (
                                        <button
                                            type="button"
                                            onClick={onOpenInEditor}
                                            className="inline-flex items-center gap-2 px-5 py-2.5 rounded-sm font-mono text-sm font-medium bg-blue-500/10 border border-blue-500/30 text-blue-400 hover:bg-blue-500/20 hover:text-blue-300 hover:border-blue-500/50 transition-colors duration-200"
                                        >
                                            <Film className="w-4 h-4" />
                                            Open in Editor
                                        </button>
                                    )}
                                    <button
                                        type="button"
                                        onClick={fpHook.reset}
                                        className="inline-flex items-center gap-2 px-5 py-2.5 rounded-sm font-mono text-sm font-medium bg-white text-black hover:bg-zinc-200 transition-colors duration-200"
                                    >
                                        <RefreshCcw className="w-4 h-4" />
                                        Start New Production
                                    </button>
                                </div>
                            </div>
                        </motion.div>
                    );
                }

                // State 3: Pipeline failed or cancelled → error view with retry
                if (!fpHook.isRunning && (fpHook.error || fpHook.result?.success === false)) {
                    const errorMsg = fpHook.error || fpHook.result?.error || 'Pipeline failed';
                    const wasCancelled = fpHook.tasks.some(t => t.status === 'cancelled');
                    return (
                        <motion.div key="pipeline-error" {...quickFade} className="h-full flex items-center justify-center p-8">
                            <div className="w-full max-w-2xl mx-auto text-center">
                                <div className={`w-16 h-16 mx-auto mb-6 rounded-full flex items-center justify-center ${wasCancelled ? 'bg-zinc-500/10 border border-zinc-500/30' : 'bg-red-500/10 border border-red-500/30'}`}>
                                    {wasCancelled
                                        ? <X className="w-8 h-8 text-zinc-400" />
                                        : <AlertCircle className="w-8 h-8 text-red-400" />
                                    }
                                </div>
                                <h2 className="text-2xl font-medium text-zinc-100 mb-2">
                                    {wasCancelled ? 'Production Cancelled' : 'Production Failed'}
                                </h2>
                                <p className="text-zinc-400 text-sm mb-4">{errorMsg}</p>
                                {/* Task summary showing what completed */}
                                {fpHook.tasks.length > 0 && (
                                    <div className="mb-8">
                                        <PipelineProgress
                                            executionProgress={fpHook.executionProgress}
                                            tasks={fpHook.tasks}
                                            currentPhase={wasCancelled ? 'Cancelled' : 'Failed'}
                                            isRunning={false}
                                            onCancel={() => { }}
                                            summaryOnly
                                        />
                                    </div>
                                )}
                                <div className="flex items-center justify-center gap-3">
                                    <button
                                        type="button"
                                        onClick={fpHook.reset}
                                        className="inline-flex items-center gap-2 px-6 py-2.5 rounded-sm font-mono text-sm font-medium border border-zinc-700 text-zinc-300 hover:border-zinc-500 hover:text-zinc-100 transition-colors duration-200"
                                    >
                                        <RefreshCcw className="w-4 h-4" />
                                        Start Over
                                    </button>
                                </div>
                            </div>
                        </motion.div>
                    );
                }

                // State 4: movie-animation selected → IdeaView with synced idea/genre
                if (isMovieAnimation) {
                    return (
                        <motion.div key="idea" {...quickFade} className="h-full">
                            <IdeaView
                                initialTopic={fpHook.idea || initialTopic}
                                onGenerate={(topic, genre) => onGenerateIdea?.(topic, genre)}
                                onApplyTemplate={onApplyTemplate}
                                isProcessing={isProcessing}
                            />
                        </motion.div>
                    );
                }

                // State 5: FormatSelector (no format yet, or a non-movie format selected but idle)
                const selectedMeta = fpHook.selectedFormat ? formatRegistry.getFormat(fpHook.selectedFormat) : null;
                return (
                    <motion.div key="format-selector" {...quickFade} className="h-full overflow-y-auto">
                        {/* Error banner from previous failed attempt */}
                        {fpHook.error && !fpHook.isRunning && !fpHook.result && (
                            <div className="mx-6 mt-6 p-4 rounded-sm bg-red-500/10 border border-red-500/30 flex items-start gap-3">
                                <AlertCircle className="w-5 h-5 text-red-400 shrink-0 mt-0.5" />
                                <div className="flex-1">
                                    <p className="text-sm text-zinc-100 font-medium">Something went wrong</p>
                                    <p className="text-xs text-zinc-500 mt-1">{fpHook.error}</p>
                                </div>
                            </div>
                        )}
                        <FormatSelector
                            selectedFormat={fpHook.selectedFormat}
                            onFormatSelect={fpHook.setFormat}
                            selectedGenre={fpHook.selectedGenre}
                            onGenreSelect={fpHook.setGenre}
                            idea={fpHook.idea}
                            onIdeaChange={fpHook.setIdea}
                            onExecute={() => onFormatExecute?.()}
                            isProcessing={fpHook.isRunning}
                        />
                        {/* Reference document upload for research formats */}
                        {selectedMeta?.requiresResearch && (
                            <div className="px-6 pb-12 max-w-3xl mx-auto">
                                <div className="mb-3">
                                    <span className="font-mono text-[11px] font-medium tracking-[0.15em] uppercase text-zinc-500">
                                        Reference Documents (Optional)
                                    </span>
                                </div>
                                <ReferenceDocumentUpload
                                    documents={fpHook.referenceDocuments}
                                    onDocumentsChange={fpHook.setReferenceDocuments}
                                />
                            </div>
                        )}
                    </motion.div>
                );
            }

            // Fallback: no format pipeline hook — existing IdeaView (backward compat)
            return (
                <motion.div key="idea" {...quickFade} className="h-full">
                    <IdeaView
                        initialTopic={initialTopic}
                        onGenerate={(topic, genre) => onGenerateIdea?.(topic, genre)}
                        onApplyTemplate={onApplyTemplate}
                        isProcessing={isProcessing}
                    />
                </motion.div>
            );
        }

        if (activeMainTab === 'breakdown') {
            if (isBreakdownProcessing) {
                return (
                    <motion.div key="breakdown-progress" {...quickFade}>
                        <BreakdownProgress currentStage="creating" />
                    </motion.div>
                );
            }

            const breakdownTabs: { id: StoryStep; label: string }[] = [
                { id: 'breakdown', label: t('story.sceneBreakdown') },
                { id: 'script', label: t('story.script') },
                { id: 'characters', label: t('story.cast') },
            ];

            return (
                <motion.div key="breakdown" {...quickFade} className="flex flex-col h-full">
                    {renderSubNav(breakdownTabs)}
                    <div className="flex-1 overflow-y-auto bg-black">
                        <AnimatePresence mode="wait">
                            {subTab === 'breakdown' && (
                                <motion.div key="scene-breakdown" {...quickFade} className="p-8 max-w-5xl mx-auto w-full">
                                    <h2 className="font-sans text-2xl font-medium tracking-tight text-zinc-100 mb-8">
                                        {t('story.sceneBreakdown')}
                                    </h2>
                                    <div className="space-y-4">
                                        {storyState.breakdown.map((scene) => (
                                            <SceneCard
                                                key={scene.id}
                                                sceneNumber={scene.sceneNumber}
                                                heading={scene.heading}
                                                content={scene.action}
                                                onRegenerate={(num, feedback) => onRegenerateScene?.(num, feedback)}
                                                isProcessing={isProcessing}
                                            />
                                        ))}
                                    </div>
                                </motion.div>
                            )}
                            {subTab === 'script' && (
                                <motion.div key="script" {...quickFade}>
                                    <ScriptView script={storyState.script} />
                                </motion.div>
                            )}
                            {subTab === 'characters' && (
                                <motion.div key="characters" {...quickFade}>
                                    <CharacterView
                                        characters={storyState.characters}
                                        reports={storyState.consistencyReports}
                                        onVerify={onVerifyConsistency}
                                        isProcessing={isProcessing}
                                        onAdd={onAddCharacter}
                                        onEdit={onEditCharacter}
                                        onDelete={onDeleteCharacter}
                                        onGenerateImage={onGenerateCharacterImage}
                                    />
                                </motion.div>
                            )}
                        </AnimatePresence>
                    </div>
                </motion.div>
            );
        }

        if (activeMainTab === 'storyboard') {
            if (isStoryboardProcessing) {
                return (
                    <motion.div key="storyboard-progress" {...quickFade}>
                        <StoryboardProgress currentStage={storyState.shots?.length ? 'storyboard' : 'shotlist'} />
                    </motion.div>
                );
            }

            const storyboardTabs: { id: StoryStep; label: string }[] = [
                { id: 'shots', label: t('story.shotList') },
                { id: 'style', label: t('story.visualStyle') },
                { id: 'storyboard', label: t('story.storyboard') },
                { id: 'narration', label: t('story.narration') },
                { id: 'animation', label: t('story.animation') },
                { id: 'export', label: t('story.export') },
            ];

            return (
                <motion.div key="storyboard" {...quickFade} className="flex flex-col h-full">
                    {renderSubNav(storyboardTabs)}
                    <div className={`flex-1 bg-black ${subTab === 'storyboard' ? 'overflow-hidden flex flex-col' : 'overflow-y-auto'}`}>
                        <AnimatePresence mode="wait">
                            {subTab === 'shots' && (
                                <motion.div key="shots" {...quickFade} className="p-6">
                                    <div className="flex justify-between items-center mb-8">
                                        <h2 className="font-sans text-2xl font-medium tracking-tight text-zinc-100">
                                            {t('story.shotBreakdown')}
                                        </h2>
                                        {storyState.isLocked && (
                                            <div className="flex items-center gap-2.5 px-4 py-2.5 bg-blue-500/10 text-blue-400 rounded-sm border border-blue-500/30">
                                                <Lock className="w-4 h-4" />
                                                <div className="flex flex-col">
                                                    <span className="text-sm font-mono font-bold tracking-wide">{t('story.locked')}</span>
                                                    <span className="text-[10px] text-zinc-500">Structure finalized</span>
                                                </div>
                                            </div>
                                        )}
                                    </div>
                                    <div className="space-y-8">
                                        {storyState.breakdown.map((scene, idx) => {
                                            const sceneShots = storyState.shots?.filter(s => s.sceneId === scene.id) || [];
                                            const moodLighting = sceneShots[0]?.lighting || '';
                                            return (
                                                <div key={scene.id} className="rounded-sm overflow-hidden border border-zinc-800">
                                                    {/* Scene Header */}
                                                    <div className="px-5 py-3 bg-zinc-950 flex items-center justify-between border-b border-zinc-800">
                                                        <div className="flex items-center gap-3 min-w-0">
                                                            <span className="font-mono text-xs font-bold text-blue-400 bg-blue-500/10 px-2.5 py-1 rounded-sm border border-blue-500/20 shrink-0">
                                                                SCENE {scene.sceneNumber}
                                                            </span>
                                                            <span className="font-mono text-sm font-medium text-zinc-100 truncate" dir="auto">
                                                                {scene.heading}
                                                            </span>
                                                            {moodLighting && (
                                                                <span className="flex items-center gap-1.5 text-[11px] text-zinc-500 shrink-0">
                                                                    <span className="w-1.5 h-1.5 rounded-full bg-amber-400/70" />
                                                                    {moodLighting}
                                                                </span>
                                                            )}
                                                        </div>
                                                        {!sceneShots.length && onGenerateShots && (
                                                            <button
                                                                onClick={() => onGenerateShots(idx)}
                                                                className="text-xs px-4 py-1.5 rounded-sm bg-blue-500 hover:bg-blue-600 text-white font-medium transition-colors duration-200 shrink-0 ml-4"
                                                            >
                                                                {t('story.generateShots')}
                                                            </button>
                                                        )}
                                                    </div>

                                                    {/* Shots Table */}
                                                    {sceneShots.length > 0 ? (
                                                        <div className="overflow-x-auto">
                                                            <table className="w-full text-left border-collapse">
                                                                <thead>
                                                                    <tr className="border-b border-zinc-800 bg-zinc-900/60">
                                                                        <th className="py-2 px-2 w-6" />
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 w-16">Scene</th>
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 w-14">Shot</th>
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 min-w-[220px]">Description</th>
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 w-20">Dialogue</th>
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 w-16">ERT</th>
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 w-28">Size</th>
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 w-28">Perspective</th>
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 w-28">Movement</th>
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 w-28">Equipment</th>
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 w-28">Focal Length</th>
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 w-24">Aspect Ratio</th>
                                                                        <th className="py-2 px-3 font-mono text-[10px] tracking-[0.12em] uppercase text-zinc-600 w-16">Notes</th>
                                                                    </tr>
                                                                </thead>
                                                                <tbody>
                                                                    {sceneShots.map((shot) => {
                                                                        const equipment = shot.equipment ?? deriveEquipment(shot.movement);
                                                                        const focalLength = shot.focalLength ?? deriveFocalLength(shot.shotType ?? shot.cameraAngle ?? '');
                                                                        const hasDialogue = !!shot.scriptSegment;
                                                                        return (
                                                                            <tr
                                                                                key={shot.id}
                                                                                className="border-b border-zinc-800/50 hover:bg-zinc-800/25 transition-colors group"
                                                                            >
                                                                                <td className="py-3 px-2 text-zinc-700 group-hover:text-zinc-500">
                                                                                    <GripVertical className="w-3.5 h-3.5" />
                                                                                </td>
                                                                                <td className="py-3 px-3 font-mono text-xs text-zinc-500">
                                                                                    {scene.sceneNumber}
                                                                                </td>
                                                                                <td className="py-3 px-3 font-mono text-xs text-blue-400 font-bold">
                                                                                    {shot.shotNumber}
                                                                                </td>
                                                                                <td className="py-3 px-3 text-xs text-zinc-300 leading-relaxed" dir="auto">
                                                                                    {shot.description}
                                                                                </td>
                                                                                <td className="py-3 px-3">
                                                                                    <button
                                                                                        className={`p-1.5 rounded-sm transition-colors ${hasDialogue ? 'text-blue-400 hover:text-blue-300' : 'text-zinc-700 hover:text-zinc-500'}`}
                                                                                        title={shot.scriptSegment || undefined}
                                                                                    >
                                                                                        <MessageCircle className="w-3.5 h-3.5" />
                                                                                    </button>
                                                                                </td>
                                                                                <td className="py-3 px-3 text-xs text-zinc-400 whitespace-nowrap">
                                                                                    {shot.duration} sec
                                                                                </td>
                                                                                <td className="py-3 px-3 text-xs text-zinc-400 whitespace-nowrap">
                                                                                    {shot.shotType}
                                                                                </td>
                                                                                <td className="py-3 px-3 text-xs text-zinc-400 whitespace-nowrap">
                                                                                    {shot.cameraAngle}
                                                                                </td>
                                                                                <td className="py-3 px-3 text-xs text-zinc-400 whitespace-nowrap">
                                                                                    {shot.movement}
                                                                                </td>
                                                                                <td className="py-3 px-3 text-xs text-zinc-400 whitespace-nowrap">
                                                                                    {equipment}
                                                                                </td>
                                                                                <td className="py-3 px-3 text-xs text-zinc-400 whitespace-nowrap">
                                                                                    {focalLength}
                                                                                </td>
                                                                                <td className="py-3 px-3 text-xs text-zinc-400 whitespace-nowrap">
                                                                                    {shot.aspectRatio ?? storyState.aspectRatio ?? '16:9'}
                                                                                </td>
                                                                                <td className="py-3 px-3">
                                                                                    <div className="flex items-center gap-1.5">
                                                                                        {shot.notes && (
                                                                                            <span className="text-[10px] text-zinc-500 truncate max-w-[80px]" title={shot.notes}>
                                                                                                {shot.notes}
                                                                                            </span>
                                                                                        )}
                                                                                        <button
                                                                                            onClick={() => setEditingShot(shot)}
                                                                                            className="p-1.5 rounded-sm text-zinc-600 hover:text-zinc-300 hover:bg-zinc-800 transition-colors shrink-0"
                                                                                            title="Edit shot"
                                                                                        >
                                                                                            <Pencil className="w-3.5 h-3.5" />
                                                                                        </button>
                                                                                    </div>
                                                                                </td>
                                                                            </tr>
                                                                        );
                                                                    })}
                                                                </tbody>
                                                            </table>
                                                        </div>
                                                    ) : (
                                                        <div className="text-center py-12">
                                                            <ImageIcon className="w-12 h-12 mx-auto mb-4 text-zinc-800" />
                                                            <p className="text-zinc-600 text-sm">
                                                                {t('story.noShotsGenerated')}
                                                            </p>
                                                        </div>
                                                    )}
                                                </div>
                                            );
                                        })}
                                    </div>
                                </motion.div>
                            )}
                            {subTab === 'style' && (
                                <motion.div key="style" {...quickFade}>
                                    <StyleSelector
                                        selectedStyle={(storyState.visualStyle || 'CINEMATIC') as VisualStyleKey}
                                        onSelectStyle={(style) => onUpdateVisualStyle?.(style)}
                                        aspectRatio={(storyState.aspectRatio || '16:9') as AspectRatioId}
                                        onSelectAspectRatio={(ratio) => onUpdateAspectRatio?.(ratio)}
                                        imageProvider={storyState.imageProvider || 'gemini'}
                                        onSelectImageProvider={onUpdateImageProvider}
                                    />
                                </motion.div>
                            )}
                            {subTab === 'storyboard' && (
                                <motion.div key="storyboard-view" {...quickFade} className="h-full">
                                    <StoryboardView
                                        shots={storyState.shotlist}
                                        scenes={storyState.breakdown}
                                        scenesWithVisuals={storyState.scenesWithVisuals}
                                        onGenerateVisuals={onGenerateVisuals}
                                        isProcessing={isProcessing}
                                        onUpdateDuration={onUpdateShotDuration}
                                        onGenerateVideo={onGenerateVideo}
                                        onEditShot={(shotId) => {
                                            const shot = storyState.shotlist.find(s => s.id === shotId);
                                            if (shot) setEditingShot(shot);
                                        }}
                                    />
                                </motion.div>
                            )}
                            {subTab === 'narration' && (
                                <motion.div key="narration-view" {...quickFade} className="p-8 max-w-5xl mx-auto w-full">
                                    <div className="flex justify-between items-center mb-8">
                                        <h2 className="font-sans text-2xl font-medium tracking-tight text-zinc-100">
                                            {t('story.narration')}
                                        </h2>
                                        {onGenerateNarration && (
                                            <button
                                                onClick={onGenerateNarration}
                                                disabled={isProcessing}
                                                className="px-5 py-2.5 rounded-sm text-sm font-medium flex items-center gap-2 bg-blue-500 hover:bg-blue-600 text-white disabled:opacity-50 transition-colors duration-200"
                                            >
                                                <Mic className="w-4 h-4" />
                                                {allScenesHaveNarration?.() ? t('story.regenerateNarration') : t('story.generateNarration')}
                                            </button>
                                        )}
                                    </div>
                                    <p className="text-zinc-500 text-sm mb-6">
                                        {t('story.narrationDescription')}
                                    </p>
                                    <div className="space-y-4">
                                        {storyState.narrationSegments?.map((segment, idx) => (
                                            <div
                                                key={segment.sceneId}
                                                className="bg-zinc-900 border border-zinc-800 p-5 rounded-sm"
                                            >
                                                <div className="flex items-center justify-between mb-3">
                                                    <span className="font-mono text-xs text-blue-400">
                                                        SCENE {String(idx + 1).padStart(2, '0')}
                                                    </span>
                                                    <span className="font-mono text-xs text-zinc-600">
                                                        {segment.duration.toFixed(1)}s
                                                    </span>
                                                </div>
                                                <p className="text-zinc-400 text-sm mb-3" dir="auto">
                                                    {segment.text}
                                                </p>
                                                {segment.audioUrl && (
                                                    <audio src={segment.audioUrl} controls className="w-full h-8 opacity-80" />
                                                )}
                                            </div>
                                        ))}
                                        {(!storyState.narrationSegments || storyState.narrationSegments.length === 0) && (
                                            <div className="text-center py-16 text-zinc-600">
                                                <Mic className="w-12 h-12 mx-auto mb-4 opacity-30" />
                                                <p className="text-sm">{t('story.noNarrationYet')}</p>
                                                <p className="text-xs mt-2 text-zinc-700">{t('story.clickGenerateNarration')}</p>
                                            </div>
                                        )}
                                    </div>
                                </motion.div>
                            )}
                            {subTab === 'animation' && (
                                <motion.div key="animation-view" {...quickFade} className="p-8 max-w-6xl mx-auto w-full">
                                    <div className="flex justify-between items-center mb-8">
                                        <h2 className="font-sans text-2xl font-medium tracking-tight text-zinc-100">
                                            {t('story.animation')}
                                        </h2>
                                        {onAnimateShots && (
                                            <button
                                                onClick={() => onAnimateShots()}
                                                disabled={isProcessing}
                                                className="px-5 py-2.5 rounded-sm text-sm font-medium flex items-center gap-2 bg-blue-500 hover:bg-blue-600 text-white disabled:opacity-50 transition-colors duration-200"
                                            >
                                                <Video className="w-4 h-4" />
                                                {allShotsHaveAnimation?.() ? t('story.regenerateAll') : t('story.animateAllShots')}
                                            </button>
                                        )}
                                    </div>
                                    <p className="text-zinc-500 text-sm mb-6">
                                        {t('story.animationDescription')}
                                    </p>
                                    <div className="grid grid-cols-2 md:grid-cols-3 lg:grid-cols-4 gap-4">
                                        {storyState.shotlist.map((shot, idx) => {
                                            const animated = animatedShotsMap.get(shot.id);
                                            return (
                                                <div
                                                    key={shot.id}
                                                    className="bg-zinc-900 border border-zinc-800 rounded-sm overflow-hidden group"
                                                >
                                                    <div className="aspect-video bg-zinc-950 relative">
                                                        {animated?.videoUrl ? (
                                                            <video
                                                                src={animated.videoUrl}
                                                                className="w-full h-full object-cover"
                                                                loop muted playsInline
                                                                onMouseEnter={(e) => e.currentTarget.play()}
                                                                onMouseLeave={(e) => { e.currentTarget.pause(); e.currentTarget.currentTime = 0; }}
                                                            />
                                                        ) : shot.imageUrl ? (
                                                            <img src={shot.imageUrl} alt={shot.description} className="w-full h-full object-cover opacity-60" />
                                                        ) : (
                                                            <div className="w-full h-full flex items-center justify-center">
                                                                <Video className="w-8 h-8 text-zinc-800" />
                                                            </div>
                                                        )}
                                                        {animatingShotIds.has(shot.id) && (
                                                            <div className="absolute inset-0 flex items-center justify-center bg-black/60">
                                                                <Loader2 className="w-8 h-8 text-white animate-spin" />
                                                            </div>
                                                        )}
                                                        {!animated && !animatingShotIds.has(shot.id) && shot.imageUrl && onAnimateShots && (
                                                            <button
                                                                onClick={() => {
                                                                    const shotId = shot.id;
                                                                    setAnimatingShotIds(prev => new Set(prev).add(shotId));
                                                                    try {
                                                                        onAnimateShots(idx);
                                                                    } catch {
                                                                        // Clear spinner immediately on synchronous failure;
                                                                        // async failures are cleared by the isProcessing → false effect.
                                                                        setAnimatingShotIds(prev => {
                                                                            const next = new Set(prev);
                                                                            next.delete(shotId);
                                                                            return next;
                                                                        });
                                                                    }
                                                                }}
                                                                disabled={isProcessing}
                                                                className="absolute inset-0 flex items-center justify-center bg-black/50 opacity-0 group-hover:opacity-100 transition-opacity duration-200"
                                                            >
                                                                <Play className="w-8 h-8 text-white" />
                                                            </button>
                                                        )}
                                                        {animated && (
                                                            <div className="absolute top-2 right-2 px-2 py-0.5 bg-emerald-500/80 text-white text-[10px] rounded-sm uppercase tracking-wider font-mono">
                                                                {t('story.animated')}
                                                            </div>
                                                        )}
                                                    </div>
                                                    <div className="p-3">
                                                        <span className="font-mono text-[10px] text-blue-400">
                                                            SHOT {shot.shotNumber}
                                                        </span>
                                                        <p className="text-xs text-zinc-500 truncate mt-1">
                                                            {shot.description}
                                                        </p>
                                                    </div>
                                                </div>
                                            );
                                        })}
                                        {storyState.shotlist.length === 0 && (
                                            <div className="col-span-full text-center py-16 text-zinc-600">
                                                <Video className="w-12 h-12 mx-auto mb-4 opacity-30" />
                                                <p className="text-sm">{t('story.noShotsToAnimate')}</p>
                                                <p className="text-xs mt-2 text-zinc-700">{t('story.generateStoryboardFirst')}</p>
                                            </div>
                                        )}
                                    </div>
                                </motion.div>
                            )}
                            {subTab === 'export' && (
                                <motion.div key="export-view" {...quickFade} className="p-8 max-w-3xl mx-auto w-full">
                                    <h2 className="font-sans text-2xl font-medium tracking-tight text-zinc-100 mb-8 text-center">
                                        {t('story.exportVideo')}
                                    </h2>
                                    <div className="bg-zinc-900 border border-zinc-800 rounded-sm p-8">
                                        {/* Preview */}
                                        {storyState.finalVideoUrl ? (
                                            <div className="mb-8">
                                                <video src={storyState.finalVideoUrl} controls className="w-full rounded-sm" />
                                            </div>
                                        ) : (
                                            <div className="aspect-video bg-zinc-950 rounded-sm flex items-center justify-center mb-8 border border-zinc-800">
                                                <Film className="w-16 h-16 text-zinc-800" />
                                            </div>
                                        )}
                                        {/* Stats */}
                                        <div className="grid grid-cols-3 gap-4 mb-8">
                                            <div className="text-center p-4 bg-zinc-950 rounded-sm border border-zinc-800">
                                                <div className="text-2xl font-sans font-medium text-blue-400">
                                                    {storyState.breakdown.length}
                                                </div>
                                                <div className="font-mono text-[10px] text-zinc-600 uppercase tracking-widest">{t('story.scenes')}</div>
                                            </div>
                                            <div className="text-center p-4 bg-zinc-950 rounded-sm border border-zinc-800">
                                                <div className="text-2xl font-sans font-medium text-orange-400">
                                                    {storyState.shotlist.length}
                                                </div>
                                                <div className="font-mono text-[10px] text-zinc-600 uppercase tracking-widest">{t('story.shots')}</div>
                                            </div>
                                            <div className="text-center p-4 bg-zinc-950 rounded-sm border border-zinc-800">
                                                <div className="text-2xl font-sans font-medium text-emerald-400">
                                                    {storyState.narrationSegments?.reduce((sum, s) => sum + s.duration, 0).toFixed(0) || 0}s
                                                </div>
                                                <div className="font-mono text-[10px] text-zinc-600 uppercase tracking-widest">{t('story.duration')}</div>
                                            </div>
                                        </div>
                                        {/* Actions */}
                                        <div className="flex flex-col gap-3">
                                            {!storyState.finalVideoUrl ? (
                                                <button
                                                    onClick={onExportFinalVideo}
                                                    disabled={isProcessing || !storyState.narrationSegments?.length || !storyState.shotlist.some(s => s.imageUrl)}
                                                    className="w-full py-4 rounded-sm text-sm font-medium flex items-center justify-center gap-2 bg-blue-500 hover:bg-blue-600 text-white disabled:opacity-50 transition-colors duration-200"
                                                >
                                                    {isProcessing ? (
                                                        <><Loader2 className="w-5 h-5 animate-spin" />{t('story.renderingVideo')}</>
                                                    ) : (
                                                        <><Film className="w-5 h-5" />{t('story.exportVideo')}</>
                                                    )}
                                                </button>
                                            ) : (
                                                <button
                                                    onClick={onDownloadVideo}
                                                    className="w-full bg-emerald-600 hover:bg-emerald-500 text-white py-4 rounded-sm text-sm font-medium flex items-center justify-center gap-2 transition-colors duration-200"
                                                >
                                                    <Download className="w-5 h-5" />
                                                    {t('story.downloadVideo')}
                                                </button>
                                            )}
                                            {!storyState.shotlist.some(s => s.imageUrl) && (
                                                <p className="text-center text-xs text-orange-400">
                                                    {t('story.generateStoryboardFirst')}
                                                </p>
                                            )}
                                            {storyState.shotlist.some(s => s.imageUrl) && !storyState.narrationSegments?.length && (
                                                <p className="text-center text-xs text-orange-400">
                                                    {t('story.generateNarrationBeforeExport')}
                                                </p>
                                            )}
                                        </div>
                                    </div>

                                    <div className="mt-6">
                                        <ExportOptionsPanel
                                            storyState={storyState}
                                            onImportProject={onImportProject}
                                            onExportVideo={onExportFinalVideo}
                                        />
                                    </div>
                                </motion.div>
                            )}
                        </AnimatePresence>
                    </div>
                </motion.div>
            );
        }
        return null;
    };

    return (
        <div className="flex flex-col h-full bg-black font-sans relative overflow-hidden">
            {/* Main Top Navigation */}
            <div className="relative z-10 flex items-center justify-between px-6 py-3 border-b border-zinc-800 bg-zinc-950">
                {/* Step Navigation */}
                <div className="flex items-center gap-1">
                    {mainTabs.map((tab, index) => {
                        const isActive = activeMainTab === tab.id;
                        const stepOrder: MainStep[] = ['idea', 'breakdown', 'storyboard'];
                        const currentIdx = stepOrder.indexOf(getHighLevelStep(storyState.currentStep));
                        const tabIdx = stepOrder.indexOf(tab.id);
                        const isAccessible = tabIdx <= currentIdx;
                        const isCompleted = tabIdx < currentIdx;

                        return (
                            <React.Fragment key={tab.id}>
                                <button
                                    onClick={() => {
                                        if (isAccessible) {
                                            setActiveMainTab(tab.id);
                                            if (tab.id === 'breakdown') setSubTab('breakdown');
                                            if (tab.id === 'storyboard') setSubTab('shots');
                                            if (tab.id === 'idea') setSubTab('idea');
                                        }
                                    }}
                                    disabled={!isAccessible}
                                    className={`
                                        relative flex items-center gap-2.5 px-4 py-2 rounded-sm transition-all duration-200
                                        ${isActive ? 'bg-zinc-800 border border-zinc-700' : 'border border-transparent'}
                                        ${!isAccessible ? 'opacity-40 cursor-not-allowed' : isActive ? 'opacity-100' : 'opacity-70 hover:opacity-100'}
                                    `}
                                >
                                    <span className={`
                                        w-5 h-5 rounded-sm flex items-center justify-center text-[10px] font-mono font-bold shrink-0 transition-all duration-200
                                        ${isActive ? 'bg-white text-black' : isCompleted ? 'bg-emerald-500/20 text-emerald-400' : 'bg-zinc-800 text-zinc-400'}
                                    `}>
                                        {isCompleted ? <Check className="w-3 h-3" /> : tab.number}
                                    </span>
                                    <span className={`
                                        font-sans text-[13px] font-medium whitespace-nowrap transition-colors duration-200
                                        ${isActive ? 'text-zinc-100' : isCompleted ? 'text-emerald-400/80' : 'text-zinc-400'}
                                    `}>
                                        {tab.label}
                                    </span>
                                </button>

                                {index < mainTabs.length - 1 && (
                                    <div className={`w-6 h-px mx-0.5 shrink-0 ${isCompleted ? 'bg-emerald-500/30' : 'bg-zinc-800'}`} />
                                )}
                            </React.Fragment>
                        );
                    })}
                </div>

                {/* Action Buttons */}
                <div className="flex items-center gap-2">
                    {activeMainTab !== 'idea' && (
                        <div className="flex items-center gap-0.5 mr-1">
                            <button
                                onClick={onUndo}
                                disabled={!canUndo}
                                className="p-1.5 rounded-sm text-zinc-600 hover:text-zinc-300 hover:bg-zinc-800 disabled:opacity-20 transition-all duration-200"
                                title="Undo (Ctrl+Z)"
                            >
                                <Undo2 className="w-3.5 h-3.5" />
                            </button>
                            <button
                                onClick={onRedo}
                                disabled={!canRedo}
                                className="p-1.5 rounded-sm text-zinc-600 hover:text-zinc-300 hover:bg-zinc-800 disabled:opacity-20 transition-all duration-200"
                                title="Redo (Ctrl+Y)"
                            >
                                <Redo2 className="w-3.5 h-3.5" />
                            </button>
                            {projectId && (
                                <button
                                    onClick={() => setShowVersionHistory(true)}
                                    className="p-1.5 rounded-sm text-zinc-600 hover:text-zinc-300 hover:bg-zinc-800 transition-all duration-200"
                                    title="Version History"
                                >
                                    <History className="w-3.5 h-3.5" />
                                </button>
                            )}
                            <div className="w-px h-4 bg-zinc-800 mx-1" />
                        </div>
                    )}

                    {/* Context-sensitive action button */}
                    {activeMainTab === 'breakdown' && subTab === 'breakdown' && storyState.breakdown.length > 0 && (
                        <button
                            onClick={() => handleTabNavigation('script')}
                            disabled={isProcessing}
                            className="flex items-center gap-2 px-4 py-1.5 rounded-sm font-sans text-[12px] font-medium bg-white text-black hover:bg-zinc-200 disabled:opacity-40 transition-all duration-200"
                        >
                            {isProcessing ? t('studio.generating') : t('story.createScript')}
                        </button>
                    )}
                    {activeMainTab === 'breakdown' && subTab === 'script' && (
                        <div className="flex items-center gap-2">
                            {onExportScript && (
                                <button
                                    onClick={onExportScript}
                                    className="p-1.5 rounded-sm text-zinc-600 hover:text-zinc-300 hover:bg-zinc-800 transition-all duration-200"
                                    title="Export"
                                >
                                    <Download className="w-3.5 h-3.5" />
                                </button>
                            )}
                            <button
                                onClick={handleProceed}
                                className="flex items-center gap-2 px-4 py-1.5 rounded-sm font-sans text-[12px] font-medium bg-white text-black hover:bg-zinc-200 transition-all duration-200"
                            >
                                {!storyState.isLocked && <Lock className="w-3 h-3" />}
                                {storyState.isLocked ? t('story.continueToCast') : t('story.lockScript')}
                            </button>
                        </div>
                    )}
                    {activeMainTab === 'breakdown' && subTab === 'characters' && (
                        <button
                            onClick={onNextStep}
                            className="flex items-center gap-2 px-4 py-1.5 rounded-sm font-sans text-[12px] font-medium bg-white text-black hover:bg-zinc-200 transition-all duration-200"
                        >
                            {t('story.createShotList')}
                        </button>
                    )}
                    {activeMainTab === 'storyboard' && subTab === 'shots' && (
                        <button
                            onClick={() => setSubTab('style')}
                            className="flex items-center gap-2 px-4 py-1.5 rounded-sm font-sans text-[12px] font-medium bg-white text-black hover:bg-zinc-200 transition-all duration-200"
                        >
                            {t('story.selectStyle')}
                        </button>
                    )}
                    {activeMainTab === 'storyboard' && subTab === 'style' && (
                        <button
                            onClick={onNextStep}
                            className="flex items-center gap-2 px-4 py-1.5 rounded-sm font-sans text-[12px] font-medium bg-white text-black hover:bg-zinc-200 transition-all duration-200"
                        >
                            {t('story.generateStoryboard')}
                        </button>
                    )}
                    {activeMainTab === 'storyboard' && subTab === 'storyboard' && storyState.scenesWithVisuals?.length && (
                        <button
                            onClick={() => setSubTab('narration')}
                            className="flex items-center gap-2 px-4 py-1.5 rounded-sm font-sans text-[12px] font-medium bg-white text-black hover:bg-zinc-200 transition-all duration-200"
                        >
                            <Mic className="w-3 h-3" />
                            {t('story.addNarration')}
                        </button>
                    )}
                    {activeMainTab === 'storyboard' && subTab === 'narration' && storyState.narrationSegments?.length && (
                        <button
                            onClick={() => setSubTab('animation')}
                            className="flex items-center gap-2 px-4 py-1.5 rounded-sm font-sans text-[12px] font-medium bg-white text-black hover:bg-zinc-200 transition-all duration-200"
                        >
                            <Video className="w-3 h-3" />
                            {t('story.animateShots')}
                        </button>
                    )}
                    {activeMainTab === 'storyboard' && subTab === 'animation' && (
                        <button
                            onClick={() => setSubTab('export')}
                            className="flex items-center gap-2 px-4 py-1.5 rounded-sm font-sans text-[12px] font-medium bg-white text-black hover:bg-zinc-200 transition-all duration-200"
                        >
                            <Film className="w-3 h-3" />
                            {t('story.export')}
                        </button>
                    )}
                </div>
            </div>

            {/* Progress Bar */}
            {isProcessing && (
                <div className="h-0.5 w-full overflow-hidden relative z-10 bg-zinc-900">
                    <motion.div
                        className="h-full bg-blue-500"
                        initial={{ width: 0 }}
                        animate={{ width: `${progress.percent}%` }}
                        transition={{ duration: 0.3, ease: 'easeOut' }}
                    />
                </div>
            )}

            {/* Error Banner */}
            <AnimatePresence>
                {error && (
                    <motion.div
                        initial={{ opacity: 0, y: -20 }}
                        animate={{ opacity: 1, y: 0 }}
                        exit={{ opacity: 0, y: -20 }}
                        transition={{ duration: 0.15 }}
                        className="relative z-10 mx-6 mt-4 p-4 rounded-sm bg-red-500/10 border border-red-500/30 flex items-start gap-3"
                    >
                        <AlertCircle className="w-5 h-5 text-red-400 shrink-0 mt-0.5" />
                        <div className="flex-1">
                            <p className="text-sm text-zinc-100 font-medium">{t('common.somethingWentWrong')}</p>
                            <p className="text-xs text-zinc-500 mt-1">{error}</p>
                        </div>
                        <div className="flex items-center gap-2 shrink-0">
                            {onRetry && (
                                <button
                                    onClick={onRetry}
                                    className="px-3 py-1.5 text-xs font-medium bg-zinc-800 hover:bg-zinc-700 text-zinc-300 rounded-sm transition-colors duration-200 flex items-center gap-1.5"
                                >
                                    <RefreshCcw className="w-3 h-3" />
                                    {t('common.retry')}
                                </button>
                            )}
                            {onClearError && (
                                <button onClick={onClearError} className="p-1.5 text-zinc-600 hover:text-zinc-300 transition-colors duration-200">
                                    <X className="w-4 h-4" />
                                </button>
                            )}
                        </div>
                    </motion.div>
                )}
            </AnimatePresence>

            {/* Content Area */}
            <div className="flex-1 overflow-hidden relative z-10">
                <AnimatePresence mode="wait">
                    {renderMainContent()}
                </AnimatePresence>
            </div>

            {/* Stage Progress Helper */}
            {stageProgress && activeMainTab === 'storyboard' && !isProcessing && (
                <motion.div
                    initial={{ opacity: 0, y: 20 }}
                    animate={{ opacity: 1, y: 0 }}
                    transition={{ duration: 0.2 }}
                    className="fixed bottom-6 right-6 z-50"
                >
                    <div className="bg-zinc-900 border border-zinc-800 rounded-sm p-4">
                        <div className="flex items-center gap-4 font-mono text-xs">
                            <span className="text-zinc-500">
                                SCENES: <span className="text-zinc-300">{stageProgress.totalScenes}</span>
                            </span>
                            <span className="w-px h-4 bg-zinc-800" />
                            <div className="flex items-center gap-2">
                                <span className={stageProgress.shotsComplete ? "text-emerald-400" : "text-blue-400"}>
                                    SHOTS: {stageProgress.scenesWithShots}/{stageProgress.totalScenes}
                                </span>
                                {stageProgress.shotsComplete
                                    ? <CheckCircle2 className="w-3 h-3 text-emerald-400" />
                                    : <Loader2 className="w-3 h-3 text-blue-400 animate-spin" />
                                }
                            </div>
                            <span className="w-px h-4 bg-zinc-800" />
                            <div className="flex items-center gap-2">
                                <span className={stageProgress.visualsComplete ? "text-emerald-400" : "text-orange-400"}>
                                    VISUALS: {stageProgress.scenesWithVisuals}/{stageProgress.totalScenes}
                                </span>
                                {stageProgress.visualsComplete
                                    ? <CheckCircle2 className="w-3 h-3 text-emerald-400" />
                                    : <Circle className="w-3 h-3 text-orange-400" />
                                }
                            </div>
                        </div>
                    </div>
                </motion.div>
            )}

            {/* Shot Editor Modal */}
            {editingShot && (() => {
                const sceneId = editingShot.sceneId;
                const scene = storyState.breakdown.find(s => s.id === sceneId);
                const shotsInScene = storyState.shotlist.filter(s => s.sceneId === sceneId);
                const shotIndexInScene = shotsInScene.findIndex(s => s.id === editingShot.id);
                return (
                    <ShotEditorModal
                        shot={editingShot}
                        sceneNumber={scene?.sceneNumber ?? 0}
                        sceneHeading={scene?.heading ?? ''}
                        sceneLighting={editingShot.lighting}
                        shotIndexInScene={shotIndexInScene >= 0 ? shotIndexInScene : 0}
                        totalShotsInScene={shotsInScene.length}
                        onClose={() => setEditingShot(null)}
                        onSave={(shotId, updates) => {
                            onUpdateShot?.(shotId, updates);
                            setEditingShot(null);
                        }}
                        onRetry={(shotId) => {
                            const idx = storyState.breakdown.findIndex(s => s.id === sceneId);
                            onGenerateVisuals?.(idx >= 0 ? idx : undefined);
                            setEditingShot(null);
                        }}
                        onNavigate={(dir) => {
                            // Use ref to avoid stale closure on rapid clicks
                            const currentShot = editingShotRef.current;
                            if (!currentShot) return;
                            const idx = shotsInScene.findIndex(s => s.id === currentShot.id);
                            const nextIdx = dir === 'next' ? idx + 1 : idx - 1;
                            const nextShot = shotsInScene[nextIdx];
                            if (nextShot) setEditingShot(nextShot);
                        }}
                        isProcessing={isProcessing}
                    />
                );
            })()}

            {/* Lock Warning Dialog */}
            <LockWarningDialog
                isOpen={showLockDialog}
                onClose={() => setShowLockDialog(false)}
                onConfirmLock={() => {
                    onLockStory?.();
                    setShowLockDialog(false);
                }}
                estimatedCost={estimateProjectCost(storyState)}
                sceneCount={storyState.breakdown.length}
                estimatedShots={storyState.breakdown.length * 5}
            />

            {/* Version History Panel */}
            <AnimatePresence>
                {showVersionHistory && projectId && (
                    <motion.div
                        initial={{ opacity: 0 }}
                        animate={{ opacity: 1 }}
                        exit={{ opacity: 0 }}
                        transition={{ duration: 0.15 }}
                        className="fixed inset-0 bg-black/80 backdrop-blur-sm flex items-center justify-center p-4 z-50"
                        onClick={() => setShowVersionHistory(false)}
                    >
                        <motion.div
                            initial={{ scale: 0.97, opacity: 0 }}
                            animate={{ scale: 1, opacity: 1 }}
                            exit={{ scale: 0.97, opacity: 0 }}
                            transition={{ duration: 0.15 }}
                            className="w-full max-w-xl h-[70vh]"
                            onClick={(e) => e.stopPropagation()}
                        >
                            <VersionHistoryPanel
                                projectId={projectId}
                                currentState={storyState}
                                onRestore={(state) => {
                                    onImportProject?.(state);
                                    setShowVersionHistory(false);
                                }}
                            />
                        </motion.div>
                    </motion.div>
                )}
            </AnimatePresence>

            {/* Checkpoint Approval Overlay */}
            <AnimatePresence>
                {formatPipelineHook?.activeCheckpoint && (() => {
                    const cp = formatPipelineHook.activeCheckpoint;
                    const d = cp.data ?? {};
                    const phase = cp.phase;

                    // Build preview content from checkpoint data
                    let previewContent: React.ReactNode = null;
                    const scenes = d.scenes as { heading: string; action: string }[] | undefined;
                    const visuals = d.visuals as { sceneId: string; imageUrl: string }[] | undefined;

                    if (scenes && scenes.length > 0) {
                        previewContent = (
                            <div className="space-y-2">
                                {d.sceneCount ? <p className="text-xs text-zinc-500 mb-2">{String(d.sceneCount)} scenes {d.estimatedDuration ? `· ${d.estimatedDuration}` : ''}</p> : null}
                                {scenes.map((s, i) => (
                                    <div key={i} className="flex gap-3 items-start">
                                        <span className="font-mono text-[10px] text-blue-400 shrink-0 mt-0.5">{String(i + 1).padStart(2, '0')}</span>
                                        <div className="min-w-0">
                                            <p className="text-sm text-zinc-200 font-medium">{s.heading}</p>
                                            <p className="text-xs text-zinc-500 line-clamp-2" dir="auto">{s.action}</p>
                                        </div>
                                    </div>
                                ))}
                            </div>
                        );
                    }

                    if (visuals && visuals.length > 0) {
                        previewContent = (
                            <div>
                                {d.visualCount != null && <p className="text-xs text-zinc-500 mb-2">{d.visualCount as number}/{d.totalScenes as number ?? '?'} visuals generated</p>}
                                <div className="grid grid-cols-3 gap-2">
                                    {visuals.map((v, i) => (
                                        <div key={v.sceneId} className="aspect-video bg-zinc-950 rounded-sm overflow-hidden border border-zinc-800">
                                            <img src={v.imageUrl} alt={`Scene ${i + 1}`} className="w-full h-full object-cover" />
                                        </div>
                                    ))}
                                </div>
                            </div>
                        );
                    }

                    if (phase.includes('final') || phase.includes('assembly')) {
                        const stats = [
                            d.sceneCount != null && `${d.sceneCount} scenes`,
                            d.visualCount != null && `${d.visualCount} visuals`,
                            d.narrationCount != null && `${d.narrationCount} narrations`,
                            d.totalDuration != null && `${Math.round(d.totalDuration as number)}s total`,
                        ].filter(Boolean);
                        if (stats.length > 0 && !scenes && !visuals) {
                            previewContent = (
                                <div className="flex flex-wrap gap-3">
                                    {stats.map((s, i) => (
                                        <span key={i} className="px-2.5 py-1 bg-zinc-800 rounded-sm text-xs font-mono text-zinc-300">{s}</span>
                                    ))}
                                </div>
                            );
                        }
                    }

                    if (d.sourceCount != null && !scenes && !visuals) {
                        previewContent = (
                            <div className="space-y-1">
                                <p className="text-sm text-zinc-300">{d.sourceCount as number} sources found</p>
                                {d.confidence != null && <p className="text-xs text-zinc-500">Confidence: {Math.round((d.confidence as number) * 100)}%</p>}
                            </div>
                        );
                    }

                    return (
                        <motion.div
                            initial={{ opacity: 0 }}
                            animate={{ opacity: 1 }}
                            exit={{ opacity: 0 }}
                            transition={{ duration: 0.15 }}
                            className="fixed inset-0 bg-black/80 backdrop-blur-sm flex items-center justify-center p-4 z-50"
                        >
                            <motion.div
                                initial={{ scale: 0.97, opacity: 0 }}
                                animate={{ scale: 1, opacity: 1 }}
                                exit={{ scale: 0.97, opacity: 0 }}
                                transition={{ duration: 0.15 }}
                                className={`w-full ${visuals && visuals.length > 0 ? 'max-w-3xl' : 'max-w-2xl'} max-h-[80vh] overflow-y-auto`}
                                onClick={(e) => e.stopPropagation()}
                            >
                                <div className="bg-zinc-900 border border-zinc-700 rounded-sm p-6">
                                    <CheckpointApproval
                                        checkpointId={cp.checkpointId}
                                        phase={cp.phase}
                                        title={`Review: ${cp.phase.replace(/-/g, ' ')}`}
                                        description={previewContent ? undefined : "Review the generated content before the pipeline continues to the next phase."}
                                        previewData={previewContent}
                                        onApprove={() => formatPipelineHook.approveCheckpoint()}
                                        onRequestChanges={(_id, changeRequest) => formatPipelineHook.rejectCheckpoint(changeRequest)}
                                    />
                                </div>
                            </motion.div>
                        </motion.div>
                    );
                })()}
            </AnimatePresence>
        </div>
    );
};

export default StoryWorkspace;
````

## File: packages/frontend/components/story/StoryWorkspaceErrorBoundary.tsx
````typescript
/**
 * StoryWorkspaceErrorBoundary.tsx
 * Error boundary with recovery options for StoryWorkspace.
 */

import React from "react";
import { AlertCircle, RotateCcw, RefreshCw } from "lucide-react";
import { Button } from "@/components/ui/button";
import type { StoryState } from "@/types";

interface StoryWorkspaceErrorBoundaryProps {
  children: React.ReactNode;
  storyState: StoryState;
  onRestore?: () => void;
}

interface StoryWorkspaceErrorBoundaryState {
  hasError: boolean;
  error: Error | null;
  errorId: string | null;
}

export class StoryWorkspaceErrorBoundary extends React.Component<
  StoryWorkspaceErrorBoundaryProps,
  StoryWorkspaceErrorBoundaryState
> {
  constructor(props: StoryWorkspaceErrorBoundaryProps) {
    super(props);
    this.state = { hasError: false, error: null, errorId: null };
  }

  static getDerivedStateFromError(error: Error): StoryWorkspaceErrorBoundaryState {
    const errorId = `ERR-${Date.now().toString(36).toUpperCase()}-${Math.random().toString(36).substring(2, 6).toUpperCase()}`;
    return { hasError: true, error, errorId };
  }

  componentDidCatch(error: Error, errorInfo: React.ErrorInfo): void {
    console.error("[StoryWorkspaceError] Caught error:", error);
    console.error("[StoryWorkspaceError] Component stack:", errorInfo.componentStack);
    console.error("[StoryWorkspaceError] Story State at error:", {
      step: this.props.storyState.currentStep,
      hasBreakdownSteps: this.props.storyState.breakdown?.length || 0,
      hasBreakdown: !!this.props.storyState.breakdown,
      hasScript: !!this.props.storyState.script,
      charactersCount: this.props.storyState.characters?.length || 0,
      shotsCount: this.props.storyState.shotlist?.length || 0,
    });
  }

  handleRetry = (): void => {
    this.setState({ hasError: false, error: null, errorId: null });
  };

  handleRestore = (): void => {
    this.props.onRestore?.();
    this.setState({ hasError: false, error: null, errorId: null });
  };

  render(): React.ReactNode {
    if (this.state.hasError) {
      return (
        <div
          className="flex flex-col items-center justify-center min-h-[60vh] p-8 bg-black"
          role="alert"
          aria-live="assertive"
          aria-atomic="true"
        >
          {/* Error Icon */}
          <div className="mb-6">
            <div className="w-20 h-20 rounded-sm bg-red-500/10 border border-red-500/30 flex items-center justify-center">
              <AlertCircle className="w-10 h-10 text-red-400" />
            </div>
          </div>

          {/* Error Message */}
          <h2 className="font-sans text-2xl font-medium tracking-tight text-zinc-100 mb-2">
            Story Workspace Crashed
          </h2>

          <p className="text-zinc-500 text-sm text-center max-w-md mb-6">
            An unexpected error occurred while working on your story. Don't worry — your work is auto-saved.
          </p>

          {/* Error Details */}
          {this.state.error && (
            <details className="mb-6 text-sm text-zinc-600 max-w-md w-full">
              <summary className="cursor-pointer hover:text-zinc-400 font-mono text-xs mb-2">
                Technical Details
              </summary>
              <pre className="mt-2 p-3 bg-zinc-900 rounded-sm text-xs overflow-auto max-h-32 border border-zinc-800 text-zinc-400">
                {this.state.error.message}
              </pre>
            </details>
          )}

          {/* Error Reference ID */}
          {this.state.errorId && (
            <p
              className="font-mono text-[10px] text-zinc-700 mb-8"
              aria-label={`Error reference ID: ${this.state.errorId}`}
            >
              Reference: <code className="px-2 py-1 bg-zinc-900 rounded-sm border border-zinc-800">{this.state.errorId}</code>
            </p>
          )}

          {/* Recovery Actions */}
          <div className="flex flex-col sm:flex-row gap-3">
            <Button
              onClick={this.handleRestore}
              className="bg-blue-500 hover:bg-blue-600 text-white rounded-sm font-sans"
            >
              <RotateCcw className="w-4 h-4 mr-2" />
              Restore Last Save
            </Button>

            <Button
              onClick={this.handleRetry}
              variant="outline"
              className="border-zinc-800 text-zinc-300 hover:bg-zinc-800 rounded-sm font-sans"
            >
              <RefreshCw className="w-4 h-4 mr-2" />
              Try Again
            </Button>

            <Button
              onClick={() => window.location.reload()}
              variant="outline"
              className="border-zinc-800 text-zinc-500 hover:bg-zinc-900 rounded-sm font-sans"
            >
              Refresh Page
            </Button>
          </div>

          {/* Help Text */}
          <p className="mt-8 text-xs text-zinc-700 text-center max-w-sm">
            If the problem persists, try refreshing the page or starting a new project.
            Your previous work is saved in version history.
          </p>
        </div>
      );
    }

    return this.props.children;
  }
}
````

## File: packages/frontend/components/story/StyleSelector.tsx
````typescript
/**
 * StyleSelector.tsx
 * Visual style gallery with sharp utility design.
 */

import React from 'react';
import { motion } from 'framer-motion';
import { Check, RectangleHorizontal, Cpu } from 'lucide-react';
import {
    VISUAL_STYLES,
    ASPECT_RATIOS,
    type VisualStyleKey,
    type AspectRatioId,
} from '@/constants/visualStyles';

interface StyleSelectorProps {
    selectedStyle: VisualStyleKey;
    onSelectStyle: (style: VisualStyleKey) => void;
    aspectRatio: AspectRatioId;
    onSelectAspectRatio: (ratio: AspectRatioId) => void;
    imageProvider?: 'gemini' | 'deapi';
    onSelectImageProvider?: (provider: 'gemini' | 'deapi') => void;
}

export const StyleSelector: React.FC<StyleSelectorProps> = ({
    selectedStyle,
    onSelectStyle,
    aspectRatio,
    onSelectAspectRatio,
    imageProvider = 'gemini',
    onSelectImageProvider,
}) => {
    const styles = Object.values(VISUAL_STYLES);

    const stylesByCategory = {
        cinematic: styles.filter(s => s.category === 'cinematic'),
        artistic: styles.filter(s => s.category === 'artistic'),
        stylized: styles.filter(s => s.category === 'stylized'),
        modern: styles.filter(s => s.category === 'modern'),
    };

    const categoryLabels: Record<string, string> = {
        cinematic: 'Cinematic',
        artistic: 'Artistic',
        stylized: 'Stylized',
        modern: 'Modern',
    };

    return (
        <div className="p-8 max-w-6xl mx-auto">
            {/* Header */}
            <div className="mb-12">
                <h2 className="font-sans text-2xl font-medium tracking-tight text-zinc-100 mb-2">
                    Visual Direction
                </h2>
                <p className="text-zinc-500 text-sm">
                    Choose a style that matches your story's mood
                </p>
            </div>

            {/* Aspect Ratio Selector */}
            <div className="mb-12">
                <div className="flex items-center gap-3 mb-4">
                    <RectangleHorizontal className="w-4 h-4 text-zinc-600" />
                    <span className="font-mono text-xs text-zinc-600 uppercase tracking-widest">
                        Frame Ratio
                    </span>
                </div>
                <div className="flex flex-wrap gap-3">
                    {ASPECT_RATIOS.slice(0, 4).map((ratio) => {
                        const isSelected = aspectRatio === ratio.id;
                        return (
                            <button
                                key={ratio.id}
                                onClick={() => onSelectAspectRatio(ratio.id)}
                                className={`
                                    px-5 py-3 rounded-sm transition-colors duration-200
                                    ${isSelected
                                        ? 'bg-blue-500/10 border border-blue-500/50 text-blue-400'
                                        : 'bg-zinc-900 border border-zinc-800 text-zinc-400 hover:border-zinc-600'
                                    }
                                `}
                            >
                                <span className="font-sans text-sm font-medium">
                                    {ratio.label}
                                </span>
                                <span className={`ml-2 text-xs ${isSelected ? 'text-blue-400/70' : 'text-zinc-600'}`}>
                                    {ratio.description}
                                </span>
                            </button>
                        );
                    })}
                </div>
            </div>

            {/* Image Provider Selector */}
            {onSelectImageProvider && (
                <div className="mb-12">
                    <div className="flex items-center gap-3 mb-4">
                        <Cpu className="w-4 h-4 text-zinc-600" />
                        <span className="font-mono text-xs text-zinc-600 uppercase tracking-widest">
                            Image Engine
                        </span>
                    </div>
                    <div className="flex flex-wrap gap-3">
                        {([
                            { id: 'gemini' as const, label: 'Imagen 4', desc: 'Google AI (default)' },
                            { id: 'deapi' as const, label: 'FLUX.2 Klein', desc: 'DeAPI, fast generation' },
                        ]).map((provider) => {
                            const isSelected = imageProvider === provider.id;
                            return (
                                <button
                                    key={provider.id}
                                    onClick={() => onSelectImageProvider(provider.id)}
                                    className={`
                                        px-5 py-3 rounded-sm transition-colors duration-200
                                        ${isSelected
                                            ? 'bg-blue-500/10 border border-blue-500/50 text-blue-400'
                                            : 'bg-zinc-900 border border-zinc-800 text-zinc-400 hover:border-zinc-600'
                                        }
                                    `}
                                >
                                    <span className="font-sans text-sm font-medium">
                                        {provider.label}
                                    </span>
                                    <span className={`ml-2 text-xs ${isSelected ? 'text-blue-400/70' : 'text-zinc-600'}`}>
                                        {provider.desc}
                                    </span>
                                </button>
                            );
                        })}
                    </div>
                </div>
            )}

            {/* Style Grid by Category */}
            <div className="space-y-12">
                {Object.entries(stylesByCategory).map(([category, categoryStyles]) => (
                    <div key={category}>
                        {/* Category Label */}
                        <div className="flex items-center gap-4 mb-6">
                            <span className="font-mono text-xs text-zinc-600 uppercase tracking-[0.2em]">
                                {categoryLabels[category]}
                            </span>
                            <div className="flex-1 h-px bg-zinc-800" />
                        </div>

                        {/* Style Cards Grid */}
                        <div className="grid grid-cols-2 sm:grid-cols-3 lg:grid-cols-4 gap-4">
                            {categoryStyles.map((style) => {
                                const isSelected = selectedStyle === style.id;
                                return (
                                    <button
                                        key={style.id}
                                        onClick={() => onSelectStyle(style.id as VisualStyleKey)}
                                        className={`
                                            group relative flex flex-col overflow-hidden rounded-sm transition-all duration-200 ease-out
                                            hover:-translate-y-0.5
                                            ${isSelected
                                                ? 'border-2 border-blue-500 ring-1 ring-blue-500/20'
                                                : 'border border-zinc-800 hover:border-zinc-600'
                                            }
                                        `}
                                    >
                                        {/* Preview Area */}
                                        <div className="aspect-video relative overflow-hidden bg-zinc-950">
                                            {/* Gradient Background */}
                                            <div className={`absolute inset-0 ${getStyleGradient(style.id)}`} />

                                            {/* Sample Image */}
                                            {style.sampleImage && (
                                                <img
                                                    src={style.sampleImage}
                                                    alt={style.name}
                                                    className="absolute inset-0 w-full h-full object-cover"
                                                    onError={(e) => {
                                                        (e.target as HTMLImageElement).style.display = 'none';
                                                    }}
                                                />
                                            )}

                                            {/* Style Name Overlay */}
                                            <div className="absolute inset-x-0 bottom-0 p-3 bg-gradient-to-t from-black to-transparent">
                                                <span className="font-sans text-sm font-medium text-zinc-100">
                                                    {style.name}
                                                </span>
                                            </div>

                                            {/* Selection Checkmark */}
                                            {isSelected && (
                                                <div className="absolute top-2 right-2 w-6 h-6 rounded-sm bg-blue-500 flex items-center justify-center">
                                                    <Check className="w-3.5 h-3.5 text-white" />
                                                </div>
                                            )}
                                        </div>

                                        {/* Description */}
                                        <div className="p-3 bg-zinc-900">
                                            <p className="text-xs text-zinc-500 line-clamp-2">
                                                {style.description}
                                            </p>
                                        </div>
                                    </button>
                                );
                            })}
                        </div>
                    </div>
                ))}
            </div>

            {/* Selected Style Preview */}
            {selectedStyle && VISUAL_STYLES[selectedStyle] && (
                <div className="mt-12 p-6 bg-zinc-900 border border-zinc-800 rounded-sm">
                    <div className="flex items-center gap-6">
                        {/* Mini Preview */}
                        <div className="shrink-0 w-24 h-14 rounded-sm overflow-hidden relative">
                            <div className={`w-full h-full ${getStyleGradient(selectedStyle)}`} />
                        </div>

                        {/* Info */}
                        <div className="flex-1 min-w-0">
                            <h3 className="font-sans text-lg font-medium text-zinc-100 mb-1">
                                {VISUAL_STYLES[selectedStyle].name}
                            </h3>
                            <p className="text-sm text-zinc-500">
                                {VISUAL_STYLES[selectedStyle].description}
                            </p>
                        </div>

                        {/* Badge */}
                        <div className="shrink-0">
                            <span className="inline-flex items-center gap-2 px-3 py-1.5 rounded-sm bg-blue-500/10 border border-blue-500/30">
                                <Check className="w-4 h-4 text-blue-400" />
                                <span className="font-mono text-xs text-blue-400 uppercase tracking-wider">
                                    Selected
                                </span>
                            </span>
                        </div>
                    </div>
                </div>
            )}
        </div>
    );
};

function getStyleGradient(styleId: string): string {
    const gradients: Record<string, string> = {
        CINEMATIC: 'bg-gradient-to-br from-amber-900/40 to-slate-900/60',
        NOIR: 'bg-gradient-to-br from-zinc-700 to-black',
        COMIC: 'bg-gradient-to-br from-red-500/30 to-yellow-500/30',
        ANIME: 'bg-gradient-to-br from-pink-500/30 to-blue-500/30',
        WATERCOLOR: 'bg-gradient-to-br from-blue-300/30 to-pink-300/30',
        OIL_PAINTING: 'bg-gradient-to-br from-amber-700/40 to-rose-800/40',
        CYBERPUNK: 'bg-gradient-to-br from-purple-600/40 to-cyan-500/40',
        DARK_FANTASY: 'bg-gradient-to-br from-slate-800 to-purple-950',
        PHOTOREALISTIC: 'bg-gradient-to-br from-emerald-800/30 to-blue-800/30',
        PIXEL_ART: 'bg-gradient-to-br from-green-500/30 to-blue-500/30',
        MINIMALIST: 'bg-gradient-to-br from-zinc-200/20 to-zinc-400/20',
        SURREALIST: 'bg-gradient-to-br from-orange-500/30 to-indigo-600/40',
    };

    return gradients[styleId] || 'bg-gradient-to-br from-zinc-700 to-zinc-900';
}

export default StyleSelector;
````

## File: packages/frontend/components/story/TemplatesGallery.tsx
````typescript
/**
 * TemplatesGallery - Browse and apply project templates.
 */

import React, { useState, useMemo } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import {
  Layout,
  Film,
  ShoppingBag,
  GraduationCap,
  Smartphone,
  Palette,
  Clock,
  ChevronRight,
  Search,
  X,
  ArrowRight,
  Layers,
  Eye,
  Ratio,
} from 'lucide-react';
import { cn } from '@/lib/utils';
import type { StoryState } from '@/types';
import { useLanguage } from '@/i18n/useLanguage';
import {
  getAllTemplates,
  getTemplatesByCategory,
  getTemplateCategories,
  applyTemplate,
  type ProjectTemplate,
} from '@/services/projectTemplatesService';

interface TemplatesGalleryProps {
  onApplyTemplate: (state: Partial<StoryState>) => void;
  onClose?: () => void;
  className?: string;
}

const categoryIcons: Record<string, React.ReactNode> = {
  narrative: <Film className="w-3.5 h-3.5" />,
  commercial: <ShoppingBag className="w-3.5 h-3.5" />,
  educational: <GraduationCap className="w-3.5 h-3.5" />,
  social: <Smartphone className="w-3.5 h-3.5" />,
  experimental: <Palette className="w-3.5 h-3.5" />,
};

const defaultDifficultyColor = { text: 'text-zinc-400', bg: 'bg-zinc-500/10' };
const difficultyColors: Record<string, { text: string; bg: string }> = {
  beginner: { text: 'text-emerald-400', bg: 'bg-emerald-500/10' },
  intermediate: { text: 'text-orange-400', bg: 'bg-orange-500/10' },
  advanced: { text: 'text-red-400', bg: 'bg-red-500/10' },
};

/** Translate a template field, falling back to the original value */
function useTemplateTranslation() {
  const { t } = useLanguage();

  return {
    tName: (template: ProjectTemplate) =>
      t(`story.templates.items.${template.id}.name`, { defaultValue: template.name }),
    tDesc: (template: ProjectTemplate) =>
      t(`story.templates.items.${template.id}.description`, { defaultValue: template.description }),
    tGenre: (genre: string) =>
      t(`story.templates.genres.${genre}`, { defaultValue: genre }),
    tTag: (tag: string) =>
      t(`story.templates.tags.${tag}`, { defaultValue: tag }),
    tDifficulty: (diff: string) =>
      t(`story.templates.difficulty.${diff}`, { defaultValue: diff }),
    tCategory: (cat: string) =>
      t(`story.templates.categories.${cat}`, { defaultValue: cat }),
    tStyle: (style: string) =>
      t(`story.templates.styles.${style}`, { defaultValue: style }),
    tSceneHeading: (template: ProjectTemplate, sceneNum: number) =>
      t(`story.templates.items.${template.id}.scene${sceneNum}_heading`, { defaultValue: '' }),
    tSceneAction: (template: ProjectTemplate, sceneNum: number) =>
      t(`story.templates.items.${template.id}.scene${sceneNum}_action`, { defaultValue: '' }),
  };
}

function TemplateCard({
  template,
  onClick,
  isSelected,
}: {
  template: ProjectTemplate;
  onClick: () => void;
  isSelected: boolean;
}) {
  const colors = difficultyColors[template.difficulty] ?? defaultDifficultyColor;
  const tt = useTemplateTranslation();

  return (
    <motion.div
      layout
      onClick={onClick}
      className={cn(
        'group relative cursor-pointer rounded-sm border transition-all duration-200 hover:-translate-y-0.5',
        isSelected
          ? 'border-blue-500/50 bg-blue-500/5'
          : 'border-zinc-800 bg-zinc-900 hover:border-zinc-700'
      )}
    >
      <div className="p-4">
        {/* Top row: difficulty + genre */}
        <div className="flex items-center gap-2 mb-3">
          <span className={cn('font-mono text-[10px] uppercase tracking-widest px-2 py-0.5 rounded-sm', colors.text, colors.bg)}>
            {tt.tDifficulty(template.difficulty)}
          </span>
          <span className="font-mono text-[10px] uppercase tracking-widest px-2 py-0.5 rounded-sm bg-zinc-800 text-zinc-500">
            {tt.tGenre(template.genre)}
          </span>
        </div>

        {/* Title */}
        <h4 className="font-sans text-sm font-medium text-zinc-100 mb-1.5 leading-snug">
          {tt.tName(template)}
        </h4>

        {/* Description */}
        <p className="text-xs text-zinc-600 leading-relaxed line-clamp-2 mb-3">
          {tt.tDesc(template)}
        </p>

        {/* Meta row */}
        <div className="flex items-center gap-4 text-zinc-700">
          <span className="flex items-center gap-1.5 font-mono text-[10px]">
            <Layers className="w-3 h-3" />
            {template.templateScenes.length}
          </span>
          <span className="flex items-center gap-1.5 font-mono text-[10px]">
            <Clock className="w-3 h-3" />
            {template.estimatedDuration}
          </span>
        </div>

        {/* Tags */}
        {template.tags.length > 0 && (
          <div className="flex flex-wrap gap-1.5 mt-3 pt-3 border-t border-zinc-800">
            {template.tags.slice(0, 3).map((tag) => (
              <span
                key={tag}
                className="font-mono text-[9px] text-zinc-600 px-1.5 py-0.5 rounded-sm bg-zinc-950 border border-zinc-800"
              >
                {tt.tTag(tag)}
              </span>
            ))}
            {template.tags.length > 3 && (
              <span className="font-mono text-[9px] text-zinc-700">
                +{template.tags.length - 3}
              </span>
            )}
          </div>
        )}
      </div>

      {/* Hover arrow */}
      <div className="absolute top-4 right-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200">
        <ChevronRight className="w-4 h-4 text-zinc-600" />
      </div>
    </motion.div>
  );
}

function TemplatePreview({
  template,
  onApply,
  onClose,
}: {
  template: ProjectTemplate;
  onApply: () => void;
  onClose: () => void;
}) {
  const { t } = useLanguage();
  const tt = useTemplateTranslation();
  const colors = difficultyColors[template.difficulty] ?? defaultDifficultyColor;

  return (
    <motion.div
      initial={{ opacity: 0, x: 12 }}
      animate={{ opacity: 1, x: 0 }}
      exit={{ opacity: 0, x: 12 }}
      transition={{ duration: 0.15, ease: 'easeOut' }}
      className="h-full flex flex-col"
    >
      {/* Preview header */}
      <div className="flex items-start justify-between p-5 pb-0">
        <div className="flex-1 min-w-0">
          <div className="flex items-center gap-2 mb-2">
            <span className={cn('font-mono text-[10px] uppercase tracking-widest px-2 py-0.5 rounded-sm', colors.text, colors.bg)}>
              {tt.tDifficulty(template.difficulty)}
            </span>
          </div>
          <h3 className="font-sans text-lg font-medium text-zinc-100 leading-snug">
            {tt.tName(template)}
          </h3>
        </div>
        <button
          onClick={onClose}
          className="p-1 -mr-1 text-zinc-700 hover:text-zinc-400 transition-colors duration-200"
        >
          <X className="w-4 h-4" />
        </button>
      </div>

      {/* Scrollable content */}
      <div className="flex-1 overflow-y-auto px-5 py-4 space-y-5">
        <p className="text-sm text-zinc-500 leading-relaxed">
          {tt.tDesc(template)}
        </p>

        {/* Stats grid */}
        <div className="grid grid-cols-2 gap-2">
          {[
            { label: t('story.templates.duration'), value: template.estimatedDuration, icon: Clock },
            { label: t('story.templates.scenes'), value: `${template.templateScenes.length}`, icon: Layers },
            { label: t('story.templates.style'), value: tt.tStyle(template.visualStyle), icon: Eye },
            { label: t('story.templates.ratio'), value: template.aspectRatio, icon: Ratio },
          ].map(({ label, value, icon: Icon }) => (
            <div
              key={label}
              className="p-3 rounded-sm bg-zinc-950 border border-zinc-800"
            >
              <div className="flex items-center gap-1.5 mb-1">
                <Icon className="w-3 h-3 text-zinc-700" />
                <span className="font-mono text-[10px] uppercase tracking-widest text-zinc-600">
                  {label}
                </span>
              </div>
              <p className="text-sm font-medium text-zinc-300">{value}</p>
            </div>
          ))}
        </div>

        {/* Scene breakdown */}
        <div>
          <span className="font-mono text-[10px] uppercase tracking-widest text-zinc-600 block mb-2.5">
            {t('story.templates.scenes')}
          </span>
          <div className="space-y-1.5">
            {template.templateScenes.map((scene, i) => (
              <div
                key={i}
                className="flex items-center gap-3 px-3 py-2.5 rounded-sm bg-zinc-950 border border-zinc-800"
              >
                <span className="w-5 h-5 rounded-sm flex items-center justify-center font-mono text-[10px] bg-zinc-800 text-zinc-500 flex-shrink-0">
                  {scene.sceneNumber}
                </span>
                <div className="flex-1 min-w-0">
                  <p className="font-mono text-[11px] text-zinc-400 truncate">
                    {tt.tSceneHeading(template, scene.sceneNumber) || scene.heading}
                  </p>
                  <p className="text-[11px] text-zinc-600 line-clamp-1 mt-0.5">
                    {tt.tSceneAction(template, scene.sceneNumber) || scene.action}
                  </p>
                </div>
                {scene.duration && (
                  <span className="font-mono text-[10px] text-zinc-700 flex-shrink-0">{scene.duration}s</span>
                )}
              </div>
            ))}
          </div>
        </div>

        {/* Suggested styles */}
        {template.suggestedVisualStyles.length > 0 && (
          <div>
            <span className="font-mono text-[10px] uppercase tracking-widest text-zinc-600 block mb-2.5">
              {t('story.templates.visualStyles')}
            </span>
            <div className="flex flex-wrap gap-1.5">
              {template.suggestedVisualStyles.map((style) => (
                <span
                  key={style}
                  className="text-xs px-2.5 py-1 rounded-sm text-zinc-400 bg-zinc-950 border border-zinc-800"
                >
                  {tt.tStyle(style)}
                </span>
              ))}
            </div>
          </div>
        )}
      </div>

      {/* Apply button */}
      <div className="p-4 pt-3 border-t border-zinc-800">
        <button
          onClick={onApply}
          className="w-full flex items-center justify-center gap-2.5 py-3 rounded-sm bg-white text-black font-sans text-sm font-medium hover:bg-zinc-200 transition-colors duration-200"
        >
          <span>{t('story.templates.useTemplate')}</span>
          <ArrowRight className="w-3.5 h-3.5" />
        </button>
      </div>
    </motion.div>
  );
}

export function TemplatesGallery({
  onApplyTemplate,
  onClose,
  className,
}: TemplatesGalleryProps) {
  const { t, language } = useLanguage();
  const tt = useTemplateTranslation();
  const [selectedCategory, setSelectedCategory] = useState<string | null>(null);
  const [searchQuery, setSearchQuery] = useState('');
  const [selectedTemplate, setSelectedTemplate] = useState<ProjectTemplate | null>(null);

  const categories = getTemplateCategories();

  const templates = useMemo(() => {
    if (searchQuery) {
      const lowerQuery = searchQuery.toLowerCase();
      return getAllTemplates().filter((tmpl) => {
        const name = tt.tName(tmpl).toLowerCase();
        const desc = tt.tDesc(tmpl).toLowerCase();
        const genre = tt.tGenre(tmpl.genre).toLowerCase();
        const tags = tmpl.tags.map((tag) => tt.tTag(tag).toLowerCase());
        return (
          name.includes(lowerQuery) ||
          desc.includes(lowerQuery) ||
          genre.includes(lowerQuery) ||
          tags.some((tag) => tag.includes(lowerQuery)) ||
          tmpl.name.toLowerCase().includes(lowerQuery) ||
          tmpl.description.toLowerCase().includes(lowerQuery) ||
          tmpl.genre.toLowerCase().includes(lowerQuery) ||
          tmpl.tags.some((tag) => tag.toLowerCase().includes(lowerQuery))
        );
      });
    }
    if (selectedCategory) {
      return getTemplatesByCategory(selectedCategory as ProjectTemplate['category']);
    }
    return getAllTemplates();
  // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [selectedCategory, searchQuery, language]);

  const handleApply = () => {
    if (!selectedTemplate) return;
    const state = applyTemplate(selectedTemplate);
    onApplyTemplate(state);
    onClose?.();
  };

  return (
    <div
      className={cn('flex flex-col h-full rounded-sm overflow-hidden bg-zinc-950 border border-zinc-800', className)}
    >
      {/* Header */}
      <div className="flex items-center justify-between px-5 py-4 border-b border-zinc-800">
        <div className="flex items-center gap-3">
          <div className="w-7 h-7 rounded-sm flex items-center justify-center bg-zinc-900 border border-zinc-800">
            <Layout className="w-3.5 h-3.5 text-zinc-500" />
          </div>
          <h3 className="font-sans text-sm font-medium text-zinc-100">{t('story.templates.title')}</h3>
          <span className="font-mono text-[10px] text-zinc-600">{templates.length}</span>
        </div>
        {onClose && (
          <button
            onClick={onClose}
            className="p-1.5 rounded-sm text-zinc-600 hover:text-zinc-300 hover:bg-zinc-800 transition-all duration-200"
          >
            <X className="w-4 h-4" />
          </button>
        )}
      </div>

      {/* Search + Filters */}
      <div className="px-5 py-3 space-y-3 border-b border-zinc-800">
        {/* Search */}
        <div className="relative">
          <Search className="absolute left-3 top-1/2 -translate-y-1/2 w-3.5 h-3.5 text-zinc-700" />
          <input
            type="text"
            value={searchQuery}
            onChange={(e) => {
              setSearchQuery(e.target.value);
              setSelectedCategory(null);
            }}
            placeholder={t('story.templates.searchPlaceholder')}
            className="w-full pl-9 pr-8 py-2 rounded-sm font-sans text-sm text-zinc-200 bg-zinc-900 border border-zinc-800 placeholder:text-zinc-700 focus:outline-none focus:border-blue-500 transition-colors duration-200"
          />
          {searchQuery && (
            <button
              onClick={() => setSearchQuery('')}
              className="absolute right-2.5 top-1/2 -translate-y-1/2 text-zinc-600 hover:text-zinc-300 transition-colors"
            >
              <X className="w-3.5 h-3.5" />
            </button>
          )}
        </div>

        {/* Category pills */}
        <div className="flex items-center gap-1.5 overflow-x-auto pb-0.5 -mx-1 px-1">
          <button
            onClick={() => {
              setSelectedCategory(null);
              setSearchQuery('');
            }}
            className={cn(
              'px-3 py-1.5 rounded-sm text-xs font-sans whitespace-nowrap transition-all duration-200 border',
              !selectedCategory && !searchQuery
                ? 'bg-blue-500/10 border-blue-500/30 text-blue-400'
                : 'border-zinc-800 text-zinc-500 hover:text-zinc-300'
            )}
          >
            {t('common.all')}
          </button>
          {categories.map((cat) => (
            <button
              key={cat.id}
              onClick={() => {
                setSelectedCategory(cat.id);
                setSearchQuery('');
              }}
              className={cn(
                'flex items-center gap-1.5 px-3 py-1.5 rounded-sm text-xs font-sans whitespace-nowrap transition-all duration-200 border',
                selectedCategory === cat.id
                  ? 'bg-blue-500/10 border-blue-500/30 text-blue-400'
                  : 'border-zinc-800 text-zinc-500 hover:text-zinc-300'
              )}
            >
              {categoryIcons[cat.id]}
              {tt.tCategory(cat.id)}
            </button>
          ))}
        </div>
      </div>

      {/* Content */}
      <div className="flex-1 overflow-hidden flex">
        {/* Template Grid */}
        <div className="flex-1 overflow-y-auto p-4">
          {templates.length === 0 ? (
            <div className="flex flex-col items-center justify-center py-16">
              <div className="w-12 h-12 rounded-sm flex items-center justify-center bg-zinc-900 border border-zinc-800 mb-4">
                <Search className="w-5 h-5 text-zinc-700" />
              </div>
              <p className="text-sm text-zinc-500 mb-1">{t('story.templates.noTemplatesFound')}</p>
              {searchQuery && (
                <button
                  onClick={() => setSearchQuery('')}
                  className="text-xs text-zinc-600 hover:text-zinc-300 transition-colors mt-1"
                >
                  {t('story.templates.clearSearch')}
                </button>
              )}
            </div>
          ) : (
            <div className="grid grid-cols-1 sm:grid-cols-2 gap-3">
              <AnimatePresence mode="popLayout">
                {templates.map((template) => (
                  <TemplateCard
                    key={template.id}
                    template={template}
                    isSelected={selectedTemplate?.id === template.id}
                    onClick={() => setSelectedTemplate(template)}
                  />
                ))}
              </AnimatePresence>
            </div>
          )}
        </div>

        {/* Preview Panel */}
        <AnimatePresence>
          {selectedTemplate && (
            <motion.div
              initial={{ width: 0, opacity: 0 }}
              animate={{ width: 320, opacity: 1 }}
              exit={{ width: 0, opacity: 0 }}
              transition={{ duration: 0.2, ease: 'easeOut' }}
              className="border-l border-zinc-800 overflow-hidden flex-shrink-0"
            >
              <div className="w-80 h-full">
                <TemplatePreview
                  template={selectedTemplate}
                  onApply={handleApply}
                  onClose={() => setSelectedTemplate(null)}
                />
              </div>
            </motion.div>
          )}
        </AnimatePresence>
      </div>
    </div>
  );
}

export default TemplatesGallery;
````

## File: packages/frontend/components/story/VersionHistoryPanel.tsx
````typescript
/**
 * VersionHistoryPanel - Browse and restore project snapshots.
 */

import React, { useState, useEffect, useCallback } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import {
  History,
  Clock,
  Save,
  RotateCcw,
  Trash2,
  Tag,
  X,
  Check,
  Edit3,
} from 'lucide-react';
import { cn } from '@/lib/utils';
import type { StoryState } from '@/types';
import {
  getSnapshots,
  createSnapshot,
  deleteSnapshot,
  renameSnapshot,
  getHistoryStats,
  type VersionSnapshot,
  type VersionHistoryStats,
} from '@/services/versionHistoryService';
import { formatRelativeTime, formatAbsoluteTime } from '@/utils/timeFormatting';
import { Tooltip, TooltipContent, TooltipProvider, TooltipTrigger } from '@/components/ui/tooltip';

interface VersionHistoryPanelProps {
  projectId: string;
  currentState: StoryState;
  onRestore: (state: StoryState) => void;
  className?: string;
}

export function VersionHistoryPanel({
  projectId,
  currentState,
  onRestore,
  className,
}: VersionHistoryPanelProps) {
  const [snapshots, setSnapshots] = useState<VersionSnapshot[]>([]);
  const [stats, setStats] = useState<VersionHistoryStats | null>(null);
  const [loading, setLoading] = useState(true);
  const [selectedSnapshot, setSelectedSnapshot] = useState<VersionSnapshot | null>(null);
  const [showSaveDialog, setShowSaveDialog] = useState(false);
  const [saveName, setSaveName] = useState('');
  const [saveDescription, setSaveDescription] = useState('');
  const [filter, setFilter] = useState<'all' | 'manual' | 'auto'>('all');
  const [confirmRestore, setConfirmRestore] = useState<string | null>(null);
  const [editingId, setEditingId] = useState<string | null>(null);
  const [editName, setEditName] = useState('');

  const loadSnapshots = useCallback(async () => {
    setLoading(true);
    try {
      const type = filter === 'all' ? undefined : filter;
      const [snapshotList, historyStats] = await Promise.all([
        getSnapshots(projectId, { type, limit: 50 }),
        getHistoryStats(projectId),
      ]);
      setSnapshots(snapshotList);
      setStats(historyStats);
    } catch (error) {
      console.error('[VersionHistory] Failed to load snapshots:', error);
    } finally {
      setLoading(false);
    }
  }, [projectId, filter]);

  useEffect(() => {
    loadSnapshots();
  }, [loadSnapshots]);

  const handleSaveSnapshot = async () => {
    if (!saveName.trim()) return;
    try {
      await createSnapshot(projectId, currentState, saveName, saveDescription, 'manual');
      setShowSaveDialog(false);
      setSaveName('');
      setSaveDescription('');
      await loadSnapshots();
    } catch (error) {
      console.error('[VersionHistory] Failed to save snapshot:', error);
    }
  };

  const handleDelete = async (snapshotId: string) => {
    try {
      await deleteSnapshot(snapshotId);
      setSnapshots(prev => prev.filter(s => s.id !== snapshotId));
      if (selectedSnapshot?.id === snapshotId) {
        setSelectedSnapshot(null);
      }
    } catch (error) {
      console.error('[VersionHistory] Failed to delete snapshot:', error);
    }
  };

  const handleRestore = (snapshot: VersionSnapshot) => {
    onRestore(snapshot.state);
    setConfirmRestore(null);
    setSelectedSnapshot(null);
  };

  const handleRename = async (snapshotId: string) => {
    if (!editName.trim()) return;
    try {
      await renameSnapshot(snapshotId, editName);
      setSnapshots(prev =>
        prev.map(s => (s.id === snapshotId ? { ...s, name: editName } : s))
      );
      setEditingId(null);
      setEditName('');
    } catch (error) {
      console.error('[VersionHistory] Failed to rename snapshot:', error);
    }
  };

  const getTypeIcon = (type: VersionSnapshot['type']) => {
    switch (type) {
      case 'manual':
        return <Save className="w-3 h-3" />;
      case 'auto':
        return <Clock className="w-3 h-3" />;
      case 'checkpoint':
        return <Tag className="w-3 h-3" />;
    }
  };

  const getTypeColor = (type: VersionSnapshot['type']) => {
    switch (type) {
      case 'manual':
        return 'text-blue-400 bg-blue-500/20';
      case 'auto':
        return 'text-zinc-400 bg-zinc-500/20';
      case 'checkpoint':
        return 'text-orange-400 bg-orange-500/20';
    }
  };

  return (
    <div className={cn('flex flex-col h-full bg-zinc-950 rounded-sm border border-zinc-800', className)}>
      {/* Header */}
      <div className="flex items-center justify-between p-4 border-b border-zinc-800">
        <div className="flex items-center gap-2">
          <History className="w-5 h-5 text-blue-400" />
          <h3 className="font-sans font-medium text-zinc-100">Version History</h3>
        </div>
        <button
          onClick={() => setShowSaveDialog(true)}
          className="flex items-center gap-1.5 px-3 py-1.5 text-sm bg-blue-500 hover:bg-blue-600 text-white rounded-sm transition-colors duration-200"
        >
          <Save className="w-4 h-4" />
          Save Version
        </button>
      </div>

      {/* Stats */}
      {stats && (
        <div className="px-4 py-2 border-b border-zinc-800/50 font-mono text-[10px] text-zinc-600 flex items-center gap-4">
          <span>{stats.totalSnapshots} versions</span>
          <span>{stats.manualSnapshots} saved</span>
          <span>{stats.autoSnapshots} auto</span>
          {stats.totalSizeBytes > 0 && (
            <span>{(stats.totalSizeBytes / 1024 / 1024).toFixed(1)} MB</span>
          )}
        </div>
      )}

      {/* Filter Tabs */}
      <div className="flex items-center gap-1 p-2 border-b border-zinc-800/50">
        {(['all', 'manual', 'auto'] as const).map((f) => (
          <button
            key={f}
            onClick={() => setFilter(f)}
            className={cn(
              'px-3 py-1 text-xs rounded-sm transition-colors duration-200 capitalize',
              filter === f
                ? 'bg-blue-500 text-white'
                : 'text-zinc-500 hover:text-zinc-100 hover:bg-zinc-800'
            )}
          >
            {f}
          </button>
        ))}
      </div>

      {/* Snapshot List */}
      <div className="flex-1 overflow-y-auto p-2 space-y-1">
        {loading ? (
          <div className="flex items-center justify-center py-8">
            <div className="animate-spin w-6 h-6 border-2 border-blue-400 border-t-transparent rounded-full" />
          </div>
        ) : snapshots.length === 0 ? (
          <div className="text-center py-8 text-zinc-600">
            <History className="w-8 h-8 mx-auto mb-2 opacity-50" />
            <p className="text-sm">No versions saved yet</p>
            <p className="text-xs mt-1 text-zinc-700">Click "Save Version" to create a snapshot</p>
          </div>
        ) : (
          <AnimatePresence>
            {snapshots.map((snapshot) => (
              <motion.div
                key={snapshot.id}
                initial={{ opacity: 0, y: -10 }}
                animate={{ opacity: 1, y: 0 }}
                exit={{ opacity: 0, height: 0 }}
                transition={{ duration: 0.15 }}
                className={cn(
                  'group p-3 rounded-sm border transition-colors duration-200 cursor-pointer',
                  selectedSnapshot?.id === snapshot.id
                    ? 'border-blue-500/50 bg-blue-500/10'
                    : 'border-zinc-800 hover:border-zinc-700 bg-zinc-900'
                )}
                onClick={() => setSelectedSnapshot(snapshot)}
              >
                <div className="flex items-start justify-between gap-2">
                  <div className="flex-1 min-w-0">
                    {editingId === snapshot.id ? (
                      <div className="flex items-center gap-2">
                        <input
                          type="text"
                          value={editName}
                          onChange={(e) => setEditName(e.target.value)}
                          onKeyDown={(e) => {
                            if (e.key === 'Enter') handleRename(snapshot.id);
                            if (e.key === 'Escape') setEditingId(null);
                          }}
                          className="flex-1 px-2 py-1 text-sm bg-zinc-950 border border-zinc-700 rounded-sm text-zinc-100 focus:outline-none focus:border-blue-500"
                          autoFocus
                        />
                        <button
                          onClick={(e) => {
                            e.stopPropagation();
                            handleRename(snapshot.id);
                          }}
                          className="p-1 text-emerald-400 hover:bg-emerald-500/20 rounded-sm"
                        >
                          <Check className="w-4 h-4" />
                        </button>
                      </div>
                    ) : (
                      <div className="flex items-center gap-2">
                        <span className={cn('p-1 rounded-sm', getTypeColor(snapshot.type))}>
                          {getTypeIcon(snapshot.type)}
                        </span>
                        <span className="font-sans font-medium text-zinc-100 text-sm truncate">
                          {snapshot.name}
                        </span>
                        <button
                          onClick={(e) => {
                            e.stopPropagation();
                            setEditingId(snapshot.id);
                            setEditName(snapshot.name);
                          }}
                          className="p-1 text-zinc-700 hover:text-zinc-400 opacity-0 group-hover:opacity-100 transition-opacity duration-200"
                        >
                          <Edit3 className="w-3 h-3" />
                        </button>
                      </div>
                    )}
                    <TooltipProvider>
                      <Tooltip delayDuration={200}>
                        <TooltipTrigger asChild>
                          <div className="flex items-center gap-2 mt-1 text-xs text-zinc-600 cursor-help">
                            <Clock className="w-3 h-3" />
                            <span>{formatRelativeTime(snapshot.timestamp)}</span>
                          </div>
                        </TooltipTrigger>
                        <TooltipContent side="right" className="text-xs">
                          {formatAbsoluteTime(snapshot.timestamp)}
                        </TooltipContent>
                      </Tooltip>
                    </TooltipProvider>
                    {snapshot.metadata && (
                      <div className="flex items-center gap-2 mt-1 font-mono text-[10px] text-zinc-700">
                        <span>{snapshot.metadata.sceneCount} scenes</span>
                        <span>·</span>
                        <span>{snapshot.metadata.shotCount} shots</span>
                        <span>·</span>
                        <span>{snapshot.metadata.step}</span>
                      </div>
                    )}
                  </div>
                  <div className="flex items-center gap-1">
                    {confirmRestore === snapshot.id ? (
                      <div className="flex items-center gap-1 bg-orange-500/10 border border-orange-500/20 rounded-sm p-1">
                        <button
                          onClick={(e) => {
                            e.stopPropagation();
                            handleRestore(snapshot);
                          }}
                          className="p-1.5 text-emerald-400 hover:bg-emerald-500/20 rounded-sm"
                          title="Confirm restore"
                        >
                          <Check className="w-4 h-4" />
                        </button>
                        <button
                          onClick={(e) => {
                            e.stopPropagation();
                            setConfirmRestore(null);
                          }}
                          className="p-1.5 text-red-400 hover:bg-red-500/20 rounded-sm"
                          title="Cancel"
                        >
                          <X className="w-4 h-4" />
                        </button>
                      </div>
                    ) : (
                      <>
                        <button
                          onClick={(e) => {
                            e.stopPropagation();
                            setConfirmRestore(snapshot.id);
                          }}
                          className="p-1.5 text-zinc-700 hover:text-blue-400 hover:bg-blue-500/20 rounded-sm transition-colors duration-200"
                          title="Restore this version"
                        >
                          <RotateCcw className="w-4 h-4" />
                        </button>
                        <button
                          onClick={(e) => {
                            e.stopPropagation();
                            handleDelete(snapshot.id);
                          }}
                          className="p-1.5 text-zinc-700 hover:text-red-400 hover:bg-red-500/20 rounded-sm transition-colors duration-200"
                          title="Delete"
                        >
                          <Trash2 className="w-4 h-4" />
                        </button>
                      </>
                    )}
                  </div>
                </div>
              </motion.div>
            ))}
          </AnimatePresence>
        )}
      </div>

      {/* Save Dialog */}
      <AnimatePresence>
        {showSaveDialog && (
          <motion.div
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            exit={{ opacity: 0 }}
            transition={{ duration: 0.15 }}
            className="absolute inset-0 bg-black/80 backdrop-blur-sm flex items-center justify-center p-4 z-50"
            onClick={() => setShowSaveDialog(false)}
          >
            <motion.div
              initial={{ scale: 0.97, opacity: 0 }}
              animate={{ scale: 1, opacity: 1 }}
              exit={{ scale: 0.97, opacity: 0 }}
              transition={{ duration: 0.15 }}
              className="w-full max-w-md bg-zinc-900 rounded-sm border border-zinc-800 p-6"
              onClick={(e) => e.stopPropagation()}
            >
              <h4 className="font-sans text-lg font-medium text-zinc-100 mb-4">Save Version</h4>

              <div className="space-y-4">
                <div>
                  <label className="block text-sm text-zinc-500 mb-1">Version Name</label>
                  <input
                    type="text"
                    value={saveName}
                    onChange={(e) => setSaveName(e.target.value)}
                    placeholder="e.g., Before major changes"
                    className="w-full px-3 py-2 bg-zinc-950 border border-zinc-800 rounded-sm text-zinc-100 placeholder:text-zinc-700 focus:outline-none focus:border-blue-500"
                    autoFocus
                  />
                </div>
                <div>
                  <label className="block text-sm text-zinc-500 mb-1">Description (optional)</label>
                  <textarea
                    value={saveDescription}
                    onChange={(e) => setSaveDescription(e.target.value)}
                    placeholder="Notes about this version..."
                    className="w-full px-3 py-2 bg-zinc-950 border border-zinc-800 rounded-sm text-zinc-100 placeholder:text-zinc-700 resize-none h-20 focus:outline-none focus:border-blue-500"
                  />
                </div>
              </div>

              <div className="flex items-center justify-end gap-3 mt-6">
                <button
                  onClick={() => setShowSaveDialog(false)}
                  className="px-4 py-2 text-zinc-500 hover:text-zinc-100 transition-colors duration-200"
                >
                  Cancel
                </button>
                <button
                  onClick={handleSaveSnapshot}
                  disabled={!saveName.trim()}
                  className="px-4 py-2 bg-blue-500 hover:bg-blue-600 disabled:opacity-50 disabled:cursor-not-allowed text-white rounded-sm transition-colors duration-200"
                >
                  Save Version
                </button>
              </div>
            </motion.div>
          </motion.div>
        )}
      </AnimatePresence>
    </div>
  );
}

export default VersionHistoryPanel;
````

## File: packages/frontend/components/TimelineEditor/AudioClip.tsx
````typescript
/**
 * AudioClip Component
 * 
 * Renders an audio clip in the Graphite Timeline with:
 * - Waveform visualization with vertical bars
 * - Cyan coloring consistent with plasma theme
 * - Clip title and duration display
 * 
 * Requirements: 5.5, 6.1, 6.2, 6.3
 */

import { useMemo } from "react";
import { TimelineClip } from "./graphite-timeline-utils";
import "./graphite-timeline.css";

// --- Types ---

export interface AudioClipProps {
  /** Clip data containing id, name, duration, etc. */
  clip: TimelineClip;
  /** Current zoom level (pixels per second) */
  zoom: number;
  /** Whether this clip is currently selected */
  isSelected: boolean;
  /** Callback when clip is clicked */
  onClick: () => void;
  /** Optional left position override (otherwise calculated from startTime * zoom) */
  left?: number;
  /** Number of waveform bars to generate (default: calculated from width) */
  waveformBars?: number;
  /** Optional amplitude data for waveform (values 0-1) */
  amplitudeData?: number[];
}

// --- Helper Functions ---

/**
 * Formats duration in seconds to a display string (e.g., "4.2s")
 */
function formatDuration(seconds: number): string {
  return `${seconds.toFixed(1)}s`;
}

/**
 * Generates pseudo-random waveform bar heights.
 * Uses a seeded approach based on clip ID for consistency.
 * 
 * @param count - Number of bars to generate
 * @param seed - Seed string for consistent randomization
 * @returns Array of heights (5-35 pixels)
 */
function generateWaveformHeights(count: number, seed: string): number[] {
  const heights: number[] = [];
  let hash = 0;
  
  // Simple hash from seed string
  for (let i = 0; i < seed.length; i++) {
    hash = ((hash << 5) - hash) + seed.charCodeAt(i);
    hash = hash & hash; // Convert to 32-bit integer
  }
  
  for (let i = 0; i < count; i++) {
    // Pseudo-random based on hash and index
    const pseudoRandom = Math.abs(Math.sin(hash + i * 0.1)) * 0.8 + 0.2;
    // Height between 5 and 35 pixels
    heights.push(5 + pseudoRandom * 30);
  }
  
  return heights;
}

/**
 * Calculates the number of waveform bars based on clip width.
 * Aims for approximately 1 bar every 4-5 pixels.
 */
function calculateBarCount(width: number): number {
  return Math.max(10, Math.floor(width / 5));
}

// --- Component ---

export function AudioClip({
  clip,
  zoom,
  isSelected,
  onClick,
  left,
  waveformBars,
  amplitudeData,
}: AudioClipProps) {
  // Calculate width based on duration and zoom level
  const width = clip.duration * zoom;
  
  // Calculate left position from startTime if not provided
  const leftPosition = left ?? clip.startTime * zoom;

  // Calculate number of bars
  const barCount = waveformBars ?? calculateBarCount(width);

  // Generate or use provided waveform heights
  const waveformHeights = useMemo(() => {
    if (amplitudeData && amplitudeData.length > 0) {
      // Use provided amplitude data, scale to pixel heights
      return amplitudeData.map(amp => 5 + amp * 30);
    }
    // Generate pseudo-random heights based on clip ID
    return generateWaveformHeights(barCount, clip.id);
  }, [amplitudeData, barCount, clip.id]);

  // Build class names for hover/selected states
  const classNames = [
    "graphite-clip",
    "graphite-audio-clip",
    isSelected ? "selected" : "",
  ].filter(Boolean).join(" ");

  return (
    <div
      className={classNames}
      style={{
        left: `${leftPosition}px`,
        width: `${width}px`,
      }}
      onClick={(e) => {
        e.stopPropagation();
        onClick();
      }}
      role="button"
      tabIndex={0}
      aria-label={`Audio clip: ${clip.name}`}
      aria-selected={isSelected}
      onKeyDown={(e) => {
        if (e.key === "Enter" || e.key === " ") {
          e.preventDefault();
          onClick();
        }
      }}
    >
      {/* Waveform Visualization - Requirements 6.1, 6.2, 6.3 */}
      <div className="graphite-audio-wave" aria-hidden="true">
        {waveformHeights.map((height, index) => (
          <div
            key={index}
            className="graphite-wave-bar"
            style={{ height: `${height}px` }}
          />
        ))}
      </div>

      {/* Clip Info - Requirement 5.5 */}
      <div className="graphite-clip-info">
        <span className="graphite-clip-title" title={clip.name}>
          {clip.name}
        </span>
        <span className="graphite-clip-duration">
          {formatDuration(clip.duration)}
        </span>
      </div>
    </div>
  );
}

export default AudioClip;
````

## File: packages/frontend/components/TimelineEditor/AudioTimelineEditor.tsx
````typescript
/**
 * AudioTimelineEditor Component
 *
 * Main wrapper component that provides a modernized timeline UI while maintaining
 * backward compatibility with the existing GraphiteTimeline props interface.
 *
 * This component:
 * 1. Accepts all GraphiteTimelineProps for backward compatibility
 * 2. Uses useTimelineAdapter hook to convert data to internal format
 * 3. Composes sub-components (TrackSidebar, VideoPreview, TimelinePanel, TimelineControls)
 * 4. Manages internal state (zoom, volume, import modal)
 * 5. Wires up callbacks to convert clip IDs back to scene IDs
 *
 * @see .kiro/specs/timeline-editor-replacement/design.md for architecture details
 * @requirements 10.1, 10.3, 10.4, 10.5
 */

"use client";

import { useState, useCallback, useRef, useEffect } from "react";
import { cn } from "@/lib/utils";
import type { Scene, NarrationSegment, VideoSFXPlan } from "@/types";
import type { MediaFile, SubtitleCue, VideoClip, AudioClip } from "@/types/audio-editor";
import { useTimelineAdapter } from "@/hooks/useTimelineAdapter";
import { useTimelineKeyboard } from "@/hooks/useTimelineKeyboard";
import {
  TrackSidebar,
  VideoPreview,
  TimelinePanel,
  TimelineControls,
  ImportMediaModal,
} from "./editor";

// --- Accessibility Helper Functions ---

/**
 * Format duration in seconds to a human-readable string for screen reader announcements.
 * Outputs format like "5 seconds", "1 minute 30 seconds", etc.
 *
 * @param seconds - Duration in seconds
 * @returns Human-readable duration string
 *
 * @requirements 8.3
 * @validates Property 15: Aria Announcement Content
 */
export function formatDurationForAnnouncement(seconds: number): string {
  if (seconds < 0 || !Number.isFinite(seconds)) {
    return "0 seconds";
  }

  const totalSeconds = Math.floor(seconds);
  const minutes = Math.floor(totalSeconds / 60);
  const remainingSeconds = totalSeconds % 60;

  if (minutes === 0) {
    return remainingSeconds === 1 ? "1 second" : `${remainingSeconds} seconds`;
  }

  const minuteStr = minutes === 1 ? "1 minute" : `${minutes} minutes`;
  
  if (remainingSeconds === 0) {
    return minuteStr;
  }

  const secondStr = remainingSeconds === 1 ? "1 second" : `${remainingSeconds} seconds`;
  return `${minuteStr} ${secondStr}`;
}

/**
 * Update the aria-live region with an announcement for screen readers.
 * The announcement will be read aloud by screen readers due to aria-live="assertive".
 *
 * @param message - The message to announce
 * @param regionId - The ID of the aria-live region element (default: "timeline-announcements")
 *
 * @requirements 8.3, 8.5
 */
export function announceToScreenReader(
  message: string,
  regionId: string = "timeline-announcements"
): void {
  const region = document.getElementById(regionId);
  if (region) {
    // Clear and set to trigger announcement even if same message
    region.textContent = "";
    // Use requestAnimationFrame to ensure the clear is processed first
    requestAnimationFrame(() => {
      region.textContent = message;
    });
  }
}

/**
 * Find a clip by ID from the combined video and audio clips arrays.
 *
 * @param clipId - The clip ID to find
 * @param videoClips - Array of video clips
 * @param audioClips - Array of audio clips
 * @returns The found clip with its name and duration, or null if not found
 */
function findClipById(
  clipId: string,
  videoClips: VideoClip[],
  audioClips: AudioClip[]
): { name: string; duration: number } | null {
  // Check video clips first
  const videoClip = videoClips.find((clip) => clip.id === clipId);
  if (videoClip) {
    return { name: videoClip.name, duration: videoClip.duration };
  }

  // Check audio clips
  const audioClip = audioClips.find((clip) => clip.id === clipId);
  if (audioClip) {
    // Audio clips don't have a name property, so we generate one from the ID
    const clipName = audioClip.id.startsWith("audio-")
      ? `Audio clip ${audioClip.id.replace("audio-", "")}`
      : `Clip ${audioClip.id}`;
    return { name: clipName, duration: audioClip.duration };
  }

  return null;
}

// --- Types ---

/**
 * Props interface for AudioTimelineEditor.
 * Maintains backward compatibility with GraphiteTimelineProps while adding
 * new optional props for extended functionality.
 *
 * @requirements 10.1, 10.5
 */
export interface AudioTimelineEditorProps {
  // Existing GraphiteTimeline props (backward compatible)
  /** Array of Scene objects from content plan */
  scenes: Scene[];
  /** Map of scene IDs to thumbnail URLs */
  visuals?: Record<string, string>;
  /** Array of NarrationSegment objects */
  narrationSegments?: NarrationSegment[];
  /** Current playback time in seconds */
  currentTime: number;
  /** Total duration in seconds */
  duration: number;
  /** Whether playback is currently active */
  isPlaying: boolean;
  /** Callback when play/pause is toggled */
  onPlayPause: () => void;
  /** Callback when user seeks to a new position */
  onSeek: (time: number) => void;
  /** Callback when a scene is selected */
  onSceneSelect?: (sceneId: string) => void;
  /** Currently selected scene ID */
  selectedSceneId?: string | null;
  /** Project name to display */
  projectName?: string;
  /** Optional additional CSS class */
  className?: string;
  /** SFX plan for ambient tracks */
  sfxPlan?: VideoSFXPlan | null;
  /** Callback when a clip should be deleted */
  onDeleteClip?: (clipId: string) => void;
  /** Optional data-testid for E2E testing */
  "data-testid"?: string;

  // New optional props for extended functionality
  /** Callback when a video file is imported */
  onImportVideo?: (file: MediaFile) => void;
  /** Callback when an image file is imported */
  onImportImage?: (file: MediaFile) => void;
  /** Callback when subtitles are imported */
  onImportSubtitles?: (cues: SubtitleCue[]) => void;
}

// --- Constants ---

const DEFAULT_ZOOM = 50;
const DEFAULT_VOLUME = 80;

// --- Component ---

/**
 * AudioTimelineEditor - Main timeline editor component.
 *
 * Provides a modernized timeline UI with video preview, track sidebar,
 * and media import capabilities while maintaining backward compatibility
 * with the existing application state and callbacks.
 *
 * @example
 * ```tsx
 * <AudioTimelineEditor
 *   scenes={contentPlan.scenes}
 *   visuals={generatedVisuals}
 *   narrationSegments={narrationSegments}
 *   currentTime={currentTime}
 *   duration={totalDuration}
 *   isPlaying={isPlaying}
 *   onPlayPause={handlePlayPause}
 *   onSeek={handleSeek}
 *   onSceneSelect={handleSceneSelect}
 *   selectedSceneId={selectedSceneId}
 *   sfxPlan={sfxPlan}
 * />
 * ```
 *
 * @requirements 10.1, 10.3, 10.4, 10.5
 */
export function AudioTimelineEditor({
  scenes,
  visuals = {},
  narrationSegments = [],
  currentTime,
  duration: _duration,
  isPlaying,
  onPlayPause,
  onSeek,
  onSceneSelect,
  selectedSceneId = null,
  projectName = "Untitled Project",
  className,
  sfxPlan = null,
  onDeleteClip,
  onImportVideo,
  onImportImage,
  onImportSubtitles,
  "data-testid": dataTestId,
}: AudioTimelineEditorProps) {
  // --- Internal State ---
  const [zoom, setZoom] = useState(DEFAULT_ZOOM);
  const [volume, setVolume] = useState(DEFAULT_VOLUME);
  const [isImportModalOpen, setIsImportModalOpen] = useState(false);
  const [selectedTrackId, setSelectedTrackId] = useState<string | null>(null);
  const [importedVideo, setImportedVideo] = useState<MediaFile | null>(null);
  const [importedSubtitles, setImportedSubtitles] = useState<SubtitleCue[]>([]);
  const [isFocused, setIsFocused] = useState(false);
  const [selectedClipIndex, setSelectedClipIndex] = useState<number | null>(null);

  // --- Refs ---
  const containerRef = useRef<HTMLDivElement>(null);
  const isInitialRenderRef = useRef(true);

  // --- Use Timeline Adapter ---
  // Convert external props to internal data model
  const {
    tracks,
    audioClips,
    videoClips,
    imageClips,
    subtitles: adapterSubtitles,
    selectedClipId,
    handleClipSelect,
    handleDeleteClip: _handleDeleteClip,
  } = useTimelineAdapter({
    scenes,
    visuals,
    narrationSegments,
    sfxPlan,
    selectedSceneId,
    onSceneSelect,
    onDeleteClip,
  });

  // Combine adapter subtitles with imported subtitles
  const allSubtitles = [...adapterSubtitles, ...importedSubtitles];

  // Calculate total clip count for keyboard navigation
  const allClips = [...videoClips, ...audioClips];
  const clipCount = allClips.length;

  // Calculate duration from scenes if not provided
  const calculatedDuration = scenes.reduce((sum, s) => sum + s.duration, 0) || _duration;

  // --- Screen Reader Announcements ---
  // Requirements: 8.3, 8.5
  // Property 15: Aria Announcement Content

  /**
   * Announce clip selection changes to screen readers.
   * When a clip is selected, announces the clip name and duration.
   *
   * @requirements 8.3
   * @validates Property 15: Aria Announcement Content
   */
  useEffect(() => {
    if (selectedClipId) {
      const clipInfo = findClipById(selectedClipId, videoClips, audioClips);
      if (clipInfo) {
        const durationStr = formatDurationForAnnouncement(clipInfo.duration);
        const announcement = `Selected: ${clipInfo.name}, duration ${durationStr}`;
        announceToScreenReader(announcement);
      }
    }
  }, [selectedClipId, videoClips, audioClips]);

  /**
   * Announce playback state changes to screen readers.
   * Announces "Playing" when playback starts and "Paused" when playback stops.
   * Skips the initial render to avoid announcing on component mount.
   *
   * @requirements 8.5
   */
  useEffect(() => {
    // Skip announcement on initial render
    if (isInitialRenderRef.current) {
      isInitialRenderRef.current = false;
      return;
    }
    
    const announcement = isPlaying ? "Playing" : "Paused";
    announceToScreenReader(announcement);
  }, [isPlaying]);

  // --- Keyboard Navigation ---
  // Integrate useTimelineKeyboard hook for keyboard shortcuts
  // Requirements: 7.1-7.7

  const handleNextClip = useCallback(() => {
    if (clipCount === 0) return;
    setSelectedClipIndex((prev) => {
      const next = prev === null ? 0 : (prev + 1) % clipCount;
      // Also update the clip selection
      const clip = allClips[next];
      if (clip) {
        handleClipSelect(clip.id);
      }
      return next;
    });
  }, [clipCount, allClips, handleClipSelect]);

  const handlePrevClip = useCallback(() => {
    if (clipCount === 0) return;
    setSelectedClipIndex((prev) => {
      const next = prev === null ? clipCount - 1 : (prev - 1 + clipCount) % clipCount;
      // Also update the clip selection
      const clip = allClips[next];
      if (clip) {
        handleClipSelect(clip.id);
      }
      return next;
    });
  }, [clipCount, allClips, handleClipSelect]);

  const handleJumpToStart = useCallback(() => {
    onSeek(0);
  }, [onSeek]);

  const handleJumpToEnd = useCallback(() => {
    onSeek(calculatedDuration);
  }, [onSeek, calculatedDuration]);

  const handleKeyboardDeleteClip = useCallback(
    (index: number) => {
      const clip = allClips[index];
      if (clip && onDeleteClip) {
        onDeleteClip(clip.id);
      }
    },
    [allClips, onDeleteClip]
  );

  const handleKeyboardSelectClip = useCallback(
    (index: number | null) => {
      setSelectedClipIndex(index);
      if (index !== null) {
        const clip = allClips[index];
        if (clip) {
          handleClipSelect(clip.id);
        }
      } else {
        handleClipSelect(null);
      }
    },
    [allClips, handleClipSelect]
  );

  // Use the keyboard navigation hook
  useTimelineKeyboard({
    isActive: isFocused && !isImportModalOpen,
    duration: calculatedDuration,
    currentTime,
    isPlaying,
    selectedClipIndex,
    clipCount,
    onTimeChange: onSeek,
    onPlayPause,
    onSelectClip: handleKeyboardSelectClip,
    onDeleteClip: handleKeyboardDeleteClip,
    onNextClip: handleNextClip,
    onPrevClip: handlePrevClip,
    onJumpToStart: handleJumpToStart,
    onJumpToEnd: handleJumpToEnd,
  });

  // --- Handlers ---

  /**
   * Handle zoom level change.
   * Clamps value to valid range [10, 100].
   */
  const handleZoomChange = useCallback((value: number) => {
    setZoom(Math.max(10, Math.min(100, value)));
  }, []);

  /**
   * Handle volume change.
   * Clamps value to valid range [0, 100].
   */
  const handleVolumeChange = useCallback((value: number) => {
    setVolume(Math.max(0, Math.min(100, value)));
  }, []);

  /**
   * Handle track selection in sidebar.
   */
  const handleSelectTrack = useCallback((trackId: string) => {
    setSelectedTrackId(trackId);
  }, []);

  /**
   * Handle text update for a track.
   * Currently a no-op as tracks are derived from scenes.
   * Future: Could update scene narration scripts.
   */
  const handleUpdateTrackText = useCallback(
    (trackId: string, text: string) => {
      // TODO: Implement track text editing
      // This would need to update the scene's narrationScript
      console.log("[AudioTimelineEditor] Track text update:", trackId, text);
    },
    []
  );

  /**
   * Handle generate audio request for a track.
   * Currently a no-op - would trigger narration generation.
   */
  const handleGenerateAudio = useCallback((trackId: string) => {
    // TODO: Implement audio generation trigger
    console.log("[AudioTimelineEditor] Generate audio for track:", trackId);
  }, []);

  /**
   * Handle adding a new voiceover track.
   */
  const handleAddVoiceoverTrack = useCallback(() => {
    // TODO: Implement adding new voiceover track
    console.log("[AudioTimelineEditor] Add voiceover track");
  }, []);

  /**
   * Handle adding a new SFX track.
   */
  const handleAddSfxTrack = useCallback(() => {
    // TODO: Implement adding new SFX track
    console.log("[AudioTimelineEditor] Add SFX track");
  }, []);

  /**
   * Handle adding a new subtitle track.
   */
  const handleAddSubtitleTrack = useCallback(() => {
    // TODO: Implement adding new subtitle track
    console.log("[AudioTimelineEditor] Add subtitle track");
  }, []);

  /**
   * Handle opening the import modal.
   */
  const handleOpenImportModal = useCallback(() => {
    setIsImportModalOpen(true);
  }, []);

  /**
   * Handle video import.
   * Stores the video locally and calls external callback if provided.
   */
  const handleImportVideo = useCallback(
    (file: MediaFile) => {
      setImportedVideo(file);
      onImportVideo?.(file);
    },
    [onImportVideo]
  );

  /**
   * Handle image import.
   * Calls external callback if provided.
   */
  const handleImportImage = useCallback(
    (file: MediaFile) => {
      onImportImage?.(file);
    },
    [onImportImage]
  );

  /**
   * Handle subtitle import.
   * Stores subtitles locally and calls external callback if provided.
   */
  const handleImportSubtitles = useCallback(
    (cues: SubtitleCue[]) => {
      setImportedSubtitles((prev) => [...prev, ...cues]);
      onImportSubtitles?.(cues);
    },
    [onImportSubtitles]
  );

  /**
   * Handle time update from video preview.
   * Syncs video playback with timeline.
   */
  const handleVideoTimeUpdate = useCallback(
    (time: number) => {
      // Only update if significantly different to avoid feedback loops
      if (Math.abs(time - currentTime) > 0.1) {
        onSeek(time);
      }
    },
    [currentTime, onSeek]
  );

  // --- Render ---

  // Unique ID for keyboard instructions (for aria-describedby)
  const keyboardInstructionsId = "timeline-keyboard-instructions";

  return (
    <div
      ref={containerRef}
      className={cn(
        "flex h-full flex-col overflow-hidden bg-background",
        className
      )}
      role="application"
      aria-label={`Audio Timeline Editor for ${projectName}`}
      aria-describedby={keyboardInstructionsId}
      tabIndex={0}
      data-testid={dataTestId}
      onFocus={() => setIsFocused(true)}
      onBlur={(e) => {
        // Only blur if focus is leaving the container entirely
        if (!containerRef.current?.contains(e.relatedTarget as Node)) {
          setIsFocused(false);
        }
      }}
    >
      {/* Visually hidden keyboard instructions for screen readers */}
      {/* Requirements: 8.2, 8.6 */}
      <div
        id={keyboardInstructionsId}
        className="sr-only"
        aria-live="polite"
      >
        Keyboard shortcuts: Space or K to play/pause, Left arrow to seek back 1 second, 
        Right arrow to seek forward 1 second, Home to jump to start, End to jump to end, 
        Tab to navigate between clips, Delete to remove selected clip.
      </div>

      {/* Aria-live region for dynamic announcements (selection, playback state) */}
      {/* Requirements: 8.3, 8.5 */}
      <div
        id="timeline-announcements"
        className="sr-only"
        aria-live="assertive"
        aria-atomic="true"
      />

      {/* Top Section: Sidebar + Video Preview */}
      <div className="flex flex-1 min-h-0">
        {/* Track Sidebar - Left Panel */}
        <TrackSidebar
          tracks={tracks}
          selectedTrackId={selectedTrackId}
          onSelectTrack={handleSelectTrack}
          onUpdateText={handleUpdateTrackText}
          onGenerateAudio={handleGenerateAudio}
        />

        {/* Video Preview - Center Panel */}
        <VideoPreview
          video={importedVideo}
          currentTime={currentTime}
          isPlaying={isPlaying}
          subtitles={allSubtitles}
          onTimeUpdate={handleVideoTimeUpdate}
          onPlayPause={onPlayPause}
        />
      </div>

      {/* Bottom Section: Timeline Controls + Timeline Panel */}
      <div className="flex flex-col border-t border-border">
        {/* Timeline Controls */}
        <TimelineControls
          isPlaying={isPlaying}
          onPlayPause={onPlayPause}
          zoom={zoom}
          onZoomChange={handleZoomChange}
          volume={volume}
          onVolumeChange={handleVolumeChange}
          selectedClipId={selectedClipId}
          onAddVoiceoverTrack={handleAddVoiceoverTrack}
          onAddSfxTrack={handleAddSfxTrack}
          onAddSubtitleTrack={handleAddSubtitleTrack}
          onOpenImportModal={handleOpenImportModal}
        />

        {/* Timeline Panel */}
        <TimelinePanel
          tracks={tracks}
          clips={audioClips}
          subtitles={allSubtitles}
          videoClips={videoClips}
          imageClips={imageClips}
          currentTime={currentTime}
          zoom={zoom}
          selectedClipId={selectedClipId}
          onSelectClip={handleClipSelect}
          onSeek={onSeek}
        />
      </div>

      {/* Import Media Modal */}
      <ImportMediaModal
        open={isImportModalOpen}
        onOpenChange={setIsImportModalOpen}
        onImportVideo={handleImportVideo}
        onImportImage={handleImportImage}
        onImportSubtitles={handleImportSubtitles}
      />
    </div>
  );
}

export default AudioTimelineEditor;
````

## File: packages/frontend/components/TimelineEditor/editor/ImageClipComponent.tsx
````typescript
"use client";

import { cn } from "@/lib/utils";
import { ImageIcon } from "lucide-react";
import type { ImageClip } from "@/types/audio-editor";

interface ImageClipProps {
  clip: ImageClip;
  pixelsPerSecond: number;
  isSelected: boolean;
  onSelect: () => void;
}

/**
 * Format duration in seconds to a human-readable string.
 */
function formatDuration(seconds: number): string {
  const mins = Math.floor(seconds / 60);
  const secs = Math.floor(seconds % 60);
  if (mins > 0) {
    return `${mins} minute${mins !== 1 ? "s" : ""} ${secs} second${secs !== 1 ? "s" : ""}`;
  }
  return `${secs} second${secs !== 1 ? "s" : ""}`;
}

export function ImageClipComponent({
  clip,
  pixelsPerSecond,
  isSelected,
  onSelect,
}: ImageClipProps) {
  const width = clip.duration * pixelsPerSecond;
  const left = clip.startTime * pixelsPerSecond;

  return (
    <div
      className={cn(
        "absolute top-1 bottom-1 cursor-pointer rounded-md overflow-hidden transition-all",
        "bg-amber-500/20 border-2",
        isSelected
          ? "border-amber-500 ring-2 ring-amber-500/30"
          : "border-amber-500/50 hover:border-amber-500/70"
      )}
      style={{ left, width, minWidth: 40 }}
      onClick={(e) => {
        e.stopPropagation();
        onSelect();
      }}
      role="button"
      tabIndex={0}
      aria-label={`Image clip: ${clip.name}, duration ${formatDuration(clip.duration)}`}
      aria-selected={isSelected}
      onKeyDown={(e) => {
        if (e.key === "Enter" || e.key === " ") {
          e.preventDefault();
          onSelect();
        }
      }}
    >
      {/* Image thumbnail */}
      <div
        className="h-full w-full bg-cover bg-center"
        style={{
          backgroundImage: `url(${clip.imageUrl || "/generic-image-thumbnail.png"})`,
        }}
      />

      {/* Icon badge */}
      <div className="absolute top-1 left-1 rounded bg-amber-500 p-0.5">
        <ImageIcon className="h-2.5 w-2.5 text-white" />
      </div>

      {/* Clip name overlay */}
      <div className="absolute inset-x-0 bottom-0 bg-gradient-to-t from-black/60 to-transparent px-1.5 py-0.5">
        <span className="text-[9px] font-medium text-white truncate block">
          {clip.name}
        </span>
      </div>

      {/* Resize handles when selected */}
      {isSelected && (
        <>
          <div className="absolute left-0 top-0 bottom-0 w-1.5 cursor-ew-resize bg-amber-500 hover:bg-amber-400" />
          <div className="absolute right-0 top-0 bottom-0 w-1.5 cursor-ew-resize bg-amber-500 hover:bg-amber-400" />
        </>
      )}
    </div>
  );
}
````

## File: packages/frontend/components/TimelineEditor/editor/ImportMediaModal.tsx
````typescript
"use client";

import type React from "react";

import { useState, useRef, useCallback } from "react";
import {
  Dialog,
  DialogContent,
  DialogHeader,
  DialogTitle,
  DialogDescription,
} from "@/components/ui/dialog";
import { Button } from "@/components/ui/button";
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs";
import { Input } from "@/components/ui/input";
import { Label } from "@/components/ui/label";
import { Video, ImageIcon, FileText, Upload, Link, X } from "lucide-react";
import type { MediaFile, SubtitleCue } from "@/types/audio-editor";

interface ImportMediaModalProps {
  open: boolean;
  onOpenChange: (open: boolean) => void;
  onImportVideo: (file: MediaFile) => void;
  onImportImage: (file: MediaFile) => void;
  onImportSubtitles: (cues: SubtitleCue[]) => void;
}

export function ImportMediaModal({
  open,
  onOpenChange,
  onImportVideo,
  onImportImage,
  onImportSubtitles,
}: ImportMediaModalProps) {
  const [activeTab, setActiveTab] = useState("video");
  const [urlInput, setUrlInput] = useState("");
  const [dragOver, setDragOver] = useState(false);
  const fileInputRef = useRef<HTMLInputElement>(null);

  const handleFileSelect = useCallback(
    (e: React.ChangeEvent<HTMLInputElement>) => {
      const file = e.target.files?.[0];
      if (!file) return;

      const url = URL.createObjectURL(file);
      const mediaFile: MediaFile = {
        id: `media-${Date.now()}`,
        type: activeTab as "video" | "image" | "subtitle",
        name: file.name,
        url,
      };

      if (activeTab === "video") {
        onImportVideo(mediaFile);
      } else if (activeTab === "image") {
        onImportImage(mediaFile);
      } else if (activeTab === "subtitle") {
        // Parse subtitle file (simplified SRT parsing)
        const reader = new FileReader();
        reader.onload = (event) => {
          const content = event.target?.result as string;
          const cues = parseSubtitles(content);
          onImportSubtitles(cues);
        };
        reader.readAsText(file);
      }

      onOpenChange(false);
    },
    [activeTab, onImportVideo, onImportImage, onImportSubtitles, onOpenChange]
  );

  const handleUrlImport = useCallback(() => {
    if (!urlInput.trim()) return;

    const mediaFile: MediaFile = {
      id: `media-${Date.now()}`,
      type: activeTab as "video" | "image" | "subtitle",
      name: urlInput.split("/").pop() || "Imported Media",
      url: urlInput,
    };

    if (activeTab === "video") {
      onImportVideo(mediaFile);
    } else if (activeTab === "image") {
      onImportImage(mediaFile);
    }

    setUrlInput("");
    onOpenChange(false);
  }, [activeTab, urlInput, onImportVideo, onImportImage, onOpenChange]);

  const handleDrop = useCallback(
    (e: React.DragEvent) => {
      e.preventDefault();
      setDragOver(false);

      const file = e.dataTransfer.files[0];
      if (!file) return;

      const url = URL.createObjectURL(file);
      const mediaFile: MediaFile = {
        id: `media-${Date.now()}`,
        type: activeTab as "video" | "image" | "subtitle",
        name: file.name,
        url,
      };

      if (activeTab === "video" && file.type.startsWith("video/")) {
        onImportVideo(mediaFile);
        onOpenChange(false);
      } else if (activeTab === "image" && file.type.startsWith("image/")) {
        onImportImage(mediaFile);
        onOpenChange(false);
      } else if (activeTab === "subtitle") {
        const reader = new FileReader();
        reader.onload = (event) => {
          const content = event.target?.result as string;
          const cues = parseSubtitles(content);
          onImportSubtitles(cues);
          onOpenChange(false);
        };
        reader.readAsText(file);
      }
    },
    [activeTab, onImportVideo, onImportImage, onImportSubtitles, onOpenChange]
  );

  const getAcceptTypes = () => {
    switch (activeTab) {
      case "video":
        return "video/*";
      case "image":
        return "image/*";
      case "subtitle":
        return ".srt,.vtt,.txt";
      default:
        return "*/*";
    }
  };

  const getTabIcon = (tab: string) => {
    switch (tab) {
      case "video":
        return <Video className="h-4 w-4" />;
      case "image":
        return <ImageIcon className="h-4 w-4" />;
      case "subtitle":
        return <FileText className="h-4 w-4" />;
      default:
        return null;
    }
  };

  return (
    <Dialog open={open} onOpenChange={onOpenChange}>
      <DialogContent className="sm:max-w-lg">
        <DialogHeader>
          <DialogTitle>Import Media</DialogTitle>
          <DialogDescription>
            Upload or link video files, images, or subtitle files to your
            project.
          </DialogDescription>
        </DialogHeader>

        <Tabs value={activeTab} onValueChange={setActiveTab} className="mt-4">
          <TabsList className="grid w-full grid-cols-3">
            <TabsTrigger value="video" className="gap-2">
              {getTabIcon("video")}
              Video
            </TabsTrigger>
            <TabsTrigger value="image" className="gap-2">
              {getTabIcon("image")}
              Image
            </TabsTrigger>
            <TabsTrigger value="subtitle" className="gap-2">
              {getTabIcon("subtitle")}
              Subtitles
            </TabsTrigger>
          </TabsList>

          <TabsContent value={activeTab} className="mt-4 space-y-4">
            {/* Drag and drop zone */}
            <div
              className={`relative flex min-h-[160px] cursor-pointer flex-col items-center justify-center rounded-lg border-2 border-dashed transition-colors ${
                dragOver
                  ? "border-primary bg-primary/5"
                  : "border-muted-foreground/25 hover:border-muted-foreground/50"
              }`}
              onDragOver={(e) => {
                e.preventDefault();
                setDragOver(true);
              }}
              onDragLeave={() => setDragOver(false)}
              onDrop={handleDrop}
              onClick={() => fileInputRef.current?.click()}
            >
              <input
                ref={fileInputRef}
                type="file"
                accept={getAcceptTypes()}
                onChange={handleFileSelect}
                className="sr-only"
                aria-label={`Upload ${activeTab} file`}
              />
              <Upload className="mb-2 h-10 w-10 text-muted-foreground" />
              <p className="text-sm font-medium">
                Drag and drop or click to upload
              </p>
              <p className="mt-1 text-xs text-muted-foreground">
                {activeTab === "video" && "MP4, WebM, MOV up to 500MB"}
                {activeTab === "image" && "PNG, JPG, GIF up to 10MB"}
                {activeTab === "subtitle" && "SRT, VTT, or TXT files"}
              </p>
            </div>

            {/* URL import (not for subtitles) */}
            {activeTab !== "subtitle" && (
              <div className="space-y-2">
                <Label htmlFor="url-input" className="text-sm">
                  Or import from URL
                </Label>
                <div className="flex gap-2">
                  <div className="relative flex-1">
                    <Link className="absolute left-3 top-1/2 h-4 w-4 -translate-y-1/2 text-muted-foreground" />
                    <Input
                      id="url-input"
                      placeholder={`Paste ${activeTab} URL...`}
                      value={urlInput}
                      onChange={(e) => setUrlInput(e.target.value)}
                      className="pl-9"
                    />
                    {urlInput && (
                      <Button
                        variant="ghost"
                        size="icon"
                        className="absolute right-1 top-1/2 h-6 w-6 -translate-y-1/2"
                        onClick={() => setUrlInput("")}
                      >
                        <X className="h-3 w-3" />
                      </Button>
                    )}
                  </div>
                  <Button onClick={handleUrlImport} disabled={!urlInput.trim()}>
                    Import
                  </Button>
                </div>
              </div>
            )}
          </TabsContent>
        </Tabs>
      </DialogContent>
    </Dialog>
  );
}

/**
 * Simple SRT/VTT subtitle parser
 * Parses subtitle files and returns an array of SubtitleCue objects
 */
function parseSubtitles(content: string): SubtitleCue[] {
  const cues: SubtitleCue[] = [];
  const trackId = `subtitle-${Date.now()}`;

  // Try to detect format and parse
  const lines = content.trim().split("\n");
  let i = 0;

  // Skip VTT header if present
  if (lines[0]?.includes("WEBVTT")) {
    i = 1;
    while (i < lines.length && lines[i]?.trim() === "") i++;
  }

  while (i < lines.length) {
    const currentLine = lines[i];
    if (!currentLine) {
      i++;
      continue;
    }

    // Skip cue number for SRT
    if (/^\d+$/.test(currentLine.trim())) {
      i++;
    }

    // Look for timestamp line
    const timestampLine = lines[i];
    if (!timestampLine) {
      i++;
      continue;
    }

    const timestampMatch = timestampLine.match(
      /(\d{2}):(\d{2}):(\d{2})[,.](\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2})[,.](\d{3})/
    );

    if (timestampMatch) {
      const startTime =
        Number.parseInt(timestampMatch[1] || "0") * 3600 +
        Number.parseInt(timestampMatch[2] || "0") * 60 +
        Number.parseInt(timestampMatch[3] || "0") +
        Number.parseInt(timestampMatch[4] || "0") / 1000;

      const endTime =
        Number.parseInt(timestampMatch[5] || "0") * 3600 +
        Number.parseInt(timestampMatch[6] || "0") * 60 +
        Number.parseInt(timestampMatch[7] || "0") +
        Number.parseInt(timestampMatch[8] || "0") / 1000;

      i++;

      // Collect text lines until empty line
      const textLines: string[] = [];
      while (i < lines.length) {
        const textLine = lines[i];
        if (!textLine || textLine.trim() === "") break;
        textLines.push(textLine.trim());
        i++;
      }

      if (textLines.length > 0) {
        cues.push({
          id: `cue-${Date.now()}-${cues.length}`,
          trackId,
          startTime,
          endTime,
          text: textLines.join(" "),
        });
      }
    }

    i++;
  }

  return cues;
}
````

## File: packages/frontend/components/TimelineEditor/editor/index.ts
````typescript
/**
 * Editor Sub-Components
 *
 * This directory contains the sub-components for the AudioTimelineEditor.
 * These components are adapted from the new-time-line reference implementation
 * with imports updated to use the project's path aliases (@/).
 *
 * @see .kiro/specs/timeline-editor-replacement/design.md for architecture details
 */

// Main editor panels
export { TrackSidebar } from "./TrackSidebar";
export { VideoPreview } from "./VideoPreview";
export { TimelinePanel } from "./TimelinePanel";
export { TimelineControls } from "./TimelineControls";
export { ImportMediaModal } from "./ImportMediaModal";

// Clip components
export { WaveformClip } from "./WaveformClip";
export { SubtitleClip } from "./SubtitleClip";
export { VideoClipComponent } from "./VideoClipComponent";
export { ImageClipComponent } from "./ImageClipComponent";
````

## File: packages/frontend/components/TimelineEditor/editor/SubtitleClip.tsx
````typescript
"use client";

import type { SubtitleCue } from "@/types/audio-editor";
import { cn } from "@/lib/utils";

interface SubtitleClipProps {
  cue: SubtitleCue;
  pixelsPerSecond: number;
  isSelected: boolean;
  onSelect: () => void;
}

export function SubtitleClip({
  cue,
  pixelsPerSecond,
  isSelected,
  onSelect,
}: SubtitleClipProps) {
  const width = (cue.endTime - cue.startTime) * pixelsPerSecond;
  const left = cue.startTime * pixelsPerSecond;

  return (
    <div
      className={cn(
        "absolute top-1/2 -translate-y-1/2 h-8 rounded-md cursor-pointer transition-all overflow-hidden",
        "bg-emerald-500/20 border border-emerald-500/40",
        isSelected &&
          "ring-2 ring-emerald-500 ring-offset-1 ring-offset-background"
      )}
      style={{ left, width: Math.max(width, 20) }}
      onClick={(e) => {
        e.stopPropagation();
        onSelect();
      }}
      role="button"
      tabIndex={0}
      aria-label={`Subtitle: ${cue.text}`}
      aria-selected={isSelected}
      onKeyDown={(e) => {
        if (e.key === "Enter" || e.key === " ") {
          e.preventDefault();
          onSelect();
        }
      }}
    >
      <div className="flex h-full items-center px-2">
        <span className="text-[10px] text-emerald-700 dark:text-emerald-300 truncate font-medium">
          {cue.text}
        </span>
      </div>
      {/* Resize handles */}
      {isSelected && (
        <>
          <div className="absolute left-0 top-0 h-full w-1.5 cursor-ew-resize bg-emerald-500/50 hover:bg-emerald-500" />
          <div className="absolute right-0 top-0 h-full w-1.5 cursor-ew-resize bg-emerald-500/50 hover:bg-emerald-500" />
        </>
      )}
    </div>
  );
}
````

## File: packages/frontend/components/TimelineEditor/editor/TimelineControls.tsx
````typescript
"use client";

import type React from "react";
import {
  Play,
  Pause,
  Volume2,
  PlusCircle,
  Upload,
  Video,
  ImageIcon,
  FileText,
} from "lucide-react";
import { Button } from "@/components/ui/button";
import { Slider } from "@/components/ui/slider";
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
  DropdownMenuSeparator,
  DropdownMenuLabel,
} from "@/components/ui/dropdown-menu";

interface TimelineControlsProps {
  isPlaying: boolean;
  onPlayPause: () => void;
  zoom: number;
  onZoomChange: (value: number) => void;
  volume: number;
  onVolumeChange: (value: number) => void;
  selectedClipId: string | null;
  onAddVoiceoverTrack: () => void;
  onAddSfxTrack: () => void;
  onAddSubtitleTrack: () => void;
  onOpenImportModal: () => void;
}

export function TimelineControls({
  isPlaying,
  onPlayPause,
  zoom,
  onZoomChange,
  volume,
  onVolumeChange,
  selectedClipId,
  onAddVoiceoverTrack,
  onAddSfxTrack,
  onAddSubtitleTrack,
  onOpenImportModal,
}: TimelineControlsProps) {
  return (
    <div className="flex items-center justify-between border-b border-border bg-card px-4 py-2">
      {/* Left controls */}
      <div className="flex items-center gap-2">
        <Button variant="ghost" size="icon" className="h-8 w-8">
          <Settings2Icon className="h-4 w-4" />
        </Button>
      </div>

      {/* Center - Play button and track actions */}
      <div className="flex flex-col items-center gap-2">
        <Button
          variant="ghost"
          size="icon"
          className="h-10 w-10"
          onClick={onPlayPause}
          aria-label={isPlaying ? "Pause" : "Play"}
        >
          {isPlaying ? (
            <Pause className="h-5 w-5" />
          ) : (
            <Play className="h-5 w-5" />
          )}
        </Button>

        {/* Track action buttons */}
        <div className="flex items-center gap-2">
          <Button
            variant="outline"
            size="sm"
            className="h-7 gap-1.5 text-xs bg-transparent"
            onClick={onAddVoiceoverTrack}
          >
            <PlusCircle className="h-3.5 w-3.5" />
            Add Voiceover Track
          </Button>
          <Button
            variant="outline"
            size="sm"
            className="h-7 gap-1.5 text-xs bg-transparent"
            onClick={onAddSfxTrack}
          >
            <PlusCircle className="h-3.5 w-3.5" />
            Add SFX Track
          </Button>
          <Button
            variant="outline"
            size="sm"
            className="h-7 gap-1.5 text-xs bg-transparent"
            onClick={onAddSubtitleTrack}
          >
            <FileText className="h-3.5 w-3.5" />
            Add Subtitle Track
          </Button>
          <DropdownMenu>
            <DropdownMenuTrigger asChild>
              <Button
                variant="outline"
                size="sm"
                className="h-7 gap-1.5 text-xs bg-transparent"
              >
                <Upload className="h-3.5 w-3.5" />
                Import Media
              </Button>
            </DropdownMenuTrigger>
            <DropdownMenuContent align="center">
              <DropdownMenuLabel className="text-xs">
                Import Files
              </DropdownMenuLabel>
              <DropdownMenuSeparator />
              <DropdownMenuItem onClick={onOpenImportModal} className="gap-2">
                <Video className="h-4 w-4" />
                Import Video
              </DropdownMenuItem>
              <DropdownMenuItem onClick={onOpenImportModal} className="gap-2">
                <ImageIcon className="h-4 w-4" />
                Import Image
              </DropdownMenuItem>
              <DropdownMenuItem onClick={onOpenImportModal} className="gap-2">
                <FileText className="h-4 w-4" />
                Import Subtitles
              </DropdownMenuItem>
            </DropdownMenuContent>
          </DropdownMenu>
        </div>
      </div>

      {/* Right controls */}
      <div className="flex items-center gap-4">
        {/* Zoom control */}
        <div className="flex items-center gap-2">
          <span className="text-xs text-muted-foreground">Zoom</span>
          <Slider
            value={[zoom]}
            onValueChange={([value]) => onZoomChange(value ?? 10)}
            min={10}
            max={100}
            step={1}
            className="w-24"
            aria-label="Timeline zoom"
          />
        </div>

        {/* Selection info */}
        <div className="flex flex-col items-end gap-1">
          <span className="text-xs text-muted-foreground">
            {selectedClipId ? "1 Clip Selected" : "No Selection"}
          </span>

          {/* Volume control */}
          <div className="flex items-center gap-2">
            <Volume2 className="h-3.5 w-3.5 text-muted-foreground" />
            <Slider
              value={[volume]}
              onValueChange={([value]) => onVolumeChange(value ?? 100)}
              min={0}
              max={100}
              step={1}
              className="w-20"
              aria-label="Volume"
            />
          </div>
        </div>

        {/* Action buttons */}
        <div className="flex flex-col gap-1">
          <Button
            variant="outline"
            size="sm"
            className="h-7 text-xs bg-transparent"
            disabled={!selectedClipId}
          >
            Create Voice from Selection
          </Button>
          <Button
            variant="outline"
            size="sm"
            className="h-7 text-xs bg-transparent"
          >
            Generate Audio
          </Button>
        </div>
      </div>
    </div>
  );
}

function Settings2Icon(props: React.SVGProps<SVGSVGElement>) {
  return (
    <svg
      xmlns="http://www.w3.org/2000/svg"
      width="24"
      height="24"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
      {...props}
    >
      <path d="M20 7h-9" />
      <path d="M14 17H5" />
      <circle cx="17" cy="17" r="3" />
      <circle cx="7" cy="7" r="3" />
    </svg>
  );
}
````

## File: packages/frontend/components/TimelineEditor/editor/TimelinePanel.tsx
````typescript
"use client";

import type React from "react";

import { useRef, useCallback } from "react";
import { Settings, Lock, Type, Video, ImageIcon } from "lucide-react";
import { Button } from "@/components/ui/button";
import { WaveformClip } from "./WaveformClip";
import { SubtitleClip } from "./SubtitleClip";
import { VideoClipComponent } from "./VideoClipComponent";
import { ImageClipComponent } from "./ImageClipComponent";
import type {
  Track,
  AudioClip,
  SubtitleCue,
  VideoClip,
  ImageClip,
} from "@/types/audio-editor";

interface TimelinePanelProps {
  tracks: Track[];
  clips: AudioClip[];
  subtitles: SubtitleCue[];
  videoClips: VideoClip[];
  imageClips: ImageClip[];
  currentTime: number;
  zoom: number;
  selectedClipId: string | null;
  onSelectClip: (id: string | null) => void;
  onSeek: (time: number) => void;
}

export function TimelinePanel({
  tracks,
  clips,
  subtitles,
  videoClips,
  imageClips,
  currentTime,
  zoom,
  selectedClipId,
  onSelectClip,
  onSeek,
}: TimelinePanelProps) {
  const timelineRef = useRef<HTMLDivElement>(null);
  const duration = 60;
  const pixelsPerSecond = (zoom / 100) * 20 + 10;

  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return `${mins.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}`;
  };

  const handleTimelineClick = useCallback(
    (e: React.MouseEvent) => {
      if (!timelineRef.current) return;
      const rect = timelineRef.current.getBoundingClientRect();
      const x = e.clientX - rect.left - 140;
      const time = Math.max(0, Math.min(duration, x / pixelsPerSecond));
      onSeek(time);
    },
    [pixelsPerSecond, duration, onSeek]
  );

  const narratorTracks = tracks.filter((t) => t.type === "narrator");
  const sfxTracks = tracks.filter((t) => t.type === "sfx");

  const narratorClips = clips.filter((c) =>
    narratorTracks.some((t) => t.id === c.trackId)
  );
  const sfxClips = clips.filter((c) => sfxTracks.some((t) => t.id === c.trackId));

  const markers = [];
  for (let i = 0; i <= duration; i += 5) {
    markers.push(i);
  }

  return (
    <div
      ref={timelineRef}
      className="relative overflow-x-auto bg-timeline-bg"
      onClick={handleTimelineClick}
    >
      {/* Time ruler */}
      <div className="sticky top-0 z-10 flex h-8 border-b border-border bg-background/80 backdrop-blur-sm">
        <div className="w-[140px] shrink-0" />
        <div className="relative flex-1">
          {markers.map((time) => (
            <div
              key={time}
              className="absolute top-0 flex h-full flex-col items-center justify-end"
              style={{ left: time * pixelsPerSecond }}
            >
              <span className="mb-1 text-[10px] text-muted-foreground">
                {formatTime(time)}
              </span>
              <div className="h-2 w-px bg-border" />
            </div>
          ))}
        </div>
      </div>

      {/* Playhead */}
      <div
        className="absolute bottom-0 top-8 z-20 w-px bg-foreground"
        style={{ left: 140 + currentTime * pixelsPerSecond }}
      >
        <div className="absolute -left-1.5 -top-1 h-3 w-3 rounded-full bg-foreground" />
      </div>

      {/* Video Track */}
      <div className="flex min-h-[70px] border-b border-border">
        <div className="flex w-[140px] shrink-0 items-center gap-2 border-r border-border bg-card px-3">
          <div className="flex items-center gap-2">
            <Video className="h-4 w-4 text-violet-500" />
            <div className="flex flex-col">
              <span className="text-xs font-medium">Video Track</span>
              <span className="text-[10px] text-muted-foreground">
                {videoClips.length} clip{videoClips.length !== 1 ? "s" : ""}
              </span>
            </div>
          </div>
          <div className="ml-auto flex items-center gap-1">
            <Button variant="ghost" size="icon" className="h-6 w-6">
              <Lock className="h-3 w-3" />
            </Button>
          </div>
        </div>
        <div className="relative flex-1 py-1 bg-violet-500/5">
          {videoClips.map((clip) => (
            <VideoClipComponent
              key={clip.id}
              clip={clip}
              pixelsPerSecond={pixelsPerSecond}
              isSelected={clip.id === selectedClipId}
              onSelect={() => onSelectClip(clip.id)}
            />
          ))}
          {videoClips.length === 0 && (
            <div className="absolute inset-0 flex items-center justify-center text-xs text-muted-foreground/50">
              Import video to add clips
            </div>
          )}
        </div>
      </div>

      {/* Image Track */}
      <div className="flex min-h-[60px] border-b border-border">
        <div className="flex w-[140px] shrink-0 items-center gap-2 border-r border-border bg-card px-3">
          <div className="flex items-center gap-2">
            <ImageIcon className="h-4 w-4 text-amber-500" />
            <div className="flex flex-col">
              <span className="text-xs font-medium">Image Track</span>
              <span className="text-[10px] text-muted-foreground">
                {imageClips.length} image{imageClips.length !== 1 ? "s" : ""}
              </span>
            </div>
          </div>
          <div className="ml-auto flex items-center gap-1">
            <Button variant="ghost" size="icon" className="h-6 w-6">
              <Lock className="h-3 w-3" />
            </Button>
          </div>
        </div>
        <div className="relative flex-1 py-1 bg-amber-500/5">
          {imageClips.map((clip) => (
            <ImageClipComponent
              key={clip.id}
              clip={clip}
              pixelsPerSecond={pixelsPerSecond}
              isSelected={clip.id === selectedClipId}
              onSelect={() => onSelectClip(clip.id)}
            />
          ))}
          {imageClips.length === 0 && (
            <div className="absolute inset-0 flex items-center justify-center text-xs text-muted-foreground/50">
              Import images to add clips
            </div>
          )}
        </div>
      </div>

      {/* Narrator track row */}
      <div className="flex min-h-[60px] border-b border-border">
        <div className="flex w-[140px] shrink-0 items-center gap-2 border-r border-border bg-card px-3">
          <div className="flex flex-col">
            <span className="text-xs font-medium">Primary Narrator</span>
            <span className="text-[10px] text-muted-foreground">
              Narration Track - Original
            </span>
          </div>
          <div className="ml-auto flex items-center gap-1">
            <Button variant="ghost" size="icon" className="h-6 w-6">
              <Settings className="h-3 w-3" />
            </Button>
            <Button variant="ghost" size="icon" className="h-6 w-6">
              <Lock className="h-3 w-3" />
            </Button>
          </div>
        </div>
        <div className="relative flex-1 py-2">
          {narratorClips.map((clip) => (
            <WaveformClip
              key={clip.id}
              clip={clip}
              type="narrator"
              pixelsPerSecond={pixelsPerSecond}
              isSelected={clip.id === selectedClipId}
              onSelect={() => onSelectClip(clip.id)}
            />
          ))}
        </div>
      </div>

      {/* SFX track row */}
      <div className="flex min-h-[60px] border-b border-border">
        <div className="flex w-[140px] shrink-0 items-center gap-2 border-r border-border bg-card px-3">
          <div className="flex flex-col">
            <span className="text-xs font-medium">New SFX Track</span>
            <span className="text-[10px] text-muted-foreground">
              SFX Track - English
            </span>
          </div>
          <div className="ml-auto flex items-center gap-1">
            <Button variant="ghost" size="icon" className="h-6 w-6">
              <Lock className="h-3 w-3" />
            </Button>
          </div>
        </div>
        <div className="relative flex-1 py-2">
          {sfxClips.map((clip) => (
            <WaveformClip
              key={clip.id}
              clip={clip}
              type="sfx"
              pixelsPerSecond={pixelsPerSecond}
              isSelected={clip.id === selectedClipId}
              onSelect={() => onSelectClip(clip.id)}
            />
          ))}
        </div>
      </div>

      {/* Subtitle Track */}
      <div className="flex min-h-[50px]">
        <div className="flex w-[140px] shrink-0 items-center gap-2 border-r border-border bg-card px-3">
          <div className="flex items-center gap-2">
            <Type className="h-4 w-4 text-emerald-500" />
            <div className="flex flex-col">
              <span className="text-xs font-medium">Subtitles</span>
              <span className="text-[10px] text-muted-foreground">
                {subtitles.length} cue{subtitles.length !== 1 ? "s" : ""}
              </span>
            </div>
          </div>
          <div className="ml-auto flex items-center gap-1">
            <Button variant="ghost" size="icon" className="h-6 w-6">
              <Lock className="h-3 w-3" />
            </Button>
          </div>
        </div>
        <div className="relative flex-1 py-2 bg-emerald-500/5">
          {subtitles.map((cue) => (
            <SubtitleClip
              key={cue.id}
              cue={cue}
              pixelsPerSecond={pixelsPerSecond}
              isSelected={cue.id === selectedClipId}
              onSelect={() => onSelectClip(cue.id)}
            />
          ))}
          {subtitles.length === 0 && (
            <div className="absolute inset-0 flex items-center justify-center text-xs text-muted-foreground/50">
              Import subtitles or add manually
            </div>
          )}
        </div>
      </div>
    </div>
  );
}
````

## File: packages/frontend/components/TimelineEditor/editor/TrackSidebar.tsx
````typescript
"use client";

import { Textarea } from "@/components/ui/textarea";
import { Button } from "@/components/ui/button";
import { ScrollArea } from "@/components/ui/scroll-area";
import { cn } from "@/lib/utils";
import type { Track } from "@/types/audio-editor";

interface TrackSidebarProps {
  tracks: Track[];
  selectedTrackId: string | null;
  onSelectTrack: (id: string) => void;
  onUpdateText: (id: string, text: string) => void;
  onGenerateAudio: (id: string) => void;
}

export function TrackSidebar({
  tracks,
  selectedTrackId,
  onSelectTrack,
  onUpdateText,
  onGenerateAudio,
}: TrackSidebarProps) {
  return (
    <ScrollArea className="w-[400px] border-r border-border bg-card lg:w-[480px]">
      <div className="flex flex-col gap-3 p-4">
        {tracks.map((track) => (
          <TrackCard
            key={track.id}
            track={track}
            isSelected={track.id === selectedTrackId}
            onSelect={() => onSelectTrack(track.id)}
            onUpdateText={(text) => onUpdateText(track.id, text)}
            onGenerateAudio={() => onGenerateAudio(track.id)}
          />
        ))}
      </div>
    </ScrollArea>
  );
}

interface TrackCardProps {
  track: Track;
  isSelected: boolean;
  onSelect: () => void;
  onUpdateText: (text: string) => void;
  onGenerateAudio: () => void;
}

function TrackCard({
  track,
  isSelected,
  onSelect,
  onUpdateText,
  onGenerateAudio,
}: TrackCardProps) {
  const isNarrator = track.type === "narrator";

  return (
    <div
      className={cn(
        "flex flex-col gap-2 rounded-lg border-2 bg-card p-3 transition-all",
        isSelected
          ? "border-primary shadow-sm"
          : "border-transparent hover:border-border"
      )}
      onClick={onSelect}
      role="button"
      tabIndex={0}
      onKeyDown={(e) => e.key === "Enter" && onSelect()}
    >
      {/* Track header */}
      <div className="flex items-center gap-2">
        <div
          className={cn(
            "h-2.5 w-2.5 rounded-full",
            isNarrator ? "bg-narrator" : "bg-sfx"
          )}
          aria-hidden="true"
        />
        <span className="text-xs font-medium text-muted-foreground">
          {track.name}
        </span>
      </div>

      {/* Text input */}
      <Textarea
        value={track.text}
        onChange={(e) => onUpdateText(e.target.value)}
        placeholder={
          isNarrator
            ? "Enter narration text..."
            : "Enter sound effect description..."
        }
        className="min-h-[60px] resize-none border-0 bg-transparent p-0 text-sm focus-visible:ring-0 focus-visible:ring-offset-0"
        onClick={(e) => e.stopPropagation()}
      />

      {/* Generate button */}
      <Button
        variant="ghost"
        size="sm"
        className="self-end text-xs text-muted-foreground hover:text-foreground"
        onClick={(e) => {
          e.stopPropagation();
          onGenerateAudio();
        }}
      >
        Generate Audio
      </Button>
    </div>
  );
}
````

## File: packages/frontend/components/TimelineEditor/editor/VideoClipComponent.tsx
````typescript
"use client";

import { cn } from "@/lib/utils";
import type { VideoClip } from "@/types/audio-editor";

interface VideoClipProps {
  clip: VideoClip;
  pixelsPerSecond: number;
  isSelected: boolean;
  onSelect: () => void;
}

/**
 * Format duration in seconds to a human-readable string.
 */
function formatDuration(seconds: number): string {
  const mins = Math.floor(seconds / 60);
  const secs = Math.floor(seconds % 60);
  if (mins > 0) {
    return `${mins} minute${mins !== 1 ? "s" : ""} ${secs} second${secs !== 1 ? "s" : ""}`;
  }
  return `${secs} second${secs !== 1 ? "s" : ""}`;
}

export function VideoClipComponent({
  clip,
  pixelsPerSecond,
  isSelected,
  onSelect,
}: VideoClipProps) {
  const width = clip.duration * pixelsPerSecond;
  const left = clip.startTime * pixelsPerSecond;

  return (
    <div
      className={cn(
        "absolute top-1 bottom-1 cursor-pointer rounded-md overflow-hidden transition-all",
        "bg-violet-500/20 border-2",
        isSelected
          ? "border-violet-500 ring-2 ring-violet-500/30"
          : "border-violet-500/50 hover:border-violet-500/70"
      )}
      style={{ left, width, minWidth: 60 }}
      onClick={(e) => {
        e.stopPropagation();
        onSelect();
      }}
      role="button"
      tabIndex={0}
      aria-label={`Video clip: ${clip.name}, duration ${formatDuration(clip.duration)}`}
      aria-selected={isSelected}
      onKeyDown={(e) => {
        if (e.key === "Enter" || e.key === " ") {
          e.preventDefault();
          onSelect();
        }
      }}
    >
      {/* Thumbnail strip */}
      <div className="flex h-full">
        {Array.from({ length: Math.max(1, Math.floor(width / 50)) }).map(
          (_, i) => (
            <div
              key={i}
              className="h-full w-[50px] shrink-0 bg-cover bg-center border-r border-violet-500/30 last:border-r-0"
              style={{
                backgroundImage: `url(${clip.thumbnailUrl || "/video-frame.png"})`,
              }}
            />
          )
        )}
      </div>

      {/* Clip name overlay */}
      <div className="absolute inset-x-0 bottom-0 bg-gradient-to-t from-black/60 to-transparent px-2 py-1">
        <span className="text-[10px] font-medium text-white truncate block">
          {clip.name}
        </span>
      </div>

      {/* Resize handles when selected */}
      {isSelected && (
        <>
          <div className="absolute left-0 top-0 bottom-0 w-1.5 cursor-ew-resize bg-violet-500 hover:bg-violet-400" />
          <div className="absolute right-0 top-0 bottom-0 w-1.5 cursor-ew-resize bg-violet-500 hover:bg-violet-400" />
        </>
      )}
    </div>
  );
}
````

## File: packages/frontend/components/TimelineEditor/editor/VideoPreview.tsx
````typescript
"use client";

import { useState, useRef, useEffect } from "react";
import { Search, Play, Pause, Maximize2 } from "lucide-react";
import { Button } from "@/components/ui/button";
import type { MediaFile, SubtitleCue } from "@/types/audio-editor";

interface VideoPreviewProps {
  video?: MediaFile | null;
  currentTime?: number;
  isPlaying?: boolean;
  subtitles?: SubtitleCue[];
  onTimeUpdate?: (time: number) => void;
  onPlayPause?: () => void;
}

export function VideoPreview({
  video,
  currentTime = 0,
  isPlaying = false,
  subtitles = [],
  onTimeUpdate,
  onPlayPause,
}: VideoPreviewProps) {
  const videoRef = useRef<HTMLVideoElement>(null);
  const [currentSubtitle, setCurrentSubtitle] = useState<string>("");

  useEffect(() => {
    const el = videoRef.current;
    if (!el || !video) return;

    if (isPlaying) {
      const playVideoSafe = async () => {
        try {
          if (el.readyState >= 3) {
            await el.play();
          } else {
            el.oncanplay = async () => {
              try { await el.play(); } catch (err) {
                if (err instanceof Error && err.name !== 'AbortError') {
                  console.error("Playback failed on canplay", err);
                }
              }
            };
          }
        } catch (err) {
          if (err instanceof Error && err.name !== 'AbortError') {
            console.error("Playback failed", err);
          }
        }
      };
      playVideoSafe();
    } else {
      el.pause();
    }
  }, [isPlaying, video]);

  useEffect(() => {
    const activeCue = subtitles.find(
      (cue) => currentTime >= cue.startTime && currentTime <= cue.endTime
    );
    setCurrentSubtitle(activeCue?.text || "");
  }, [currentTime, subtitles]);

  const handleTimeUpdate = () => {
    if (videoRef.current && onTimeUpdate) {
      onTimeUpdate(videoRef.current.currentTime);
    }
  };

  return (
    <div className="relative flex flex-1 items-center justify-center bg-muted/30 p-4">
      {/* Zoom/Search button */}
      <Button
        variant="secondary"
        size="icon"
        className="absolute left-4 top-4 z-10 h-8 w-8 rounded-full bg-card/80 backdrop-blur-sm"
        aria-label="Zoom preview"
      >
        <Search className="h-4 w-4" />
      </Button>

      {/* Video preview container */}
      <div className="relative aspect-video w-full max-w-3xl overflow-hidden rounded-lg shadow-lg bg-black">
        {video ? (
          <>
            <video
              ref={videoRef}
              src={video.url}
              className="h-full w-full object-contain"
              onTimeUpdate={handleTimeUpdate}
              playsInline
            />
            {/* Play/Pause overlay button */}
            <Button
              variant="ghost"
              size="icon"
              className="absolute left-1/2 top-1/2 h-16 w-16 -translate-x-1/2 -translate-y-1/2 rounded-full bg-black/50 text-white opacity-0 transition-opacity hover:bg-black/70 hover:opacity-100 focus:opacity-100"
              onClick={onPlayPause}
              aria-label={isPlaying ? "Pause" : "Play"}
            >
              {isPlaying ? (
                <Pause className="h-8 w-8" />
              ) : (
                <Play className="h-8 w-8 ml-1" />
              )}
            </Button>
          </>
        ) : (
          // Default placeholder image
          <div className="h-full w-full flex items-center justify-center bg-muted/50">
            <span className="text-muted-foreground text-sm">
              No video loaded
            </span>
          </div>
        )}

        {/* Subtitle overlay */}
        {currentSubtitle && (
          <div className="absolute bottom-8 left-1/2 -translate-x-1/2 max-w-[80%]">
            <p className="rounded bg-black/75 px-4 py-2 text-center text-sm font-medium text-white md:text-base">
              {currentSubtitle}
            </p>
          </div>
        )}

        {/* Gradient overlay */}
        <div className="absolute inset-0 bg-gradient-to-t from-black/20 to-transparent pointer-events-none" />

        {/* Fullscreen button */}
        <Button
          variant="ghost"
          size="icon"
          className="absolute bottom-4 right-4 h-8 w-8 rounded bg-black/50 text-white hover:bg-black/70"
          aria-label="Fullscreen"
        >
          <Maximize2 className="h-4 w-4" />
        </Button>
      </div>
    </div>
  );
}
````

## File: packages/frontend/components/TimelineEditor/editor/WaveformClip.tsx
````typescript
"use client";

import { Maximize2 } from "lucide-react";
import { cn } from "@/lib/utils";
import type { AudioClip } from "@/types/audio-editor";

interface WaveformClipProps {
  clip: AudioClip;
  type: "narrator" | "sfx";
  pixelsPerSecond: number;
  isSelected: boolean;
  onSelect: () => void;
}

export function WaveformClip({
  clip,
  type,
  pixelsPerSecond,
  isSelected,
  onSelect,
}: WaveformClipProps) {
  const width = clip.duration * pixelsPerSecond;
  const left = clip.startTime * pixelsPerSecond;

  return (
    <div
      className={cn(
        "absolute flex h-10 cursor-pointer items-center rounded-md transition-all",
        type === "narrator"
          ? "bg-narrator-waveform/20 hover:bg-narrator-waveform/30"
          : "bg-sfx-waveform/20 hover:bg-sfx-waveform/30",
        isSelected && "ring-2 ring-primary ring-offset-1"
      )}
      style={{ left, width }}
      onClick={(e) => {
        e.stopPropagation();
        onSelect();
      }}
      role="button"
      tabIndex={0}
      onKeyDown={(e) => {
        if (e.key === "Enter" || e.key === " ") {
          e.preventDefault();
          onSelect();
        }
      }}
      aria-label={`Audio clip, ${clip.duration.toFixed(1)} seconds`}
      aria-selected={isSelected}
    >
      {/* Waveform visualization */}
      <div className="flex h-full flex-1 items-center gap-px px-2">
        {clip.waveformData.map((amplitude, index) => (
          <div
            key={index}
            className={cn(
              "w-0.5 rounded-full",
              type === "narrator" ? "bg-narrator-waveform" : "bg-sfx-waveform"
            )}
            style={{ height: `${amplitude * 100}%` }}
          />
        ))}
      </div>

      {/* Resize handle */}
      <div className="absolute right-1 top-1">
        <Maximize2 className="h-3 w-3 rotate-90 text-muted-foreground/50" />
      </div>
    </div>
  );
}
````

## File: packages/frontend/components/TimelineEditor/FooterNav.tsx
````typescript
/**
 * FooterNav Component
 * 
 * Footer section of the Graphite Timeline containing:
 * - Zoom controls (zoom in/out buttons and slider)
 * - Project overview minimap
 * - Total duration display
 * 
 * Requirements: 8.1, 8.2, 9.1, 9.2, 9.3, 9.4
 */

import React, { useCallback, useRef } from "react";
import { formatTimecode, TimelineTrack } from "./graphite-timeline-utils";
import "./graphite-timeline.css";

// --- SVG Icons ---

const ZoomOutIcon = () => (
  <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
    <circle cx="11" cy="11" r="8" />
    <line x1="21" y1="21" x2="16.65" y2="16.65" />
    <line x1="8" y1="11" x2="14" y2="11" />
  </svg>
);

const ZoomInIcon = () => (
  <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
    <circle cx="11" cy="11" r="8" />
    <line x1="21" y1="21" x2="16.65" y2="16.65" />
    <line x1="11" y1="8" x2="11" y2="14" />
    <line x1="8" y1="11" x2="14" y2="11" />
  </svg>
);

// --- Types ---

export interface FooterNavProps {
  /** Current zoom level (pixels per second) */
  zoom: number;
  /** Minimum zoom level */
  minZoom: number;
  /** Maximum zoom level */
  maxZoom: number;
  /** Total project duration in seconds */
  duration: number;
  /** Start time of visible region in seconds */
  visibleStart: number;
  /** End time of visible region in seconds */
  visibleEnd: number;
  /** Callback when zoom level changes */
  onZoomChange: (zoom: number) => void;
  /** Timeline tracks for minimap display */
  tracks?: TimelineTrack[];
  /** Frames per second for duration display (default: 24) */
  fps?: number;
}

// --- Sub-Components ---

/**
 * ZoomControls - Zoom in/out buttons with slider
 * Requirements: 8.1, 8.2
 */
interface ZoomControlsProps {
  zoom: number;
  minZoom: number;
  maxZoom: number;
  onZoomChange: (zoom: number) => void;
}

function ZoomControls({ zoom, minZoom, maxZoom, onZoomChange }: ZoomControlsProps) {
  const sliderRef = useRef<HTMLDivElement>(null);
  
  // Calculate slider handle position (0-100%)
  const handlePosition = ((zoom - minZoom) / (maxZoom - minZoom)) * 100;
  
  const handleZoomOut = useCallback(() => {
    const newZoom = Math.max(minZoom, zoom - 10);
    onZoomChange(newZoom);
  }, [zoom, minZoom, onZoomChange]);
  
  const handleZoomIn = useCallback(() => {
    const newZoom = Math.min(maxZoom, zoom + 10);
    onZoomChange(newZoom);
  }, [zoom, maxZoom, onZoomChange]);
  
  const handleSliderClick = useCallback((e: React.MouseEvent<HTMLDivElement>) => {
    if (!sliderRef.current) return;
    
    const rect = sliderRef.current.getBoundingClientRect();
    const clickX = e.clientX - rect.left;
    const percentage = Math.max(0, Math.min(1, clickX / rect.width));
    const newZoom = minZoom + percentage * (maxZoom - minZoom);
    onZoomChange(newZoom);
  }, [minZoom, maxZoom, onZoomChange]);
  
  const handleSliderDrag = useCallback((e: React.MouseEvent<HTMLDivElement>) => {
    if (e.buttons !== 1 || !sliderRef.current) return;
    
    const rect = sliderRef.current.getBoundingClientRect();
    const dragX = e.clientX - rect.left;
    const percentage = Math.max(0, Math.min(1, dragX / rect.width));
    const newZoom = minZoom + percentage * (maxZoom - minZoom);
    onZoomChange(newZoom);
  }, [minZoom, maxZoom, onZoomChange]);
  
  return (
    <div className="graphite-zoom-controls">
      <button
        className="graphite-btn graphite-btn-small"
        onClick={handleZoomOut}
        aria-label="Zoom out"
        title="Zoom out"
        disabled={zoom <= minZoom}
      >
        <ZoomOutIcon />
      </button>
      
      <div
        ref={sliderRef}
        className="graphite-zoom-slider"
        onClick={handleSliderClick}
        onMouseMove={handleSliderDrag}
        role="slider"
        aria-label="Zoom level"
        aria-valuemin={minZoom}
        aria-valuemax={maxZoom}
        aria-valuenow={zoom}
        tabIndex={0}
      >
        <div
          className="graphite-zoom-handle"
          style={{ left: `calc(${handlePosition}% - 7px)` }}
        />
      </div>
      
      <button
        className="graphite-btn graphite-btn-small"
        onClick={handleZoomIn}
        aria-label="Zoom in"
        title="Zoom in"
        disabled={zoom >= maxZoom}
      >
        <ZoomInIcon />
      </button>
    </div>
  );
}

/**
 * ProjectOverview - Minimap showing all clips and visible region
 * Requirements: 9.1, 9.2, 9.3, 9.4
 */
interface ProjectOverviewProps {
  tracks: TimelineTrack[];
  duration: number;
  visibleStart: number;
  visibleEnd: number;
}

function ProjectOverview({ tracks, duration, visibleStart, visibleEnd }: ProjectOverviewProps) {
  // Don't render if no duration
  if (duration <= 0) {
    return <div className="graphite-project-overview" />;
  }
  
  // Calculate visible region position and width as percentages
  const visibleLeft = (visibleStart / duration) * 100;
  const visibleWidth = ((visibleEnd - visibleStart) / duration) * 100;
  
  // Flatten all clips from all tracks for minimap display
  const allClips = tracks.flatMap(track => 
    track.clips.map(clip => ({
      ...clip,
      trackType: track.type,
    }))
  );
  
  return (
    <div className="graphite-project-overview" aria-label="Project overview minimap">
      {/* Render mini clips */}
      {allClips.map(clip => {
        const clipLeft = (clip.startTime / duration) * 100;
        const clipWidth = (clip.duration / duration) * 100;
        
        return (
          <div
            key={clip.id}
            className={`graphite-mini-clip graphite-mini-clip--${clip.trackType}`}
            style={{
              left: `${clipLeft}%`,
              width: `${Math.max(clipWidth, 0.5)}%`, // Minimum width for visibility
            }}
            title={clip.name}
          />
        );
      })}
      
      {/* Visible region indicator */}
      <div
        className="graphite-visible-region"
        style={{
          left: `${visibleLeft}%`,
          width: `${Math.max(visibleWidth, 1)}%`, // Minimum width for visibility
        }}
        aria-label={`Visible region: ${visibleStart.toFixed(1)}s to ${visibleEnd.toFixed(1)}s`}
      />
    </div>
  );
}

// --- Main Component ---

export function FooterNav({
  zoom,
  minZoom,
  maxZoom,
  duration,
  visibleStart,
  visibleEnd,
  onZoomChange,
  tracks = [],
  fps = 24,
}: FooterNavProps) {
  return (
    <footer className="graphite-footer-nav">
      {/* Zoom Controls - Requirements 8.1, 8.2 */}
      <ZoomControls
        zoom={zoom}
        minZoom={minZoom}
        maxZoom={maxZoom}
        onZoomChange={onZoomChange}
      />
      
      {/* Project Overview Minimap - Requirements 9.1, 9.2, 9.3, 9.4 */}
      <ProjectOverview
        tracks={tracks}
        duration={duration}
        visibleStart={visibleStart}
        visibleEnd={visibleEnd}
      />
      
      {/* Duration Display - Requirement 9.3 */}
      <div className="graphite-duration-display">
        DURATION: <span>{formatTimecode(duration, fps)}</span>
      </div>
    </footer>
  );
}

export default FooterNav;
````

## File: packages/frontend/components/TimelineEditor/graphite-timeline-utils.ts
````typescript
/**
 * Graphite Timeline Utility Functions
 * 
 * Core utility functions for the Graphite Timeline component including:
 * - Timecode formatting
 * - Tick interval calculation
 * - Track data transformation
 * - Selection state management
 */

import { Scene, NarrationSegment, VideoSFXPlan } from "@/types";

// --- Types ---

export interface TimelineClip {
  id: string;
  trackId: string;
  startTime: number;
  duration: number;
  name: string;
  thumbnail?: string;
  type: "video" | "audio" | "fx" | "music";
}

export interface TimelineTrack {
  id: string;
  name: string;
  type: "video" | "audio" | "fx" | "music";
  clips: TimelineClip[];
}

// ... existing interfaces ...

export interface TickInterval {
  major: number;
  minor: number;
}

// --- Timecode Formatting ---

/**
 * Formats a time value in seconds to HH:MM:SS:FF timecode format.
 * 
 * @param seconds - Time value in seconds (non-negative)
 * @param fps - Frames per second (default: 24)
 * @returns Zero-padded timecode string in HH:MM:SS:FF format
 * 
 * @example
 * formatTimecode(3661.5) // "01:01:01:12"
 * formatTimecode(0) // "00:00:00:00"
 * formatTimecode(59.99, 30) // "00:00:59:29"
 */
export function formatTimecode(seconds: number, fps: number = 24): string {
  // Ensure non-negative
  const time = Math.max(0, seconds);

  const hrs = Math.floor(time / 3600);
  const mins = Math.floor((time % 3600) / 60);
  const secs = Math.floor(time % 60);
  const frames = Math.floor((time % 1) * fps);

  return `${hrs.toString().padStart(2, "0")}:${mins.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}:${frames.toString().padStart(2, "0")}`;
}


// --- Tick Interval Calculation ---

/**
 * Calculates major and minor tick intervals based on zoom level.
 * Higher zoom levels result in finer granularity (smaller intervals).
 * 
 * The relationship is monotonic: as zoom increases, intervals decrease or stay the same.
 * 
 * @param zoom - Current zoom level (pixels per second)
 * @returns Object with major and minor tick intervals in seconds
 * 
 * @example
 * getTickInterval(100) // { major: 1, minor: 0.2 }
 * getTickInterval(50)  // { major: 5, minor: 1 }
 * getTickInterval(20)  // { major: 10, minor: 2 }
 * getTickInterval(10)  // { major: 30, minor: 5 }
 */
export function getTickInterval(zoom: number): TickInterval {
  if (zoom >= 100) return { major: 1, minor: 0.2 };   // Every second
  if (zoom >= 50) return { major: 5, minor: 1 };     // Every 5 seconds
  if (zoom >= 20) return { major: 10, minor: 2 };    // Every 10 seconds
  return { major: 30, minor: 5 };                     // Every 30 seconds
}

// ... existing code ...

export function buildTracks(
  scenes: Scene[],
  visuals: Record<string, string>,
  narrationSegments: NarrationSegment[],
  sfxPlan: VideoSFXPlan | null = null
): TimelineTrack[] {
  // Video track from scenes - calculate cumulative start times
  let videoStartTime = 0;
  const videoClips: TimelineClip[] = scenes.map((scene) => {
    const clip: TimelineClip = {
      id: scene.id,
      trackId: "video",
      startTime: videoStartTime,
      duration: scene.duration,
      name: scene.name,
      thumbnail: visuals[scene.id],
      type: "video",
    };
    videoStartTime += scene.duration;
    return clip;
  });

  // Audio track from narration segments - calculate cumulative start times
  let audioStartTime = 0;
  const audioClips: TimelineClip[] = narrationSegments.map((segment, i) => {
    const scene = scenes.find((s) => s.id === segment.sceneId);
    const clip: TimelineClip = {
      id: `audio-${segment.sceneId}`,
      trackId: "audio",
      startTime: audioStartTime,
      duration: segment.audioDuration,
      name: scene?.name || `Narration ${i + 1}`,
      type: "audio",
    };
    audioStartTime += segment.audioDuration;
    return clip;
  });

  // FX / Ambient track from SFX plan
  let fxStartTime = 0;
  const fxClips: TimelineClip[] = [];

  if (sfxPlan) {
    sfxPlan.scenes.forEach((scenePlan) => {
      // Find the narration segment or scene duration for timing
      const narration = narrationSegments.find(n => n.sceneId === scenePlan.sceneId);
      const scene = scenes.find(s => s.id === scenePlan.sceneId);
      const duration = narration?.audioDuration || scene?.duration || 0;

      if (scenePlan.ambientTrack) {
        fxClips.push({
          id: `fx-${scenePlan.sceneId}`,
          trackId: "fx",
          startTime: fxStartTime,
          duration: duration,
          name: scenePlan.ambientTrack.name,
          type: "fx",
        });
      }
      fxStartTime += duration;
    });
  }

  // Music track from generated music (Suno AI)
  const musicClips: TimelineClip[] = [];

  if (sfxPlan?.generatedMusic) {
    // Calculate total video duration for the music track
    const totalDuration = scenes.reduce((sum, s) => sum + s.duration, 0);

    musicClips.push({
      id: `music-${sfxPlan.generatedMusic.trackId}`,
      trackId: "music",
      startTime: 0,
      // Use the shorter of music duration or video duration
      duration: Math.min(sfxPlan.generatedMusic.duration, totalDuration),
      name: sfxPlan.generatedMusic.title,
      type: "music",
    });
  }

  // Build tracks array - include music track only if there's generated music
  const tracks: TimelineTrack[] = [
    { id: "video", name: "Video 01", type: "video", clips: videoClips },
    { id: "audio", name: "Narration", type: "audio", clips: audioClips },
    { id: "fx", name: "FX / Ambient", type: "fx", clips: fxClips },
  ];

  // Add music track if there's generated music
  if (musicClips.length > 0) {
    tracks.push({ id: "music", name: "Music", type: "music", clips: musicClips });
  }

  return tracks;
}


// --- Selection State Management ---

/**
 * Selection state for the timeline.
 * Supports single-selection only (Requirement 10.4).
 */
export interface SelectionState {
  /** ID of the currently selected clip, or null if none selected */
  selectedClipId: string | null;
}

/**
 * Extracts the scene ID from a clip ID.
 * Audio clips use the pattern "audio-{sceneId}", video clips use the scene ID directly.
 * 
 * @param clipId - The clip ID to extract scene ID from
 * @returns The scene ID
 * 
 * @example
 * extractSceneId("scene-1") // "scene-1"
 * extractSceneId("audio-scene-1") // "scene-1"
 */
export function extractSceneId(clipId: string): string {
  if (clipId.startsWith("audio-")) {
    return clipId.replace("audio-", "");
  }
  return clipId;
}

/**
 * Creates the initial selection state.
 * 
 * @param initialSelectedId - Optional initial selected clip ID
 * @returns Initial selection state
 */
export function createInitialSelectionState(initialSelectedId?: string | null): SelectionState {
  return {
    selectedClipId: initialSelectedId ?? null,
  };
}

/**
 * Selects a clip by ID.
 * Implements single-selection: selecting a new clip deselects the previous one.
 * (Requirement 10.1, 10.4)
 * 
 * @param state - Current selection state
 * @param clipId - ID of the clip to select
 * @returns New selection state with the clip selected
 */
export function selectClip(state: SelectionState, clipId: string): SelectionState {
  return {
    selectedClipId: clipId,
  };
}

/**
 * Clears the current selection.
 * (Requirement 10.2)
 * 
 * @param state - Current selection state
 * @returns New selection state with no clip selected
 */
export function clearSelection(state: SelectionState): SelectionState {
  return {
    selectedClipId: null,
  };
}

/**
 * Checks if a specific clip is selected.
 * 
 * @param state - Current selection state
 * @param clipId - ID of the clip to check
 * @returns True if the clip is selected
 */
export function isClipSelected(state: SelectionState, clipId: string): boolean {
  return state.selectedClipId === clipId;
}

/**
 * Handles a clip click event.
 * Selects the clicked clip and returns the scene ID for the callback.
 * (Requirement 10.1, 10.3)
 * 
 * @param state - Current selection state
 * @param clipId - ID of the clicked clip
 * @returns Object with new state and scene ID for callback
 */
export function handleClipClick(
  state: SelectionState,
  clipId: string
): { newState: SelectionState; sceneId: string } {
  const newState = selectClip(state, clipId);
  const sceneId = extractSceneId(clipId);
  return { newState, sceneId };
}

/**
 * Handles a click outside of clips (on empty lane area).
 * Clears the selection.
 * (Requirement 10.2)
 * 
 * @param state - Current selection state
 * @returns New selection state with no clip selected
 */
export function handleOutsideClick(state: SelectionState): SelectionState {
  return clearSelection(state);
}
````

## File: packages/frontend/components/TimelineEditor/graphite-timeline.css
````css
/* =============================================================================
   Graphite Timeline - CSS Foundation
   A professional dark interface for video editing with graphite textures
   Requirements: 1.1, 1.2, 1.3, 12.1 + Accessibility
   ============================================================================= */

/* -----------------------------------------------------------------------------
   CSS Custom Properties - Graphite Theme
   ----------------------------------------------------------------------------- */
:root {
  /* Graphite Color Palette - Clean dark interface */
  --graphite-bg: rgba(10, 10, 12, 0.95);
  --graphite-deep: rgba(15, 15, 18, 0.9);
  --graphite-mid: rgba(35, 35, 40, 0.6);
  --graphite-light: rgba(50, 50, 55, 0.5);
  --graphite-edge: rgba(255, 255, 255, 0.12);

  /* Track Type Colors - Distinct for each track */
  --track-video: rgba(79, 70, 229, 0.15);
  /* Indigo tint */
  --track-audio: rgba(34, 197, 94, 0.12);
  /* Green tint */
  --track-fx: rgba(251, 146, 60, 0.12);
  /* Orange tint */
  --track-music: rgba(168, 85, 247, 0.12);
  /* Purple tint */

  /* Waveform Colors */
  --waveform-audio: #22c55e;
  /* Green for narration */
  --waveform-fx: #fb923c;
  /* Orange for FX */
  --waveform-music: #a855f7;
  /* Purple for music */

  /* Accent Colors */
  --plasma-cyan: #00f2ff;
  --plasma-glow: rgba(0, 242, 255, 0.4);
  --laser-red: #ff3e3e;

  /* Focus Colors (high contrast for accessibility) */
  --focus-ring: #00f2ff;
  --focus-ring-offset: #0a0a0a;

  /* Text Colors */
  --graphite-text-main: #e0e0e0;
  --graphite-text-dim: #888;

  /* Spacing & Sizing */
  --graphite-block-radius: 6px;
  --graphite-track-height: 110px;
  --graphite-clip-height: 85px;
  --graphite-ruler-height: 35px;
  --graphite-transport-height: 80px;
  --graphite-footer-height: 60px;
  --graphite-label-width: 140px;

  /* Transitions */
  --graphite-transition-fast: 0.2s cubic-bezier(0.23, 1, 0.32, 1);
  --graphite-transition-smooth: 0.3s ease;
}

/* -----------------------------------------------------------------------------
   Screen Reader Only Utility (Accessibility)
   ----------------------------------------------------------------------------- */
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  white-space: nowrap;
  border: 0;
}

/* Extend .sr-only when it's focusable */
.sr-only-focusable:focus,
.sr-only-focusable:active {
  position: static;
  width: auto;
  height: auto;
  padding: 0;
  margin: 0;
  overflow: visible;
  clip: auto;
  white-space: normal;
}

/* -----------------------------------------------------------------------------
   Base Container Styles
   ----------------------------------------------------------------------------- */
.graphite-timeline {
  background-color: var(--graphite-bg);
  color: var(--graphite-text-main);
  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
  display: flex;
  flex-direction: column;
  position: relative;
  overflow: hidden;
  height: 100%;
  width: 100%;
}

/* Focus styles for timeline container (keyboard navigation) */
.graphite-timeline:focus {
  outline: none;
}

.graphite-timeline:focus-visible {
  outline: 2px solid var(--focus-ring);
  outline-offset: 2px;
}

.graphite-timeline--focused {
  box-shadow: 0 0 0 2px var(--focus-ring-offset), 0 0 0 4px var(--focus-ring);
}

.graphite-timeline-workspace {
  background: var(--graphite-deep);
  backdrop-filter: blur(10px);
  border: 1px solid var(--graphite-edge);
  display: flex;
  flex-direction: column;
  position: relative;
  overflow: hidden;
  flex: 1;
  border-radius: 12px;
  margin: 8px;
}

/* -----------------------------------------------------------------------------
   Noise Texture Overlay (Requirement 1.3)
   ----------------------------------------------------------------------------- */
.graphite-noise {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  opacity: 0.01;
  /* Reduced from 0.03 - much subtler */
  pointer-events: none;
  z-index: 1;
  mix-blend-mode: overlay;
}

.graphite-noise rect {
  width: 100%;
  height: 100%;
}

/* -----------------------------------------------------------------------------
   Transport Bar Styles
   ----------------------------------------------------------------------------- */
.graphite-transport-bar {
  height: var(--graphite-transport-height);
  background: rgba(0, 0, 0, 0.3);
  border-bottom: 1px solid var(--graphite-edge);
  display: flex;
  align-items: center;
  padding: 0 30px;
  justify-content: space-between;
  z-index: 10;
  flex-shrink: 0;
}

.graphite-transport-btns {
  display: flex;
  gap: 15px;
}

.graphite-btn {
  background: var(--graphite-light);
  border: 1px solid var(--graphite-edge);
  width: 45px;
  height: 45px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: var(--graphite-block-radius);
  transition: all var(--graphite-transition-fast);
  position: relative;
  color: var(--graphite-text-main);
  cursor: pointer;
}

.graphite-btn:hover {
  background: var(--graphite-edge);
  transform: translateY(-2px);
  box-shadow: 0 4px 10px rgba(0, 0, 0, 0.4);
}

.graphite-btn:active {
  transform: translateY(0);
}

/* Focus styles for buttons (keyboard navigation) */
.graphite-btn:focus {
  outline: none;
}

.graphite-btn:focus-visible {
  outline: 2px solid var(--focus-ring);
  outline-offset: 2px;
  box-shadow: 0 0 10px var(--plasma-glow);
}

.graphite-btn.primary {
  border-color: var(--plasma-cyan);
  color: var(--plasma-cyan);
}

.graphite-btn.primary:hover {
  box-shadow: 0 0 15px var(--plasma-glow), 0 4px 10px rgba(0, 0, 0, 0.4);
}

.graphite-btn.primary:focus-visible {
  box-shadow: 0 0 20px var(--plasma-glow);
}

/* Timecode Display */
.graphite-timecode-display {
  font-family: 'JetBrains Mono', monospace;
  font-size: 2rem;
  font-weight: 300;
  color: var(--plasma-cyan);
  letter-spacing: -1px;
  text-shadow: 0 0 15px var(--plasma-glow);
  background: #000;
  padding: 5px 20px;
  border-radius: var(--graphite-block-radius);
  box-shadow: inset 0 2px 10px rgba(0, 0, 0, 0.5);
  border: 1px solid var(--graphite-light);
}

/* Project Info */
.graphite-project-info {
  display: flex;
  gap: 20px;
  align-items: center;
}

.graphite-project-name {
  font-size: 10px;
  font-weight: 800;
  color: var(--graphite-text-dim);
  letter-spacing: 1px;
  text-transform: uppercase;
}

/* -----------------------------------------------------------------------------
   Timeline Area Styles
   ----------------------------------------------------------------------------- */
.graphite-timeline-area {
  flex-grow: 1;
  display: flex;
  flex-direction: column;
  position: relative;
  overflow: hidden;
  background: transparent;
}

/* Time Ruler */
.graphite-ruler {
  height: var(--graphite-ruler-height);
  min-height: 35px;
  background: var(--graphite-deep);
  border-bottom: 1px solid var(--graphite-edge);
  position: relative;
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  color: var(--graphite-text-dim);
  flex-shrink: 0;
  overflow-x: auto;
  overflow-y: hidden;
}

/* Hide scrollbars on ruler but keep functionality (Requirement 11.5) */
.graphite-ruler::-webkit-scrollbar {
  display: none;
}

.graphite-ruler {
  -ms-overflow-style: none;
  scrollbar-width: none;
}

/* Inner container for ruler content - enables scroll synchronization */
.graphite-ruler-inner {
  position: relative;
  height: 100%;
}

.graphite-ruler-mark {
  position: absolute;
  bottom: 0;
  width: 1px;
  height: 8px;
  background: var(--graphite-text-dim);
}

.graphite-ruler-mark.major {
  height: 15px;
  background: var(--graphite-text-main);
}

.graphite-ruler-label {
  position: absolute;
  top: 4px;
  font-size: 9px;
  color: var(--graphite-text-main);
  white-space: nowrap;
  pointer-events: none;
}

/* -----------------------------------------------------------------------------
   Tracks Container
   ----------------------------------------------------------------------------- */
.graphite-tracks-container {
  display: flex;
  flex: 1;
  position: relative;
  overflow: hidden;
}

/* Track Labels (Left Sidebar) */
.graphite-track-labels {
  width: var(--graphite-label-width);
  background: var(--graphite-deep);
  border-right: 2px solid #000;
  display: flex;
  flex-direction: column;
  flex-shrink: 0;
  z-index: 5;
}

/* Ruler spacer - empty area above track labels that aligns with the ruler */
.graphite-ruler-spacer {
  height: var(--graphite-ruler-height);
  background: var(--graphite-deep);
  border-bottom: 1px solid var(--graphite-edge);
  flex-shrink: 0;
}

.graphite-label-block {
  height: var(--graphite-track-height);
  padding: 15px;
  border-bottom: 1px solid var(--graphite-edge);
  display: flex;
  flex-direction: column;
  justify-content: flex-end;
}

.graphite-label-block span {
  font-size: 10px;
  font-weight: 800;
  text-transform: uppercase;
  letter-spacing: 2px;
  color: var(--graphite-text-dim);
}

/* Track Lanes */
.graphite-track-lanes {
  flex-grow: 1;
  position: relative;
  background-image: linear-gradient(var(--graphite-deep) 1px, transparent 1px);
  background-size: 100% var(--graphite-track-height);
  overflow-x: auto;
  overflow-y: hidden;
}

/* Track lanes inner container - holds clips with full timeline width */
.graphite-track-lanes-inner {
  display: flex;
  flex-direction: column;
  min-width: 100%;
}

/* Timeline content area - contains ruler and track lanes, scrolls together */
.graphite-timeline-content {
  flex: 1;
  display: flex;
  flex-direction: column;
  overflow: hidden;
  min-width: 0;
  /* Allow flex item to shrink below content size */
}

/* Hide scrollbars but keep functionality (Requirement 11.5) */
.graphite-track-lanes::-webkit-scrollbar {
  display: none;
}

.graphite-track-lanes {
  -ms-overflow-style: none;
  scrollbar-width: none;
}

.graphite-lane {
  height: var(--graphite-track-height);
  position: relative;
  display: flex;
  align-items: center;
  padding: 0 20px;
  border-bottom: 1px solid rgba(255, 255, 255, 0.02);
}

/* Track Type-Specific Background Colors (Requirement 4.5) */
.graphite-lane--video {
  background: var(--track-video);
  border-left: 3px solid rgba(79, 70, 229, 0.5);
}

.graphite-lane--audio {
  background: var(--track-audio);
  border-left: 3px solid rgba(34, 197, 94, 0.5);
}

.graphite-lane--fx {
  background: var(--track-fx);
  border-left: 3px solid rgba(251, 146, 60, 0.5);
}

.graphite-lane--music {
  background: var(--track-music);
  border-left: 3px solid rgba(168, 85, 247, 0.5);
}

/* -----------------------------------------------------------------------------
   Clip Styles (Requirement 1.4, 1.5) + Accessibility Focus Styles
   ----------------------------------------------------------------------------- */
.graphite-clip {
  position: absolute;
  height: var(--graphite-clip-height);
  background: linear-gradient(135deg, var(--graphite-light) 0%, var(--graphite-mid) 100%);
  border: 1px solid var(--graphite-edge);
  border-radius: var(--graphite-block-radius);
  box-shadow:
    4px 4px 15px rgba(0, 0, 0, 0.5),
    inset 1px 1px 0 rgba(255, 255, 255, 0.05);
  display: flex;
  flex-direction: column;
  overflow: hidden;
  transition: border-color var(--graphite-transition-smooth);
  cursor: pointer;
}

/* Hover state with cyan plasma glow (Requirement 1.4) */
.graphite-clip:hover {
  border-color: var(--plasma-cyan);
  z-index: 5;
}

/* Focus styles for clips (keyboard navigation) */
.graphite-clip:focus {
  outline: none;
}

.graphite-clip:focus-visible {
  outline: 2px solid var(--focus-ring);
  outline-offset: 2px;
  z-index: 10;
  box-shadow:
    0 0 15px var(--plasma-glow),
    4px 4px 15px rgba(0, 0, 0, 0.5);
}

/* Selected/Active state with enhanced glow (Requirement 1.5) */
.graphite-clip.active,
.graphite-clip.selected,
.graphite-clip[aria-selected="true"] {
  border-color: var(--plasma-cyan);
  box-shadow:
    0 0 20px var(--plasma-glow),
    4px 4px 15px rgba(0, 0, 0, 0.5);
}

/* Clip Thumbnail */
.graphite-clip-thumb {
  height: 55px;
  background: #000;
  width: 100%;
  opacity: 0.85;
  background-size: cover;
  background-position: center;
  transition: opacity var(--graphite-transition-smooth);
}

.graphite-clip:hover .graphite-clip-thumb,
.graphite-clip:focus-visible .graphite-clip-thumb {
  opacity: 1;
}

/* Clip Info */
.graphite-clip-info {
  padding: 6px 10px;
  display: flex;
  justify-content: space-between;
  align-items: center;
  flex: 1;
}

.graphite-clip-title {
  font-size: 10px;
  font-weight: 600;
  text-transform: uppercase;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  color: var(--graphite-text-main);
}

.graphite-clip-duration {
  font-family: 'JetBrains Mono', monospace;
  font-size: 9px;
  opacity: 0.5;
  color: var(--graphite-text-main);
}

/* -----------------------------------------------------------------------------
   Audio Clip & Waveform Styles
   ----------------------------------------------------------------------------- */
.graphite-audio-clip {
  height: 65px;
  background: rgba(20, 22, 25, 0.9);
  border: 1px solid rgba(255, 255, 255, 0.08);
}

/* Track-specific audio clip styling */
.graphite-lane--audio .graphite-audio-clip {
  background: rgba(34, 197, 94, 0.08);
  border-color: rgba(34, 197, 94, 0.25);
}

.graphite-lane--fx .graphite-audio-clip {
  background: rgba(251, 146, 60, 0.08);
  border-color: rgba(251, 146, 60, 0.25);
}

.graphite-lane--music .graphite-audio-clip {
  background: rgba(168, 85, 247, 0.1);
  border-color: rgba(168, 85, 247, 0.3);
}

/* Focus styles for audio clips */
.graphite-audio-clip:focus {
  outline: none;
}

.graphite-audio-clip:focus-visible {
  outline: 2px solid var(--focus-ring);
  outline-offset: 2px;
  z-index: 10;
}

.graphite-audio-clip[aria-selected="true"] {
  box-shadow: 0 0 15px var(--plasma-glow);
  border-color: var(--plasma-cyan);
}

.graphite-audio-wave {
  height: 42px;
  width: 100%;
  position: relative;
  display: flex;
  align-items: center;
  gap: 1px;
  padding: 0 8px;
}

/* Base waveform bar - defaults to cyan */
.graphite-wave-bar {
  width: 2px;
  background: var(--plasma-cyan);
  opacity: 0.6;
  border-radius: 1px;
}

/* Track-specific waveform colors */
.graphite-lane--audio .graphite-wave-bar {
  background: var(--waveform-audio);
  opacity: 0.7;
}

.graphite-lane--fx .graphite-wave-bar {
  background: var(--waveform-fx);
  opacity: 0.7;
}

.graphite-lane--music .graphite-wave-bar {
  background: var(--waveform-music);
  opacity: 0.7;
}

/* -----------------------------------------------------------------------------
   Playhead Styles
   ----------------------------------------------------------------------------- */
.graphite-playhead {
  position: absolute;
  top: 0;
  bottom: 0;
  width: 2px;
  background: var(--laser-red);
  box-shadow: 0 0 10px var(--laser-red);
  z-index: 100;
  pointer-events: none;
}

.graphite-playhead-handle {
  position: absolute;
  top: 0;
  left: -8px;
  width: 18px;
  height: 35px;
  background: var(--laser-red);
  clip-path: polygon(0% 0%, 100% 0%, 100% 70%, 50% 100%, 0% 70%);
}

/* -----------------------------------------------------------------------------
   Footer Navigation Styles
   ----------------------------------------------------------------------------- */
.graphite-footer-nav {
  height: var(--graphite-footer-height);
  background: var(--graphite-mid);
  border-top: 1px solid #000;
  display: flex;
  align-items: center;
  padding: 0 30px;
  gap: 40px;
  flex-shrink: 0;
}

/* Zoom Controls */
.graphite-zoom-controls {
  display: flex;
  align-items: center;
  gap: 15px;
}

.graphite-zoom-slider {
  width: 150px;
  height: 4px;
  background: #000;
  border-radius: 2px;
  position: relative;
}

.graphite-zoom-handle {
  position: absolute;
  width: 14px;
  height: 14px;
  background: var(--graphite-text-main);
  border-radius: 50%;
  top: -5px;
  box-shadow: 0 0 10px rgba(255, 255, 255, 0.3);
  cursor: pointer;
  transition: transform var(--graphite-transition-fast);
}

.graphite-zoom-handle:hover {
  transform: scale(1.2);
}

.graphite-zoom-handle:focus {
  outline: none;
}

.graphite-zoom-handle:focus-visible {
  outline: 2px solid var(--focus-ring);
  outline-offset: 2px;
}

/* Project Overview Minimap */
.graphite-project-overview {
  flex-grow: 1;
  height: 30px;
  background: #000;
  border-radius: var(--graphite-block-radius);
  position: relative;
  overflow: hidden;
  border: 1px solid var(--graphite-light);
}

.graphite-mini-clip {
  position: absolute;
  height: 100%;
  background: var(--graphite-edge);
  opacity: 0.5;
}

.graphite-visible-region {
  position: absolute;
  top: 0;
  height: 100%;
  border: 1px solid #fff;
  opacity: 0.3;
  background: rgba(255, 255, 255, 0.05);
}

/* Duration Display */
.graphite-duration-display {
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  color: var(--graphite-text-dim);
  white-space: nowrap;
}

.graphite-duration-display span {
  color: #fff;
}

/* Small Button Variant for Zoom Controls */
.graphite-btn-small {
  width: 28px;
  height: 28px;
  padding: 0;
}

.graphite-btn-small:disabled {
  opacity: 0.4;
  cursor: not-allowed;
  transform: none;
}

.graphite-btn-small:disabled:hover {
  background: var(--graphite-light);
  transform: none;
  box-shadow: none;
}

/* Mini Clip Track Type Colors */
.graphite-mini-clip--video {
  background: var(--graphite-edge);
}

.graphite-mini-clip--audio {
  background: var(--plasma-cyan);
  opacity: 0.3;
}

.graphite-mini-clip--fx {
  background: var(--laser-red);
  opacity: 0.3;
}

.graphite-mini-clip--music {
  background: #8a2be2;
  opacity: 0.3;
}

/* -----------------------------------------------------------------------------
   Animation Keyframes (Requirement 12.1)
   ----------------------------------------------------------------------------- */
@keyframes graphite-reveal {
  from {
    opacity: 0;
    transform: translateY(10px);
  }

  to {
    opacity: 1;
    transform: translateY(0);
  }
}

@keyframes graphite-fade-in {
  from {
    opacity: 0;
  }

  to {
    opacity: 1;
  }
}

@keyframes graphite-glow-pulse {

  0%,
  100% {
    box-shadow: 0 0 10px var(--plasma-glow);
  }

  50% {
    box-shadow: 0 0 20px var(--plasma-glow);
  }
}

/* Focus indicator pulse animation */
@keyframes graphite-focus-pulse {

  0%,
  100% {
    box-shadow: 0 0 0 2px var(--focus-ring-offset), 0 0 0 4px var(--focus-ring);
  }

  50% {
    box-shadow: 0 0 0 2px var(--focus-ring-offset), 0 0 0 6px var(--focus-ring);
  }
}

/* Staggered reveal animation on mount */
.graphite-timeline-workspace>* {
  animation: graphite-reveal 0.8s cubic-bezier(0.16, 1, 0.3, 1) both;
}

.graphite-transport-bar {
  animation-delay: 0.1s;
}

.graphite-timeline-area {
  animation-delay: 0.2s;
}

.graphite-footer-nav {
  animation-delay: 0.3s;
}

/* Clip hover transition effects (Requirement 12.2) */
.graphite-clip {
  transition:
    border-color var(--graphite-transition-smooth),
    box-shadow var(--graphite-transition-smooth),
    transform var(--graphite-transition-fast);
}

/* Button hover lift effect (Requirement 12.4) */
.graphite-btn {
  transition:
    background var(--graphite-transition-fast),
    transform var(--graphite-transition-fast),
    box-shadow var(--graphite-transition-fast);
}

/* Playhead smooth transition for click-to-seek (Requirement 12.3) */
.graphite-playhead.animating {
  transition: left 0.2s cubic-bezier(0.23, 1, 0.32, 1);
}

/* -----------------------------------------------------------------------------
   High Contrast Mode Support (Accessibility)
   ----------------------------------------------------------------------------- */
@media (prefers-contrast: high) {
  :root {
    --graphite-edge: #555;
    --plasma-cyan: #00ffff;
    --focus-ring: #ffff00;
  }

  .graphite-clip:focus-visible {
    outline-width: 3px;
  }

  .graphite-btn:focus-visible {
    outline-width: 3px;
  }
}

/* -----------------------------------------------------------------------------
   Reduced Motion Support (Accessibility)
   ----------------------------------------------------------------------------- */
@media (prefers-reduced-motion: reduce) {

  .graphite-timeline-workspace>*,
  .graphite-clip,
  .graphite-btn,
  .graphite-playhead.animating {
    animation: none;
    transition: none;
  }

  .graphite-zoom-handle {
    transition: none;
  }
}

/* -----------------------------------------------------------------------------
   Utility Classes
   ----------------------------------------------------------------------------- */
.graphite-mono {
  font-family: 'JetBrains Mono', monospace;
}

.graphite-uppercase {
  text-transform: uppercase;
  letter-spacing: 2px;
}

.graphite-text-dim {
  color: var(--graphite-text-dim);
}

.graphite-text-cyan {
  color: var(--plasma-cyan);
}

.graphite-glow-cyan {
  text-shadow: 0 0 15px var(--plasma-glow);
}

/* -----------------------------------------------------------------------------
   Skip Link for Keyboard Navigation (Accessibility)
   ----------------------------------------------------------------------------- */
.graphite-skip-link {
  position: absolute;
  top: -40px;
  left: 0;
  background: var(--plasma-cyan);
  color: var(--graphite-bg);
  padding: 8px 16px;
  z-index: 1000;
  font-weight: bold;
  text-decoration: none;
}

.graphite-skip-link:focus {
  top: 0;
}
````

## File: packages/frontend/components/TimelineEditor/GraphiteClip.tsx
````typescript
/**
 * GraphiteClip Component
 * 
 * Renders a video clip in the Graphite Timeline with:
 * - Thumbnail with luminosity blend mode
 * - Clip title (uppercase, truncated)
 * - Duration badge
 * - Hover and selected states with plasma glow
 * 
 * Requirements: 5.1, 5.2, 5.3, 5.4, 5.6, 1.4, 1.5
 */

import { TimelineClip } from "./graphite-timeline-utils";
import "./graphite-timeline.css";

// --- Types ---

export interface GraphiteClipProps {
  /** Clip data containing id, name, duration, thumbnail, etc. */
  clip: TimelineClip;
  /** Current zoom level (pixels per second) */
  zoom: number;
  /** Whether this clip is currently selected */
  isSelected: boolean;
  /** Callback when clip is clicked */
  onClick: () => void;
  /** Optional left position override (otherwise calculated from startTime * zoom) */
  left?: number;
}

// --- Helper Functions ---

/**
 * Formats duration in seconds to a display string (e.g., "4.2s")
 */
function formatDuration(seconds: number): string {
  return `${seconds.toFixed(1)}s`;
}

// --- Component ---

export function GraphiteClip({
  clip,
  zoom,
  isSelected,
  onClick,
  left,
}: GraphiteClipProps) {
  // Calculate width based on duration and zoom level
  const width = clip.duration * zoom;
  
  // Calculate left position from startTime if not provided
  const leftPosition = left ?? clip.startTime * zoom;

  // Build class names for hover/selected states
  const classNames = [
    "graphite-clip",
    isSelected ? "selected" : "",
  ].filter(Boolean).join(" ");

  return (
    <div
      className={classNames}
      style={{
        left: `${leftPosition}px`,
        width: `${width}px`,
      }}
      onClick={(e) => {
        e.stopPropagation();
        onClick();
      }}
      role="button"
      tabIndex={0}
      aria-label={`Clip: ${clip.name}`}
      aria-selected={isSelected}
      onKeyDown={(e) => {
        if (e.key === "Enter" || e.key === " ") {
          e.preventDefault();
          onClick();
        }
      }}
    >
      {/* Thumbnail with luminosity blend mode - Requirements 5.1, 5.4, 5.6 */}
      <div
        className="graphite-clip-thumb"
        style={{
          backgroundImage: clip.thumbnail ? `url(${clip.thumbnail})` : undefined,
          backgroundColor: clip.thumbnail ? undefined : "var(--graphite-mid)",
        }}
        aria-hidden="true"
      />

      {/* Clip Info - Requirements 5.2, 5.3 */}
      <div className="graphite-clip-info">
        <span className="graphite-clip-title" title={clip.name}>
          {clip.name}
        </span>
        <span className="graphite-clip-duration">
          {formatDuration(clip.duration)}
        </span>
      </div>
    </div>
  );
}

export default GraphiteClip;
````

## File: packages/frontend/components/TimelineEditor/GraphiteTimeline.tsx
````typescript
/**
 * GraphiteTimeline Component
 * 
 * Main timeline component that assembles all sub-components:
 * - TransportBar: Playback controls and timecode display
 * - TimeRuler: Time scale with major/minor ticks
 * - TrackLabel: Track names in left sidebar
 * - TrackLane: Track lanes with clips
 * - Playhead: Current position indicator
 * - FooterNav: Zoom controls and project overview
 * 
 * Accessibility Features:
 * - Full keyboard navigation via useTimelineKeyboard hook
 * - ARIA roles and labels for screen readers
 * - Focus management and visual indicators
 * - Live regions for time announcements
 * 
 * Requirements: All (1.1-12.1) + Accessibility
 */

import { useState, useMemo, useCallback, useEffect, useRef } from "react";
import { Scene, NarrationSegment } from "@/types";
import { buildTracks, TimelineClip } from "./graphite-timeline-utils";
import { TransportBar } from "./TransportBar";
import { TimeRuler } from "./TimeRuler";
import { TrackLabel } from "./TrackLabel";
import { TrackLane } from "./TrackLane";
import { Playhead, usePlayheadSeek } from "./Playhead";
import { FooterNav } from "./FooterNav";
import { useTimelineScroll } from "./useTimelineScroll";
import { useTimelineSelection } from "@/hooks/useTimelineSelection";
import { useTimelineKeyboard } from "@/hooks/useTimelineKeyboard";
import "./graphite-timeline.css";

// --- Types ---

export interface GraphiteTimelineProps {
  /** Array of Scene objects from content plan */
  scenes: Scene[];
  /** Map of scene IDs to thumbnail URLs */
  visuals?: Record<string, string>;
  /** Array of NarrationSegment objects */
  narrationSegments?: NarrationSegment[];
  /** Current playback time in seconds */
  currentTime: number;
  /** Total duration in seconds */
  duration: number;
  /** Whether playback is currently active */
  isPlaying: boolean;
  /** Callback when play/pause is toggled */
  onPlayPause: () => void;
  /** Callback when user seeks to a new position */
  onSeek: (time: number) => void;
  /** Callback when a scene is selected */
  onSceneSelect?: (sceneId: string) => void;
  /** Currently selected scene ID */
  selectedSceneId?: string | null;
  /** Project name to display in transport bar */
  projectName?: string;
  /** Optional additional CSS class */
  className?: string;
  /** SFX plan for ambient tracks */
  sfxPlan?: import("@/types").VideoSFXPlan | null;
  /** Callback when a clip should be deleted */
  onDeleteClip?: (clipId: string) => void;
}

// --- Constants ---

const MIN_ZOOM = 10; // pixels per second
const MAX_ZOOM = 200;
const DEFAULT_ZOOM = 50;
const SKIP_INTERVAL = 5; // seconds

// --- Helper Functions ---

/**
 * Formats time in seconds to a human-readable string for screen readers
 * @param seconds - Time in seconds
 * @returns Formatted string like "1 minute 30 seconds"
 */
function formatTimeForScreenReader(seconds: number): string {
  const mins = Math.floor(seconds / 60);
  const secs = Math.floor(seconds % 60);

  if (mins === 0) {
    return `${secs} second${secs !== 1 ? 's' : ''}`;
  }
  return `${mins} minute${mins !== 1 ? 's' : ''} ${secs} second${secs !== 1 ? 's' : ''}`;
}


// --- Noise Texture SVG ---

const NoiseTexture = () => (
  <svg className="graphite-noise" aria-hidden="true">
    <filter id="graphite-noise-filter">
      <feTurbulence
        type="fractalNoise"
        baseFrequency="0.8"
        numOctaves="4"
        stitchTiles="stitch"
      />
    </filter>
    <rect filter="url(#graphite-noise-filter)" />
  </svg>
);

// --- Component ---

export function GraphiteTimeline({
  scenes,
  visuals = {},
  narrationSegments = [],
  currentTime,
  duration,
  isPlaying,
  onPlayPause,
  onSeek,
  onSceneSelect,
  selectedSceneId,
  projectName = "UNTITLED",
  className = "",
  sfxPlan = null,
  onDeleteClip,
}: GraphiteTimelineProps) {
  // --- State ---
  const [zoom, setZoom] = useState(DEFAULT_ZOOM);
  const [isFocused, setIsFocused] = useState(false);

  // --- Refs ---
  const timelineRef = useRef<HTMLDivElement>(null);
  const liveRegionRef = useRef<HTMLDivElement>(null);

  // --- Build Tracks ---
  // Memoize visuals to prevent unnecessary rebuilds when object reference changes but content is same
  const visualsJson = useMemo(() => JSON.stringify(visuals), [visuals]);

  const tracks = useMemo(() => {
    let parsedVisuals: Record<string, string> = {};
    try {
      parsedVisuals = JSON.parse(visualsJson);
    } catch (e) {
      console.error("[GraphiteTimeline] Failed to parse visualsJson:", e);
    }
    const t = buildTracks(scenes, parsedVisuals, narrationSegments, sfxPlan);
    // Only log in development and limit frequency
    if (process.env.NODE_ENV === 'development') {
      console.log("[GraphiteTimeline] Tracks built:", t.length, "tracks,", t.find(tr => tr.id === "fx")?.clips.length || 0, "FX clips");
    }
    return t;
  }, [scenes, visualsJson, narrationSegments, sfxPlan]);

  // --- Flatten all clips for index-based navigation ---
  const allClips = useMemo(() => {
    const clips: Array<TimelineClip & { trackId: string }> = [];
    tracks.forEach(track => {
      track.clips.forEach(clip => {
        clips.push({ ...clip, trackId: track.id });
      });
    });
    // Sort by start time for logical navigation order
    return clips.sort((a, b) => a.startTime - b.startTime);
  }, [tracks]);

  // --- Scroll Management ---
  const {
    scrollLeft,
    trackLanesRef,
    rulerRef,
    handleScroll,
    getVisibleRange,
  } = useTimelineScroll({
    currentTime,
    duration,
    zoom,
    isPlaying,
  });

  // --- Selection Management ---
  const {
    selectedClipId,
    handleSelectClip,
    handleClearSelection,
    setSelectedClipId,
  } = useTimelineSelection({
    initialSelectedId: selectedSceneId,
    onSceneSelect,
  });

  // --- Get selected clip index ---
  const selectedClipIndex = useMemo(() => {
    if (!selectedClipId) return null;
    const index = allClips.findIndex(clip => clip.id === selectedClipId);
    return index >= 0 ? index : null;
  }, [selectedClipId, allClips]);

  // Sync external selectedSceneId with internal state
  useEffect(() => {
    if (selectedSceneId !== undefined) {
      // The selectedSceneId could be a scene ID or null
      // We need to check if it matches any clip ID
    }
  }, [selectedSceneId]);

  // --- Playhead Seek ---
  const {
    isAnimating,
    handleMouseDown: handleSeekMouseDown,
    handleClick: handleSeekClick,
  } = usePlayheadSeek({
    zoom,
    scrollLeft,
    duration,
    onSeek,
  });

  // --- Transport Handlers ---
  const handleSkipBack = useCallback(() => {
    onSeek(Math.max(0, currentTime - SKIP_INTERVAL));
  }, [currentTime, onSeek]);

  const handleSkipForward = useCallback(() => {
    onSeek(Math.min(duration, currentTime + SKIP_INTERVAL));
  }, [currentTime, duration, onSeek]);

  // --- Zoom Handler ---
  const handleZoomChange = useCallback((newZoom: number) => {
    setZoom(Math.max(MIN_ZOOM, Math.min(MAX_ZOOM, newZoom)));
  }, []);

  // --- Keyboard Navigation Handlers ---
  const handleNextClip = useCallback(() => {
    if (allClips.length === 0) return;
    const nextIndex = selectedClipIndex === null ? 0 : Math.min(selectedClipIndex + 1, allClips.length - 1);
    const clip = allClips[nextIndex];
    if (clip) {
      setSelectedClipId(clip.id);
      // Also seek to the clip start
      onSeek(clip.startTime);
    }
  }, [allClips, selectedClipIndex, setSelectedClipId, onSeek]);

  const handlePrevClip = useCallback(() => {
    if (allClips.length === 0) return;
    const prevIndex = selectedClipIndex === null ? allClips.length - 1 : Math.max(selectedClipIndex - 1, 0);
    const clip = allClips[prevIndex];
    if (clip) {
      setSelectedClipId(clip.id);
      // Also seek to the clip start
      onSeek(clip.startTime);
    }
  }, [allClips, selectedClipIndex, setSelectedClipId, onSeek]);

  const handleDeleteSelectedClip = useCallback((index: number) => {
    const clip = allClips[index];
    if (clip && onDeleteClip) {
      onDeleteClip(clip.id);
      // Clear selection after delete
      setSelectedClipId(null);
    }
  }, [allClips, onDeleteClip, setSelectedClipId]);

  const handleJumpToStart = useCallback(() => {
    onSeek(0);
  }, [onSeek]);

  const handleJumpToEnd = useCallback(() => {
    onSeek(duration);
  }, [onSeek, duration]);

  // --- Integrate Keyboard Navigation ---
  useTimelineKeyboard({
    isActive: isFocused,
    duration,
    currentTime,
    isPlaying,
    selectedClipIndex,
    clipCount: allClips.length,
    onTimeChange: onSeek,
    onPlayPause,
    onSelectClip: (index) => {
      if (index === null) {
        setSelectedClipId(null);
      } else {
        const clip = allClips[index];
        if (clip) {
          setSelectedClipId(clip.id);
        }
      }
    },
    onDeleteClip: onDeleteClip ? handleDeleteSelectedClip : undefined,
    onNextClip: handleNextClip,
    onPrevClip: handlePrevClip,
    onJumpToStart: handleJumpToStart,
    onJumpToEnd: handleJumpToEnd,
  });

  // --- Live Region Updates for Screen Readers ---
  const lastAnnouncedTimeRef = useRef<number>(currentTime);

  useEffect(() => {
    // Only announce significant time changes (more than 0.5 seconds)
    // to avoid flooding screen readers during playback
    if (Math.abs(currentTime - lastAnnouncedTimeRef.current) >= 0.5 && !isPlaying) {
      lastAnnouncedTimeRef.current = currentTime;
      // The live region will be read by screen readers
    }
  }, [currentTime, isPlaying]);

  // --- Focus Handler ---
  const handleFocus = useCallback(() => {
    setIsFocused(true);
  }, []);

  const handleBlur = useCallback((e: React.FocusEvent) => {
    // Only blur if focus moved outside the timeline container
    if (!timelineRef.current?.contains(e.relatedTarget as Node)) {
      setIsFocused(false);
    }
  }, []);

  // --- Visible Range for Minimap ---
  const visibleRange = getVisibleRange();

  // --- Calculate Timeline Width ---
  const timelineWidth = Math.max(duration * zoom, 800);

  // --- Calculate Track Container Height ---
  const trackContainerHeight = tracks.length * 120; // --graphite-track-height

  // --- Class Names ---
  const containerClassNames = [
    "graphite-timeline",
    isFocused && "graphite-timeline--focused",
    className,
  ].filter(Boolean).join(" ");

  return (
    <div
      ref={timelineRef}
      className={containerClassNames}
      tabIndex={0}
      role="application"
      aria-label={`Timeline editor for ${projectName}. Use arrow keys to navigate time, Tab to select clips, Space to play/pause.`}
      aria-describedby="timeline-instructions"
      aria-activedescendant={selectedClipId || undefined}
      onFocus={handleFocus}
      onBlur={handleBlur}
    >
      {/* Screen reader instructions (visually hidden) */}
      <div id="timeline-instructions" className="sr-only">
        Press Space or K to play/pause. Use left/right arrows to move 1 second, Shift+arrows for 5 seconds, Ctrl+arrows for frame-by-frame.
        Press Home to jump to start, End to jump to end. Tab to navigate clips, Delete to remove selected clip, Escape to deselect.
        Number keys 0-9 jump to that percentage of the timeline.
      </div>

      {/* Live region for time announcements (screen readers) */}
      <div
        ref={liveRegionRef}
        role="status"
        aria-live="polite"
        aria-atomic="true"
        className="sr-only"
      >
        {!isPlaying && `Current time: ${formatTimeForScreenReader(currentTime)} of ${formatTimeForScreenReader(duration)}`}
        {isPlaying && 'Playing'}
      </div>

      {/* Selected clip announcement for screen readers */}
      {selectedClipId && (
        <div
          role="status"
          aria-live="assertive"
          aria-atomic="true"
          className="sr-only"
        >
          {(() => {
            const clip = allClips.find(c => c.id === selectedClipId);
            if (clip) {
              return `Selected clip: ${clip.name}, duration ${formatTimeForScreenReader(clip.duration)}`;
            }
            return '';
          })()}
        </div>
      )}

      <div className="graphite-timeline-workspace">
        {/* Noise Texture Overlay - Requirement 1.3 */}
        <NoiseTexture />

        {/* Transport Bar - Requirements 2.1-2.6 */}
        <TransportBar
          currentTime={currentTime}
          isPlaying={isPlaying}
          projectName={projectName}
          onPlayPause={onPlayPause}
          onSkipBack={handleSkipBack}
          onSkipForward={handleSkipForward}
        />

        {/* Timeline Area */}
        <div
          className="graphite-timeline-area"
          role="region"
          aria-label="Timeline tracks"
        >
          {/* Tracks Container - holds labels and lanes side by side */}
          <div className="graphite-tracks-container">
            {/* Track Labels (Left Sidebar) - Requirement 4.2, 11.3 */}
            <div className="graphite-track-labels" role="group" aria-label="Track labels">
              {/* Ruler spacer to align with ruler */}
              <div className="graphite-ruler-spacer" aria-hidden="true" />
              {tracks.map((track) => (
                <TrackLabel key={track.id} name={track.name} />
              ))}
            </div>

            {/* Timeline Content (Ruler + Track Lanes) */}
            <div className="graphite-timeline-content">
              {/* Time Ruler - Requirements 3.1-3.5 */}
              <TimeRuler
                ref={rulerRef}
                duration={duration}
                zoom={zoom}
                scrollLeft={scrollLeft}
              />

              {/* Track Lanes Container */}
              <div
                ref={trackLanesRef}
                className="graphite-track-lanes"
                onScroll={handleScroll}
                role="list"
                aria-label="Track lanes with clips"
              >
                <div
                  className="graphite-track-lanes-inner"
                  style={{ width: `${timelineWidth}px` }}
                >
                  {tracks.map((track) => (
                    <TrackLane
                      key={track.id}
                      track={track}
                      zoom={zoom}
                      scrollLeft={0} // Clips position relative to inner container
                      selectedClipId={selectedClipId}
                      onClipSelect={handleSelectClip}
                      onLaneClick={handleClearSelection}
                      onSeekClick={handleSeekClick}
                      onSeekMouseDown={handleSeekMouseDown}
                    />
                  ))}
                </div>

                {/* Playhead - Requirements 7.1-7.6 */}
                <Playhead
                  currentTime={currentTime}
                  zoom={zoom}
                  scrollLeft={scrollLeft}
                  height={trackContainerHeight}
                  duration={duration}
                  isAnimating={isAnimating}
                />
              </div>
            </div>
          </div>
        </div>

        {/* Footer Navigation - Requirements 8.1-9.4 */}
        <FooterNav
          zoom={zoom}
          minZoom={MIN_ZOOM}
          maxZoom={MAX_ZOOM}
          duration={duration}
          visibleStart={visibleRange.start}
          visibleEnd={visibleRange.end}
          onZoomChange={handleZoomChange}
          tracks={tracks}
        />
      </div>
    </div>
  );
}

export default GraphiteTimeline;

// Backward compatibility alias - can be used as drop-in replacement for VideoTimeline
export { GraphiteTimeline as VideoTimeline };
````

## File: packages/frontend/components/TimelineEditor/index.ts
````typescript
/**
 * TimelineEditor - Barrel Export
 * 
 * Groups all timeline-related components for the video editing interface.
 * Provides a clean public API while hiding internal implementation details.
 * 
 * The AudioTimelineEditor is the new modernized timeline component that replaces
 * GraphiteTimeline while maintaining backward compatibility.
 */

// New AudioTimelineEditor component (recommended)
export { AudioTimelineEditor } from "./AudioTimelineEditor";
export type { AudioTimelineEditorProps } from "./AudioTimelineEditor";

// Backward-compatible aliases - AudioTimelineEditor exported as GraphiteTimeline and VideoTimeline
export { AudioTimelineEditor as GraphiteTimeline } from "./AudioTimelineEditor";
export { AudioTimelineEditor as VideoTimeline } from "./AudioTimelineEditor";
export type { AudioTimelineEditorProps as GraphiteTimelineProps } from "./AudioTimelineEditor";

// Adapter functions for data conversion
export {
  scenesToVideoTrack,
  narrationToAudioTrack,
  sfxPlanToTracks,
  clipIdToSceneId,
  sceneIdToClipId,
  convertToTimelineData,
  TRACK_IDS,
  CLIP_PREFIXES,
} from "./timelineAdapter";

// Accessibility helpers
export {
  formatDurationForAnnouncement,
  announceToScreenReader,
} from "./AudioTimelineEditor";

// Editor sub-components (exported for advanced customization)
export {
  TrackSidebar,
  VideoPreview,
  TimelinePanel,
  TimelineControls,
  ImportMediaModal,
  WaveformClip,
  SubtitleClip,
  VideoClipComponent,
  ImageClipComponent,
} from "./editor";

// Legacy components (deprecated - use AudioTimelineEditor instead)
// These are kept for backward compatibility with existing code
export { TransportBar } from "./TransportBar";
export { TimeRuler } from "./TimeRuler";
export { TrackLabel } from "./TrackLabel";
export { TrackLane } from "./TrackLane";
export { Playhead, usePlayheadSeek } from "./Playhead";
export { FooterNav } from "./FooterNav";
export { GraphiteClip } from "./GraphiteClip";
export { AudioClip } from "./AudioClip";

// Legacy utilities
export { buildTracks } from "./graphite-timeline-utils";
export { useTimelineScroll } from "./useTimelineScroll";
````

## File: packages/frontend/components/TimelineEditor/Playhead.tsx
````typescript
/**
 * Playhead Component
 * 
 * Renders the current playback position indicator in the Graphite Timeline.
 * Features:
 * - Red vertical line with glow shadow
 * - Triangular handle at the top
 * - Position based on currentTime, zoom, and scrollLeft
 * - Smooth animation when clicking to seek
 * 
 * Requirements: 7.1, 7.2, 7.3, 7.4, 7.6
 */

import React, { useRef, useCallback, useState, useEffect } from "react";
import "./graphite-timeline.css";

// --- Types ---

export interface PlayheadProps {
  /** Current playback time in seconds */
  currentTime: number;
  /** Zoom level (pixels per second) */
  zoom: number;
  /** Horizontal scroll offset in pixels */
  scrollLeft: number;
  /** Height of the playhead line (typically track container height) */
  height: number;
  /** Total duration of the timeline in seconds */
  duration: number;
  /** Whether to animate position changes (for click-to-seek) */
  isAnimating?: boolean;
}

// --- Component ---

export function Playhead({
  currentTime,
  zoom,
  scrollLeft,
  height,
  duration,
  isAnimating = false,
}: PlayheadProps) {
  // Calculate the left position based on currentTime and zoom
  // The playhead position is relative to the track lanes container
  const leftPosition = currentTime * zoom - scrollLeft;

  // Build class names for animation state
  const classNames = [
    "graphite-playhead",
    isAnimating ? "animating" : "",
  ].filter(Boolean).join(" ");

  return (
    <div
      className={classNames}
      style={{
        left: `${leftPosition}px`,
        height: `${height}px`,
      }}
      role="slider"
      aria-label="Playhead position"
      aria-valuenow={currentTime}
      aria-valuemin={0}
      aria-valuemax={duration}
    >
      {/* Triangular handle at the top - Requirement 7.1 */}
      <div className="graphite-playhead-handle" aria-hidden="true" />
    </div>
  );
}

// --- Seek Handler Hook ---

export interface UsePlayheadSeekOptions {
  /** Zoom level (pixels per second) */
  zoom: number;
  /** Horizontal scroll offset in pixels */
  scrollLeft: number;
  /** Total duration of the timeline in seconds */
  duration: number;
  /** Callback when user seeks to a new position */
  onSeek: (time: number) => void;
}

export interface UsePlayheadSeekResult {
  /** Whether the user is currently dragging */
  isDragging: boolean;
  /** Whether the playhead should animate (after click, not during drag) */
  isAnimating: boolean;
  /** Handler for mousedown events on the track lanes */
  handleMouseDown: (e: React.MouseEvent) => void;
  /** Handler for click events on the track lanes (click-to-seek) */
  handleClick: (e: React.MouseEvent) => void;
}

/**
 * Custom hook for handling playhead seek interactions.
 * Supports both click-to-seek and drag-to-scrub functionality.
 * 
 * Requirements: 7.3, 7.4, 7.6
 */
export function usePlayheadSeek({
  zoom,
  scrollLeft,
  duration,
  onSeek,
}: UsePlayheadSeekOptions): UsePlayheadSeekResult {
  const [isDragging, setIsDragging] = useState(false);
  const [isAnimating, setIsAnimating] = useState(false);
  const containerRef = useRef<HTMLElement | null>(null);

  /**
   * Calculates time from a mouse position relative to the track lanes.
   * Clamps the result to [0, duration] range.
   * Requirement 7.3: Clamp to valid range
   */
  const calculateTimeFromPosition = useCallback(
    (clientX: number, container: HTMLElement): number => {
      const rect = container.getBoundingClientRect();
      const x = clientX - rect.left + scrollLeft;
      const time = x / zoom;
      // Clamp to valid range [0, duration]
      return Math.max(0, Math.min(duration, time));
    },
    [zoom, scrollLeft, duration]
  );

  /**
   * Handle click-to-seek with animation.
   * Requirement 7.3: Click to jump playhead
   * Requirement 7.6: Animate smooth transitions on click
   */
  const handleClick = useCallback(
    (e: React.MouseEvent) => {
      // Don't handle if clicking on a clip
      if ((e.target as HTMLElement).closest(".graphite-clip")) {
        return;
      }

      const container = e.currentTarget as HTMLElement;
      const time = calculateTimeFromPosition(e.clientX, container);
      
      // Enable animation for click-to-seek
      setIsAnimating(true);
      onSeek(time);
      
      // Disable animation after transition completes
      setTimeout(() => setIsAnimating(false), 200);
    },
    [calculateTimeFromPosition, onSeek]
  );

  /**
   * Handle mousedown for drag-to-scrub.
   * Requirement 7.4: Drag to scrub
   */
  const handleMouseDown = useCallback(
    (e: React.MouseEvent) => {
      // Don't handle if clicking on a clip
      if ((e.target as HTMLElement).closest(".graphite-clip")) {
        return;
      }

      // Only handle left mouse button
      if (e.button !== 0) return;

      const container = e.currentTarget as HTMLElement;
      containerRef.current = container;
      setIsDragging(true);
      setIsAnimating(false); // No animation during drag

      // Initial seek on mousedown
      const time = calculateTimeFromPosition(e.clientX, container);
      onSeek(time);
    },
    [calculateTimeFromPosition, onSeek]
  );

  // Handle mousemove and mouseup for drag-to-scrub
  useEffect(() => {
    if (!isDragging) return;

    const handleMouseMove = (e: MouseEvent) => {
      if (!containerRef.current) return;
      const time = calculateTimeFromPosition(e.clientX, containerRef.current);
      onSeek(time);
    };

    const handleMouseUp = () => {
      setIsDragging(false);
      containerRef.current = null;
    };

    // Add listeners to document for drag outside container
    document.addEventListener("mousemove", handleMouseMove);
    document.addEventListener("mouseup", handleMouseUp);

    return () => {
      document.removeEventListener("mousemove", handleMouseMove);
      document.removeEventListener("mouseup", handleMouseUp);
    };
  }, [isDragging, calculateTimeFromPosition, onSeek]);

  return {
    isDragging,
    isAnimating,
    handleMouseDown,
    handleClick,
  };
}

export default Playhead;
````

## File: packages/frontend/components/TimelineEditor/timelineAdapter.ts
````typescript
/**
 * Timeline Adapter Functions
 *
 * Converts between the existing data model (Scene, NarrationSegment, VideoSFXPlan)
 * and the new AudioTimelineEditor's internal data model (Track, AudioClip, VideoClip).
 *
 * This adapter layer enables backward compatibility, allowing the new timeline component
 * to be used as a drop-in replacement without requiring changes to the rest of the application.
 *
 * @see .kiro/specs/timeline-editor-replacement/design.md for architecture details
 * @requirements 9.1, 9.2, 9.3, 9.4, 9.5
 */

import type { Scene, NarrationSegment, VideoSFXPlan } from "@/types";
import type { Track, AudioClip, VideoClip } from "@/types/audio-editor";

/** Track ID constants for consistent referencing */
export const TRACK_IDS = {
  VIDEO: "video-track",
  NARRATOR: "narrator-track",
  SFX: "sfx-track",
  AMBIENT: "ambient-track",
  MUSIC: "music-track",
} as const;

/** Clip ID prefixes for different track types */
export const CLIP_PREFIXES = {
  VIDEO: "video-",
  AUDIO: "audio-",
  SFX: "sfx-",
  AMBIENT: "ambient-",
  MUSIC: "music-",
} as const;

/**
 * Generates placeholder waveform data for audio clips.
 * In a real implementation, this would be extracted from the actual audio.
 *
 * @param duration - Duration in seconds
 * @param samplesPerSecond - Number of waveform samples per second
 * @returns Array of normalized waveform values (0-1)
 */
function generatePlaceholderWaveform(
  duration: number,
  samplesPerSecond: number = 10
): number[] {
  const sampleCount = Math.max(1, Math.floor(duration * samplesPerSecond));
  const waveform: number[] = [];

  for (let i = 0; i < sampleCount; i++) {
    // Generate a pseudo-random but deterministic waveform pattern
    const t = i / sampleCount;
    const value = 0.3 + 0.4 * Math.sin(t * Math.PI * 4) + 0.3 * Math.random();
    waveform.push(Math.min(1, Math.max(0, value)));
  }

  return waveform;
}

/**
 * Converts Scene[] to a video Track with VideoClip[].
 *
 * Conversion logic:
 * - Scene.id → VideoClip.id (direct mapping)
 * - Scene.name → VideoClip.name (direct mapping)
 * - Scene.duration → VideoClip.duration (direct mapping)
 * - visuals[scene.id] → VideoClip.thumbnailUrl (lookup from visuals map)
 * - Cumulative scene durations → VideoClip.startTime (sum of previous scene durations)
 *
 * @param scenes - Array of Scene objects from ContentPlan
 * @param visuals - Map of scene IDs to thumbnail URLs
 * @returns Object containing the video track and array of video clips
 *
 * @requirements 9.1, 9.5
 * @validates Requirements 1.1, 1.2, 1.3, 1.4
 */
export function scenesToVideoTrack(
  scenes: Scene[],
  visuals: Record<string, string>
): { track: Track; clips: VideoClip[] } {
  const track: Track = {
    id: TRACK_IDS.VIDEO,
    type: "video",
    name: "Video",
    text: "",
    isGenerated: scenes.length > 0,
  };

  let cumulativeStartTime = 0;
  const clips: VideoClip[] = scenes.map((scene) => {
    const clip: VideoClip = {
      id: scene.id,
      trackId: TRACK_IDS.VIDEO,
      startTime: cumulativeStartTime,
      duration: scene.duration,
      thumbnailUrl: visuals[scene.id] || "",
      name: scene.name,
    };

    cumulativeStartTime += scene.duration;
    return clip;
  });

  return { track, clips };
}

/**
 * Converts NarrationSegment[] to a narrator Track with AudioClip[].
 *
 * Conversion logic:
 * - NarrationSegment.sceneId → AudioClip.id (prefixed with "audio-")
 * - NarrationSegment.audioDuration → AudioClip.duration (direct mapping)
 * - Cumulative narration durations → AudioClip.startTime (sum of previous segment durations)
 *
 * @param segments - Array of NarrationSegment objects
 * @param scenes - Array of Scene objects (used for ordering and fallback durations)
 * @returns Object containing the narrator track and array of audio clips
 *
 * @requirements 9.2, 9.5
 * @validates Requirements 1.5
 */
export function narrationToAudioTrack(
  segments: NarrationSegment[],
  scenes: Scene[]
): { track: Track; clips: AudioClip[] } {
  const track: Track = {
    id: TRACK_IDS.NARRATOR,
    type: "narrator",
    name: "Narrator",
    text: segments.map((s) => s.transcript).join(" "),
    isGenerated: segments.length > 0,
  };

  // Order segments by scene order
  const orderedSegments = [...segments].sort((a, b) => {
    const aIndex = scenes.findIndex((s) => s.id === a.sceneId);
    const bIndex = scenes.findIndex((s) => s.id === b.sceneId);
    return aIndex - bIndex;
  });

  let cumulativeStartTime = 0;
  const clips: AudioClip[] = orderedSegments.map((segment) => {
    const clip: AudioClip = {
      id: `${CLIP_PREFIXES.AUDIO}${segment.sceneId}`,
      trackId: TRACK_IDS.NARRATOR,
      startTime: cumulativeStartTime,
      duration: segment.audioDuration,
      waveformData: generatePlaceholderWaveform(segment.audioDuration),
    };

    cumulativeStartTime += segment.audioDuration;
    return clip;
  });

  return { track, clips };
}

/**
 * Converts VideoSFXPlan to SFX Track(s) with AudioClip[].
 *
 * Creates clips for:
 * - Ambient tracks from each scene's SFX plan
 * - Background music if present
 * - Generated music if present
 *
 * @param sfxPlan - VideoSFXPlan object or null
 * @param scenes - Array of Scene objects (used for timing reference)
 * @param narrationSegments - Array of NarrationSegment objects (used for timing alignment)
 * @returns Object containing the SFX tracks and array of audio clips
 *
 * @requirements 9.3, 9.5
 * @validates Requirements 1.6
 */
export function sfxPlanToTracks(
  sfxPlan: VideoSFXPlan | null,
  scenes: Scene[],
  _narrationSegments: NarrationSegment[]
): { tracks: Track[]; clips: AudioClip[] } {
  const tracks: Track[] = [];
  const clips: AudioClip[] = [];

  // Create SFX track
  const sfxTrack: Track = {
    id: TRACK_IDS.SFX,
    type: "sfx",
    name: "Sound Effects",
    text: "",
    isGenerated: sfxPlan !== null && sfxPlan.scenes.length > 0,
  };
  tracks.push(sfxTrack);

  if (!sfxPlan) {
    return { tracks, clips };
  }

  // Calculate scene start times for positioning SFX clips
  const sceneStartTimes = new Map<string, number>();
  let cumulativeTime = 0;
  for (const scene of scenes) {
    sceneStartTimes.set(scene.id, cumulativeTime);
    cumulativeTime += scene.duration;
  }

  // Process ambient tracks from each scene
  for (const sceneSfx of sfxPlan.scenes) {
    const startTime = sceneStartTimes.get(sceneSfx.sceneId) ?? 0;
    const scene = scenes.find((s) => s.id === sceneSfx.sceneId);
    const sceneDuration = scene?.duration ?? 0;

    if (sceneSfx.ambientTrack) {
      const ambientClip: AudioClip = {
        id: `${CLIP_PREFIXES.AMBIENT}${sceneSfx.sceneId}`,
        trackId: TRACK_IDS.SFX,
        startTime,
        duration:
          sceneSfx.ambientTrack.duration > 0
            ? sceneSfx.ambientTrack.duration
            : sceneDuration,
        waveformData: generatePlaceholderWaveform(sceneDuration),
      };
      clips.push(ambientClip);
    }
  }

  // Add background music track if present
  if (sfxPlan.backgroundMusic || sfxPlan.generatedMusic) {
    const musicTrack: Track = {
      id: TRACK_IDS.MUSIC,
      type: "sfx",
      name: "Music",
      text: sfxPlan.generatedMusic?.title || sfxPlan.backgroundMusic?.name || "",
      isGenerated: true,
    };
    tracks.push(musicTrack);

    // Calculate total duration
    const totalDuration = scenes.reduce((sum, s) => sum + s.duration, 0);

    if (sfxPlan.generatedMusic) {
      const musicClip: AudioClip = {
        id: `${CLIP_PREFIXES.MUSIC}generated`,
        trackId: TRACK_IDS.MUSIC,
        startTime: 0,
        duration: sfxPlan.generatedMusic.duration || totalDuration,
        waveformData: generatePlaceholderWaveform(
          sfxPlan.generatedMusic.duration || totalDuration
        ),
      };
      clips.push(musicClip);
    } else if (sfxPlan.backgroundMusic) {
      const musicClip: AudioClip = {
        id: `${CLIP_PREFIXES.MUSIC}background`,
        trackId: TRACK_IDS.MUSIC,
        startTime: 0,
        duration:
          sfxPlan.backgroundMusic.duration > 0
            ? sfxPlan.backgroundMusic.duration
            : totalDuration,
        waveformData: generatePlaceholderWaveform(totalDuration),
      };
      clips.push(musicClip);
    }
  }

  return { tracks, clips };
}

/**
 * Extracts the scene ID from a clip ID.
 *
 * Handles various clip ID patterns:
 * - "audio-{sceneId}" → sceneId
 * - "sfx-{sceneId}" → sceneId
 * - "ambient-{sceneId}" → sceneId
 * - "video-{sceneId}" → sceneId
 * - "{sceneId}" (no prefix) → sceneId
 *
 * @param clipId - The clip ID to extract scene ID from
 * @returns The extracted scene ID
 *
 * @requirements 9.4
 * @validates Requirements 3.2, 3.5, 10.4
 */
export function clipIdToSceneId(clipId: string): string {
  // Check for known prefixes and remove them
  for (const prefix of Object.values(CLIP_PREFIXES)) {
    if (clipId.startsWith(prefix)) {
      return clipId.slice(prefix.length);
    }
  }

  // No prefix found, return as-is (video clips use scene ID directly)
  return clipId;
}

/**
 * Converts a scene ID to a clip ID for a specific track type.
 *
 * @param sceneId - The scene ID to convert
 * @param trackType - The type of track ("video", "narrator", "sfx", "ambient", "music")
 * @returns The clip ID for the specified track type
 *
 * @requirements 9.4
 * @validates Requirements 3.2, 3.5, 10.4
 */
export function sceneIdToClipId(
  sceneId: string,
  trackType: "video" | "narrator" | "sfx" | "ambient" | "music"
): string {
  switch (trackType) {
    case "video":
      // Video clips use scene ID directly (no prefix)
      return sceneId;
    case "narrator":
      return `${CLIP_PREFIXES.AUDIO}${sceneId}`;
    case "sfx":
      return `${CLIP_PREFIXES.SFX}${sceneId}`;
    case "ambient":
      return `${CLIP_PREFIXES.AMBIENT}${sceneId}`;
    case "music":
      return `${CLIP_PREFIXES.MUSIC}${sceneId}`;
    default:
      return sceneId;
  }
}

/**
 * Converts all data sources to the timeline editor's internal format.
 * This is a convenience function that combines all adapter functions.
 *
 * @param scenes - Array of Scene objects from ContentPlan
 * @param visuals - Map of scene IDs to thumbnail URLs
 * @param narrationSegments - Array of NarrationSegment objects
 * @param sfxPlan - VideoSFXPlan object or null
 * @returns Object containing all tracks and clips
 */
export function convertToTimelineData(
  scenes: Scene[],
  visuals: Record<string, string>,
  narrationSegments: NarrationSegment[],
  sfxPlan: VideoSFXPlan | null
): {
  tracks: Track[];
  videoClips: VideoClip[];
  audioClips: AudioClip[];
} {
  const { track: videoTrack, clips: videoClips } = scenesToVideoTrack(
    scenes,
    visuals
  );
  const { track: narratorTrack, clips: narratorClips } = narrationToAudioTrack(
    narrationSegments,
    scenes
  );
  const { tracks: sfxTracks, clips: sfxClips } = sfxPlanToTracks(
    sfxPlan,
    scenes,
    narrationSegments
  );

  return {
    tracks: [videoTrack, narratorTrack, ...sfxTracks],
    videoClips,
    audioClips: [...narratorClips, ...sfxClips],
  };
}
````

## File: packages/frontend/components/TimelineEditor/TimeRuler.tsx
````typescript
/**
 * TimeRuler Component
 * 
 * Renders the time scale ruler at the top of the Graphite Timeline.
 * Features:
 * - Major and minor tick marks based on zoom level (Requirements 3.1, 3.2)
 * - Timecode labels at major ticks (Requirement 3.3)
 * - Zoom-responsive tick intervals (Requirement 3.4)
 * - Horizontal scroll synchronization with track lanes (Requirements 3.5, 11.2)
 * 
 * Requirements: 3.1, 3.2, 3.3, 3.4, 3.5, 11.2
 */

import React, { forwardRef, useMemo } from "react";
import { getTickInterval } from "./graphite-timeline-utils";
import "./graphite-timeline.css";

// --- Types ---

export interface TimeRulerProps {
  /** Total duration in seconds */
  duration: number;
  /** Current zoom level (pixels per second) */
  zoom: number;
  /** Horizontal scroll offset in pixels (for positioning) - used for sync tracking */
  scrollLeft?: number;
  /** Frames per second for timecode display (default: 24) */
  fps?: number;
  /** Optional additional CSS class */
  className?: string;
}

// --- Helper Functions ---

/**
 * Formats time for ruler labels (shorter format than full timecode).
 * Shows MM:SS for times under an hour, HH:MM:SS for longer.
 * 
 * @param seconds - Time value in seconds
 * @returns Formatted time string
 */
export function formatRulerLabel(seconds: number): string {
  const hrs = Math.floor(seconds / 3600);
  const mins = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  
  if (hrs > 0) {
    return `${hrs}:${mins.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}`;
  }
  return `${mins}:${secs.toString().padStart(2, "0")}`;
}

/**
 * Generates tick positions for the ruler based on duration and zoom level.
 * Uses getTickInterval to determine appropriate spacing based on zoom.
 * 
 * @param duration - Total duration in seconds
 * @param zoom - Current zoom level (pixels per second)
 * @returns Object containing arrays of major and minor tick positions
 */
export function generateTicks(
  duration: number,
  zoom: number
): { majorTicks: number[]; minorTicks: number[] } {
  const { major, minor } = getTickInterval(zoom);
  
  const majorTicks: number[] = [];
  const minorTicks: number[] = [];
  
  // Generate major ticks at configurable intervals (Requirement 3.1)
  for (let t = 0; t <= duration; t += major) {
    // Round to avoid floating point precision issues
    majorTicks.push(Math.round(t * 1000) / 1000);
  }
  
  // Generate minor ticks between major ticks for finer granularity (Requirement 3.2)
  for (let t = 0; t <= duration; t += minor) {
    const roundedT = Math.round(t * 1000) / 1000;
    // Check if this is not a major tick position (with small epsilon for floating point)
    const isMajorTick = majorTicks.some(mt => Math.abs(mt - roundedT) < 0.001);
    if (!isMajorTick) {
      minorTicks.push(roundedT);
    }
  }
  
  return { majorTicks, minorTicks };
}

// --- Component ---

/**
 * TimeRuler component with forwardRef for scroll synchronization.
 * 
 * The ref is used by useTimelineScroll to sync ruler scroll with track lanes.
 * When the track lanes scroll, the ruler's scrollLeft is updated to match,
 * ensuring the time scale stays aligned with the clips below.
 * 
 * Scroll synchronization (Requirements 3.5, 11.2):
 * - The ruler container has overflow-x: auto (hidden scrollbar via CSS)
 * - The inner container width matches the total timeline width
 * - The useTimelineScroll hook syncs scrollLeft between ruler and track lanes
 */
export const TimeRuler = forwardRef<HTMLDivElement, TimeRulerProps>(
  function TimeRuler(
    {
      duration,
      zoom,
      scrollLeft = 0,
      fps = 24,
      className = "",
    },
    ref
  ) {
    // Calculate total width based on duration and zoom
    const totalWidth = Math.max(duration * zoom, 800);
    
    // Generate tick positions - memoized for performance
    const { majorTicks, minorTicks } = useMemo(
      () => generateTicks(duration, zoom),
      [duration, zoom]
    );
    
    const classNames = [
      "graphite-ruler",
      className,
    ].filter(Boolean).join(" ");

    return (
      <div
        ref={ref}
        className={classNames}
        role="presentation"
        aria-label="Time ruler"
        style={{ minHeight: '35px' }}
      >
        {/* Inner container with full width for scrolling */}
        <div
          className="graphite-ruler-inner"
          style={{
            width: `${totalWidth}px`,
            minWidth: "100%",
            height: "100%",
            position: "relative",
          }}
        >
          {/* Minor ticks - smaller marks between major ticks (Requirement 3.2) */}
          {minorTicks.map((t) => (
            <div
              key={`minor-${t}`}
              className="graphite-ruler-mark"
              style={{ left: `${t * zoom}px` }}
              data-time={t}
            />
          ))}
          
          {/* Major ticks with timecode labels (Requirements 3.1, 3.3) */}
          {majorTicks.map((t) => (
            <React.Fragment key={`major-${t}`}>
              <div
                className="graphite-ruler-mark major"
                style={{ left: `${t * zoom}px` }}
                data-time={t}
              />
              <span
                className="graphite-ruler-label"
                style={{ left: `${t * zoom + 4}px` }}
              >
                {formatRulerLabel(t)}
              </span>
            </React.Fragment>
          ))}
        </div>
      </div>
    );
  }
);

export default TimeRuler;
````

## File: packages/frontend/components/TimelineEditor/TrackLabel.tsx
````typescript
/**
 * TrackLabel Component
 * 
 * Renders a track label in the left sidebar of the Graphite Timeline.
 * Displays the track name with uppercase styling and graphite background.
 * 
 * Requirements: 4.2
 */

import "./graphite-timeline.css";

// --- Types ---

export interface TrackLabelProps {
  /** Track name to display */
  name: string;
  /** Optional height override (default: uses CSS variable --graphite-track-height) */
  height?: number;
  /** Optional additional CSS class */
  className?: string;
}

// --- Component ---

export function TrackLabel({
  name,
  height,
  className = "",
}: TrackLabelProps) {
  const classNames = [
    "graphite-label-block",
    className,
  ].filter(Boolean).join(" ");

  return (
    <div
      className={classNames}
      style={height ? { height: `${height}px` } : undefined}
      role="rowheader"
      aria-label={`Track: ${name}`}
    >
      <span>{name}</span>
    </div>
  );
}

export default TrackLabel;
````

## File: packages/frontend/components/TimelineEditor/TrackLane.tsx
````typescript
/**
 * TrackLane Component
 * 
 * Renders a single track lane in the Graphite Timeline containing clips.
 * Supports different track types (video, audio, fx) with type-specific styling.
 * Handles clip selection via click events.
 * Supports click-to-seek and drag-to-scrub for playhead positioning.
 * 
 * Requirements: 4.5, 10.1, 7.3, 7.4
 */

import React from "react";
import { TimelineTrack, TimelineClip } from "./graphite-timeline-utils";
import { GraphiteClip } from "./GraphiteClip";
import { AudioClip } from "./AudioClip";
import "./graphite-timeline.css";

// --- Types ---

export interface TrackLaneProps {
  /** Track data containing clips and metadata */
  track: TimelineTrack;
  /** Current zoom level (pixels per second) */
  zoom: number;
  /** Horizontal scroll offset in pixels */
  scrollLeft: number;
  /** ID of the currently selected clip (null if none) */
  selectedClipId: string | null;
  /** Callback when a clip is clicked */
  onClipSelect: (clipId: string) => void;
  /** Callback when clicking on empty lane area (to deselect) */
  onLaneClick?: () => void;
  /** Callback for click-to-seek (Requirement 7.3) */
  onSeekClick?: (e: React.MouseEvent) => void;
  /** Callback for drag-to-scrub mousedown (Requirement 7.4) */
  onSeekMouseDown?: (e: React.MouseEvent) => void;
  /** Optional height override */
  height?: number;
  /** Optional additional CSS class */
  className?: string;
}

// --- Helper Functions ---

/**
 * Gets the CSS class modifier for track type-specific background colors.
 * Requirement 4.5: Visually distinguish track types using different background colors.
 */
function getTrackTypeClass(type: TimelineTrack["type"]): string {
  switch (type) {
    case "video":
      return "graphite-lane--video";
    case "audio":
      return "graphite-lane--audio";
    case "fx":
      return "graphite-lane--fx";
    case "music":
      return "graphite-lane--music";
    default:
      return "";
  }
}

/**
 * Renders the appropriate clip component based on track type.
 */
function renderClip(
  clip: TimelineClip,
  trackType: TimelineTrack["type"],
  zoom: number,
  isSelected: boolean,
  onSelect: () => void
) {
  if (trackType === "audio" || trackType === "fx" || trackType === "music") {
    return (
      <AudioClip
        key={clip.id}
        clip={clip}
        zoom={zoom}
        isSelected={isSelected}
        onClick={onSelect}
      />
    );
  }

  return (
    <GraphiteClip
      key={clip.id}
      clip={clip}
      zoom={zoom}
      isSelected={isSelected}
      onClick={onSelect}
    />
  );
}

// --- Component ---

export function TrackLane({
  track,
  zoom,
  scrollLeft,
  selectedClipId,
  onClipSelect,
  onLaneClick,
  onSeekClick,
  onSeekMouseDown,
  height,
  className = "",
}: TrackLaneProps) {
  const trackTypeClass = getTrackTypeClass(track.type);

  const classNames = [
    "graphite-lane",
    trackTypeClass,
    className,
  ].filter(Boolean).join(" ");

  // Handle click on lane area
  // Supports both deselection and click-to-seek (Requirement 7.3)
  const handleLaneClick = (e: React.MouseEvent) => {
    // Only trigger if clicking directly on the lane, not on a clip
    if ((e.target as HTMLElement).closest(".graphite-clip")) {
      return;
    }

    // Call deselect handler
    if (onLaneClick) {
      onLaneClick();
    }

    // Call seek handler for click-to-seek
    if (onSeekClick) {
      onSeekClick(e);
    }
  };

  // Handle mousedown for drag-to-scrub (Requirement 7.4)
  const handleMouseDown = (e: React.MouseEvent) => {
    // Only trigger if clicking directly on the lane, not on a clip
    if ((e.target as HTMLElement).closest(".graphite-clip")) {
      return;
    }

    if (onSeekMouseDown) {
      onSeekMouseDown(e);
    }
  };

  return (
    <div
      className={classNames}
      style={height ? { height: `${height}px` } : undefined}
      onClick={handleLaneClick}
      onMouseDown={handleMouseDown}
      role="row"
      aria-label={`${track.name} track lane`}
    >
      {track.clips.map((clip) =>
        renderClip(
          clip,
          track.type,
          zoom,
          clip.id === selectedClipId,
          () => onClipSelect(clip.id)
        )
      )}
    </div>
  );
}

export default TrackLane;
````

## File: packages/frontend/components/TimelineEditor/TransportBar.tsx
````typescript
/**
 * TransportBar Component
 * 
 * Header section of the Graphite Timeline containing:
 * - Playback controls (play/pause, skip back, skip forward)
 * - Timecode display with cyan glow styling
 * - Project name display
 * - Add button placeholder
 * 
 * Accessibility Features:
 * - Keyboard shortcut hints in tooltips
 * - ARIA labels for all interactive elements
 * - aria-pressed for toggle states
 * 
 * Requirements: 2.1, 2.2, 2.3, 2.4, 2.5, 2.6 + Accessibility
 */

import React from "react";
import { formatTimecode } from "./graphite-timeline-utils";
import "./graphite-timeline.css";

// --- SVG Icons ---

const SkipBackIcon = () => (
  <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true">
    <path d="M6 6h2v12H6zm3.5 6l8.5 6V6z" />
  </svg>
);

const PlayIcon = () => (
  <svg width="22" height="22" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true">
    <path d="M8 5v14l11-7z" />
  </svg>
);

const PauseIcon = () => (
  <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true">
    <path d="M6 19h4V5H6v14zm8-14v14h4V5h-4z" />
  </svg>
);

const SkipForwardIcon = () => (
  <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true">
    <path d="M6 18l8.5-6L6 6v12zM16 6v12h2V6h-2z" />
  </svg>
);

const AddIcon = () => (
  <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" aria-hidden="true">
    <path d="M12 5v14M5 12h14" />
  </svg>
);

// --- Types ---

export interface TransportBarProps {
  /** Current playback time in seconds */
  currentTime: number;
  /** Whether playback is currently active */
  isPlaying: boolean;
  /** Project name to display in header */
  projectName?: string;
  /** Callback when play/pause button is clicked */
  onPlayPause: () => void;
  /** Callback when skip back button is clicked */
  onSkipBack: () => void;
  /** Callback when skip forward button is clicked */
  onSkipForward: () => void;
  /** Callback when add button is clicked (optional) */
  onAdd?: () => void;
  /** Frames per second for timecode display (default: 24) */
  fps?: number;
}

// --- Component ---

export function TransportBar({
  currentTime,
  isPlaying,
  projectName = "UNTITLED",
  onPlayPause,
  onSkipBack,
  onSkipForward,
  onAdd,
  fps = 24,
}: TransportBarProps) {
  return (
    <header className="graphite-transport-bar" role="toolbar" aria-label="Playback controls">
      {/* Playback Controls - Requirements 2.1, 2.4, 2.5 */}
      <div className="graphite-transport-btns" role="group" aria-label="Transport controls">
        <button
          className="graphite-btn"
          onClick={onSkipBack}
          aria-label="Skip backward 5 seconds (J or Left Arrow)"
          title="Skip backward 5 seconds (J or ←)"
          type="button"
        >
          <SkipBackIcon />
        </button>
        
        <button
          className="graphite-btn primary"
          onClick={onPlayPause}
          aria-label={isPlaying ? "Pause (Space or K)" : "Play (Space or K)"}
          aria-pressed={isPlaying}
          title={isPlaying ? "Pause (Space or K)" : "Play (Space or K)"}
          type="button"
        >
          {isPlaying ? <PauseIcon /> : <PlayIcon />}
        </button>
        
        <button
          className="graphite-btn"
          onClick={onSkipForward}
          aria-label="Skip forward 5 seconds (L or Right Arrow)"
          title="Skip forward 5 seconds (L or →)"
          type="button"
        >
          <SkipForwardIcon />
        </button>
      </div>

      {/* Timecode Display - Requirement 2.2 */}
      <div 
        className="graphite-timecode-display"
        role="timer"
        aria-label={`Current time: ${formatTimecode(currentTime, fps)}`}
        aria-live="off"
      >
        {formatTimecode(currentTime, fps)}
      </div>

      {/* Project Info - Requirements 2.3, 2.6 */}
      <div className="graphite-project-info">
        <span className="graphite-project-name" aria-label={`Project name: ${projectName}`}>
          PROJECT: {projectName.toUpperCase()}
        </span>
        
        <button
          className="graphite-btn"
          onClick={onAdd}
          aria-label="Add new clip or media"
          title="Add new clip or media"
          type="button"
        >
          <AddIcon />
        </button>
      </div>
    </header>
  );
}

export default TransportBar;
````

## File: packages/frontend/components/TimelineEditor/useTimelineScroll.ts
````typescript
/**
 * useTimelineScroll Hook
 * 
 * Custom hook for managing horizontal scroll state in the Graphite Timeline.
 * Handles:
 * - Tracking scrollLeft state
 * - Syncing ruler and track lanes scroll positions
 * - Auto-scroll during playback to keep playhead visible
 * 
 * Requirements: 11.1, 11.2, 11.3, 11.4, 11.5
 */

import { useState, useCallback, useRef, useEffect } from "react";

// --- Types ---

export interface UseTimelineScrollOptions {
  /** Current playback time in seconds */
  currentTime: number;
  /** Total duration in seconds */
  duration: number;
  /** Current zoom level (pixels per second) */
  zoom: number;
  /** Whether playback is currently active */
  isPlaying: boolean;
  /** Width of the track labels sidebar in pixels */
  labelsWidth?: number;
  /** Margin from edge before auto-scroll triggers (in pixels) */
  autoScrollMargin?: number;
}

export interface UseTimelineScrollResult {
  /** Current horizontal scroll offset in pixels */
  scrollLeft: number;
  /** Ref to attach to the scrollable track lanes container */
  trackLanesRef: React.RefObject<HTMLDivElement | null>;
  /** Ref to attach to the ruler container (for sync) */
  rulerRef: React.RefObject<HTMLDivElement | null>;
  /** Handler for scroll events on track lanes */
  handleScroll: (e: React.UIEvent<HTMLDivElement>) => void;
  /** Programmatically set scroll position */
  setScrollLeft: (value: number) => void;
  /** Calculate visible time range based on current scroll and container width */
  getVisibleRange: () => { start: number; end: number };
  /** Scroll to make a specific time visible */
  scrollToTime: (time: number) => void;
}

// --- Constants ---

const DEFAULT_LABELS_WIDTH = 140;
const DEFAULT_AUTO_SCROLL_MARGIN = 100;

// --- Hook Implementation ---

/**
 * Custom hook for managing timeline horizontal scroll state.
 * 
 * Features:
 * - Tracks scrollLeft state for positioning playhead and clips
 * - Syncs ruler scroll with track lanes (Requirement 11.2)
 * - Auto-scrolls during playback to keep playhead visible (Requirement 11.4)
 * - Provides refs for attaching to scrollable containers
 * 
 * @param options - Configuration options
 * @returns Scroll state and handlers
 */
export function useTimelineScroll({
  currentTime,
  duration,
  zoom,
  isPlaying,
  labelsWidth = DEFAULT_LABELS_WIDTH,
  autoScrollMargin = DEFAULT_AUTO_SCROLL_MARGIN,
}: UseTimelineScrollOptions): UseTimelineScrollResult {
  // Scroll state
  const [scrollLeft, setScrollLeftState] = useState(0);
  
  // Refs for scrollable containers
  const trackLanesRef = useRef<HTMLDivElement>(null);
  const rulerRef = useRef<HTMLDivElement>(null);
  
  // Track if we're programmatically scrolling to avoid feedback loops
  const isProgrammaticScroll = useRef(false);

  /**
   * Handle scroll events from track lanes.
   * Syncs the ruler scroll position to match.
   * Requirement 11.2: Ruler scrolls in sync with track lanes
   */
  const handleScroll = useCallback((e: React.UIEvent<HTMLDivElement>) => {
    const newScrollLeft = e.currentTarget.scrollLeft;
    setScrollLeftState(newScrollLeft);
    
    // Sync ruler scroll position (Requirement 11.2)
    if (rulerRef.current && !isProgrammaticScroll.current) {
      rulerRef.current.scrollLeft = newScrollLeft;
    }
  }, []);

  /**
   * Programmatically set scroll position.
   * Updates both track lanes and ruler.
   */
  const setScrollLeft = useCallback((value: number) => {
    const clampedValue = Math.max(0, value);
    setScrollLeftState(clampedValue);
    
    isProgrammaticScroll.current = true;
    
    if (trackLanesRef.current) {
      trackLanesRef.current.scrollLeft = clampedValue;
    }
    if (rulerRef.current) {
      rulerRef.current.scrollLeft = clampedValue;
    }
    
    // Reset flag after a tick
    requestAnimationFrame(() => {
      isProgrammaticScroll.current = false;
    });
  }, []);

  /**
   * Calculate the visible time range based on current scroll and container width.
   * Used for the project overview minimap.
   */
  const getVisibleRange = useCallback((): { start: number; end: number } => {
    if (!trackLanesRef.current) {
      return { start: 0, end: duration };
    }
    
    const containerWidth = trackLanesRef.current.clientWidth;
    const visibleStart = scrollLeft / zoom;
    const visibleEnd = (scrollLeft + containerWidth) / zoom;
    
    return {
      start: Math.max(0, visibleStart),
      end: Math.min(duration, visibleEnd),
    };
  }, [scrollLeft, zoom, duration]);

  /**
   * Scroll to make a specific time visible.
   * Centers the time in the viewport if possible.
   */
  const scrollToTime = useCallback((time: number) => {
    if (!trackLanesRef.current) return;
    
    const containerWidth = trackLanesRef.current.clientWidth;
    const targetPosition = time * zoom;
    
    // Center the time in the viewport
    const newScrollLeft = Math.max(0, targetPosition - containerWidth / 2);
    setScrollLeft(newScrollLeft);
  }, [zoom, setScrollLeft]);

  /**
   * Auto-scroll during playback to keep playhead visible.
   * Requirement 11.4: Auto-scroll when playhead approaches edge
   */
  useEffect(() => {
    if (!isPlaying || !trackLanesRef.current) return;
    
    const containerWidth = trackLanesRef.current.clientWidth;
    const playheadPosition = currentTime * zoom;
    
    // Calculate visible bounds
    const visibleStart = scrollLeft;
    const visibleEnd = scrollLeft + containerWidth;
    
    // Check if playhead is approaching the right edge
    if (playheadPosition > visibleEnd - autoScrollMargin) {
      // Scroll to keep playhead visible with margin
      const newScrollLeft = playheadPosition - containerWidth + autoScrollMargin;
      setScrollLeft(Math.max(0, newScrollLeft));
    }
    // Check if playhead is before the visible area (e.g., after seeking)
    else if (playheadPosition < visibleStart + autoScrollMargin) {
      // Scroll to show playhead with margin from left
      const newScrollLeft = playheadPosition - autoScrollMargin;
      setScrollLeft(Math.max(0, newScrollLeft));
    }
  }, [currentTime, zoom, isPlaying, scrollLeft, autoScrollMargin, setScrollLeft]);

  return {
    scrollLeft,
    trackLanesRef,
    rulerRef,
    handleScroll,
    setScrollLeft,
    getVisibleRange,
    scrollToTime,
  };
}

export default useTimelineScroll;
````

## File: packages/frontend/components/TimelinePlayer.tsx
````typescript
import React, { useRef, useEffect, useState } from "react";
import {
  Play,
  Pause,
  RotateCcw,
  Activity,
  Circle,
  Waves,
  Sparkles,
} from "lucide-react";
import { SubtitleItem } from "@/types";
import { Button } from "@/components/ui/button";
import { cn, isRTL } from "@/lib/utils";
import { Card } from "@/components/ui/card";

interface TimelinePlayerProps {
  audioUrl: string;
  subtitles: SubtitleItem[];
  currentTime: number;
  duration: number;
  isPlaying: boolean;
  onPlayPause: () => void;
  onSeek: (time: number) => void;
  onTimeUpdate: (time: number) => void;
  onDurationChange: (duration: number) => void;
  onEnded: () => void;
  /** Content mode - "music" shows visualizer, "story" hides it */
  contentMode?: "music" | "story";
}

type VisualizerMode = "bars" | "circular" | "wave" | "particles";

class ParticleSystem {
  particles: {
    x: number;
    y: number;
    vx: number;
    vy: number;
    life: number;
    color: string;
    size: number;
  }[] = [];

  update(width: number, height: number, bassEnergy: number) {
    if (bassEnergy > 230) {
      const count = Math.floor(bassEnergy / 20);
      for (let i = 0; i < count; i++) {
        this.particles.push({
          x: width / 2,
          y: height / 2,
          vx: (Math.random() - 0.5) * 10,
          vy: (Math.random() - 0.5) * 10,
          life: 1.0,
          color: `hsl(${Math.random() * 60 + 180}, 100%, 70%)`,
          size: Math.random() * 4 + 1,
        });
      }
    }

    for (let i = this.particles.length - 1; i >= 0; i--) {
      const p = this.particles[i];
      if (!p) {
        this.particles.splice(i, 1);
        continue;
      }
      p.x += p.vx;
      p.y += p.vy;
      p.life -= 0.02;
      p.size *= 0.95;

      if (p.life <= 0 || p.x < 0 || p.x > width || p.y < 0 || p.y > height) {
        this.particles.splice(i, 1);
      }
    }
  }

  draw(ctx: CanvasRenderingContext2D) {
    ctx.globalCompositeOperation = "lighter";
    this.particles.forEach((p) => {
      if (!p) return;
      ctx.beginPath();
      ctx.arc(p.x, p.y, p.size, 0, Math.PI * 2);
      ctx.fillStyle = p.color;
      ctx.globalAlpha = p.life;
      ctx.fill();
    });
    ctx.globalCompositeOperation = "source-over";
    ctx.globalAlpha = 1.0;
  }
}

export const TimelinePlayer: React.FC<TimelinePlayerProps> = ({
  audioUrl,
  subtitles,
  currentTime,
  duration,
  isPlaying,
  onPlayPause,
  onSeek,
  onTimeUpdate,
  onDurationChange,
  onEnded,
  contentMode = "music",
}) => {
  const audioRef = useRef<HTMLAudioElement>(null);
  const progressBarRef = useRef<HTMLDivElement>(null);
  const waveformCanvasRef = useRef<HTMLCanvasElement>(null);
  const visualizerCanvasRef = useRef<HTMLCanvasElement>(null);

  const [waveformBuffer, setWaveformBuffer] = useState<AudioBuffer | null>(
    null,
  );
  const [visualizerMode, setVisualizerMode] = useState<VisualizerMode>("wave");

  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const sourceRef = useRef<MediaElementAudioSourceNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const particleSystemRef = useRef<ParticleSystem>(new ParticleSystem());

  // Only enable visualizer for music mode
  const showVisualizer = contentMode === "music";

  useEffect(() => {
    // Skip visualizer setup for story mode
    if (!showVisualizer) {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      return;
    }

    if (!isPlaying) {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      return;
    }

    // Defer AudioContext initialization to avoid blocking the click handler
    const timeoutId = setTimeout(() => {
      const initAudioContext = () => {
        if (!audioRef.current) return;

        if (!audioContextRef.current) {
          audioContextRef.current = new (
            window.AudioContext || (window as any).webkitAudioContext
          )();
        }

        const ctx = audioContextRef.current;

        if (!sourceRef.current) {
          try {
            const source = ctx.createMediaElementSource(audioRef.current);
            sourceRef.current = source;
            const analyser = ctx.createAnalyser();
            analyserRef.current = analyser;
            
            analyser.fftSize = visualizerMode === "wave" ? 2048 : 256;
            analyser.smoothingTimeConstant = 0.8;

            source.connect(analyser);
            analyser.connect(ctx.destination);
          } catch (e) {
            console.warn("AudioContext already connected");
          }
        }

        if (analyserRef.current) {
          analyserRef.current.fftSize =
            visualizerMode === "circular"
              ? 512
              : visualizerMode === "wave"
                ? 2048
                : 256;
        }

        if (ctx.state === "suspended") {
          ctx.resume();
        }

        startRenderLoop();
      };

      initAudioContext();
    }, 0);

    return () => {
      clearTimeout(timeoutId);
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, [isPlaying, visualizerMode, showVisualizer]);

  const startRenderLoop = () => {
    const canvas = visualizerCanvasRef.current;
    const analyser = analyserRef.current;
    if (!canvas || !analyser) return;

    const ctx = canvas.getContext("2d");
    if (!ctx) return;

    const dpr = window.devicePixelRatio || 1;
    const rect = canvas.getBoundingClientRect();
    canvas.width = rect.width * dpr;
    canvas.height = rect.height * dpr;
    ctx.scale(dpr, dpr);

    const width = rect.width;
    const height = rect.height;

    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    const render = () => {
      if (!analyser) return;
      analyser.getByteFrequencyData(dataArray);

      ctx.clearRect(0, 0, width, height);

      switch (visualizerMode) {
        case "circular":
          drawCircular(ctx, width, height, dataArray, bufferLength);
          break;
        case "wave":
          drawWave(ctx, width, height, dataArray, bufferLength);
          break;
        case "particles":
          drawParticles(ctx, width, height, dataArray, bufferLength);
          break;
        default:
          drawBars(ctx, width, height, dataArray, bufferLength);
      }

      animationFrameRef.current = requestAnimationFrame(render);
    };

    render();
  };

  const drawBars = (
    ctx: CanvasRenderingContext2D,
    w: number,
    h: number,
    data: Uint8Array,
    len: number,
  ) => {
    const barWidth = (w / len) * 2.5;
    let x = 0;

    const gradient = ctx.createLinearGradient(0, h, 0, 0);
    gradient.addColorStop(0, "rgba(34, 211, 238, 0.1)");
    gradient.addColorStop(0.5, "rgba(34, 211, 238, 0.8)");
    gradient.addColorStop(1, "rgba(167, 139, 250, 0.9)");

    ctx.fillStyle = gradient;

    for (let i = 0; i < len / 2; i++) {
      const val = data[i] ?? 0;
      const barHeight = (val / 255) * h;
      const centerX = w / 2;
      const offset = i * (barWidth + 1);

      ctx.fillRect(centerX + offset, h - barHeight, barWidth, barHeight);
      ctx.fillRect(
        centerX - offset - barWidth,
        h - barHeight,
        barWidth,
        barHeight,
      );

      x += barWidth + 1;
      if (x > w) break;
    }
  };

  const drawCircular = (
    ctx: CanvasRenderingContext2D,
    w: number,
    h: number,
    data: Uint8Array,
    len: number,
  ) => {
    const cx = w / 2;
    const cy = h / 2;
    const radius = Math.min(w, h) * 0.3;

    ctx.lineWidth = 2;
    ctx.lineCap = "round";
    ctx.shadowBlur = 15;
    ctx.shadowColor = "#22d3ee";

    const angleStep = (Math.PI * 2) / len;

    ctx.strokeStyle = "rgba(34, 211, 238, 0.8)";
    ctx.beginPath();
    for (let i = 0; i < len; i += 2) {
      const value = data[i] ?? 0;
      const barHeight = (value / 255) * radius * 0.8;
      const angle = i * angleStep - Math.PI / 2;

      const xStart = cx + Math.cos(angle) * radius;
      const yStart = cy + Math.sin(angle) * radius;
      const xEnd = cx + Math.cos(angle) * (radius + barHeight);
      const yEnd = cy + Math.sin(angle) * (radius + barHeight);

      ctx.moveTo(xStart, yStart);
      ctx.lineTo(xEnd, yEnd);
    }
    ctx.stroke();

    ctx.shadowBlur = 5;
    ctx.shadowColor = "#a78bfa";
    ctx.beginPath();
    for (let i = 0; i < len; i += 4) {
      const value = data[len - i - 1] ?? 0;
      const barHeight = (value / 255) * radius * 0.5;

      const angle = i * angleStep - Math.PI / 2;

      const rOuter = radius * 0.6;
      const xStart = cx + Math.cos(angle) * (rOuter - barHeight);
      const yStart = cy + Math.sin(angle) * (rOuter - barHeight);
      const xEnd = cx + Math.cos(angle) * rOuter;
      const yEnd = cy + Math.sin(angle) * rOuter;

      ctx.moveTo(xStart, yStart);
      ctx.lineTo(xEnd, yEnd);
    }
    ctx.stroke();
    ctx.shadowBlur = 0;
  };

  const drawWave = (
    ctx: CanvasRenderingContext2D,
    w: number,
    h: number,
    data: Uint8Array,
    len: number,
  ) => {
    ctx.lineWidth = 3;
    const gradient = ctx.createLinearGradient(0, h, 0, h / 2);
    gradient.addColorStop(0, "rgba(34, 211, 238, 0.0)");
    gradient.addColorStop(0.5, "rgba(34, 211, 238, 0.2)");
    gradient.addColorStop(1, "rgba(167, 139, 250, 0.4)");

    const strokeGradient = ctx.createLinearGradient(0, 0, w, 0);
    strokeGradient.addColorStop(0, "#22d3ee");
    strokeGradient.addColorStop(0.5, "#a78bfa");
    strokeGradient.addColorStop(1, "#22d3ee");

    const sliceWidth = w / (len / 3);
    let x = 0;

    ctx.beginPath();
    ctx.moveTo(0, h);

    let lastX = 0;
    let lastY = h;

    const step = 2;
    for (let i = 0; i < len / 3; i += step) {
      const value = data[i] ?? 0;
      const percent = value / 255;
      const y = h - percent * h * 0.6 - 20;

      const nextX = i * sliceWidth * step;
      const xc = (lastX + (x + sliceWidth * step)) / 2;
      const yc = (lastY + y) / 2;

      ctx.quadraticCurveTo(lastX, lastY, xc, yc);

      lastX = x + sliceWidth * step;
      lastY = y;
      x += sliceWidth * step;
    }

    ctx.lineTo(w, h);
    ctx.closePath();
    ctx.fillStyle = gradient;
    ctx.fill();

    ctx.strokeStyle = strokeGradient;
    ctx.stroke();
  };

  const drawParticles = (
    ctx: CanvasRenderingContext2D,
    w: number,
    h: number,
    data: Uint8Array,
    _len: number,
  ) => {
    const bassEnergy = data.slice(0, 10).reduce((a, b) => a + b, 0) / 10;
    if (bassEnergy > 200) {
      ctx.fillStyle = `rgba(34, 211, 238, ${0.05 + (bassEnergy / 255) * 0.05})`;
      ctx.fillRect(0, 0, w, h);
    }
    const radius = 50 + (bassEnergy / 255) * 30;
    ctx.beginPath();
    ctx.arc(w / 2, h / 2, radius, 0, Math.PI * 2);
    ctx.fillStyle = "rgba(167, 139, 250, 0.2)";
    ctx.fill();
    particleSystemRef.current.update(w, h, bassEnergy);
    particleSystemRef.current.draw(ctx);
  };

  useEffect(() => {
    if (audioRef.current) {
      if (isPlaying) {
        audioRef.current.play().catch((e) => console.error("Play error:", e));
      } else {
        audioRef.current.pause();
      }
    }
  }, [isPlaying]);

  // Sync audio element to currentTime prop when it changes externally (e.g., scene selection)
  // Using a smaller threshold (0.1s) to ensure accurate seeking when clicking thumbnails
  useEffect(() => {
    if (
      audioRef.current &&
      Math.abs(audioRef.current.currentTime - currentTime) > 0.1
    ) {
      audioRef.current.currentTime = currentTime;
    }
  }, [currentTime]);

  useEffect(() => {
    const loadAudioData = async () => {
      try {
        const response = await fetch(audioUrl);
        const arrayBuffer = await response.arrayBuffer();
        const audioContext = new (
          window.AudioContext || (window as any).webkitAudioContext
        )();
        const decodedBuffer = await audioContext.decodeAudioData(arrayBuffer);
        setWaveformBuffer(decodedBuffer);
      } catch (error) {
        console.error("Failed to load audio waveform", error);
      }
    };
    loadAudioData();
  }, [audioUrl]);

  useEffect(() => {
    const canvas = waveformCanvasRef.current;
    if (!canvas || !waveformBuffer) return;
    const ctx = canvas.getContext("2d");
    if (!ctx) return;
    const parent = canvas.parentElement;
    if (parent) {
      canvas.width = parent.clientWidth;
      canvas.height = parent.clientHeight;
    }
    const width = canvas.width;
    const height = canvas.height;
    const data = waveformBuffer.getChannelData(0);
    const step = Math.ceil(data.length / width);
    const amp = height / 2;
    ctx.clearRect(0, 0, width, height);
    ctx.fillStyle = "#334155";
    for (let i = 0; i < width; i++) {
      let min = 1.0;
      let max = -1.0;
      for (let j = 0; j < step; j++) {
        const datum = data[i * step + j] ?? 0;
        if (datum < min) min = datum;
        if (datum > max) max = datum;
      }
      const barHeight = (max - min) * amp;
      const y = (1 + min) * amp;
      ctx.fillRect(i, y, 1, Math.max(1, barHeight));
    }
  }, [waveformBuffer]);

  const handleSeek = (
    e: React.MouseEvent<HTMLDivElement> | React.TouchEvent<HTMLDivElement>,
  ) => {
    if (progressBarRef.current && duration > 0) {
      const rect = progressBarRef.current.getBoundingClientRect();
      const clientX = "touches" in e ? (e.touches[0]?.clientX ?? 0) : e.clientX;
      const x = clientX - rect.left;
      const percentage = Math.min(Math.max(x / rect.width, 0), 1);
      const newTime = percentage * duration;
      onSeek(newTime);
    }
  };

  const formatTime = (time: number) => {
    const mins = Math.floor(time / 60);
    const secs = Math.floor(time % 60);
    const ms = Math.floor((time % 1) * 100);
    return `${mins}:${secs.toString().padStart(2, "0")}.${ms.toString().padStart(2, "0")}`;
  };

  const activeSubtitle = subtitles.find(
    (s) => currentTime >= s.startTime && currentTime <= s.endTime,
  );

  let karaokeStyle: React.CSSProperties = {};
  const isActiveSubtitleRTL = activeSubtitle
    ? isRTL(activeSubtitle.text)
    : false;
  const isActiveTranslationRTL = activeSubtitle?.translation
    ? isRTL(activeSubtitle.translation)
    : false;

  if (activeSubtitle) {
    const totalDuration = activeSubtitle.endTime - activeSubtitle.startTime;
    const progress = Math.max(
      0,
      Math.min(1, (currentTime - activeSubtitle.startTime) / totalDuration),
    );
    const percent = Math.floor(progress * 100);
    // For RTL languages, reverse the gradient direction (270deg instead of 90deg)
    const gradientDirection = isActiveSubtitleRTL ? "270deg" : "90deg";
    karaokeStyle = {
      backgroundImage: `linear-gradient(${gradientDirection}, #22d3ee ${percent}%, #94a3b8 ${percent}%)`,
      WebkitBackgroundClip: "text",
      WebkitTextFillColor: "transparent",
      backgroundClip: "text",
      color: "transparent",
      direction: isActiveSubtitleRTL ? "rtl" : "ltr",
      unicodeBidi: "isolate",
    };
  }

  const visuals = [
    { id: "wave", icon: Waves, label: "Fluid" },
    { id: "circular", icon: Circle, label: "Radial" },
    { id: "particles", icon: Sparkles, label: "Particles" },
    { id: "bars", icon: Activity, label: "Bars" },
  ];

  return (
    <Card className="flex flex-col gap-4 bg-card border-border p-6 shadow-xl backdrop-blur-sm">
      <audio
        ref={audioRef}
        src={audioUrl}
        crossOrigin="anonymous"
        onTimeUpdate={(e) => onTimeUpdate(e.currentTarget.currentTime)}
        onLoadedMetadata={(e) => onDurationChange(e.currentTarget.duration)}
        onEnded={onEnded}
      />

      <div className="aspect-video w-full flex items-center justify-center text-center p-8 bg-black rounded-xl border border-border relative overflow-hidden group">
        {/* Only render visualizer canvas in music mode */}
        {showVisualizer && (
          <canvas
            ref={visualizerCanvasRef}
            className="absolute inset-0 w-full h-full opacity-60 z-0 pointer-events-none"
          />
        )}
        <div className="absolute inset-0 bg-linear-to-t from-background/90 via-transparent to-background/40 z-1 pointer-events-none"></div>

        {/* Only show visualizer controls in music mode */}
        {showVisualizer && (
          <div className="absolute top-4 right-4 z-20 flex gap-2 opacity-0 group-hover:opacity-100 transition-opacity duration-300">
            {visuals.map((v) => (
              <Button
                key={v.id}
                variant="outline"
                size="icon"
                onClick={() => setVisualizerMode(v.id as VisualizerMode)}
                className={cn(
                  "h-8 w-8 hover:bg-muted",
                  visualizerMode === v.id
                    ? "bg-primary/20 border-primary text-primary"
                    : "bg-black/40 border-border text-muted-foreground",
                )}
                title={v.label}
              >
                <v.icon size={16} />
              </Button>
            ))}
          </div>
        )}

        {/* Subtitles - positioned at bottom */}
        <div className="absolute bottom-6 left-4 right-4 z-10 flex flex-col items-center">
          {activeSubtitle ? (
            <div className="animate-in fade-in slide-in-from-bottom-2 duration-200 max-w-[90%]">
              {/* Slim background bar that fits the text */}
              <div
                className="inline-block px-4 py-2 rounded-lg"
                style={{
                  background: "rgba(0, 0, 0, 0.75)",
                  backdropFilter: "blur(4px)",
                }}
              >
                <p
                  className="text-lg md:text-xl lg:text-2xl font-semibold leading-snug"
                  style={{
                    direction: isActiveSubtitleRTL ? "rtl" : "ltr",
                    textAlign: "center",
                    color: "#ffffff",
                    textShadow: "1px 1px 2px rgba(0,0,0,0.8), -1px -1px 2px rgba(0,0,0,0.8)",
                  }}
                >
                  <span
                    style={karaokeStyle}
                    className="transition-all duration-75"
                  >
                    {activeSubtitle.text}
                  </span>
                </p>
              </div>
              {activeSubtitle.translation && (
                <div
                  className="inline-block px-3 py-1 rounded-md mt-1"
                  style={{
                    background: "rgba(0, 0, 0, 0.6)",
                  }}
                >
                  <p
                    className="text-sm md:text-base text-cyan-300 font-medium"
                    style={{
                      direction: isActiveTranslationRTL ? "rtl" : "ltr",
                      textAlign: "center",
                      unicodeBidi: "isolate",
                      textShadow: "1px 1px 2px rgba(0,0,0,0.8)",
                    }}
                  >
                    {activeSubtitle.translation}
                  </p>
                </div>
              )}
            </div>
          ) : (
            <div className="flex flex-col items-center gap-2 opacity-30">
              <div className="w-12 h-1 bg-muted-foreground rounded-full animate-pulse"></div>
              <div className="w-24 h-1 bg-muted-foreground rounded-full animate-pulse delay-75"></div>
            </div>
          )}
        </div>

        {/* Center content for music mode visualizer */}
        {showVisualizer && !activeSubtitle && (
          <div className="relative z-5 max-w-3xl flex flex-col gap-4">
            <div className="flex flex-col items-center gap-2 opacity-30">
              <div className="w-12 h-1 bg-muted-foreground rounded-full animate-pulse"></div>
              <div className="w-24 h-1 bg-muted-foreground rounded-full animate-pulse delay-75"></div>
            </div>
          </div>
        )}
      </div>

      <div className="flex flex-col gap-2 pt-2">
        <div
          ref={progressBarRef}
          className="relative h-16 bg-muted/20 rounded-lg cursor-pointer group overflow-hidden border border-border/50"
          onClick={handleSeek}
          onTouchStart={handleSeek}
          onTouchMove={handleSeek}
          style={{ touchAction: "none" }}
        >
          <canvas
            ref={waveformCanvasRef}
            className="absolute inset-0 w-full h-full opacity-60"
          />

          <div className="absolute inset-0 w-full h-full">
            {subtitles.map((sub) => {
              const left = (sub.startTime / duration) * 100;
              const width = ((sub.endTime - sub.startTime) / duration) * 100;
              const isActive =
                currentTime >= sub.startTime && currentTime <= sub.endTime;

              return (
                <div
                  key={sub.id}
                  className={`absolute bottom-0 h-3 rounded-t-sm transition-all duration-200 pointer-events-none border-x border-background/20
                    ${isActive
                      ? "bg-primary z-10 h-4 shadow-[0_0_15px_rgba(34,211,238,0.6)]"
                      : "bg-accent hover:bg-accent/80 h-3"
                    }
                  `}
                  style={{
                    left: `${left}%`,
                    width: `${Math.max(width, 0.2)}%`,
                  }}
                />
              );
            })}
          </div>

          <div
            className="absolute top-0 h-full bg-primary/20 pointer-events-none"
            style={{ width: `${(currentTime / duration) * 100}%` }}
          />

          <div
            className="absolute top-0 w-0.5 h-full bg-primary shadow-[0_0_10px_rgba(34,211,238,0.8)] z-20 pointer-events-none"
            style={{ left: `${(currentTime / duration) * 100}%` }}
          />
        </div>

        <div className="flex items-center justify-between gap-4">
          <span className="font-mono text-xs text-muted-foreground tabular-nums w-24">
            {formatTime(currentTime)}
          </span>

          <div className="flex items-center gap-2">
            <Button
              variant="ghost"
              size="icon"
              onClick={() => onSeek(0)}
              className="h-9 w-9 text-muted-foreground hover:text-foreground"
              aria-label="Reset to beginning"
            >
              <RotateCcw size={16} />
            </Button>
            <Button
              variant="default"
              size="icon"
              onClick={onPlayPause}
              className="w-14 h-14 rounded-full bg-primary hover:bg-primary/90 text-primary-foreground flex items-center justify-center transition-all active:scale-95 shadow-lg shadow-primary/30 border border-primary/20 p-0"
              aria-label={isPlaying ? "Pause" : "Play"}
            >
              {isPlaying ? (
                <Pause size={24} fill="currentColor" />
              ) : (
                <Play size={24} fill="currentColor" className="ml-1" />
              )}
            </Button>
          </div>

          <span className="font-mono text-xs text-muted-foreground tabular-nums w-24 text-right">
            {formatTime(duration)}
          </span>
        </div>
      </div>
    </Card>
  );
};
````

## File: packages/frontend/components/ui/badge.tsx
````typescript
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const badgeVariants = cva(
  "inline-flex items-center justify-center rounded-full border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&>svg]:size-3 gap-1 [&>svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden",
  {
    variants: {
      variant: {
        default:
          "border-transparent bg-primary text-primary-foreground [a&]:hover:bg-primary/90",
        secondary:
          "border-transparent bg-secondary text-secondary-foreground [a&]:hover:bg-secondary/90",
        destructive:
          "border-transparent bg-destructive text-white [a&]:hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",
        outline:
          "text-foreground [a&]:hover:bg-accent [a&]:hover:text-accent-foreground",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  }
)

function Badge({
  className,
  variant,
  asChild = false,
  ...props
}: React.ComponentProps<"span"> &
  VariantProps<typeof badgeVariants> & { asChild?: boolean }) {
  const Comp = asChild ? Slot : "span"

  return (
    <Comp
      data-slot="badge"
      className={cn(badgeVariants({ variant }), className)}
      {...props}
    />
  )
}

export { Badge, badgeVariants }
````

## File: packages/frontend/components/ui/button.tsx
````typescript
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const buttonVariants = cva(
  "inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
  {
    variants: {
      variant: {
        default: "bg-primary text-primary-foreground hover:bg-primary/90",
        destructive:
          "bg-destructive text-white hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",
        outline:
          "border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50",
        secondary:
          "bg-secondary text-secondary-foreground hover:bg-secondary/80",
        ghost:
          "hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50",
        glass:
          "bg-white/5 backdrop-blur-md border border-white/10 hover:bg-white/10 hover:border-white/20 text-foreground",
        cinematic:
          "btn-cinematic font-editorial text-sm tracking-wide rounded-lg hover:shadow-[0_0_30px_var(--glow-spotlight)] hover:-translate-y-px",
        editorial:
          "font-editorial text-[var(--cinema-silver)] bg-transparent border border-transparent hover:bg-white/5 hover:border-white/10 transition-all",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-9 px-4 py-2 has-[>svg]:px-3",
        sm: "h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5",
        lg: "h-10 rounded-md px-6 has-[>svg]:px-4",
        icon: "size-9",
        "icon-sm": "size-8",
        "icon-lg": "size-10",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  }
)

function Button({
  className,
  variant = "default",
  size = "default",
  asChild = false,
  ...props
}: React.ComponentProps<"button"> &
  VariantProps<typeof buttonVariants> & {
    asChild?: boolean
  }) {
  const Comp = asChild ? Slot : "button"

  return (
    <Comp
      data-slot="button"
      data-variant={variant}
      data-size={size}
      className={cn(buttonVariants({ variant, size, className }))}
      {...props}
    />
  )
}

export { Button, buttonVariants }
````

## File: packages/frontend/components/ui/card.tsx
````typescript
import * as React from "react"

import { cn } from "@/lib/utils"

function Card({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card"
      className={cn(
        "glass-panel text-card-foreground flex flex-col gap-6 rounded-xl py-6 shadow-sm",
        className
      )}
      {...props}
    />
  )
}

function CardHeader({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-header"
      className={cn(
        "@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6",
        className
      )}
      {...props}
    />
  )
}

function CardTitle({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-title"
      className={cn("leading-none font-semibold", className)}
      {...props}
    />
  )
}

function CardDescription({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-description"
      className={cn("text-muted-foreground text-sm", className)}
      {...props}
    />
  )
}

function CardAction({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-action"
      className={cn(
        "col-start-2 row-span-2 row-start-1 self-start justify-self-end",
        className
      )}
      {...props}
    />
  )
}

function CardContent({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-content"
      className={cn("px-6", className)}
      {...props}
    />
  )
}

function CardFooter({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-footer"
      className={cn("flex items-center px-6 [.border-t]:pt-6", className)}
      {...props}
    />
  )
}

export {
  Card,
  CardHeader,
  CardFooter,
  CardTitle,
  CardAction,
  CardDescription,
  CardContent,
}
````

## File: packages/frontend/components/ui/dialog.tsx
````typescript
"use client"

import * as React from "react"
import * as DialogPrimitive from "@radix-ui/react-dialog"
import { XIcon } from "lucide-react"

import { cn } from "@/lib/utils"

function Dialog({
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Root>) {
  return <DialogPrimitive.Root data-slot="dialog" {...props} />
}

function DialogTrigger({
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Trigger>) {
  return <DialogPrimitive.Trigger data-slot="dialog-trigger" {...props} />
}

function DialogPortal({
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Portal>) {
  return <DialogPrimitive.Portal data-slot="dialog-portal" {...props} />
}

function DialogClose({
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Close>) {
  return <DialogPrimitive.Close data-slot="dialog-close" {...props} />
}

function DialogOverlay({
  className,
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Overlay>) {
  return (
    <DialogPrimitive.Overlay
      data-slot="dialog-overlay"
      className={cn(
        "data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 fixed inset-0 z-50 bg-black/60 backdrop-blur-xl",
        className
      )}
      {...props}
    />
  )
}

function DialogContent({
  className,
  children,
  showCloseButton = true,
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Content> & {
  showCloseButton?: boolean
}) {
  return (
    <DialogPortal data-slot="dialog-portal">
      <DialogOverlay />
      <DialogPrimitive.Content
        data-slot="dialog-content"
        className={cn(
          "glass-panel data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 fixed top-[50%] left-[50%] z-50 grid w-full max-w-[calc(100%-2rem)] translate-x-[-50%] translate-y-[-50%] gap-4 rounded-lg p-6 shadow-lg duration-200 outline-none sm:max-w-lg",
          className
        )}
        {...props}
      >
        {children}
        {showCloseButton && (
          <DialogPrimitive.Close
            data-slot="dialog-close"
            aria-label="Close dialog"
            className="ring-offset-background focus:ring-ring data-[state=open]:bg-accent data-[state=open]:text-muted-foreground absolute top-4 right-4 rounded-xs opacity-70 transition-opacity hover:opacity-100 focus:ring-2 focus:ring-offset-2 focus:outline-hidden disabled:pointer-events-none [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4"
          >
            <XIcon aria-hidden="true" focusable="false" />
            <span className="sr-only">Close</span>
          </DialogPrimitive.Close>
        )}
      </DialogPrimitive.Content>
    </DialogPortal>
  )
}

function DialogHeader({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="dialog-header"
      className={cn("flex flex-col gap-2 text-center sm:text-left", className)}
      {...props}
    />
  )
}

function DialogFooter({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="dialog-footer"
      className={cn(
        "flex flex-col-reverse gap-2 sm:flex-row sm:justify-end",
        className
      )}
      {...props}
    />
  )
}

function DialogTitle({
  className,
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Title>) {
  return (
    <DialogPrimitive.Title
      data-slot="dialog-title"
      className={cn("text-lg leading-none font-semibold", className)}
      {...props}
    />
  )
}

function DialogDescription({
  className,
  ...props
}: React.ComponentProps<typeof DialogPrimitive.Description>) {
  return (
    <DialogPrimitive.Description
      data-slot="dialog-description"
      className={cn("text-muted-foreground text-sm", className)}
      {...props}
    />
  )
}

export {
  Dialog,
  DialogClose,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogOverlay,
  DialogPortal,
  DialogTitle,
  DialogTrigger,
}
````

## File: packages/frontend/components/ui/dropdown-menu.tsx
````typescript
"use client";

import * as React from "react";
import * as DropdownMenuPrimitive from "@radix-ui/react-dropdown-menu";
import { Check, ChevronRight, Circle } from "lucide-react";

import { cn } from "@/lib/utils";

const DropdownMenu = DropdownMenuPrimitive.Root;

const DropdownMenuTrigger = DropdownMenuPrimitive.Trigger;

const DropdownMenuGroup = DropdownMenuPrimitive.Group;

const DropdownMenuPortal = DropdownMenuPrimitive.Portal;

const DropdownMenuSub = DropdownMenuPrimitive.Sub;

const DropdownMenuRadioGroup = DropdownMenuPrimitive.RadioGroup;

const DropdownMenuSubTrigger = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubTrigger>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubTrigger> & {
    inset?: boolean;
  }
>(({ className, inset, children, ...props }, ref) => (
  <DropdownMenuPrimitive.SubTrigger
    ref={ref}
    className={cn(
      "flex cursor-default select-none items-center gap-2 rounded-sm px-2 py-1.5 text-sm outline-none focus:bg-accent data-[state=open]:bg-accent [&_svg]:pointer-events-none [&_svg]:size-4 [&_svg]:shrink-0",
      inset && "pl-8",
      className,
    )}
    {...props}
  >
    {children}
    <ChevronRight className="ml-auto" />
  </DropdownMenuPrimitive.SubTrigger>
));
DropdownMenuSubTrigger.displayName =
  DropdownMenuPrimitive.SubTrigger.displayName;

const DropdownMenuSubContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubContent>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubContent>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.SubContent
    ref={ref}
    className={cn(
      "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-lg data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className,
    )}
    {...props}
  />
));
DropdownMenuSubContent.displayName =
  DropdownMenuPrimitive.SubContent.displayName;

const DropdownMenuContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <DropdownMenuPrimitive.Portal>
    <DropdownMenuPrimitive.Content
      ref={ref}
      sideOffset={sideOffset}
      className={cn(
        "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-md",
        "data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className,
      )}
      {...props}
    />
  </DropdownMenuPrimitive.Portal>
));
DropdownMenuContent.displayName = DropdownMenuPrimitive.Content.displayName;

const DropdownMenuItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Item> & {
    inset?: boolean;
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center gap-2 rounded-sm px-2 py-1.5 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&>svg]:size-4 [&>svg]:shrink-0",
      inset && "pl-8",
      className,
    )}
    {...props}
  />
));
DropdownMenuItem.displayName = DropdownMenuPrimitive.Item.displayName;

const DropdownMenuCheckboxItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.CheckboxItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.CheckboxItem>
>(({ className, children, checked, ...props }, ref) => (
  <DropdownMenuPrimitive.CheckboxItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className,
    )}
    checked={checked}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.CheckboxItem>
));
DropdownMenuCheckboxItem.displayName =
  DropdownMenuPrimitive.CheckboxItem.displayName;

const DropdownMenuRadioItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.RadioItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.RadioItem>
>(({ className, children, ...props }, ref) => (
  <DropdownMenuPrimitive.RadioItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className,
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Circle className="h-2 w-2 fill-current" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.RadioItem>
));
DropdownMenuRadioItem.displayName = DropdownMenuPrimitive.RadioItem.displayName;

const DropdownMenuLabel = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Label> & {
    inset?: boolean;
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Label
    ref={ref}
    className={cn(
      "px-2 py-1.5 text-sm font-semibold",
      inset && "pl-8",
      className,
    )}
    {...props}
  />
));
DropdownMenuLabel.displayName = DropdownMenuPrimitive.Label.displayName;

const DropdownMenuSeparator = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
));
DropdownMenuSeparator.displayName = DropdownMenuPrimitive.Separator.displayName;

const DropdownMenuShortcut = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLSpanElement>) => {
  return (
    <span
      className={cn("ml-auto text-xs tracking-widest opacity-60", className)}
      {...props}
    />
  );
};
DropdownMenuShortcut.displayName = "DropdownMenuShortcut";

export {
  DropdownMenu,
  DropdownMenuTrigger,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuCheckboxItem,
  DropdownMenuRadioItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuShortcut,
  DropdownMenuGroup,
  DropdownMenuPortal,
  DropdownMenuSub,
  DropdownMenuSubContent,
  DropdownMenuSubTrigger,
  DropdownMenuRadioGroup,
};
````

## File: packages/frontend/components/ui/ErrorState.tsx
````typescript
/**
 * ErrorState - Consistent error display
 *
 * Provides various error state representations for different contexts.
 */

import React from 'react';
import { AlertCircle, RefreshCw, ChevronRight } from 'lucide-react';
import { motion } from 'framer-motion';
import { cn } from '@/lib/utils';
import { Button } from '@/components/ui/button';

export interface ErrorStateProps {
  /** Error title */
  title?: string;
  /** Error message to display */
  message: string;
  /** Variant of error display */
  variant?: 'inline' | 'card' | 'fullArea';
  /** Callback for retry action */
  onRetry?: () => void;
  /** Custom retry button text */
  retryText?: string;
  /** Additional actions */
  actions?: React.ReactNode;
  /** Additional class names */
  className?: string;
}

/**
 * Error state display with optional retry
 */
export function ErrorState({
  title = 'Something went wrong',
  message,
  variant = 'card',
  onRetry,
  retryText = 'Try again',
  actions,
  className,
}: ErrorStateProps) {
  if (variant === 'inline') {
    return (
      <div
        className={cn(
          'flex items-center gap-2 text-red-400 text-sm',
          className
        )}
        role="alert"
      >
        <AlertCircle className="w-4 h-4 shrink-0" aria-hidden="true" />
        <span>{message}</span>
        {onRetry && (
          <button
            onClick={onRetry}
            className="text-red-300 hover:text-red-200 underline underline-offset-2"
          >
            {retryText}
          </button>
        )}
      </div>
    );
  }

  const content = (
    <motion.div
      initial={{ opacity: 0, y: 10 }}
      animate={{ opacity: 1, y: 0 }}
      className={cn(
        'rounded-xl bg-red-500/10 border border-red-500/20 p-6',
        variant === 'fullArea' && 'max-w-md mx-auto',
        className
      )}
      role="alert"
    >
      <div className="flex items-start gap-4">
        <div className="w-10 h-10 rounded-full bg-red-500/20 flex items-center justify-center shrink-0">
          <AlertCircle className="w-5 h-5 text-red-400" aria-hidden="true" />
        </div>
        <div className="flex-1 min-w-0">
          <h3 className="text-red-200 font-medium mb-1">{title}</h3>
          <p className="text-red-200/70 text-sm">{message}</p>

          {(onRetry || actions) && (
            <div className="mt-4 flex items-center gap-3">
              {onRetry && (
                <Button
                  onClick={onRetry}
                  size="sm"
                  className="bg-red-500/20 hover:bg-red-500/30 text-red-200 border border-red-500/30"
                >
                  <RefreshCw className="w-3.5 h-3.5 me-2" aria-hidden="true" />
                  {retryText}
                </Button>
              )}
              {actions}
            </div>
          )}
        </div>
      </div>
    </motion.div>
  );

  if (variant === 'fullArea') {
    return (
      <div className="flex items-center justify-center w-full h-full min-h-[300px] p-6">
        {content}
      </div>
    );
  }

  return content;
}

/**
 * Error boundary fallback component
 */
export interface ErrorFallbackProps {
  error: Error;
  resetErrorBoundary?: () => void;
}

export function ErrorFallback({ error, resetErrorBoundary }: ErrorFallbackProps) {
  return (
    <ErrorState
      variant="fullArea"
      title="Application Error"
      message={error.message || 'An unexpected error occurred'}
      onRetry={resetErrorBoundary}
      retryText="Reload"
      actions={
        <Button
          variant="ghost"
          size="sm"
          onClick={() => window.location.reload()}
          className="text-red-200/70 hover:text-red-200"
        >
          Refresh page
          <ChevronRight className="w-3.5 h-3.5 ms-1" aria-hidden="true" />
        </Button>
      }
    />
  );
}

export default ErrorState;
````

## File: packages/frontend/components/ui/input.tsx
````typescript
import * as React from "react"

import { cn } from "@/lib/utils"

function Input({ className, type, ...props }: React.ComponentProps<"input">) {
  return (
    <input
      type={type}
      data-slot="input"
      className={cn(
        "file:text-foreground placeholder:text-muted-foreground selection:bg-primary selection:text-primary-foreground dark:bg-input/30 border-input h-9 w-full min-w-0 rounded-md border bg-transparent px-3 py-1 text-base shadow-xs transition-[color,box-shadow] outline-none file:inline-flex file:h-7 file:border-0 file:bg-transparent file:text-sm file:font-medium disabled:pointer-events-none disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",
        "focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px]",
        "aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
        className
      )}
      {...props}
    />
  )
}

export { Input }
````

## File: packages/frontend/components/ui/label.tsx
````typescript
import * as React from "react"
import * as LabelPrimitive from "@radix-ui/react-label"

import { cn } from "@/lib/utils"

function Label({
  className,
  ...props
}: React.ComponentProps<typeof LabelPrimitive.Root>) {
  return (
    <LabelPrimitive.Root
      data-slot="label"
      className={cn(
        "flex items-center gap-2 text-sm leading-none font-medium select-none group-data-[disabled=true]:pointer-events-none group-data-[disabled=true]:opacity-50 peer-disabled:cursor-not-allowed peer-disabled:opacity-50",
        className
      )}
      {...props}
    />
  )
}

export { Label }
````

## File: packages/frontend/components/ui/LoadingState.tsx
````typescript
/**
 * LoadingState - Consistent loading indicators
 *
 * Provides various loading state representations for different contexts.
 */

import React from 'react';
import { Loader2, Sparkles } from 'lucide-react';
import { motion } from 'framer-motion';
import { cn } from '@/lib/utils';

export interface LoadingStateProps {
  /** Loading message to display */
  message?: string;
  /** Variant of loading indicator */
  variant?: 'spinner' | 'dots' | 'pulse' | 'branded';
  /** Size of the loading indicator */
  size?: 'sm' | 'md' | 'lg';
  /** Additional class names */
  className?: string;
  /** Whether to show as full page/area overlay */
  fullArea?: boolean;
}

const sizeClasses = {
  sm: 'w-4 h-4',
  md: 'w-8 h-8',
  lg: 'w-12 h-12',
};

const textSizeClasses = {
  sm: 'text-xs',
  md: 'text-sm',
  lg: 'text-base',
};

/**
 * Loading indicator with multiple variants
 */
export function LoadingState({
  message,
  variant = 'spinner',
  size = 'md',
  className,
  fullArea = false,
}: LoadingStateProps) {
  const content = (
    <div className={cn('flex flex-col items-center gap-3', className)}>
      {variant === 'spinner' && (
        <Loader2
          className={cn(sizeClasses[size], 'text-violet-400 animate-spin')}
          aria-hidden="true"
        />
      )}

      {variant === 'dots' && (
        <div className="flex gap-1">
          {[0, 1, 2].map((i) => (
            <motion.div
              key={i}
              className={cn(
                'rounded-full bg-violet-400',
                size === 'sm' ? 'w-1.5 h-1.5' : size === 'md' ? 'w-2 h-2' : 'w-3 h-3'
              )}
              animate={{ scale: [1, 1.2, 1], opacity: [0.5, 1, 0.5] }}
              transition={{
                duration: 0.8,
                delay: i * 0.15,
                repeat: Infinity,
                ease: 'easeInOut',
              }}
            />
          ))}
        </div>
      )}

      {variant === 'pulse' && (
        <motion.div
          className={cn(
            'rounded-full bg-violet-500/20 border border-violet-500/30',
            sizeClasses[size]
          )}
          animate={{ scale: [1, 1.1, 1], opacity: [0.5, 1, 0.5] }}
          transition={{ duration: 1.5, repeat: Infinity, ease: 'easeInOut' }}
        />
      )}

      {variant === 'branded' && (
        <motion.div
          className={cn(
            'rounded-2xl bg-gradient-to-br from-violet-600/20 to-fuchsia-600/20 border border-white/10 flex items-center justify-center',
            size === 'sm' ? 'w-10 h-10' : size === 'md' ? 'w-16 h-16' : 'w-20 h-20'
          )}
          animate={{ scale: [1, 1.05, 1] }}
          transition={{ duration: 2, repeat: Infinity, ease: 'easeInOut' }}
        >
          <Sparkles
            className={cn(
              'text-violet-400',
              size === 'sm' ? 'w-5 h-5' : size === 'md' ? 'w-8 h-8' : 'w-10 h-10'
            )}
          />
        </motion.div>
      )}

      {message && (
        <span className={cn('text-white/60', textSizeClasses[size])}>{message}</span>
      )}
    </div>
  );

  if (fullArea) {
    return (
      <div className="flex items-center justify-center w-full h-full min-h-[200px]">
        {content}
      </div>
    );
  }

  return content;
}

/**
 * Inline loading spinner for buttons etc.
 */
export function InlineLoader({ className }: { className?: string }) {
  return (
    <Loader2
      className={cn('w-4 h-4 animate-spin', className)}
      aria-hidden="true"
    />
  );
}

/**
 * Skeleton loading placeholder
 */
export function Skeleton({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) {
  return (
    <div
      className={cn('animate-pulse rounded-md bg-white/10', className)}
      {...props}
    />
  );
}

export default LoadingState;
````

## File: packages/frontend/components/ui/MarkdownContent.tsx
````typescript
import React, { useMemo } from 'react';
import { cn } from '@/lib/utils';

interface MarkdownContentProps {
  content: string;
  className?: string;
}

/**
 * Lightweight markdown renderer for scene content.
 * Handles bold, headings, italic, and line breaks without
 * requiring a heavy markdown library.
 */
export function MarkdownContent({ content, className }: MarkdownContentProps) {
  const rendered = useMemo(() => {
    if (!content) return '';

    let html = content
      // Escape HTML entities
      .replace(/&/g, '&amp;')
      .replace(/</g, '&lt;')
      .replace(/>/g, '&gt;')
      // Headings (### -> h4, ## -> h3)
      .replace(
        /^###\s+(.+)$/gm,
        '<h4 class="heading-card mt-4 mb-2">$1</h4>'
      )
      .replace(
        /^##\s+(.+)$/gm,
        '<h3 class="heading-section mt-4 mb-2">$1</h3>'
      )
      // Bold (**text** or __text__)
      .replace(
        /\*\*(.+?)\*\*/g,
        '<strong class="font-editorial font-semibold text-[oklch(0.95_0.01_60)]">$1</strong>'
      )
      .replace(
        /__(.+?)__/g,
        '<strong class="font-editorial font-semibold text-[oklch(0.95_0.01_60)]">$1</strong>'
      )
      // Italic (*text* or _text_) - must come after bold
      .replace(
        /(?<!\*)\*(?!\*)(.+?)(?<!\*)\*(?!\*)/g,
        '<em class="font-script italic text-[oklch(0.80_0.02_60)]">$1</em>'
      )
      // Bullet lists (- item or * item)
      .replace(
        /^[\-\*]\s+(.+)$/gm,
        '<li class="text-body-editorial ps-2">$1</li>'
      )
      // Line breaks
      .replace(/\n\n/g, '</p><p class="mt-3">')
      .replace(/\n/g, '<br />');

    // Wrap standalone li elements in ul
    html = html.replace(
      /(<li[^>]*>.*?<\/li>(?:\s*<br\s*\/?>\s*)?)+/g,
      (match) => `<ul class="space-y-1 my-2 list-none">${match.replace(/<br\s*\/?>/g, '')}</ul>`
    );

    return html;
  }, [content]);

  return (
    <div
      dir="auto"
      className={cn('text-body-editorial', className)}
      dangerouslySetInnerHTML={{ __html: rendered }}
    />
  );
}

export default MarkdownContent;
````

## File: packages/frontend/components/ui/progress.tsx
````typescript
"use client"

import * as React from "react"
import * as ProgressPrimitive from "@radix-ui/react-progress"

import { cn } from "@/lib/utils"

function Progress({
  className,
  value,
  ...props
}: React.ComponentProps<typeof ProgressPrimitive.Root>) {
  return (
    <ProgressPrimitive.Root
      data-slot="progress"
      className={cn(
        "bg-primary/20 relative h-2 w-full overflow-hidden rounded-full",
        className
      )}
      {...props}
    >
      <ProgressPrimitive.Indicator
        data-slot="progress-indicator"
        className="bg-primary h-full w-full flex-1 transition-all"
        style={{ transform: `translateX(-${100 - (value || 0)}%)` }}
      />
    </ProgressPrimitive.Root>
  )
}

export { Progress }
````

## File: packages/frontend/components/ui/scroll-area.tsx
````typescript
import * as React from "react"
import * as ScrollAreaPrimitive from "@radix-ui/react-scroll-area"

import { cn } from "@/lib/utils"

function ScrollArea({
  className,
  children,
  ...props
}: React.ComponentProps<typeof ScrollAreaPrimitive.Root>) {
  return (
    <ScrollAreaPrimitive.Root
      data-slot="scroll-area"
      className={cn("relative", className)}
      {...props}
    >
      <ScrollAreaPrimitive.Viewport
        data-slot="scroll-area-viewport"
        className="focus-visible:ring-ring/50 size-full rounded-[inherit] transition-[color,box-shadow] outline-none focus-visible:ring-[3px] focus-visible:outline-1"
      >
        {children}
      </ScrollAreaPrimitive.Viewport>
      <ScrollBar />
      <ScrollAreaPrimitive.Corner />
    </ScrollAreaPrimitive.Root>
  )
}

function ScrollBar({
  className,
  orientation = "vertical",
  ...props
}: React.ComponentProps<typeof ScrollAreaPrimitive.ScrollAreaScrollbar>) {
  return (
    <ScrollAreaPrimitive.ScrollAreaScrollbar
      data-slot="scroll-area-scrollbar"
      orientation={orientation}
      className={cn(
        "flex touch-none p-px transition-colors select-none",
        orientation === "vertical" &&
          "h-full w-2.5 border-l border-l-transparent",
        orientation === "horizontal" &&
          "h-2.5 flex-col border-t border-t-transparent",
        className
      )}
      {...props}
    >
      <ScrollAreaPrimitive.ScrollAreaThumb
        data-slot="scroll-area-thumb"
        className="bg-border relative flex-1 rounded-full"
      />
    </ScrollAreaPrimitive.ScrollAreaScrollbar>
  )
}

export { ScrollArea, ScrollBar }
````

## File: packages/frontend/components/ui/select.tsx
````typescript
"use client"

import * as React from "react"
import * as SelectPrimitive from "@radix-ui/react-select"
import { Check, ChevronDown, ChevronUp } from "lucide-react"

import { cn } from "@/lib/utils"

const Select = SelectPrimitive.Root

const SelectGroup = SelectPrimitive.Group

const SelectValue = SelectPrimitive.Value

const SelectTrigger = React.forwardRef<
    React.ElementRef<typeof SelectPrimitive.Trigger>,
    React.ComponentPropsWithoutRef<typeof SelectPrimitive.Trigger>
>(({ className, children, ...props }, ref) => (
    <SelectPrimitive.Trigger
        ref={ref}
        className={cn(
            "flex h-9 w-full items-center justify-between whitespace-nowrap rounded-md border border-input bg-transparent px-3 py-2 text-sm shadow-sm ring-offset-background placeholder:text-muted-foreground focus:outline-none focus:ring-1 focus:ring-ring disabled:cursor-not-allowed disabled:opacity-50 [&>span]:line-clamp-1",
            className
        )}
        {...props}
    >
        {children}
        <SelectPrimitive.Icon asChild>
            <ChevronDown className="h-4 w-4 opacity-50" />
        </SelectPrimitive.Icon>
    </SelectPrimitive.Trigger>
))
SelectTrigger.displayName = SelectPrimitive.Trigger.displayName

const SelectScrollUpButton = React.forwardRef<
    React.ElementRef<typeof SelectPrimitive.ScrollUpButton>,
    React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollUpButton>
>(({ className, ...props }, ref) => (
    <SelectPrimitive.ScrollUpButton
        ref={ref}
        className={cn(
            "flex cursor-default items-center justify-center py-1",
            className
        )}
        {...props}
    >
        <ChevronUp className="h-4 w-4" />
    </SelectPrimitive.ScrollUpButton>
))
SelectScrollUpButton.displayName = SelectPrimitive.ScrollUpButton.displayName

const SelectScrollDownButton = React.forwardRef<
    React.ElementRef<typeof SelectPrimitive.ScrollDownButton>,
    React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollDownButton>
>(({ className, ...props }, ref) => (
    <SelectPrimitive.ScrollDownButton
        ref={ref}
        className={cn(
            "flex cursor-default items-center justify-center py-1",
            className
        )}
        {...props}
    >
        <ChevronDown className="h-4 w-4" />
    </SelectPrimitive.ScrollDownButton>
))
SelectScrollDownButton.displayName =
    SelectPrimitive.ScrollDownButton.displayName

const SelectContent = React.forwardRef<
    React.ElementRef<typeof SelectPrimitive.Content>,
    React.ComponentPropsWithoutRef<typeof SelectPrimitive.Content>
>(({ className, children, position = "popper", ...props }, ref) => (
    <SelectPrimitive.Portal>
        <SelectPrimitive.Content
            ref={ref}
            className={cn(
                "relative z-50 max-h-96 min-w-32 overflow-hidden rounded-md border bg-popover text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
                position === "popper" &&
                "data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",
                className
            )}
            position={position}
            {...props}
        >
            <SelectScrollUpButton />
            <SelectPrimitive.Viewport
                className={cn(
                    "p-1",
                    position === "popper" &&
                    "h-(--radix-select-trigger-height) w-full min-w-(--radix-select-trigger-width)"
                )}
            >
                {children}
            </SelectPrimitive.Viewport>
            <SelectScrollDownButton />
        </SelectPrimitive.Content>
    </SelectPrimitive.Portal>
))
SelectContent.displayName = SelectPrimitive.Content.displayName

const SelectLabel = React.forwardRef<
    React.ElementRef<typeof SelectPrimitive.Label>,
    React.ComponentPropsWithoutRef<typeof SelectPrimitive.Label>
>(({ className, ...props }, ref) => (
    <SelectPrimitive.Label
        ref={ref}
        className={cn("px-2 py-1.5 text-sm font-semibold", className)}
        {...props}
    />
))
SelectLabel.displayName = SelectPrimitive.Label.displayName

const SelectItem = React.forwardRef<
    React.ElementRef<typeof SelectPrimitive.Item>,
    React.ComponentPropsWithoutRef<typeof SelectPrimitive.Item>
>(({ className, children, ...props }, ref) => (
    <SelectPrimitive.Item
        ref={ref}
        className={cn(
            "relative flex w-full cursor-default select-none items-center rounded-sm py-1.5 pl-2 pr-8 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-disabled:pointer-events-none data-disabled:opacity-50",
            className
        )}
        {...props}
    >
        <span className="absolute right-2 flex h-3.5 w-3.5 items-center justify-center">
            <SelectPrimitive.ItemIndicator>
                <Check className="h-4 w-4" />
            </SelectPrimitive.ItemIndicator>
        </span>
        <SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
    </SelectPrimitive.Item>
))
SelectItem.displayName = SelectPrimitive.Item.displayName

const SelectSeparator = React.forwardRef<
    React.ElementRef<typeof SelectPrimitive.Separator>,
    React.ComponentPropsWithoutRef<typeof SelectPrimitive.Separator>
>(({ className, ...props }, ref) => (
    <SelectPrimitive.Separator
        ref={ref}
        className={cn("-mx-1 my-1 h-px bg-muted", className)}
        {...props}
    />
))
SelectSeparator.displayName = SelectPrimitive.Separator.displayName

export {
    Select,
    SelectGroup,
    SelectValue,
    SelectTrigger,
    SelectContent,
    SelectLabel,
    SelectItem,
    SelectSeparator,
    SelectScrollUpButton,
    SelectScrollDownButton,
}
````

## File: packages/frontend/components/ui/skeleton.tsx
````typescript
import * as React from "react"
import { cn } from "@/lib/utils"

interface SkeletonProps extends React.HTMLAttributes<HTMLDivElement> {
    /**
     * The variant of the skeleton
     * - text: For text placeholders (default height, full width)
     * - circular: For avatar/icon placeholders (circle shape)
     * - rectangular: For image/card placeholders (larger height)
     */
    variant?: "text" | "circular" | "rectangular"
    /**
     * Width of the skeleton (CSS value)
     */
    width?: string | number
    /**
     * Height of the skeleton (CSS value)
     */
    height?: string | number
    /**
     * Whether to animate the skeleton
     */
    animate?: boolean
}

/**
 * Skeleton component for loading states.
 * Provides visual feedback while content is loading.
 * 
 * @example
 * // Text skeleton
 * <Skeleton className="h-4 w-full" />
 * 
 * // Circular skeleton (avatar)
 * <Skeleton variant="circular" className="w-10 h-10" />
 * 
 * // Rectangular skeleton (image)
 * <Skeleton variant="rectangular" className="w-full h-48" />
 */
function Skeleton({
    className,
    variant = "text",
    width,
    height,
    animate = true,
    ...props
}: SkeletonProps) {
    const variantStyles = {
        text: "h-4 rounded",
        circular: "rounded-full aspect-square",
        rectangular: "h-24 rounded-lg",
    }

    const style: React.CSSProperties = {
        width: typeof width === "number" ? `${width}px` : width,
        height: typeof height === "number" ? `${height}px` : height,
    }

    return (
        <div
            data-slot="skeleton"
            aria-hidden="true"
            role="presentation"
            className={cn(
                "bg-muted/50",
                animate && "animate-pulse",
                variantStyles[variant],
                className
            )}
            style={style}
            {...props}
        />
    )
}

/**
 * SkeletonText - Multiple skeleton lines for paragraph loading
 */
function SkeletonText({
    lines = 3,
    className,
    ...props
}: { lines?: number } & React.HTMLAttributes<HTMLDivElement>) {
    return (
        <div className={cn("space-y-2", className)} {...props}>
            {Array.from({ length: lines }).map((_, i) => (
                <Skeleton
                    key={i}
                    variant="text"
                    className={cn(
                        "h-4",
                        // Last line is shorter for natural look
                        i === lines - 1 && "w-3/4"
                    )}
                />
            ))}
        </div>
    )
}

/**
 * SkeletonCard - Card-shaped skeleton with header and content
 */
function SkeletonCard({ className, ...props }: React.HTMLAttributes<HTMLDivElement>) {
    return (
        <div
            className={cn(
                "rounded-lg border border-border/50 p-4 space-y-4",
                className
            )}
            {...props}
        >
            {/* Header with avatar and title */}
            <div className="flex items-center gap-3">
                <Skeleton variant="circular" className="w-10 h-10" />
                <div className="flex-1 space-y-2">
                    <Skeleton variant="text" className="h-4 w-1/2" />
                    <Skeleton variant="text" className="h-3 w-1/3" />
                </div>
            </div>
            {/* Content */}
            <Skeleton variant="rectangular" className="h-32 w-full" />
            {/* Footer */}
            <div className="flex gap-2">
                <Skeleton variant="text" className="h-8 w-20" />
                <Skeleton variant="text" className="h-8 w-20" />
            </div>
        </div>
    )
}

/**
 * SkeletonImage - Image placeholder with aspect ratio support
 */
function SkeletonImage({
    aspectRatio = "16/9",
    className,
    ...props
}: { aspectRatio?: string } & React.HTMLAttributes<HTMLDivElement>) {
    return (
        <Skeleton
            variant="rectangular"
            className={cn("w-full", className)}
            style={{ aspectRatio }}
            {...props}
        />
    )
}

/**
 * SkeletonStoryCard - Skeleton for story/shot cards in Story Mode
 */
function SkeletonStoryCard({ className, ...props }: React.HTMLAttributes<HTMLDivElement>) {
    return (
        <div
            className={cn(
                "rounded-xl border border-white/10 bg-black/20 p-4 space-y-3",
                className
            )}
            {...props}
        >
            <SkeletonImage aspectRatio="16/9" className="rounded-lg" />
            <div className="space-y-2">
                <Skeleton variant="text" className="h-5 w-3/4" />
                <Skeleton variant="text" className="h-4 w-full" />
                <Skeleton variant="text" className="h-4 w-2/3" />
            </div>
            <div className="flex items-center gap-2 pt-2">
                <Skeleton variant="text" className="h-8 w-24 rounded-full" />
                <Skeleton variant="text" className="h-8 w-24 rounded-full" />
            </div>
        </div>
    )
}

/**
 * SkeletonShotGrid - Grid of skeleton shot cards
 */
function SkeletonShotGrid({
    count = 6,
    className,
    ...props
}: { count?: number } & React.HTMLAttributes<HTMLDivElement>) {
    return (
        <div
            className={cn(
                "grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-4",
                className
            )}
            {...props}
        >
            {Array.from({ length: count }).map((_, i) => (
                <SkeletonStoryCard key={i} />
            ))}
        </div>
    )
}

/**
 * SkeletonTimeline - Timeline/video editor skeleton
 */
function SkeletonTimeline({ className, ...props }: React.HTMLAttributes<HTMLDivElement>) {
    return (
        <div
            className={cn(
                "rounded-lg border border-white/10 bg-black/30 p-4 space-y-3",
                className
            )}
            {...props}
        >
            {/* Playback controls */}
            <div className="flex items-center gap-3">
                <Skeleton variant="circular" className="w-10 h-10" />
                <Skeleton variant="text" className="h-2 flex-1 rounded-full" />
                <Skeleton variant="text" className="h-4 w-16" />
            </div>
            {/* Timeline tracks */}
            <div className="space-y-2">
                {[1, 2, 3].map((i) => (
                    <div key={i} className="flex items-center gap-2">
                        <Skeleton variant="text" className="h-4 w-16" />
                        <div className="flex-1 flex gap-1">
                            {Array.from({ length: 4 + i }).map((_, j) => (
                                <Skeleton
                                    key={j}
                                    variant="rectangular"
                                    className="h-12 rounded"
                                    style={{ width: `${15 + Math.random() * 20}%` }}
                                />
                            ))}
                        </div>
                    </div>
                ))}
            </div>
        </div>
    )
}

/**
 * SkeletonCharacterCard - Character profile skeleton
 */
function SkeletonCharacterCard({ className, ...props }: React.HTMLAttributes<HTMLDivElement>) {
    return (
        <div
            className={cn(
                "rounded-xl border border-white/10 bg-black/20 p-4 flex gap-4",
                className
            )}
            {...props}
        >
            <Skeleton variant="circular" className="w-20 h-20 flex-shrink-0" />
            <div className="flex-1 space-y-2">
                <Skeleton variant="text" className="h-5 w-1/2" />
                <Skeleton variant="text" className="h-4 w-3/4" />
                <Skeleton variant="text" className="h-4 w-full" />
                <div className="flex gap-2 pt-2">
                    <Skeleton variant="text" className="h-6 w-16 rounded-full" />
                    <Skeleton variant="text" className="h-6 w-16 rounded-full" />
                </div>
            </div>
        </div>
    )
}

/**
 * SkeletonSceneBreakdown - Scene breakdown list skeleton
 */
function SkeletonSceneBreakdown({
    sceneCount = 3,
    className,
    ...props
}: { sceneCount?: number } & React.HTMLAttributes<HTMLDivElement>) {
    return (
        <div className={cn("space-y-4", className)} {...props}>
            {Array.from({ length: sceneCount }).map((_, i) => (
                <div
                    key={i}
                    className="rounded-lg border border-white/10 bg-black/20 p-4 space-y-3"
                >
                    <div className="flex items-center justify-between">
                        <Skeleton variant="text" className="h-6 w-32" />
                        <Skeleton variant="text" className="h-4 w-20" />
                    </div>
                    <SkeletonText lines={2} />
                    <div className="flex gap-2">
                        <Skeleton variant="text" className="h-6 w-20 rounded-full" />
                        <Skeleton variant="text" className="h-6 w-24 rounded-full" />
                    </div>
                </div>
            ))}
        </div>
    )
}

/**
 * SkeletonProjectCard - Project card skeleton for gallery
 */
function SkeletonProjectCard({ className, ...props }: React.HTMLAttributes<HTMLDivElement>) {
    return (
        <div
            className={cn(
                "rounded-xl border border-white/10 bg-black/20 overflow-hidden",
                className
            )}
            {...props}
        >
            <SkeletonImage aspectRatio="16/9" />
            <div className="p-4 space-y-2">
                <Skeleton variant="text" className="h-5 w-2/3" />
                <Skeleton variant="text" className="h-4 w-1/2" />
                <div className="flex items-center justify-between pt-2">
                    <Skeleton variant="text" className="h-4 w-20" />
                    <Skeleton variant="circular" className="w-8 h-8" />
                </div>
            </div>
        </div>
    )
}

export {
    Skeleton,
    SkeletonText,
    SkeletonCard,
    SkeletonImage,
    SkeletonStoryCard,
    SkeletonShotGrid,
    SkeletonTimeline,
    SkeletonCharacterCard,
    SkeletonSceneBreakdown,
    SkeletonProjectCard
}
````

## File: packages/frontend/components/ui/skip-link.tsx
````typescript
import * as React from "react"
import { cn } from "@/lib/utils"

interface SkipLinkProps extends React.AnchorHTMLAttributes<HTMLAnchorElement> {
    /**
     * The ID of the element to skip to (without #)
     * @default "main-content"
     */
    targetId?: string
    /**
     * The text to display in the skip link
     */
    children?: React.ReactNode
}

/**
 * SkipLink component for keyboard accessibility.
 * 
 * Allows keyboard users to skip navigation and jump directly to main content.
 * The link is visually hidden until focused, then appears at the top of the viewport.
 * 
 * CSS classes are defined in index.css (.skip-to-content)
 * 
 * @example
 * // In your layout component:
 * <SkipLink />
 * <Header />
 * <main id="main-content">
 *   ...content
 * </main>
 */
function SkipLink({
    targetId = "main-content",
    children,
    className,
    ...props
}: SkipLinkProps) {
    const handleClick = (e: React.MouseEvent<HTMLAnchorElement>) => {
        e.preventDefault()
        const target = document.getElementById(targetId)
        if (target) {
            // Set focus to the target element
            target.setAttribute("tabindex", "-1")
            target.focus()
            // Scroll to the target
            target.scrollIntoView({ behavior: "smooth" })
            // Remove tabindex after focus to maintain normal tab order
            target.addEventListener(
                "blur",
                () => target.removeAttribute("tabindex"),
                { once: true }
            )
        }
    }

    return (
        <a
            href={`#${targetId}`}
            onClick={handleClick}
            className={cn("skip-to-content", className)}
            {...props}
        >
            {children || "Skip to main content"}
        </a>
    )
}

export { SkipLink }
````

## File: packages/frontend/components/ui/SlidePanel.tsx
````typescript
/**
 * SlidePanel - Slide-in side panel component
 *
 * A reusable slide-in panel for editing sidebars, scene editors, etc.
 * Includes focus trapping and RTL support.
 */

import React from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import { X } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { cn } from '@/lib/utils';
import { useFocusTrap } from '@/hooks/useFocusTrap';

export interface SlidePanelProps {
  /** Whether the panel is open */
  isOpen: boolean;
  /** Callback when panel should close */
  onClose: () => void;
  /** Panel title */
  title: string;
  /** RTL layout */
  isRTL?: boolean;
  /** Panel width class */
  width?: string;
  /** Panel content */
  children: React.ReactNode;
  /** Additional class names for the panel */
  className?: string;
  /** Whether to show the close button */
  showCloseButton?: boolean;
  /** ID for the title element (for aria-labelledby) */
  titleId?: string;
}

/**
 * Slide-in panel with focus trapping
 *
 * @example
 * ```tsx
 * <SlidePanel
 *   isOpen={showEditor}
 *   onClose={() => setShowEditor(false)}
 *   title="Edit Scene"
 *   isRTL={isRTL}
 * >
 *   <SceneEditor ... />
 * </SlidePanel>
 * ```
 */
export function SlidePanel({
  isOpen,
  onClose,
  title,
  isRTL = false,
  width = 'max-w-md',
  children,
  className,
  showCloseButton = true,
  titleId,
}: SlidePanelProps) {
  const panelRef = useFocusTrap<HTMLDivElement>({
    isActive: isOpen,
    onEscape: onClose,
    returnFocusOnDeactivate: true,
  });

  const generatedTitleId = titleId || `slide-panel-title-${React.useId()}`;

  return (
    <AnimatePresence>
      {isOpen && (
        <motion.div
          initial={{ opacity: 0 }}
          animate={{ opacity: 1 }}
          exit={{ opacity: 0 }}
          className="fixed inset-0 bg-black/80 backdrop-blur-sm z-50 flex"
          style={{ justifyContent: isRTL ? 'flex-start' : 'flex-end' }}
          onClick={onClose}
          role="dialog"
          aria-modal="true"
          aria-labelledby={generatedTitleId}
        >
          <motion.div
            ref={panelRef}
            initial={{ x: isRTL ? '-100%' : '100%' }}
            animate={{ x: 0 }}
            exit={{ x: isRTL ? '-100%' : '100%' }}
            transition={{ type: 'spring', damping: 25, stiffness: 200 }}
            className={cn(
              'w-full bg-[#12121a] h-full overflow-y-auto p-4',
              width,
              isRTL ? 'border-e border-white/10' : 'border-s border-white/10',
              className
            )}
            onClick={(e) => e.stopPropagation()}
          >
            {/* Header */}
            <div
              className={cn(
                'flex items-center justify-between mb-6',
                isRTL && 'flex-row-reverse'
              )}
            >
              <h2 id={generatedTitleId} className="text-lg font-semibold text-white">
                {title}
              </h2>
              {showCloseButton && (
                <Button
                  variant="ghost"
                  size="sm"
                  onClick={onClose}
                  aria-label="Close panel"
                >
                  <X className="w-5 h-5" aria-hidden="true" />
                </Button>
              )}
            </div>

            {/* Content */}
            {children}
          </motion.div>
        </motion.div>
      )}
    </AnimatePresence>
  );
}

export default SlidePanel;
````

## File: packages/frontend/components/ui/slider.tsx
````typescript
import * as React from "react"
import * as SliderPrimitive from "@radix-ui/react-slider"

import { cn } from "@/lib/utils"

function Slider({
  className,
  defaultValue,
  value,
  min = 0,
  max = 100,
  ...props
}: React.ComponentProps<typeof SliderPrimitive.Root>) {
  const _values = React.useMemo(
    () =>
      Array.isArray(value)
        ? value
        : Array.isArray(defaultValue)
          ? defaultValue
          : [min, max],
    [value, defaultValue, min, max]
  )

  return (
    <SliderPrimitive.Root
      data-slot="slider"
      defaultValue={defaultValue}
      value={value}
      min={min}
      max={max}
      className={cn(
        "relative flex w-full touch-none items-center select-none data-[disabled]:opacity-50 data-[orientation=vertical]:h-full data-[orientation=vertical]:min-h-44 data-[orientation=vertical]:w-auto data-[orientation=vertical]:flex-col",
        className
      )}
      {...props}
    >
      <SliderPrimitive.Track
        data-slot="slider-track"
        className={cn(
          "bg-muted relative grow overflow-hidden rounded-full data-[orientation=horizontal]:h-1.5 data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-1.5"
        )}
      >
        <SliderPrimitive.Range
          data-slot="slider-range"
          className={cn(
            "bg-primary absolute data-[orientation=horizontal]:h-full data-[orientation=vertical]:w-full"
          )}
        />
      </SliderPrimitive.Track>
      {Array.from({ length: _values.length }, (_, index) => (
        <SliderPrimitive.Thumb
          data-slot="slider-thumb"
          key={index}
          className="border-primary ring-ring/50 block size-4 shrink-0 rounded-full border bg-white shadow-sm transition-[color,box-shadow] hover:ring-4 focus-visible:ring-4 focus-visible:outline-hidden disabled:pointer-events-none disabled:opacity-50"
        />
      ))}
    </SliderPrimitive.Root>
  )
}

export { Slider }
````

## File: packages/frontend/components/ui/switch.tsx
````typescript
import * as React from "react"
import * as SwitchPrimitive from "@radix-ui/react-switch"

import { cn } from "@/lib/utils"

function Switch({
  className,
  ...props
}: React.ComponentProps<typeof SwitchPrimitive.Root>) {
  return (
    <SwitchPrimitive.Root
      data-slot="switch"
      className={cn(
        "peer data-[state=checked]:bg-primary data-[state=unchecked]:bg-input focus-visible:border-ring focus-visible:ring-ring/50 dark:data-[state=unchecked]:bg-input/80 inline-flex h-[1.15rem] w-8 shrink-0 items-center rounded-full border border-transparent shadow-xs transition-all outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50",
        className
      )}
      {...props}
    >
      <SwitchPrimitive.Thumb
        data-slot="switch-thumb"
        className={cn(
          "bg-background dark:data-[state=unchecked]:bg-foreground dark:data-[state=checked]:bg-primary-foreground pointer-events-none block size-4 rounded-full ring-0 transition-transform data-[state=checked]:translate-x-[calc(100%-2px)] data-[state=unchecked]:translate-x-0"
        )}
      />
    </SwitchPrimitive.Root>
  )
}

export { Switch }
````

## File: packages/frontend/components/ui/tabs.tsx
````typescript
"use client"

import * as React from "react"
import * as TabsPrimitive from "@radix-ui/react-tabs"

import { cn } from "@/lib/utils"

function Tabs({
  className,
  ...props
}: React.ComponentProps<typeof TabsPrimitive.Root>) {
  return (
    <TabsPrimitive.Root
      data-slot="tabs"
      className={cn("flex flex-col gap-2", className)}
      {...props}
    />
  )
}

function TabsList({
  className,
  ...props
}: React.ComponentProps<typeof TabsPrimitive.List>) {
  return (
    <TabsPrimitive.List
      data-slot="tabs-list"
      className={cn(
        "bg-muted text-muted-foreground inline-flex h-9 w-fit items-center justify-center rounded-lg p-[3px]",
        className
      )}
      {...props}
    />
  )
}

function TabsTrigger({
  className,
  ...props
}: React.ComponentProps<typeof TabsPrimitive.Trigger>) {
  return (
    <TabsPrimitive.Trigger
      data-slot="tabs-trigger"
      className={cn(
        "data-[state=active]:bg-background dark:data-[state=active]:text-foreground focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:outline-ring dark:data-[state=active]:border-input dark:data-[state=active]:bg-input/30 text-foreground dark:text-muted-foreground inline-flex h-[calc(100%-1px)] flex-1 items-center justify-center gap-1.5 rounded-md border border-transparent px-2 py-1 text-sm font-medium whitespace-nowrap transition-[color,box-shadow] focus-visible:ring-[3px] focus-visible:outline-1 disabled:pointer-events-none disabled:opacity-50 data-[state=active]:shadow-sm [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
        className
      )}
      {...props}
    />
  )
}

function TabsContent({
  className,
  ...props
}: React.ComponentProps<typeof TabsPrimitive.Content>) {
  return (
    <TabsPrimitive.Content
      data-slot="tabs-content"
      className={cn("flex-1 outline-none", className)}
      {...props}
    />
  )
}

export { Tabs, TabsList, TabsTrigger, TabsContent }
````

## File: packages/frontend/components/ui/textarea.tsx
````typescript
import * as React from "react"

import { cn } from "@/lib/utils"

function Textarea({ className, ...props }: React.ComponentProps<"textarea">) {
  return (
    <textarea
      data-slot="textarea"
      className={cn(
        "border-input placeholder:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 flex field-sizing-content min-h-16 w-full rounded-md border bg-transparent px-3 py-2 text-base shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",
        className
      )}
      {...props}
    />
  )
}

export { Textarea }
````

## File: packages/frontend/components/ui/toast.tsx
````typescript
import * as React from "react";
import * as ToastPrimitives from "@radix-ui/react-toast";
import { X } from "lucide-react";
import { cn } from "@/lib/utils";

const ToastProvider = ToastPrimitives.Provider;

const ToastViewport = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Viewport>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Viewport>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Viewport
    ref={ref}
    className={cn(
      "fixed bottom-0 right-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]",
      className
    )}
    {...props}
  />
));
ToastViewport.displayName = ToastPrimitives.Viewport.displayName;

const Toast = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Root>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Root> & {
    variant?: "default" | "success" | "error" | "warning";
  }
>(({ className, variant = "default", ...props }, ref) => {
  const variantStyles = {
    default: "bg-[var(--cinema-celluloid)]/95 border-[var(--cinema-silver)]/20 text-[var(--cinema-silver)]",
    success: "bg-emerald-900/95 border-emerald-500/30 text-emerald-100",
    error: "bg-red-900/95 border-red-500/30 text-red-100",
    warning: "bg-amber-900/95 border-amber-500/30 text-amber-100",
  };

  return (
    <ToastPrimitives.Root
      ref={ref}
      className={cn(
        "group pointer-events-auto relative flex w-full items-center justify-between space-x-4 overflow-hidden rounded-lg border p-4 pr-8 shadow-lg backdrop-blur-sm transition-all data-[swipe=cancel]:translate-x-0 data-[swipe=end]:translate-x-[var(--radix-toast-swipe-end-x)] data-[swipe=move]:translate-x-[var(--radix-toast-swipe-move-x)] data-[swipe=move]:transition-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[swipe=end]:animate-out data-[state=closed]:fade-out-80 data-[state=closed]:slide-out-to-right-full data-[state=open]:slide-in-from-bottom-full data-[state=open]:sm:slide-in-from-bottom-full",
        variantStyles[variant],
        className
      )}
      {...props}
    />
  );
});
Toast.displayName = ToastPrimitives.Root.displayName;

const ToastAction = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Action>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Action>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Action
    ref={ref}
    className={cn(
      "inline-flex h-8 shrink-0 items-center justify-center rounded-md border border-[var(--cinema-silver)]/30 bg-transparent px-3 text-xs font-medium transition-colors hover:bg-[var(--cinema-silver)]/10 focus:outline-none focus:ring-2 focus:ring-[var(--cinema-spotlight)] disabled:pointer-events-none disabled:opacity-50",
      className
    )}
    {...props}
  />
));
ToastAction.displayName = ToastPrimitives.Action.displayName;

const ToastClose = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Close>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Close>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Close
    ref={ref}
    className={cn(
      "absolute right-2 top-2 rounded-md p-1 text-[var(--cinema-silver)]/50 opacity-0 transition-opacity hover:text-[var(--cinema-silver)] focus:opacity-100 focus:outline-none focus:ring-2 focus:ring-[var(--cinema-spotlight)] group-hover:opacity-100",
      className
    )}
    toast-close=""
    {...props}
  >
    <X className="h-4 w-4" />
    <span className="sr-only">Close</span>
  </ToastPrimitives.Close>
));
ToastClose.displayName = ToastPrimitives.Close.displayName;

const ToastTitle = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Title>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Title>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Title
    ref={ref}
    className={cn("font-editorial text-sm font-semibold", className)}
    {...props}
  />
));
ToastTitle.displayName = ToastPrimitives.Title.displayName;

const ToastDescription = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Description>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Description>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Description
    ref={ref}
    className={cn("text-xs opacity-90", className)}
    {...props}
  />
));
ToastDescription.displayName = ToastPrimitives.Description.displayName;

type ToastProps = React.ComponentPropsWithoutRef<typeof Toast>;
type ToastActionElement = React.ReactElement<typeof ToastAction>;

export {
  type ToastProps,
  type ToastActionElement,
  ToastProvider,
  ToastViewport,
  Toast,
  ToastTitle,
  ToastDescription,
  ToastClose,
  ToastAction,
};
````

## File: packages/frontend/components/ui/toaster.tsx
````typescript
import {
  Toast,
  ToastClose,
  ToastDescription,
  ToastProvider,
  ToastTitle,
  ToastViewport,
} from "./toast";
import { useToast } from "./use-toast";

export function Toaster() {
  const { toasts } = useToast();

  return (
    <ToastProvider>
      {toasts.map(function ({ id, title, description, action, ...props }) {
        return (
          <Toast key={id} {...props}>
            <div className="grid gap-1">
              {title && <ToastTitle>{title}</ToastTitle>}
              {description && (
                <ToastDescription>{description}</ToastDescription>
              )}
            </div>
            {action}
            <ToastClose />
          </Toast>
        );
      })}
      <ToastViewport />
    </ToastProvider>
  );
}
````

## File: packages/frontend/components/ui/tooltip.tsx
````typescript
import * as React from "react";
import * as TooltipPrimitive from "@radix-ui/react-tooltip";
import { cn } from "@/lib/utils";

const TooltipProvider = TooltipPrimitive.Provider;

const Tooltip = TooltipPrimitive.Root;

const TooltipTrigger = TooltipPrimitive.Trigger;

const TooltipContent = React.forwardRef<
  React.ElementRef<typeof TooltipPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof TooltipPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <TooltipPrimitive.Portal>
    <TooltipPrimitive.Content
      ref={ref}
      sideOffset={sideOffset}
      className={cn(
        "z-50 overflow-hidden rounded-lg bg-card border border-border/50 px-3 py-1.5 text-xs text-foreground shadow-xl shadow-black/20 animate-in fade-in-0 zoom-in-95 data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=closed]:zoom-out-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className
      )}
      {...props}
    />
  </TooltipPrimitive.Portal>
));
TooltipContent.displayName = TooltipPrimitive.Content.displayName;

export { Tooltip, TooltipTrigger, TooltipContent, TooltipProvider };
````

## File: packages/frontend/components/ui/use-toast.tsx
````typescript
import * as React from "react";
import type { ToastActionElement, ToastProps } from "./toast";

const TOAST_LIMIT = 3;
const TOAST_REMOVE_DELAY = 5000;

type ToasterToast = ToastProps & {
  id: string;
  title?: React.ReactNode;
  description?: React.ReactNode;
  action?: ToastActionElement;
  variant?: "default" | "success" | "error" | "warning";
};

const actionTypes = {
  ADD_TOAST: "ADD_TOAST",
  UPDATE_TOAST: "UPDATE_TOAST",
  DISMISS_TOAST: "DISMISS_TOAST",
  REMOVE_TOAST: "REMOVE_TOAST",
} as const;

let count = 0;

function genId() {
  count = (count + 1) % Number.MAX_SAFE_INTEGER;
  return count.toString();
}

type ActionType = typeof actionTypes;

type Action =
  | {
      type: ActionType["ADD_TOAST"];
      toast: ToasterToast;
    }
  | {
      type: ActionType["UPDATE_TOAST"];
      toast: Partial<ToasterToast>;
    }
  | {
      type: ActionType["DISMISS_TOAST"];
      toastId?: ToasterToast["id"];
    }
  | {
      type: ActionType["REMOVE_TOAST"];
      toastId?: ToasterToast["id"];
    };

interface State {
  toasts: ToasterToast[];
}

const toastTimeouts = new Map<string, ReturnType<typeof setTimeout>>();

const addToRemoveQueue = (toastId: string) => {
  if (toastTimeouts.has(toastId)) {
    return;
  }

  const timeout = setTimeout(() => {
    toastTimeouts.delete(toastId);
    dispatch({
      type: "REMOVE_TOAST",
      toastId: toastId,
    });
  }, TOAST_REMOVE_DELAY);

  toastTimeouts.set(toastId, timeout);
};

export const reducer = (state: State, action: Action): State => {
  switch (action.type) {
    case "ADD_TOAST":
      return {
        ...state,
        toasts: [action.toast, ...state.toasts].slice(0, TOAST_LIMIT),
      };

    case "UPDATE_TOAST":
      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === action.toast.id ? { ...t, ...action.toast } : t
        ),
      };

    case "DISMISS_TOAST": {
      const { toastId } = action;

      if (toastId) {
        addToRemoveQueue(toastId);
      } else {
        state.toasts.forEach((toast) => {
          addToRemoveQueue(toast.id);
        });
      }

      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === toastId || toastId === undefined
            ? {
                ...t,
                open: false,
              }
            : t
        ),
      };
    }
    case "REMOVE_TOAST":
      if (action.toastId === undefined) {
        return {
          ...state,
          toasts: [],
        };
      }
      return {
        ...state,
        toasts: state.toasts.filter((t) => t.id !== action.toastId),
      };
  }
};

const listeners: Array<(state: State) => void> = [];

let memoryState: State = { toasts: [] };

function dispatch(action: Action) {
  memoryState = reducer(memoryState, action);
  listeners.forEach((listener) => {
    listener(memoryState);
  });
}

type Toast = Omit<ToasterToast, "id">;

function toast({ ...props }: Toast) {
  const id = genId();

  const update = (props: ToasterToast) =>
    dispatch({
      type: "UPDATE_TOAST",
      toast: { ...props, id },
    });
  const dismiss = () => dispatch({ type: "DISMISS_TOAST", toastId: id });

  dispatch({
    type: "ADD_TOAST",
    toast: {
      ...props,
      id,
      open: true,
      onOpenChange: (open) => {
        if (!open) dismiss();
      },
    },
  });

  return {
    id: id,
    dismiss,
    update,
  };
}

function useToast() {
  const [state, setState] = React.useState<State>(memoryState);

  React.useEffect(() => {
    listeners.push(setState);
    return () => {
      const index = listeners.indexOf(setState);
      if (index > -1) {
        listeners.splice(index, 1);
      }
    };
  }, [state]);

  return {
    ...state,
    toast,
    dismiss: (toastId?: string) => dispatch({ type: "DISMISS_TOAST", toastId }),
  };
}

export { useToast, toast };
````

## File: packages/frontend/components/VideoEditor/CanvasPreview.tsx
````typescript
/**
 * CanvasPreview
 *
 * Live canvas preview that composites visible clips at the current time.
 * Renders video frames, image overlays, and text elements onto a canvas.
 */

import { useRef, useEffect, useCallback, useMemo } from 'react';
import type { EditorClip, AspectRatio } from './types/video-editor-types';
import './video-editor.css';

const ASPECT_DIMENSIONS: Record<AspectRatio, { w: number; h: number }> = {
  '16:9': { w: 1280, h: 720 },
  '9:16': { w: 720, h: 1280 },
  '1:1': { w: 720, h: 720 },
  '4:3': { w: 960, h: 720 },
};

interface CanvasPreviewProps {
  clips: EditorClip[];
  currentTime: number;
  aspectRatio: AspectRatio;
  isPlaying: boolean;
  className?: string;
}

export function CanvasPreview({
  clips,
  currentTime,
  aspectRatio,
  isPlaying,
  className = '',
}: CanvasPreviewProps) {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const imageCache = useRef<Map<string, HTMLImageElement>>(new Map());
  const rafRef = useRef<number>(0);

  const dims = ASPECT_DIMENSIONS[aspectRatio];

  // Get clips visible at the current time, sorted by track order
  const visibleClips = useMemo(() => {
    return clips.filter(c =>
      currentTime >= c.startTime && currentTime < c.startTime + c.duration
    );
  }, [clips, currentTime]);

  // Preload images for image/video clips
  const preloadImage = useCallback((url: string): HTMLImageElement | null => {
    const cached = imageCache.current.get(url);
    if (cached?.complete) return cached;
    if (!cached) {
      const img = new window.Image();
      img.crossOrigin = 'anonymous';
      img.src = url;
      imageCache.current.set(url, img);
      img.onload = () => renderFrame();
    }
    return null;
  }, []);

  const renderFrame = useCallback(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    const ctx = canvas.getContext('2d');
    if (!ctx) return;

    // Clear
    ctx.fillStyle = '#000';
    ctx.fillRect(0, 0, dims.w, dims.h);

    // Render clips bottom-to-top (first = background)
    for (const clip of visibleClips) {
      if (clip.type === 'video' && clip.thumbnailUrl) {
        const img = preloadImage(clip.thumbnailUrl);
        if (img?.complete) {
          ctx.drawImage(img, 0, 0, dims.w, dims.h);
        }
      } else if (clip.type === 'image' && clip.imageUrl) {
        const img = preloadImage(clip.imageUrl);
        if (img?.complete) {
          // Center the image preserving aspect ratio
          const scale = Math.min(dims.w / img.width, dims.h / img.height) * 0.8;
          const drawW = img.width * scale;
          const drawH = img.height * scale;
          ctx.drawImage(img, (dims.w - drawW) / 2, (dims.h - drawH) / 2, drawW, drawH);
        }
      } else if (clip.type === 'text' && clip.text) {
        const style = clip.textStyle ?? {
          fontFamily: 'Inter',
          fontSize: 48,
          fontWeight: '700',
          color: '#ffffff',
          position: { x: 0.5, y: 0.5 },
          alignment: 'center' as const,
        };

        ctx.save();
        ctx.font = `${style.fontWeight} ${style.fontSize}px ${style.fontFamily}`;
        ctx.fillStyle = style.color;
        ctx.textAlign = style.alignment;
        ctx.textBaseline = 'middle';

        // Draw text shadow/outline
        ctx.strokeStyle = 'rgba(0,0,0,0.5)';
        ctx.lineWidth = 3;
        const x = style.position.x * dims.w;
        const y = style.position.y * dims.h;
        ctx.strokeText(clip.text, x, y);
        ctx.fillText(clip.text, x, y);
        ctx.restore();
      }
      // Audio clips don't render on canvas
    }
  }, [dims, visibleClips, preloadImage]);

  // Render on time change or clip change
  useEffect(() => {
    renderFrame();
  }, [renderFrame]);

  // Animation loop when playing
  useEffect(() => {
    if (!isPlaying) return;
    const animate = () => {
      renderFrame();
      rafRef.current = requestAnimationFrame(animate);
    };
    rafRef.current = requestAnimationFrame(animate);
    return () => cancelAnimationFrame(rafRef.current);
  }, [isPlaying, renderFrame]);

  return (
    <div className={`ve-preview ${className}`}>
      <canvas
        ref={canvasRef}
        width={dims.w}
        height={dims.h}
        className="ve-preview-canvas"
        aria-label="Video preview"
      />
      <div className="ve-aspect-badge">{aspectRatio}</div>
    </div>
  );
}
````

## File: packages/frontend/components/VideoEditor/clips/AudioTrackClip.tsx
````typescript
/**
 * AudioTrackClip — Orange audio clip with waveform visualization.
 */

import { Music } from 'lucide-react';
import { useMemo } from 'react';
import type { EditorClip } from '../types/video-editor-types';
import { TRACK_COLORS } from '../types/video-editor-types';
import '../video-editor.css';

interface AudioTrackClipProps {
  clip: EditorClip;
  zoom: number;
  isSelected: boolean;
  onSelect: (clipId: string) => void;
  onResizeStart?: (clipId: string, edge: 'left' | 'right', e: React.PointerEvent) => void;
}

/**
 * Generate pseudo-random waveform bars if no waveformData is provided.
 * Uses a seed derived from the clip ID for consistent rendering.
 */
function generateWaveform(clipId: string, barCount: number): number[] {
  let hash = 0;
  for (let i = 0; i < clipId.length; i++) {
    hash = ((hash << 5) - hash + clipId.charCodeAt(i)) | 0;
  }
  const bars: number[] = [];
  for (let i = 0; i < barCount; i++) {
    hash = ((hash * 1103515245 + 12345) & 0x7fffffff);
    bars.push(0.2 + (hash % 100) / 125);
  }
  return bars;
}

export function AudioTrackClip({ clip, zoom, isSelected, onSelect, onResizeStart }: AudioTrackClipProps) {
  const left = clip.startTime * zoom;
  const width = Math.max(clip.duration * zoom, 20);
  const barCount = Math.max(8, Math.floor(width / 4));

  const waveform = useMemo(() => {
    return clip.waveformData ?? generateWaveform(clip.id, barCount);
  }, [clip.id, clip.waveformData, barCount]);

  const maxBarHeight = 36;
  const color = TRACK_COLORS.audio.waveform;

  return (
    <div
      className={`ve-clip ve-clip--audio ${isSelected ? 'selected' : ''}`}
      style={{ left: `${left}px`, width: `${width}px` }}
      onClick={(e) => { e.stopPropagation(); onSelect(clip.id); }}
      role="button"
      aria-label={`Audio: ${clip.name}`}
      aria-selected={isSelected}
      tabIndex={0}
    >
      {isSelected && (
        <>
          <div
            className="ve-resize-handle ve-resize-handle--left"
            onPointerDown={(e) => onResizeStart?.(clip.id, 'left', e)}
          />
          <div
            className="ve-resize-handle ve-resize-handle--right"
            onPointerDown={(e) => onResizeStart?.(clip.id, 'right', e)}
          />
        </>
      )}
      <Music size={12} className="ve-clip-icon" style={{ flexShrink: 0, marginRight: 2 }} />
      <div className="ve-waveform">
        {waveform.slice(0, barCount).map((amp, i) => (
          <div
            key={i}
            className="ve-wave-bar"
            style={{
              height: `${Math.max(4, amp * maxBarHeight)}px`,
              background: color,
            }}
          />
        ))}
      </div>
    </div>
  );
}
````

## File: packages/frontend/components/VideoEditor/clips/ImageTrackClip.tsx
````typescript
/**
 * ImageTrackClip — Green image clip with thumbnail preview.
 */

import { ImageIcon } from 'lucide-react';
import type { EditorClip } from '../types/video-editor-types';
import '../video-editor.css';

interface ImageTrackClipProps {
  clip: EditorClip;
  zoom: number;
  isSelected: boolean;
  onSelect: (clipId: string) => void;
  onResizeStart?: (clipId: string, edge: 'left' | 'right', e: React.PointerEvent) => void;
}

export function ImageTrackClip({ clip, zoom, isSelected, onSelect, onResizeStart }: ImageTrackClipProps) {
  const left = clip.startTime * zoom;
  const width = Math.max(clip.duration * zoom, 20);

  return (
    <div
      className={`ve-clip ve-clip--image ${isSelected ? 'selected' : ''}`}
      style={{ left: `${left}px`, width: `${width}px` }}
      onClick={(e) => { e.stopPropagation(); onSelect(clip.id); }}
      role="button"
      aria-label={`Image: ${clip.name}`}
      aria-selected={isSelected}
      tabIndex={0}
    >
      {isSelected && (
        <>
          <div
            className="ve-resize-handle ve-resize-handle--left"
            onPointerDown={(e) => onResizeStart?.(clip.id, 'left', e)}
          />
          <div
            className="ve-resize-handle ve-resize-handle--right"
            onPointerDown={(e) => onResizeStart?.(clip.id, 'right', e)}
          />
        </>
      )}
      {clip.imageUrl ? (
        <img src={clip.imageUrl} alt={clip.name} className="ve-image-thumb" />
      ) : (
        <>
          <ImageIcon size={14} className="ve-clip-icon" />
          <span className="ve-clip-name">{clip.name}</span>
        </>
      )}
    </div>
  );
}
````

## File: packages/frontend/components/VideoEditor/clips/TextClip.tsx
````typescript
/**
 * TextClip — Blue text clip renderer for the timeline.
 */

import { Type } from 'lucide-react';
import type { EditorClip } from '../types/video-editor-types';
import '../video-editor.css';

interface TextClipProps {
  clip: EditorClip;
  zoom: number;
  isSelected: boolean;
  onSelect: (clipId: string) => void;
  onResizeStart?: (clipId: string, edge: 'left' | 'right', e: React.PointerEvent) => void;
}

export function TextClip({ clip, zoom, isSelected, onSelect, onResizeStart }: TextClipProps) {
  const left = clip.startTime * zoom;
  const width = Math.max(clip.duration * zoom, 20);

  return (
    <div
      className={`ve-clip ve-clip--text ${isSelected ? 'selected' : ''}`}
      style={{ left: `${left}px`, width: `${width}px` }}
      onClick={(e) => { e.stopPropagation(); onSelect(clip.id); }}
      role="button"
      aria-label={`Text: ${clip.text ?? clip.name}`}
      aria-selected={isSelected}
      tabIndex={0}
    >
      {/* Resize handles */}
      {isSelected && (
        <>
          <div
            className="ve-resize-handle ve-resize-handle--left"
            onPointerDown={(e) => onResizeStart?.(clip.id, 'left', e)}
          />
          <div
            className="ve-resize-handle ve-resize-handle--right"
            onPointerDown={(e) => onResizeStart?.(clip.id, 'right', e)}
          />
        </>
      )}
      <Type size={14} className="ve-clip-icon" />
      <span className="ve-clip-name">{clip.text ?? clip.name}</span>
    </div>
  );
}
````

## File: packages/frontend/components/VideoEditor/clips/VideoTrackClip.tsx
````typescript
/**
 * VideoTrackClip — Teal video clip with repeating thumbnail tiles.
 */

import { Film } from 'lucide-react';
import type { EditorClip } from '../types/video-editor-types';
import '../video-editor.css';

interface VideoTrackClipProps {
  clip: EditorClip;
  zoom: number;
  isSelected: boolean;
  onSelect: (clipId: string) => void;
  onResizeStart?: (clipId: string, edge: 'left' | 'right', e: React.PointerEvent) => void;
}

export function VideoTrackClip({ clip, zoom, isSelected, onSelect, onResizeStart }: VideoTrackClipProps) {
  const left = clip.startTime * zoom;
  const width = Math.max(clip.duration * zoom, 20);
  const tileCount = Math.max(1, Math.floor(width / 50));

  return (
    <div
      className={`ve-clip ve-clip--video ${isSelected ? 'selected' : ''}`}
      style={{ left: `${left}px`, width: `${width}px` }}
      onClick={(e) => { e.stopPropagation(); onSelect(clip.id); }}
      role="button"
      aria-label={`Video: ${clip.name}`}
      aria-selected={isSelected}
      tabIndex={0}
    >
      {isSelected && (
        <>
          <div
            className="ve-resize-handle ve-resize-handle--left"
            onPointerDown={(e) => onResizeStart?.(clip.id, 'left', e)}
          />
          <div
            className="ve-resize-handle ve-resize-handle--right"
            onPointerDown={(e) => onResizeStart?.(clip.id, 'right', e)}
          />
        </>
      )}
      {clip.thumbnailUrl ? (
        <div className="ve-thumb-strip">
          {Array.from({ length: tileCount }, (_, i) => (
            <div
              key={i}
              className="ve-thumb-tile"
              style={{ backgroundImage: `url(${clip.thumbnailUrl})` }}
            />
          ))}
        </div>
      ) : (
        <>
          <Film size={14} className="ve-clip-icon" />
          <span className="ve-clip-name">{clip.name}</span>
        </>
      )}
    </div>
  );
}
````

## File: packages/frontend/components/VideoEditor/EnhancedTransportBar.tsx
````typescript
/**
 * EnhancedTransportBar
 *
 * Full transport bar with undo/redo, split tool, playback controls,
 * timecode display, aspect ratio selector, zoom slider, and fullscreen toggle.
 */

import {
  Undo2, Redo2, Scissors,
  SkipBack, Play, Pause, SkipForward,
  Maximize2,
} from 'lucide-react';
import type { AspectRatio } from './types/video-editor-types';
import { MIN_ZOOM, MAX_ZOOM } from './types/video-editor-types';
import './video-editor.css';

function formatTimecodeCompact(seconds: number): string {
  const time = Math.max(0, seconds);
  const mins = Math.floor(time / 60);
  const secs = Math.floor(time % 60);
  const ms = Math.floor((time % 1) * 100);
  return `${mins}:${secs.toString().padStart(2, '0')}.${ms.toString().padStart(2, '0')}`;
}

interface EnhancedTransportBarProps {
  currentTime: number;
  duration: number;
  isPlaying: boolean;
  zoom: number;
  aspectRatio: AspectRatio;
  activeTool: string;
  canUndo: boolean;
  canRedo: boolean;
  onPlayPause: () => void;
  onSkipBack: () => void;
  onSkipForward: () => void;
  onUndo: () => void;
  onRedo: () => void;
  onSplit: () => void;
  onZoomChange: (zoom: number) => void;
  onAspectRatioChange: (ratio: AspectRatio) => void;
  onFullscreen: () => void;
}

export function EnhancedTransportBar({
  currentTime,
  duration,
  isPlaying,
  zoom,
  aspectRatio,
  activeTool,
  canUndo,
  canRedo,
  onPlayPause,
  onSkipBack,
  onSkipForward,
  onUndo,
  onRedo,
  onSplit,
  onZoomChange,
  onAspectRatioChange,
  onFullscreen,
}: EnhancedTransportBarProps) {
  return (
    <div className="ve-transport" role="toolbar" aria-label="Transport controls">
      {/* Left group: undo/redo/split */}
      <div className="ve-transport-group">
        <button
          className="ve-transport-btn"
          onClick={onUndo}
          disabled={!canUndo}
          title="Undo (Ctrl+Z)"
          aria-label="Undo"
        >
          <Undo2 size={16} />
        </button>
        <button
          className="ve-transport-btn"
          onClick={onRedo}
          disabled={!canRedo}
          title="Redo (Ctrl+Shift+Z)"
          aria-label="Redo"
        >
          <Redo2 size={16} />
        </button>
        <button
          className={`ve-transport-btn ${activeTool === 'split' ? 'active' : ''}`}
          onClick={onSplit}
          title="Split at playhead (S)"
          aria-label="Split clip"
        >
          <Scissors size={16} />
        </button>
      </div>

      <div className="ve-transport-divider" />

      {/* Center group: transport + timecode */}
      <div className="ve-transport-group">
        <button
          className="ve-transport-btn"
          onClick={onSkipBack}
          title="Skip back 5s (J)"
          aria-label="Skip back"
        >
          <SkipBack size={16} />
        </button>
        <button
          className="ve-transport-btn primary"
          onClick={onPlayPause}
          title={isPlaying ? 'Pause (Space)' : 'Play (Space)'}
          aria-label={isPlaying ? 'Pause' : 'Play'}
          aria-pressed={isPlaying}
        >
          {isPlaying ? <Pause size={16} /> : <Play size={16} />}
        </button>
        <button
          className="ve-transport-btn"
          onClick={onSkipForward}
          title="Skip forward 5s (L)"
          aria-label="Skip forward"
        >
          <SkipForward size={16} />
        </button>
      </div>

      {/* Timecode */}
      <div className="ve-timecode" role="timer" aria-label="Playback position">
        <span className="ve-timecode-current">{formatTimecodeCompact(currentTime)}</span>
        <span className="ve-timecode-separator">/</span>
        <span>{formatTimecodeCompact(duration)}</span>
      </div>

      <div className="ve-transport-spacer" />

      {/* Right group: aspect ratio, zoom, fullscreen */}
      <select
        className="ve-aspect-select"
        value={aspectRatio}
        onChange={(e) => onAspectRatioChange(e.target.value as AspectRatio)}
        title="Aspect ratio"
        aria-label="Aspect ratio"
      >
        <option value="16:9">16:9</option>
        <option value="9:16">9:16</option>
        <option value="1:1">1:1</option>
        <option value="4:3">4:3</option>
      </select>

      <div className="ve-zoom-container">
        <input
          type="range"
          className="ve-zoom-slider"
          min={MIN_ZOOM}
          max={MAX_ZOOM}
          value={zoom}
          onChange={(e) => onZoomChange(Number(e.target.value))}
          title={`Zoom: ${zoom}px/s`}
          aria-label="Timeline zoom"
        />
      </div>

      <button
        className="ve-transport-btn"
        onClick={onFullscreen}
        title="Fullscreen"
        aria-label="Toggle fullscreen"
      >
        <Maximize2 size={16} />
      </button>
    </div>
  );
}
````

## File: packages/frontend/components/VideoEditor/hooks/useVideoEditorStore.ts
````typescript
/**
 * Video Editor Store
 *
 * Zustand store managing all mutable state for the video editor:
 * tracks, clips, playback, selection, undo/redo, and view settings.
 */

import { create } from 'zustand';
import type {
  VideoEditorState,
  EditorTrack,
  EditorTrackType,
  EditorClip,
  EditorSnapshot,
  AspectRatio,
  ActiveTool,
  ToolPanel,
} from '../types/video-editor-types';
import { MAX_UNDO_STACK, DEFAULT_ZOOM } from '../types/video-editor-types';

let clipIdCounter = 0;
let trackIdCounter = 0;

function nextClipId(): string {
  return `clip_${Date.now()}_${++clipIdCounter}`;
}

function nextTrackId(): string {
  return `track_${Date.now()}_${++trackIdCounter}`;
}

function createSnapshot(state: { tracks: EditorTrack[]; clips: EditorClip[] }): EditorSnapshot {
  return {
    tracks: state.tracks.map(t => ({ ...t })),
    clips: state.clips.map(c => ({ ...c, textStyle: c.textStyle ? { ...c.textStyle, position: { ...c.textStyle.position } } : undefined })),
  };
}

function recalcDuration(clips: EditorClip[]): number {
  if (clips.length === 0) return 30;
  return Math.max(30, ...clips.map(c => c.startTime + c.duration));
}

export const useVideoEditorStore = create<VideoEditorState>((set, get) => ({
  // Initial state
  tracks: [],
  clips: [],
  currentTime: 0,
  duration: 30,
  isPlaying: false,
  selectedClipIds: [],
  selectedTrackId: null,
  zoom: DEFAULT_ZOOM,
  aspectRatio: '16:9' as AspectRatio,
  isFullscreen: false,
  undoStack: [],
  redoStack: [],
  activeTool: 'select' as ActiveTool,
  activeToolPanel: null as ToolPanel,

  // ---- Track Actions ----

  addTrack: (type: EditorTrackType, name?: string) => {
    const state = get();
    state.pushSnapshot();
    const track: EditorTrack = {
      id: nextTrackId(),
      type,
      name: name ?? `${type.charAt(0).toUpperCase() + type.slice(1)} ${state.tracks.filter(t => t.type === type).length + 1}`,
      isLocked: false,
      isMuted: false,
      isVisible: true,
      order: state.tracks.length,
    };
    set({ tracks: [...state.tracks, track], redoStack: [] });
  },

  removeTrack: (trackId: string) => {
    const state = get();
    state.pushSnapshot();
    set({
      tracks: state.tracks.filter(t => t.id !== trackId),
      clips: state.clips.filter(c => c.trackId !== trackId),
      selectedTrackId: state.selectedTrackId === trackId ? null : state.selectedTrackId,
      redoStack: [],
    });
  },

  reorderTrack: (trackId: string, newOrder: number) => {
    const state = get();
    state.pushSnapshot();
    const tracks = state.tracks.map(t => {
      if (t.id === trackId) return { ...t, order: newOrder };
      return t;
    }).sort((a, b) => a.order - b.order).map((t, i) => ({ ...t, order: i }));
    set({ tracks, redoStack: [] });
  },

  toggleTrackLock: (trackId: string) => {
    set(state => ({
      tracks: state.tracks.map(t => t.id === trackId ? { ...t, isLocked: !t.isLocked } : t),
    }));
  },

  toggleTrackMute: (trackId: string) => {
    set(state => ({
      tracks: state.tracks.map(t => t.id === trackId ? { ...t, isMuted: !t.isMuted } : t),
    }));
  },

  toggleTrackVisibility: (trackId: string) => {
    set(state => ({
      tracks: state.tracks.map(t => t.id === trackId ? { ...t, isVisible: !t.isVisible } : t),
    }));
  },

  // ---- Clip Actions ----

  addClip: (clipData) => {
    const state = get();
    state.pushSnapshot();
    const clip: EditorClip = { ...clipData, id: nextClipId() };
    const newClips = [...state.clips, clip];
    set({ clips: newClips, duration: recalcDuration(newClips), redoStack: [] });
  },

  removeClip: (clipId: string) => {
    const state = get();
    state.pushSnapshot();
    const newClips = state.clips.filter(c => c.id !== clipId);
    set({
      clips: newClips,
      duration: recalcDuration(newClips),
      selectedClipIds: state.selectedClipIds.filter(id => id !== clipId),
      redoStack: [],
    });
  },

  updateClip: (clipId: string, updates: Partial<EditorClip>) => {
    const state = get();
    state.pushSnapshot();
    const newClips = state.clips.map(c => c.id === clipId ? { ...c, ...updates } : c);
    set({ clips: newClips, duration: recalcDuration(newClips), redoStack: [] });
  },

  moveClip: (clipId: string, newStartTime: number, newTrackId?: string) => {
    const state = get();
    state.pushSnapshot();
    const clampedTime = Math.max(0, newStartTime);
    const newClips = state.clips.map(c => {
      if (c.id !== clipId) return c;
      return { ...c, startTime: clampedTime, ...(newTrackId ? { trackId: newTrackId } : {}) };
    });
    set({ clips: newClips, duration: recalcDuration(newClips), redoStack: [] });
  },

  resizeClip: (clipId: string, edge: 'left' | 'right', newTime: number) => {
    const state = get();
    state.pushSnapshot();
    const newClips = state.clips.map(c => {
      if (c.id !== clipId) return c;
      if (edge === 'left') {
        const clamped = Math.max(0, Math.min(newTime, c.startTime + c.duration - 0.1));
        const delta = clamped - c.startTime;
        return { ...c, startTime: clamped, duration: c.duration - delta, inPoint: c.inPoint + delta };
      } else {
        const newDuration = Math.max(0.1, newTime - c.startTime);
        return { ...c, duration: newDuration, outPoint: c.inPoint + newDuration };
      }
    });
    set({ clips: newClips, duration: recalcDuration(newClips), redoStack: [] });
  },

  splitClipAtPlayhead: (clipId: string) => {
    const state = get();
    const clip = state.clips.find(c => c.id === clipId);
    if (!clip) return;
    const splitTime = state.currentTime;
    if (splitTime <= clip.startTime || splitTime >= clip.startTime + clip.duration) return;

    state.pushSnapshot();
    const leftDuration = splitTime - clip.startTime;
    const rightDuration = clip.duration - leftDuration;

    const leftClip: EditorClip = { ...clip, duration: leftDuration, outPoint: clip.inPoint + leftDuration };
    const rightClip: EditorClip = {
      ...clip,
      id: nextClipId(),
      startTime: splitTime,
      duration: rightDuration,
      inPoint: clip.inPoint + leftDuration,
    };

    const newClips = state.clips.map(c => c.id === clipId ? leftClip : c);
    newClips.push(rightClip);
    set({ clips: newClips, duration: recalcDuration(newClips), redoStack: [] });
  },

  // ---- Playback Actions ----

  setCurrentTime: (time: number) => set({ currentTime: Math.max(0, time) }),
  setIsPlaying: (playing: boolean) => set({ isPlaying: playing }),
  togglePlayback: () => set(state => ({ isPlaying: !state.isPlaying })),

  // ---- View Actions ----

  setZoom: (zoom: number) => set({ zoom: Math.max(10, Math.min(200, zoom)) }),
  setAspectRatio: (ratio: AspectRatio) => set({ aspectRatio: ratio }),
  setFullscreen: (fullscreen: boolean) => set({ isFullscreen: fullscreen }),
  setActiveTool: (tool: ActiveTool) => set({ activeTool: tool }),
  setActiveToolPanel: (panel: ToolPanel) => set(state => ({
    activeToolPanel: state.activeToolPanel === panel ? null : panel,
  })),

  // ---- Selection Actions ----

  selectClip: (clipId: string, additive = false) => {
    set(state => ({
      selectedClipIds: additive
        ? (state.selectedClipIds.includes(clipId)
          ? state.selectedClipIds.filter(id => id !== clipId)
          : [...state.selectedClipIds, clipId])
        : [clipId],
    }));
  },

  selectTrack: (trackId: string | null) => set({ selectedTrackId: trackId }),

  deselectAll: () => set({ selectedClipIds: [], selectedTrackId: null }),

  // ---- Undo/Redo ----

  pushSnapshot: () => {
    set(state => {
      const snapshot = createSnapshot(state);
      const stack = [...state.undoStack, snapshot];
      if (stack.length > MAX_UNDO_STACK) stack.shift();
      return { undoStack: stack };
    });
  },

  undo: () => {
    const state = get();
    if (state.undoStack.length === 0) return;
    const currentSnapshot = createSnapshot(state);
    const prev = state.undoStack[state.undoStack.length - 1]!;
    set({
      tracks: prev.tracks,
      clips: prev.clips,
      duration: recalcDuration(prev.clips),
      undoStack: state.undoStack.slice(0, -1),
      redoStack: [...state.redoStack, currentSnapshot],
    });
  },

  redo: () => {
    const state = get();
    if (state.redoStack.length === 0) return;
    const currentSnapshot = createSnapshot(state);
    const next = state.redoStack[state.redoStack.length - 1]!;
    set({
      tracks: next.tracks,
      clips: next.clips,
      duration: recalcDuration(next.clips),
      redoStack: state.redoStack.slice(0, -1),
      undoStack: [...state.undoStack, currentSnapshot],
    });
  },

  // ---- Reset ----

  reset: () => {
    clipIdCounter = 0;
    trackIdCounter = 0;
    set({
      tracks: [],
      clips: [],
      currentTime: 0,
      duration: 30,
      isPlaying: false,
      selectedClipIds: [],
      selectedTrackId: null,
      zoom: DEFAULT_ZOOM,
      aspectRatio: '16:9',
      isFullscreen: false,
      undoStack: [],
      redoStack: [],
      activeTool: 'select',
      activeToolPanel: null,
    });
  },
}));
````

## File: packages/frontend/components/VideoEditor/index.ts
````typescript
/**
 * VideoEditor — Barrel Export
 */

export { VideoEditor } from './VideoEditor';
export { useVideoEditorStore } from './hooks/useVideoEditorStore';
export type {
  EditorTrack,
  EditorClip,
  EditorTrackType,
  TextStyle,
  AspectRatio,
  ActiveTool,
  ToolPanel,
  VideoEditorState,
} from './types/video-editor-types';
````

## File: packages/frontend/components/VideoEditor/MultiTrackTimeline.tsx
````typescript
/**
 * MultiTrackTimeline
 *
 * The timeline panel containing the time ruler, track lanes, and playhead.
 * Reuses TimeRuler, Playhead, and usePlayheadSeek from the existing timeline.
 */

import { useRef, useCallback, useMemo } from 'react';
import { TimeRuler } from '@/components/TimelineEditor/TimeRuler';
import { Playhead, usePlayheadSeek } from '@/components/TimelineEditor/Playhead';
import { useTimelineScroll } from '@/components/TimelineEditor/useTimelineScroll';
import type { EditorTrack, EditorClip } from './types/video-editor-types';
import { TrackRow } from './TrackRow';
import './video-editor.css';

interface MultiTrackTimelineProps {
  tracks: EditorTrack[];
  clips: EditorClip[];
  currentTime: number;
  duration: number;
  isPlaying: boolean;
  zoom: number;
  selectedClipIds: string[];
  onSeek: (time: number) => void;
  onSelectClip: (clipId: string) => void;
  onDeselectAll: () => void;
  onSelectTrack: (trackId: string) => void;
  onResizeStart?: (clipId: string, edge: 'left' | 'right', e: React.PointerEvent) => void;
}

export function MultiTrackTimeline({
  tracks,
  clips,
  currentTime,
  duration,
  isPlaying,
  zoom,
  selectedClipIds,
  onSeek,
  onSelectClip,
  onDeselectAll,
  onSelectTrack,
  onResizeStart,
}: MultiTrackTimelineProps) {
  const trackLanesContainerRef = useRef<HTMLDivElement>(null);

  // Reuse scroll sync hook from existing timeline
  const {
    scrollLeft,
    trackLanesRef,
    rulerRef,
    handleScroll,
  } = useTimelineScroll({
    currentTime,
    zoom,
    isPlaying,
    duration,
    autoScrollMargin: 100,
  });

  // Reuse playhead seek hook
  const {
    isAnimating,
    handleMouseDown: seekMouseDown,
    handleClick: seekClick,
  } = usePlayheadSeek({
    zoom,
    scrollLeft,
    duration,
    onSeek,
  });

  // Sort tracks by order
  const sortedTracks = useMemo(
    () => [...tracks].sort((a, b) => a.order - b.order),
    [tracks],
  );

  // Group clips by track
  const clipsByTrack = useMemo(() => {
    const map = new Map<string, EditorClip[]>();
    for (const track of tracks) {
      map.set(track.id, []);
    }
    for (const clip of clips) {
      const arr = map.get(clip.trackId);
      if (arr) arr.push(clip);
    }
    return map;
  }, [tracks, clips]);

  const totalWidth = Math.max(duration * zoom, 800);
  const trackHeight = 64; // --ve-track-height
  const totalTrackHeight = sortedTracks.length * trackHeight;

  // Create a combined ref handler for track lanes
  const setTrackLanesRef = useCallback((node: HTMLDivElement | null) => {
    // Update the scroll hook's ref
    (trackLanesRef as React.MutableRefObject<HTMLDivElement | null>).current = node;
    trackLanesContainerRef.current = node;
  }, [trackLanesRef]);

  return (
    <div className="ve-timeline-content">
      {/* Time ruler — syncs scroll with track lanes */}
      <TimeRuler
        ref={rulerRef}
        duration={duration}
        zoom={zoom}
        scrollLeft={scrollLeft}
      />

      {/* Track lanes with playhead */}
      <div
        ref={setTrackLanesRef}
        className="ve-track-lanes"
        onScroll={handleScroll}
        onClick={seekClick}
        onMouseDown={seekMouseDown}
      >
        <div
          className="ve-track-lanes-inner"
          style={{ width: `${totalWidth}px` }}
        >
          {sortedTracks.map(track => (
            <TrackRow
              key={track.id}
              track={track}
              clips={clipsByTrack.get(track.id) ?? []}
              zoom={zoom}
              selectedClipIds={selectedClipIds}
              onSelectClip={onSelectClip}
              onLaneClick={() => {
                onDeselectAll();
                onSelectTrack(track.id);
              }}
              onResizeStart={onResizeStart}
            />
          ))}

          {/* Empty state when no tracks */}
          {sortedTracks.length === 0 && (
            <div className="ve-empty-state" style={{ height: '120px' }}>
              <p>Add tracks using the toolbar to get started</p>
            </div>
          )}
        </div>

        {/* Playhead overlay */}
        <Playhead
          currentTime={currentTime}
          zoom={zoom}
          scrollLeft={scrollLeft}
          height={Math.max(totalTrackHeight, 120)}
          duration={duration}
          isAnimating={isAnimating}
        />
      </div>
    </div>
  );
}
````

## File: packages/frontend/components/VideoEditor/ToolPanels.tsx
````typescript
/**
 * ToolPanels
 *
 * Content panels that appear when toolbar icons are toggled.
 * Layers, Templates, Audio, Text, and Media panels.
 */

import { useState } from 'react';
import {
  Plus, Video, Image as ImageIcon, Type, Music, Trash2,
  Eye, EyeOff, GripVertical,
} from 'lucide-react';
import type { EditorTrack, EditorTrackType, ToolPanel } from './types/video-editor-types';
import { DEFAULT_TEXT_STYLE } from './types/video-editor-types';
import './video-editor.css';

// ============================================================================
// Layers Panel
// ============================================================================

interface LayersPanelProps {
  tracks: EditorTrack[];
  onToggleVisibility: (trackId: string) => void;
  onRemoveTrack: (trackId: string) => void;
  onAddTrack: (type: EditorTrackType) => void;
}

function LayersPanel({ tracks, onToggleVisibility, onRemoveTrack, onAddTrack }: LayersPanelProps) {
  const sorted = [...tracks].sort((a, b) => a.order - b.order);

  return (
    <div className="ve-tool-panel">
      <div className="ve-tool-panel-header">Layers</div>
      <div style={{ padding: '8px' }}>
        {sorted.map(track => (
          <div
            key={track.id}
            style={{
              display: 'flex', alignItems: 'center', gap: '6px',
              padding: '6px 8px', borderRadius: '6px',
              background: 'rgba(255,255,255,0.03)', marginBottom: '4px',
            }}
          >
            <GripVertical size={12} style={{ opacity: 0.3, cursor: 'grab' }} />
            <span style={{ flex: 1, fontSize: '12px' }}>{track.name}</span>
            <button
              onClick={() => onToggleVisibility(track.id)}
              style={{ background: 'none', border: 'none', cursor: 'pointer', color: 'inherit', padding: '2px' }}
              aria-label={track.isVisible ? 'Hide' : 'Show'}
            >
              {track.isVisible ? <Eye size={12} /> : <EyeOff size={12} />}
            </button>
            <button
              onClick={() => onRemoveTrack(track.id)}
              style={{ background: 'none', border: 'none', cursor: 'pointer', color: '#ef4444', padding: '2px' }}
              aria-label="Remove track"
            >
              <Trash2 size={12} />
            </button>
          </div>
        ))}
        <div style={{ display: 'flex', gap: '4px', marginTop: '8px', flexWrap: 'wrap' }}>
          {(['video', 'image', 'text', 'audio'] as const).map(type => (
            <button
              key={type}
              onClick={() => onAddTrack(type)}
              style={{
                display: 'flex', alignItems: 'center', gap: '4px',
                padding: '4px 8px', borderRadius: '4px', fontSize: '11px',
                background: 'rgba(255,255,255,0.05)', border: '1px solid rgba(255,255,255,0.1)',
                color: 'inherit', cursor: 'pointer',
              }}
            >
              <Plus size={10} /> {type}
            </button>
          ))}
        </div>
      </div>
    </div>
  );
}

// ============================================================================
// Text Panel
// ============================================================================

interface TextPanelProps {
  onAddTextClip: (text: string) => void;
}

function TextPanel({ onAddTextClip }: TextPanelProps) {
  const [text, setText] = useState('');

  const presets = [
    'Title Text',
    'Subtitle',
    'Lower Third',
    'Call to Action',
  ];

  return (
    <div className="ve-tool-panel">
      <div className="ve-tool-panel-header">Text</div>
      <div style={{ padding: '12px' }}>
        <input
          type="text"
          value={text}
          onChange={(e) => setText(e.target.value)}
          placeholder="Enter text..."
          style={{
            width: '100%', padding: '8px', borderRadius: '6px',
            background: 'rgba(0,0,0,0.3)', border: '1px solid rgba(255,255,255,0.1)',
            color: '#fff', fontSize: '13px', marginBottom: '8px',
          }}
          onKeyDown={(e) => {
            if (e.key === 'Enter' && text.trim()) {
              onAddTextClip(text.trim());
              setText('');
            }
          }}
        />
        <button
          onClick={() => {
            if (text.trim()) {
              onAddTextClip(text.trim());
              setText('');
            }
          }}
          style={{
            width: '100%', padding: '8px', borderRadius: '6px',
            background: 'rgba(59, 130, 246, 0.3)', border: '1px solid rgba(59, 130, 246, 0.5)',
            color: '#fff', fontSize: '12px', cursor: 'pointer', marginBottom: '12px',
          }}
        >
          <Type size={12} style={{ marginRight: '4px', verticalAlign: 'middle' }} />
          Add Text
        </button>

        <div style={{ fontSize: '10px', color: '#888', marginBottom: '6px', textTransform: 'uppercase', letterSpacing: '1px' }}>
          Presets
        </div>
        {presets.map(preset => (
          <button
            key={preset}
            onClick={() => onAddTextClip(preset)}
            style={{
              display: 'block', width: '100%', padding: '8px',
              borderRadius: '6px', marginBottom: '4px', textAlign: 'left',
              background: 'rgba(255,255,255,0.03)', border: '1px solid rgba(255,255,255,0.06)',
              color: '#ccc', fontSize: '12px', cursor: 'pointer',
            }}
          >
            {preset}
          </button>
        ))}
      </div>
    </div>
  );
}

// ============================================================================
// Media Panel
// ============================================================================

interface MediaPanelProps {
  onAddTrack: (type: EditorTrackType) => void;
}

function MediaPanel({ onAddTrack }: MediaPanelProps) {
  return (
    <div className="ve-tool-panel">
      <div className="ve-tool-panel-header">Media</div>
      <div style={{ padding: '12px' }}>
        <div className="ve-empty-state" style={{ height: '120px', border: '2px dashed rgba(255,255,255,0.1)', borderRadius: '8px' }}>
          <p style={{ fontSize: '12px' }}>Drop files here or import</p>
        </div>
        <div style={{ marginTop: '12px', display: 'flex', gap: '6px', flexWrap: 'wrap' }}>
          <button
            onClick={() => onAddTrack('video')}
            style={{
              display: 'flex', alignItems: 'center', gap: '4px',
              padding: '6px 10px', borderRadius: '6px', fontSize: '11px',
              background: 'rgba(20, 184, 166, 0.15)', border: '1px solid rgba(20, 184, 166, 0.3)',
              color: '#14b8a6', cursor: 'pointer',
            }}
          >
            <Video size={12} /> Video
          </button>
          <button
            onClick={() => onAddTrack('image')}
            style={{
              display: 'flex', alignItems: 'center', gap: '4px',
              padding: '6px 10px', borderRadius: '6px', fontSize: '11px',
              background: 'rgba(34, 197, 94, 0.15)', border: '1px solid rgba(34, 197, 94, 0.3)',
              color: '#22c55e', cursor: 'pointer',
            }}
          >
            <ImageIcon size={12} /> Image
          </button>
          <button
            onClick={() => onAddTrack('audio')}
            style={{
              display: 'flex', alignItems: 'center', gap: '4px',
              padding: '6px 10px', borderRadius: '6px', fontSize: '11px',
              background: 'rgba(249, 115, 22, 0.15)', border: '1px solid rgba(249, 115, 22, 0.3)',
              color: '#f97316', cursor: 'pointer',
            }}
          >
            <Music size={12} /> Audio
          </button>
        </div>
      </div>
    </div>
  );
}

// ============================================================================
// Audio Panel
// ============================================================================

function AudioPanel() {
  return (
    <div className="ve-tool-panel">
      <div className="ve-tool-panel-header">Audio</div>
      <div style={{ padding: '12px' }}>
        <div className="ve-empty-state" style={{ height: '80px' }}>
          <Music size={24} style={{ opacity: 0.3 }} />
          <p style={{ fontSize: '12px' }}>No audio files imported</p>
        </div>
      </div>
    </div>
  );
}

// ============================================================================
// Templates Panel
// ============================================================================

function TemplatesPanel() {
  return (
    <div className="ve-tool-panel">
      <div className="ve-tool-panel-header">Templates</div>
      <div style={{ padding: '12px' }}>
        <div className="ve-empty-state" style={{ height: '80px' }}>
          <p style={{ fontSize: '12px' }}>Templates coming soon</p>
        </div>
      </div>
    </div>
  );
}

// ============================================================================
// Panel Router
// ============================================================================

interface ToolPanelRouterProps {
  activePanel: ToolPanel;
  tracks: EditorTrack[];
  onToggleVisibility: (trackId: string) => void;
  onRemoveTrack: (trackId: string) => void;
  onAddTrack: (type: EditorTrackType) => void;
  onAddTextClip: (text: string) => void;
}

export function ToolPanelRouter({
  activePanel,
  tracks,
  onToggleVisibility,
  onRemoveTrack,
  onAddTrack,
  onAddTextClip,
}: ToolPanelRouterProps) {
  if (!activePanel) return null;

  switch (activePanel) {
    case 'layers':
      return <LayersPanel tracks={tracks} onToggleVisibility={onToggleVisibility} onRemoveTrack={onRemoveTrack} onAddTrack={onAddTrack} />;
    case 'text':
      return <TextPanel onAddTextClip={onAddTextClip} />;
    case 'media':
      return <MediaPanel onAddTrack={onAddTrack} />;
    case 'audio':
      return <AudioPanel />;
    case 'templates':
      return <TemplatesPanel />;
    default:
      return null;
  }
}
````

## File: packages/frontend/components/VideoEditor/TrackLabelPanel.tsx
````typescript
/**
 * TrackLabelPanel
 *
 * Left sidebar of the timeline showing track names, type icons,
 * lock/mute toggle buttons for each track.
 */

import {
  Video, Image, Type, Music,
  Lock, Unlock, Eye, EyeOff,
} from 'lucide-react';
import type { EditorTrack, EditorTrackType } from './types/video-editor-types';
import './video-editor.css';

const TRACK_ICONS: Record<EditorTrackType, React.ElementType> = {
  video: Video,
  image: Image,
  text: Type,
  audio: Music,
};

interface TrackLabelPanelProps {
  tracks: EditorTrack[];
  selectedTrackId: string | null;
  onSelectTrack: (trackId: string) => void;
  onToggleLock: (trackId: string) => void;
  onToggleVisibility: (trackId: string) => void;
}

export function TrackLabelPanel({
  tracks,
  selectedTrackId,
  onSelectTrack,
  onToggleLock,
  onToggleVisibility,
}: TrackLabelPanelProps) {
  const sorted = [...tracks].sort((a, b) => a.order - b.order);

  return (
    <div className="ve-track-labels" role="list" aria-label="Track labels">
      <div className="ve-track-label-spacer" />
      {sorted.map(track => {
        const Icon = TRACK_ICONS[track.type];
        return (
          <div
            key={track.id}
            className={`ve-track-label ${selectedTrackId === track.id ? 'selected' : ''}`}
            onClick={() => onSelectTrack(track.id)}
            role="listitem"
            aria-label={`${track.name} track`}
          >
            <div className="ve-track-label-icon">
              <Icon size={14} />
            </div>
            <span className="ve-track-label-name">{track.name}</span>
            <div className="ve-track-label-actions">
              <button
                className={`ve-track-action-btn ${track.isLocked ? 'active' : ''}`}
                onClick={(e) => { e.stopPropagation(); onToggleLock(track.id); }}
                title={track.isLocked ? 'Unlock track' : 'Lock track'}
                aria-label={track.isLocked ? 'Unlock track' : 'Lock track'}
              >
                {track.isLocked ? <Lock size={12} /> : <Unlock size={12} />}
              </button>
              <button
                className={`ve-track-action-btn ${!track.isVisible ? 'muted' : ''}`}
                onClick={(e) => { e.stopPropagation(); onToggleVisibility(track.id); }}
                title={track.isVisible ? 'Hide track' : 'Show track'}
                aria-label={track.isVisible ? 'Hide track' : 'Show track'}
              >
                {track.isVisible ? <Eye size={12} /> : <EyeOff size={12} />}
              </button>
            </div>
          </div>
        );
      })}
    </div>
  );
}
````

## File: packages/frontend/components/VideoEditor/TrackRow.tsx
````typescript
/**
 * TrackRow
 *
 * A single track lane in the timeline. Renders clips of the appropriate type
 * and handles click-to-seek on empty areas.
 */

import type { EditorTrack, EditorClip } from './types/video-editor-types';
import { TextClip } from './clips/TextClip';
import { VideoTrackClip } from './clips/VideoTrackClip';
import { ImageTrackClip } from './clips/ImageTrackClip';
import { AudioTrackClip } from './clips/AudioTrackClip';
import './video-editor.css';

interface TrackRowProps {
  track: EditorTrack;
  clips: EditorClip[];
  zoom: number;
  selectedClipIds: string[];
  onSelectClip: (clipId: string) => void;
  onLaneClick?: () => void;
  onSeekClick?: (e: React.MouseEvent) => void;
  onResizeStart?: (clipId: string, edge: 'left' | 'right', e: React.PointerEvent) => void;
}

export function TrackRow({
  track,
  clips,
  zoom,
  selectedClipIds,
  onSelectClip,
  onLaneClick,
  onSeekClick,
  onResizeStart,
}: TrackRowProps) {
  const handleLaneClick = (e: React.MouseEvent) => {
    // Only fire if clicking on the lane background, not on a clip
    if ((e.target as HTMLElement).closest('.ve-clip')) return;
    onLaneClick?.();
    onSeekClick?.(e);
  };

  const renderClip = (clip: EditorClip) => {
    const isSelected = selectedClipIds.includes(clip.id);
    const props = { clip, zoom, isSelected, onSelect: onSelectClip, onResizeStart };

    switch (clip.type) {
      case 'text': return <TextClip key={clip.id} {...props} />;
      case 'video': return <VideoTrackClip key={clip.id} {...props} />;
      case 'image': return <ImageTrackClip key={clip.id} {...props} />;
      case 'audio': return <AudioTrackClip key={clip.id} {...props} />;
      default: return null;
    }
  };

  return (
    <div
      className={`ve-lane ve-lane--${track.type}`}
      onClick={handleLaneClick}
      role="row"
      aria-label={`${track.name} lane`}
    >
      {clips.map(renderClip)}
    </div>
  );
}
````

## File: packages/frontend/components/VideoEditor/types/video-editor-types.ts
````typescript
/**
 * Video Editor Types
 *
 * Type definitions for the professional video editor component.
 * Defines the mutable clip/track model used for interactive editing.
 */

// ============================================================================
// Track Types
// ============================================================================

export type EditorTrackType = 'text' | 'video' | 'image' | 'audio';

export interface EditorTrack {
  id: string;
  type: EditorTrackType;
  name: string;
  isLocked: boolean;
  isMuted: boolean;
  isVisible: boolean;
  order: number;
}

// ============================================================================
// Clip Types
// ============================================================================

export interface TextStyle {
  fontFamily: string;
  fontSize: number;
  fontWeight: string;
  color: string;
  backgroundColor?: string;
  /** Normalized position 0-1 on canvas */
  position: { x: number; y: number };
  alignment: 'left' | 'center' | 'right';
}

export interface EditorClip {
  id: string;
  trackId: string;
  type: EditorTrackType;
  startTime: number;
  duration: number;
  name: string;

  // Video-specific
  thumbnailUrl?: string;
  sourceUrl?: string;

  // Image-specific
  imageUrl?: string;

  // Text-specific
  text?: string;
  textStyle?: TextStyle;

  // Audio-specific
  waveformData?: number[];

  // Trimming (source-relative)
  inPoint: number;
  outPoint: number;
}

// ============================================================================
// Editor State
// ============================================================================

export type AspectRatio = '16:9' | '9:16' | '1:1' | '4:3';
export type ActiveTool = 'select' | 'split' | 'text' | 'hand';
export type ToolPanel = 'layers' | 'templates' | 'audio' | 'text' | 'media' | null;

export interface EditorSnapshot {
  tracks: EditorTrack[];
  clips: EditorClip[];
}

export interface VideoEditorState {
  // Tracks and clips
  tracks: EditorTrack[];
  clips: EditorClip[];

  // Playback
  currentTime: number;
  duration: number;
  isPlaying: boolean;

  // Selection
  selectedClipIds: string[];
  selectedTrackId: string | null;

  // View
  zoom: number;
  aspectRatio: AspectRatio;
  isFullscreen: boolean;

  // Undo
  undoStack: EditorSnapshot[];
  redoStack: EditorSnapshot[];

  // Tool state
  activeTool: ActiveTool;
  activeToolPanel: ToolPanel;

  // Actions — Tracks
  addTrack: (type: EditorTrackType, name?: string) => void;
  removeTrack: (trackId: string) => void;
  reorderTrack: (trackId: string, newOrder: number) => void;
  toggleTrackLock: (trackId: string) => void;
  toggleTrackMute: (trackId: string) => void;
  toggleTrackVisibility: (trackId: string) => void;

  // Actions — Clips
  addClip: (clip: Omit<EditorClip, 'id'>) => void;
  removeClip: (clipId: string) => void;
  updateClip: (clipId: string, updates: Partial<EditorClip>) => void;
  moveClip: (clipId: string, newStartTime: number, newTrackId?: string) => void;
  resizeClip: (clipId: string, edge: 'left' | 'right', newTime: number) => void;
  splitClipAtPlayhead: (clipId: string) => void;

  // Actions — Playback
  setCurrentTime: (time: number) => void;
  setIsPlaying: (playing: boolean) => void;
  togglePlayback: () => void;

  // Actions — View
  setZoom: (zoom: number) => void;
  setAspectRatio: (ratio: AspectRatio) => void;
  setFullscreen: (fullscreen: boolean) => void;
  setActiveTool: (tool: ActiveTool) => void;
  setActiveToolPanel: (panel: ToolPanel) => void;

  // Actions — Selection
  selectClip: (clipId: string, additive?: boolean) => void;
  selectTrack: (trackId: string | null) => void;
  deselectAll: () => void;

  // Actions — Undo/Redo
  undo: () => void;
  redo: () => void;
  pushSnapshot: () => void;

  // Actions — Reset
  reset: () => void;
}

// ============================================================================
// Constants
// ============================================================================

export const MIN_ZOOM = 10;
export const MAX_ZOOM = 200;
export const DEFAULT_ZOOM = 50;
export const MAX_UNDO_STACK = 50;

export const TRACK_COLORS: Record<EditorTrackType, {
  bg: string;
  border: string;
  clip: string;
  clipBorder: string;
  waveform: string;
}> = {
  text: {
    bg: 'rgba(59, 130, 246, 0.12)',
    border: 'rgba(59, 130, 246, 0.5)',
    clip: 'rgba(59, 130, 246, 0.25)',
    clipBorder: 'rgba(59, 130, 246, 0.6)',
    waveform: '#3b82f6',
  },
  video: {
    bg: 'rgba(20, 184, 166, 0.12)',
    border: 'rgba(20, 184, 166, 0.5)',
    clip: 'rgba(20, 184, 166, 0.25)',
    clipBorder: 'rgba(20, 184, 166, 0.6)',
    waveform: '#14b8a6',
  },
  image: {
    bg: 'rgba(34, 197, 94, 0.12)',
    border: 'rgba(34, 197, 94, 0.5)',
    clip: 'rgba(34, 197, 94, 0.25)',
    clipBorder: 'rgba(34, 197, 94, 0.6)',
    waveform: '#22c55e',
  },
  audio: {
    bg: 'rgba(249, 115, 22, 0.12)',
    border: 'rgba(249, 115, 22, 0.5)',
    clip: 'rgba(249, 115, 22, 0.2)',
    clipBorder: 'rgba(249, 115, 22, 0.5)',
    waveform: '#f97316',
  },
};

export const DEFAULT_TEXT_STYLE: TextStyle = {
  fontFamily: 'Inter',
  fontSize: 48,
  fontWeight: '700',
  color: '#ffffff',
  position: { x: 0.5, y: 0.5 },
  alignment: 'center',
};
````

## File: packages/frontend/components/VideoEditor/video-editor.css
````css
/* =============================================================================
   Video Editor - CSS Foundation
   Extends the graphite design tokens for the professional video editor layout
   ============================================================================= */

/* Import graphite tokens (already loaded via graphite-timeline.css) */

/* -----------------------------------------------------------------------------
   Additional Track Colors for Video Editor
   ----------------------------------------------------------------------------- */
:root {
  /* Text track — blue */
  --track-text: rgba(59, 130, 246, 0.12);
  --track-text-border: rgba(59, 130, 246, 0.5);
  --track-text-clip: rgba(59, 130, 246, 0.25);

  /* Video track — teal */
  --track-video-ed: rgba(20, 184, 166, 0.12);
  --track-video-ed-border: rgba(20, 184, 166, 0.5);
  --track-video-ed-clip: rgba(20, 184, 166, 0.25);

  /* Image track — green */
  --track-image: rgba(34, 197, 94, 0.12);
  --track-image-border: rgba(34, 197, 94, 0.5);
  --track-image-clip: rgba(34, 197, 94, 0.25);

  /* Audio track — orange */
  --track-audio-ed: rgba(249, 115, 22, 0.12);
  --track-audio-ed-border: rgba(249, 115, 22, 0.5);
  --track-audio-ed-clip: rgba(249, 115, 22, 0.2);

  /* Editor sizing */
  --ve-toolbar-width: 48px;
  --ve-panel-width: 280px;
  --ve-transport-height: 48px;
  --ve-track-height: 64px;
  --ve-clip-height: 48px;
  --ve-label-width: 160px;
}

/* -----------------------------------------------------------------------------
   Editor Layout
   ----------------------------------------------------------------------------- */
.video-editor {
  display: grid;
  grid-template-columns: var(--ve-toolbar-width) 1fr;
  grid-template-rows: 1fr var(--ve-transport-height) minmax(180px, 40vh);
  height: 100%;
  width: 100%;
  background: var(--graphite-bg);
  color: var(--graphite-text-main);
  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
  overflow: hidden;
  position: relative;
}

/* When tool panel is open, add panel column */
.video-editor--panel-open {
  grid-template-columns: var(--ve-toolbar-width) var(--ve-panel-width) 1fr;
}

/* -----------------------------------------------------------------------------
   Left Tool Sidebar
   ----------------------------------------------------------------------------- */
.ve-toolbar {
  grid-row: 1 / -1;
  grid-column: 1;
  background: var(--graphite-deep);
  border-right: 1px solid var(--graphite-edge);
  display: flex;
  flex-direction: column;
  align-items: center;
  padding: 8px 0;
  gap: 2px;
  z-index: 20;
}

.ve-toolbar-btn {
  width: 40px;
  height: 40px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 8px;
  cursor: pointer;
  transition: all var(--graphite-transition-fast);
  color: var(--graphite-text-dim);
  background: transparent;
  border: none;
}

.ve-toolbar-btn:hover {
  background: var(--graphite-mid);
  color: var(--graphite-text-main);
}

.ve-toolbar-btn.active {
  background: rgba(0, 242, 255, 0.1);
  color: var(--plasma-cyan);
  box-shadow: inset 0 0 10px rgba(0, 242, 255, 0.1);
}

.ve-toolbar-btn:focus-visible {
  outline: 2px solid var(--focus-ring);
  outline-offset: -2px;
}

/* -----------------------------------------------------------------------------
   Tool Panel (slides out from toolbar)
   ----------------------------------------------------------------------------- */
.ve-tool-panel {
  grid-row: 1;
  grid-column: 2;
  background: var(--graphite-deep);
  border-right: 1px solid var(--graphite-edge);
  overflow-y: auto;
  overflow-x: hidden;
}

.ve-tool-panel-header {
  padding: 16px;
  font-size: 12px;
  font-weight: 700;
  text-transform: uppercase;
  letter-spacing: 1px;
  color: var(--graphite-text-dim);
  border-bottom: 1px solid var(--graphite-edge);
}

/* -----------------------------------------------------------------------------
   Canvas Preview
   ----------------------------------------------------------------------------- */
.ve-preview {
  background: #000;
  display: flex;
  align-items: center;
  justify-content: center;
  position: relative;
  overflow: hidden;
}

.ve-preview-canvas {
  background: #000;
  max-width: 100%;
  max-height: 100%;
  object-fit: contain;
}

/* Aspect ratio badges */
.ve-aspect-badge {
  position: absolute;
  bottom: 8px;
  right: 8px;
  font-family: 'JetBrains Mono', monospace;
  font-size: 10px;
  color: var(--graphite-text-dim);
  background: rgba(0, 0, 0, 0.6);
  padding: 2px 6px;
  border-radius: 4px;
}

/* -----------------------------------------------------------------------------
   Transport Bar
   ----------------------------------------------------------------------------- */
.ve-transport {
  display: flex;
  align-items: center;
  gap: 4px;
  padding: 0 12px;
  background: var(--graphite-deep);
  border-top: 1px solid var(--graphite-edge);
  border-bottom: 1px solid var(--graphite-edge);
  height: var(--ve-transport-height);
}

.ve-transport-group {
  display: flex;
  align-items: center;
  gap: 2px;
}

.ve-transport-divider {
  width: 1px;
  height: 24px;
  background: var(--graphite-edge);
  margin: 0 8px;
}

.ve-transport-spacer {
  flex: 1;
}

.ve-transport-btn {
  width: 32px;
  height: 32px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 6px;
  cursor: pointer;
  transition: all var(--graphite-transition-fast);
  color: var(--graphite-text-dim);
  background: transparent;
  border: none;
}

.ve-transport-btn:hover {
  background: var(--graphite-mid);
  color: var(--graphite-text-main);
}

.ve-transport-btn.active {
  color: var(--plasma-cyan);
}

.ve-transport-btn.primary {
  color: var(--graphite-text-main);
  background: var(--graphite-mid);
}

.ve-transport-btn.primary:hover {
  background: var(--graphite-light);
}

.ve-transport-btn:disabled {
  opacity: 0.3;
  cursor: not-allowed;
}

.ve-transport-btn:focus-visible {
  outline: 2px solid var(--focus-ring);
  outline-offset: -2px;
}

.ve-timecode {
  font-family: 'JetBrains Mono', monospace;
  font-size: 13px;
  font-weight: 500;
  color: var(--graphite-text-main);
  padding: 4px 10px;
  background: rgba(0, 0, 0, 0.4);
  border-radius: 4px;
  border: 1px solid var(--graphite-edge);
  white-space: nowrap;
  min-width: 140px;
  text-align: center;
}

.ve-timecode-current {
  color: var(--plasma-cyan);
}

.ve-timecode-separator {
  color: var(--graphite-text-dim);
  margin: 0 2px;
}

/* Aspect ratio select */
.ve-aspect-select {
  font-family: 'JetBrains Mono', monospace;
  font-size: 11px;
  color: var(--graphite-text-dim);
  background: transparent;
  border: 1px solid var(--graphite-edge);
  border-radius: 4px;
  padding: 4px 8px;
  cursor: pointer;
}

.ve-aspect-select:hover {
  border-color: var(--graphite-text-dim);
}

/* Zoom slider in transport */
.ve-zoom-container {
  display: flex;
  align-items: center;
  gap: 6px;
  min-width: 100px;
}

.ve-zoom-slider {
  width: 80px;
  height: 3px;
  appearance: none;
  background: var(--graphite-mid);
  border-radius: 2px;
  outline: none;
  cursor: pointer;
}

.ve-zoom-slider::-webkit-slider-thumb {
  appearance: none;
  width: 12px;
  height: 12px;
  border-radius: 50%;
  background: var(--graphite-text-main);
  cursor: pointer;
}

.ve-zoom-slider::-moz-range-thumb {
  width: 12px;
  height: 12px;
  border-radius: 50%;
  background: var(--graphite-text-main);
  cursor: pointer;
  border: none;
}

/* -----------------------------------------------------------------------------
   Timeline Area (bottom panel)
   ----------------------------------------------------------------------------- */
.ve-timeline-area {
  display: flex;
  flex-direction: row;
  overflow: hidden;
  background: var(--graphite-bg);
}

/* Track Labels (left of timeline) */
.ve-track-labels {
  width: var(--ve-label-width);
  min-width: var(--ve-label-width);
  background: var(--graphite-deep);
  border-right: 1px solid rgba(255, 255, 255, 0.06);
  display: flex;
  flex-direction: column;
  flex-shrink: 0;
  z-index: 5;
  overflow: hidden;
}

.ve-track-label-spacer {
  height: var(--graphite-ruler-height);
  border-bottom: 1px solid var(--graphite-edge);
  flex-shrink: 0;
}

.ve-track-label {
  height: var(--ve-track-height);
  display: flex;
  align-items: center;
  gap: 6px;
  padding: 0 8px;
  border-bottom: 1px solid rgba(255, 255, 255, 0.03);
  cursor: pointer;
  transition: background var(--graphite-transition-fast);
}

.ve-track-label:hover {
  background: rgba(255, 255, 255, 0.03);
}

.ve-track-label.selected {
  background: rgba(0, 242, 255, 0.05);
}

.ve-track-label-icon {
  width: 20px;
  height: 20px;
  display: flex;
  align-items: center;
  justify-content: center;
  color: var(--graphite-text-dim);
  flex-shrink: 0;
}

.ve-track-label-name {
  font-size: 11px;
  font-weight: 500;
  color: var(--graphite-text-main);
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  flex: 1;
  min-width: 0;
}

.ve-track-label-actions {
  display: flex;
  gap: 2px;
  opacity: 0;
  transition: opacity var(--graphite-transition-fast);
}

.ve-track-label:hover .ve-track-label-actions {
  opacity: 1;
}

.ve-track-action-btn {
  width: 20px;
  height: 20px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 4px;
  cursor: pointer;
  color: var(--graphite-text-dim);
  background: transparent;
  border: none;
  padding: 0;
}

.ve-track-action-btn:hover {
  background: var(--graphite-mid);
  color: var(--graphite-text-main);
}

.ve-track-action-btn.active {
  color: var(--plasma-cyan);
}

.ve-track-action-btn.muted {
  color: var(--laser-red);
}

/* Timeline content */
.ve-timeline-content {
  flex: 1;
  display: flex;
  flex-direction: column;
  overflow: hidden;
  min-width: 0;
}

/* Track lanes scroll container */
.ve-track-lanes {
  flex: 1;
  overflow-x: auto;
  overflow-y: hidden;
  position: relative;
}

.ve-track-lanes-inner {
  display: flex;
  flex-direction: column;
  min-width: 100%;
  position: relative;
}

/* Individual track lane */
.ve-lane {
  height: var(--ve-track-height);
  position: relative;
  display: flex;
  align-items: center;
  border-bottom: 1px solid rgba(255, 255, 255, 0.03);
}

.ve-lane--text {
  background: var(--track-text);
  border-left: 3px solid var(--track-text-border);
}

.ve-lane--video {
  background: var(--track-video-ed);
  border-left: 3px solid var(--track-video-ed-border);
}

.ve-lane--image {
  background: var(--track-image);
  border-left: 3px solid var(--track-image-border);
}

.ve-lane--audio {
  background: var(--track-audio-ed);
  border-left: 3px solid var(--track-audio-ed-border);
}

/* -----------------------------------------------------------------------------
   Clip Styles
   ----------------------------------------------------------------------------- */
.ve-clip {
  position: absolute;
  height: var(--ve-clip-height);
  top: 50%;
  transform: translateY(-50%);
  border-radius: 6px;
  cursor: pointer;
  display: flex;
  align-items: center;
  overflow: hidden;
  transition: box-shadow var(--graphite-transition-fast), border-color var(--graphite-transition-fast);
  user-select: none;
}

.ve-clip:hover {
  z-index: 5;
}

.ve-clip.selected {
  border-color: var(--plasma-cyan) !important;
  box-shadow: 0 0 12px var(--plasma-glow);
  z-index: 10;
}

.ve-clip--text {
  background: var(--track-text-clip);
  border: 1px solid rgba(59, 130, 246, 0.4);
}

.ve-clip--video {
  background: var(--track-video-ed-clip);
  border: 1px solid rgba(20, 184, 166, 0.4);
}

.ve-clip--image {
  background: var(--track-image-clip);
  border: 1px solid rgba(34, 197, 94, 0.4);
}

.ve-clip--audio {
  background: var(--track-audio-ed-clip);
  border: 1px solid rgba(249, 115, 22, 0.3);
}

.ve-clip-name {
  font-size: 11px;
  font-weight: 500;
  padding: 0 8px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  color: var(--graphite-text-main);
  pointer-events: none;
}

/* Clip icon badge */
.ve-clip-icon {
  width: 16px;
  height: 16px;
  margin-left: 6px;
  flex-shrink: 0;
  opacity: 0.6;
}

/* Resize handles */
.ve-resize-handle {
  position: absolute;
  top: 0;
  bottom: 0;
  width: 6px;
  cursor: ew-resize;
  z-index: 15;
  transition: background var(--graphite-transition-fast);
}

.ve-resize-handle:hover {
  background: rgba(0, 242, 255, 0.3);
}

.ve-resize-handle--left {
  left: 0;
  border-radius: 6px 0 0 6px;
}

.ve-resize-handle--right {
  right: 0;
  border-radius: 0 6px 6px 0;
}

/* Waveform in audio clips */
.ve-waveform {
  display: flex;
  align-items: center;
  gap: 1px;
  height: 100%;
  padding: 0 8px;
  flex: 1;
  overflow: hidden;
}

.ve-wave-bar {
  width: 2px;
  border-radius: 1px;
  opacity: 0.7;
  flex-shrink: 0;
}

/* Thumbnail strip in video clips */
.ve-thumb-strip {
  display: flex;
  height: 100%;
  overflow: hidden;
  flex: 1;
}

.ve-thumb-tile {
  width: 50px;
  height: 100%;
  background-size: cover;
  background-position: center;
  flex-shrink: 0;
  opacity: 0.7;
}

/* Image clip thumbnail */
.ve-image-thumb {
  height: 100%;
  width: 100%;
  object-fit: cover;
  opacity: 0.75;
}

/* Drag ghost */
.ve-clip.dragging {
  opacity: 0.6;
  pointer-events: none;
}

/* Empty state */
.ve-empty-state {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  gap: 8px;
  color: var(--graphite-text-dim);
  font-size: 13px;
  height: 100%;
}

.ve-empty-state-icon {
  width: 48px;
  height: 48px;
  opacity: 0.3;
}

/* -----------------------------------------------------------------------------
   Reduced Motion & High Contrast
   ----------------------------------------------------------------------------- */
@media (prefers-reduced-motion: reduce) {
  .ve-clip,
  .ve-toolbar-btn,
  .ve-transport-btn,
  .ve-track-label-actions {
    transition: none;
  }
}

@media (prefers-contrast: high) {
  .ve-clip.selected {
    outline: 3px solid var(--focus-ring);
  }
}
````

## File: packages/frontend/components/VideoEditor/VideoEditor.tsx
````typescript
/**
 * VideoEditor
 *
 * Top-level layout component for the professional video editor.
 * Orchestrates toolbar, canvas preview, transport bar, and multi-track timeline.
 *
 * Layout:
 * ┌──────────┬─────────────────────────────────────┐
 * │ Toolbar  │ [Tool Panel?] │ Canvas Preview      │
 * │ (48px)   │ (280px)       │                     │
 * ├──────────┴───────────────┴─────────────────────┤
 * │ Enhanced Transport Bar                         │
 * ├────────────────────────────────────────────────┤
 * │ Track Labels │ MultiTrack Timeline             │
 * │ (160px)      │ (TimeRuler + Tracks + Playhead) │
 * └────────────────────────────────────────────────┘
 */

import { useCallback, useEffect, useRef } from 'react';
import { useVideoEditorStore } from './hooks/useVideoEditorStore';
import { VideoEditorToolbar } from './VideoEditorToolbar';
import { ToolPanelRouter } from './ToolPanels';
import { CanvasPreview } from './CanvasPreview';
import { EnhancedTransportBar } from './EnhancedTransportBar';
import { TrackLabelPanel } from './TrackLabelPanel';
import { MultiTrackTimeline } from './MultiTrackTimeline';
import type { EditorTrackType, ToolPanel } from './types/video-editor-types';
import { DEFAULT_TEXT_STYLE } from './types/video-editor-types';
import './video-editor.css';

const SKIP_INTERVAL = 5;

interface VideoEditorProps {
  className?: string;
}

export function VideoEditor({ className = '' }: VideoEditorProps) {
  const store = useVideoEditorStore();
  const containerRef = useRef<HTMLDivElement>(null);
  const playbackRef = useRef<number>(0);
  const lastTimeRef = useRef<number>(0);

  // ---- Playback Loop ----
  useEffect(() => {
    if (!store.isPlaying) return;
    lastTimeRef.current = performance.now();

    const tick = (now: number) => {
      const delta = (now - lastTimeRef.current) / 1000;
      lastTimeRef.current = now;
      const newTime = store.currentTime + delta;
      if (newTime >= store.duration) {
        store.setIsPlaying(false);
        store.setCurrentTime(store.duration);
        return;
      }
      store.setCurrentTime(newTime);
      playbackRef.current = requestAnimationFrame(tick);
    };

    playbackRef.current = requestAnimationFrame(tick);
    return () => cancelAnimationFrame(playbackRef.current);
  }, [store.isPlaying]);

  // ---- Transport Handlers ----
  const handlePlayPause = useCallback(() => store.togglePlayback(), [store]);
  const handleSkipBack = useCallback(() => {
    store.setCurrentTime(Math.max(0, store.currentTime - SKIP_INTERVAL));
  }, [store]);
  const handleSkipForward = useCallback(() => {
    store.setCurrentTime(Math.min(store.duration, store.currentTime + SKIP_INTERVAL));
  }, [store]);

  const handleSplit = useCallback(() => {
    if (store.selectedClipIds.length === 1) {
      store.splitClipAtPlayhead(store.selectedClipIds[0]!);
    }
  }, [store]);

  // ---- Tool Panel ----
  const handlePanelToggle = useCallback((panel: NonNullable<ToolPanel>) => {
    store.setActiveToolPanel(panel);
  }, [store]);

  // ---- Add Text Clip ----
  const handleAddTextClip = useCallback((text: string) => {
    // Find or create a text track
    let textTrack = store.tracks.find(t => t.type === 'text');
    if (!textTrack) {
      store.addTrack('text');
      textTrack = useVideoEditorStore.getState().tracks.find(t => t.type === 'text');
    }
    if (!textTrack) return;

    store.addClip({
      trackId: textTrack.id,
      type: 'text',
      startTime: store.currentTime,
      duration: 3,
      name: text,
      text,
      textStyle: { ...DEFAULT_TEXT_STYLE },
      inPoint: 0,
      outPoint: 3,
    });
  }, [store]);

  // ---- Add Track ----
  const handleAddTrack = useCallback((type: EditorTrackType) => {
    store.addTrack(type);
  }, [store]);

  // ---- Resize Handler ----
  const handleResizeStart = useCallback((clipId: string, edge: 'left' | 'right', e: React.PointerEvent) => {
    e.stopPropagation();
    e.preventDefault();
    const target = e.currentTarget as HTMLElement;
    const laneElement = target.closest('.ve-track-lanes');
    if (!laneElement) return;

    const zoom = store.zoom;

    const onPointerMove = (moveEvent: PointerEvent) => {
      const rect = laneElement.getBoundingClientRect();
      const scrollLeft = (laneElement as HTMLElement).scrollLeft;
      const x = moveEvent.clientX - rect.left + scrollLeft;
      const newTime = Math.max(0, x / zoom);
      store.resizeClip(clipId, edge, newTime);
    };

    const onPointerUp = () => {
      document.removeEventListener('pointermove', onPointerMove);
      document.removeEventListener('pointerup', onPointerUp);
    };

    document.addEventListener('pointermove', onPointerMove);
    document.addEventListener('pointerup', onPointerUp);
  }, [store]);

  // ---- Keyboard Shortcuts ----
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      // Don't handle if typing in an input
      if ((e.target as HTMLElement).tagName === 'INPUT' || (e.target as HTMLElement).tagName === 'TEXTAREA') return;

      switch (e.key) {
        case ' ':
        case 'k':
          e.preventDefault();
          store.togglePlayback();
          break;
        case 'j':
        case 'ArrowLeft':
          e.preventDefault();
          store.setCurrentTime(Math.max(0, store.currentTime - (e.shiftKey ? 5 : 1)));
          break;
        case 'l':
        case 'ArrowRight':
          e.preventDefault();
          store.setCurrentTime(Math.min(store.duration, store.currentTime + (e.shiftKey ? 5 : 1)));
          break;
        case 'Home':
          e.preventDefault();
          store.setCurrentTime(0);
          break;
        case 'End':
          e.preventDefault();
          store.setCurrentTime(store.duration);
          break;
        case 's':
          if (!e.ctrlKey && !e.metaKey) {
            e.preventDefault();
            handleSplit();
          }
          break;
        case 'z':
          if (e.ctrlKey || e.metaKey) {
            e.preventDefault();
            if (e.shiftKey) store.redo();
            else store.undo();
          }
          break;
        case 'y':
          if (e.ctrlKey || e.metaKey) {
            e.preventDefault();
            store.redo();
          }
          break;
        case 'Delete':
        case 'Backspace':
          if (store.selectedClipIds.length > 0) {
            e.preventDefault();
            for (const id of store.selectedClipIds) {
              store.removeClip(id);
            }
          }
          break;
        case 'Escape':
          store.deselectAll();
          break;
      }
    };

    window.addEventListener('keydown', handleKeyDown);
    return () => window.removeEventListener('keydown', handleKeyDown);
  }, [store, handleSplit]);

  const isPanelOpen = store.activeToolPanel !== null;

  return (
    <div
      ref={containerRef}
      className={`video-editor ${isPanelOpen ? 'video-editor--panel-open' : ''} ${className}`}
      role="application"
      aria-label="Video Editor"
      tabIndex={0}
    >
      {/* Left toolbar — spans all rows */}
      <VideoEditorToolbar
        activePanel={store.activeToolPanel}
        onPanelToggle={handlePanelToggle}
      />

      {/* Tool panel (conditionally rendered between toolbar and preview) */}
      {isPanelOpen && (
        <ToolPanelRouter
          activePanel={store.activeToolPanel}
          tracks={store.tracks}
          onToggleVisibility={store.toggleTrackVisibility}
          onRemoveTrack={store.removeTrack}
          onAddTrack={handleAddTrack}
          onAddTextClip={handleAddTextClip}
        />
      )}

      {/* Canvas preview */}
      <CanvasPreview
        clips={store.clips}
        currentTime={store.currentTime}
        aspectRatio={store.aspectRatio}
        isPlaying={store.isPlaying}
        className={isPanelOpen ? '' : ''}
      />

      {/* Transport bar — spans full width below preview */}
      <EnhancedTransportBar
        currentTime={store.currentTime}
        duration={store.duration}
        isPlaying={store.isPlaying}
        zoom={store.zoom}
        aspectRatio={store.aspectRatio}
        activeTool={store.activeTool}
        canUndo={store.undoStack.length > 0}
        canRedo={store.redoStack.length > 0}
        onPlayPause={handlePlayPause}
        onSkipBack={handleSkipBack}
        onSkipForward={handleSkipForward}
        onUndo={store.undo}
        onRedo={store.redo}
        onSplit={handleSplit}
        onZoomChange={store.setZoom}
        onAspectRatioChange={store.setAspectRatio}
        onFullscreen={() => {
          if (containerRef.current) {
            if (document.fullscreenElement) {
              document.exitFullscreen();
            } else {
              containerRef.current.requestFullscreen();
            }
          }
        }}
      />

      {/* Timeline area: labels + tracks */}
      <div
        className="ve-timeline-area"
        style={{ gridColumn: isPanelOpen ? '2 / -1' : '2 / -1' }}
      >
        <TrackLabelPanel
          tracks={store.tracks}
          selectedTrackId={store.selectedTrackId}
          onSelectTrack={store.selectTrack}
          onToggleLock={store.toggleTrackLock}
          onToggleVisibility={store.toggleTrackVisibility}
        />
        <MultiTrackTimeline
          tracks={store.tracks}
          clips={store.clips}
          currentTime={store.currentTime}
          duration={store.duration}
          isPlaying={store.isPlaying}
          zoom={store.zoom}
          selectedClipIds={store.selectedClipIds}
          onSeek={store.setCurrentTime}
          onSelectClip={(id) => store.selectClip(id)}
          onDeselectAll={store.deselectAll}
          onSelectTrack={store.selectTrack}
          onResizeStart={handleResizeStart}
        />
      </div>
    </div>
  );
}
````

## File: packages/frontend/components/VideoEditor/VideoEditorToolbar.tsx
````typescript
/**
 * VideoEditorToolbar
 *
 * Left sidebar with tool icons that toggle media/property panels.
 */

import { Layers, LayoutTemplate, Music, Type, Image } from 'lucide-react';
import type { ToolPanel } from './types/video-editor-types';
import './video-editor.css';

interface ToolbarItem {
  panel: ToolPanel;
  icon: React.ElementType;
  label: string;
}

const TOOLS: ToolbarItem[] = [
  { panel: 'layers', icon: Layers, label: 'Layers' },
  { panel: 'templates', icon: LayoutTemplate, label: 'Templates' },
  { panel: 'audio', icon: Music, label: 'Audio' },
  { panel: 'text', icon: Type, label: 'Text' },
  { panel: 'media', icon: Image, label: 'Media' },
];

interface VideoEditorToolbarProps {
  activePanel: ToolPanel;
  onPanelToggle: (panel: NonNullable<ToolPanel>) => void;
}

export function VideoEditorToolbar({ activePanel, onPanelToggle }: VideoEditorToolbarProps) {
  return (
    <div className="ve-toolbar" role="toolbar" aria-label="Editor tools">
      {TOOLS.map(({ panel, icon: Icon, label }) => (
        <button
          key={panel}
          className={`ve-toolbar-btn ${activePanel === panel ? 'active' : ''}`}
          onClick={() => onPanelToggle(panel!)}
          title={label}
          aria-label={label}
          aria-pressed={activePanel === panel}
        >
          <Icon size={18} />
        </button>
      ))}
    </div>
  );
}
````

## File: packages/frontend/components/VideoExportModal.tsx
````typescript
import React, { useEffect, useState } from "react";
import { SongData, TransitionType } from "@/types";
import {
  Download,
  Film,
  AlertCircle,
  Settings,
  Smartphone,
  Monitor,
  Cloud,
  Laptop,
  Blend,
  ZoomIn,
  ArrowRightLeft,
  CircleSlash,
  Sparkles,
  Type,
  Activity,
  Volume2,
} from "lucide-react";
import {
  exportVideoWithFFmpeg,
  exportVideoClientSide,
  ExportProgress,
  ExportConfig,
} from "@/services/ffmpeg";
import { VideoSFXPlan } from "@/types";
import { SceneAudioInfo } from "@/services/audioMixerService";
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from "@/components/ui/dialog";
import { Button } from "@/components/ui/button";
import { Progress } from "@/components/ui/progress";
import { Label } from "@/components/ui/label";
import { Slider } from "@/components/ui/slider";
import { Switch } from "@/components/ui/switch";
import { Card, CardContent } from "@/components/ui/card";
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select";
import { cn, isRTL } from "@/lib/utils";

interface VideoExportModalProps {
  songData: SongData;
  onClose: () => void;
  isOpen: boolean;
  /** Content mode - "music" includes visualizer, "story" skips it */
  contentMode?: "music" | "story";
  /** SFX plan with audio URLs for mixing */
  sfxPlan?: VideoSFXPlan | null;
  /** Scene timing information for SFX mixing */
  sceneTimings?: SceneAudioInfo[];
}

/**
 * Detect if the song's lyrics are primarily RTL (Arabic, Hebrew, etc.)
 */
function detectLyricsRTL(songData: SongData): boolean {
  if (!songData.parsedSubtitles || songData.parsedSubtitles.length === 0) {
    return false;
  }
  // Check first few subtitles to determine primary text direction
  const samplesToCheck = Math.min(5, songData.parsedSubtitles.length);
  let rtlCount = 0;
  for (let i = 0; i < samplesToCheck; i++) {
    const subtitle = songData.parsedSubtitles[i];
    if (subtitle && isRTL(subtitle.text)) {
      rtlCount++;
    }
  }
  // If majority of samples are RTL, consider the content RTL
  return rtlCount > samplesToCheck / 2;
}

export const VideoExportModal: React.FC<VideoExportModalProps> = ({
  songData,
  onClose,
  isOpen,
  contentMode = "music",
  sfxPlan,
  sceneTimings,
}) => {
  const [exportProgress, setExportProgress] = useState<ExportProgress>({
    stage: "loading",
    progress: 0,
    message: "Initializing...",
  });
  const [videoBlobUrl, setVideoBlobUrl] = useState<string | null>(null);
  const [error, setError] = useState<string | null>(null);

  // Auto-detect RTL for text animation direction
  const isLyricsRTL = detectLyricsRTL(songData);

  // Check if SFX mixing is available
  const hasSFX = !!(sfxPlan && sceneTimings && sceneTimings.length > 0 &&
    sfxPlan.scenes.some(s => s.ambientTrack?.audioUrl));

  // Configuration State
  const [config, setConfig] = useState<ExportConfig>({
    orientation: "landscape",
    useModernEffects: true,
    syncOffsetMs: -50,
    fadeOutBeforeCut: true,
    wordLevelHighlight: true,
    contentMode: contentMode,
    transitionType: "dissolve",
    transitionDuration: 1.5,
    visualizerConfig: {
      enabled: true,
      opacity: 0.15,
      maxHeightRatio: 0.25,
      zIndex: 1,
      barWidth: 3,
      barGap: 2,
      colorScheme: "cyan-purple",
    },
    textAnimationConfig: {
      revealDirection: isLyricsRTL ? "rtl" : "ltr", // Auto-detect based on lyrics
      revealDuration: 0.3,
      wordReveal: true,
    },
    // SFX config
    sfxPlan: sfxPlan,
    sceneTimings: sceneTimings,
    sfxMasterVolume: 1.0,
    musicMasterVolume: 0.5,
  });
  const [isExporting, setIsExporting] = useState(false);
  const [useCloudRender, setUseCloudRender] = useState(true); // Default to server-side for faster encoding
  const [enableSFX, setEnableSFX] = useState(hasSFX);

  // Cleanup blob URL on unmount
  useEffect(() => {
    return () => {
      if (videoBlobUrl) URL.revokeObjectURL(videoBlobUrl);
    };
  }, [videoBlobUrl]);

  const startExport = async () => {
    // Validate audio URL before starting export
    if (!songData.audioUrl) {
      setError("No audio available for export. Please wait for audio generation to complete.");
      return;
    }

    setIsExporting(true);
    setError(null);
    setVideoBlobUrl(null);

    try {
      // Build export config with SFX if enabled
      const exportConfig: ExportConfig = {
        ...config,
        // Only include SFX data if enabled
        sfxPlan: enableSFX ? sfxPlan : null,
        sceneTimings: enableSFX ? sceneTimings : undefined,
      };

      const exportFn = useCloudRender
        ? exportVideoWithFFmpeg
        : exportVideoClientSide;
      const result = await exportFn(
        songData,
        (progress) => {
          setExportProgress(progress);
        },
        exportConfig,
      );

      const url = URL.createObjectURL(result.blob);
      setVideoBlobUrl(url);
    } catch (e: any) {
      console.error("Export failed:", e);
      setError(e.message || "Export failed. Please try again.");
      setIsExporting(false);
    }
  };

  const handleDownload = () => {
    if (videoBlobUrl) {
      const a = document.createElement("a");
      a.href = videoBlobUrl;
      a.download = `${songData.fileName.replace(/\.[^/.]+$/, "")}-lyriclens-${config.orientation}.mp4`;
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
    }
  };

  const isComplete = exportProgress.stage === "complete" && videoBlobUrl;

  const getStageLabel = () => {
    switch (exportProgress.stage) {
      case "loading":
        return "Loading FFmpeg";
      case "preparing":
        return "Preparing Assets";
      case "rendering":
        return "Rendering Frames";
      case "encoding":
        return "Encoding Video";
      case "complete":
        return "Complete";
      default:
        return "Processing";
    }
  };

  return (
    <Dialog
      open={isOpen}
      onOpenChange={(open) => !open && !isExporting && onClose()}
    >
      <DialogContent className="sm:max-w-md md:max-w-lg glass-panel border-white/10 text-foreground">
        <DialogHeader>
          <DialogTitle className="flex items-center gap-2 text-xl">
            <Film className="w-5 h-5 text-primary" />
            {isComplete
              ? "Video Ready!"
              : isExporting
                ? "Exporting Video"
                : "Export Settings"}
          </DialogTitle>
          <DialogDescription className="text-muted-foreground">
            {!isExporting &&
              !isComplete &&
              "Customize your video export settings."}
            {isExporting &&
              !isComplete &&
              "Please wait while we render your video."}
            {isComplete && "Your video has been successfully generated."}
          </DialogDescription>
        </DialogHeader>

        <div className="py-4">
          {error ? (
            <div className="bg-destructive/10 border border-destructive/20 rounded-xl p-6 text-center mb-6">
              <AlertCircle className="w-12 h-12 text-destructive mx-auto mb-3" />
              <p className="text-destructive mb-4">{error}</p>
              <Button variant="destructive" onClick={() => setError(null)}>
                Try Again
              </Button>
            </div>
          ) : isExporting && !isComplete ? (
            <div className="space-y-6">
              <div className="flex justify-between text-xs uppercase tracking-wider">
                <span className="text-primary font-medium">
                  {getStageLabel()}
                </span>
                <span className="text-muted-foreground">
                  {Math.round(exportProgress.progress)}%
                </span>
              </div>

              <Progress value={exportProgress.progress} className="h-2" />

              <p className="text-sm text-muted-foreground text-center animate-pulse">
                {exportProgress.message}
              </p>

              <div className="grid grid-cols-5 gap-1 pt-2">
                {[
                  "loading",
                  "preparing",
                  "rendering",
                  "encoding",
                  "complete",
                ].map((stage, i) => {
                  const stages = [
                    "loading",
                    "preparing",
                    "rendering",
                    "encoding",
                    "complete",
                  ];
                  const currentIdx = stages.indexOf(exportProgress.stage);
                  const stageIdx = stages.indexOf(stage);
                  const isActive = stageIdx <= currentIdx;

                  return (
                    <div
                      key={stage}
                      className="flex flex-col items-center gap-1"
                    >
                      <div
                        className={cn(
                          "w-2 h-2 rounded-full transition-colors",
                          isActive ? "bg-primary" : "bg-muted",
                        )}
                      />
                      <span
                        className={cn(
                          "text-[9px] capitalize",
                          isActive ? "text-primary" : "text-muted-foreground",
                        )}
                      >
                        {stage}
                      </span>
                    </div>
                  );
                })}
              </div>
            </div>
          ) : isComplete ? (
            <div className="space-y-6">
              {videoBlobUrl && (
                <div className="relative aspect-video rounded-lg overflow-hidden bg-black border border-border">
                  <video
                    src={videoBlobUrl}
                    controls
                    className="w-full h-full"
                  />
                </div>
              )}

              <div className="flex flex-col gap-2">
                <Button
                  onClick={handleDownload}
                  className="w-full bg-primary hover:bg-primary/90 text-primary-foreground font-bold border-0"
                >
                  <Download className="mr-2 h-4 w-4" /> Download MP4
                </Button>
                <Button
                  variant="ghost"
                  className="text-muted-foreground hover:text-foreground"
                  onClick={() => {
                    setIsExporting(false);
                    setExportProgress({
                      stage: "loading",
                      progress: 0,
                      message: "Initializing...",
                    });
                    setVideoBlobUrl(null);
                  }}
                >
                  Export Another Version
                </Button>
              </div>
            </div>
          ) : (
            <div className="space-y-6">
              {/* Render Engine */}
              <div className="space-y-3">
                <Label className="text-muted-foreground">Render Engine</Label>
                <div className="grid grid-cols-2 gap-4">
                  <Card
                    className={cn(
                      "cursor-pointer transition-all border-white/10 hover:bg-white/5",
                      !useCloudRender && "border-primary bg-primary/10",
                    )}
                    onClick={() => setUseCloudRender(false)}
                  >
                    <CardContent className="flex flex-col items-center justify-center p-4 gap-2">
                      <Laptop
                        className={cn(
                          "h-6 w-6",
                          !useCloudRender
                            ? "text-primary"
                            : "text-muted-foreground",
                        )}
                      />
                      <div className="text-center">
                        <span className="text-sm font-medium">Browser</span>
                        <span className="text-[10px] text-muted-foreground block">
                          Private, No Upload
                        </span>
                      </div>
                    </CardContent>
                  </Card>
                  <Card
                    className={cn(
                      "cursor-pointer transition-all border-white/10 hover:bg-white/5",
                      useCloudRender && "border-primary bg-primary/10",
                    )}
                    onClick={() => setUseCloudRender(true)}
                  >
                    <CardContent className="flex flex-col items-center justify-center p-4 gap-2">
                      <Cloud
                        className={cn(
                          "h-6 w-6",
                          useCloudRender
                            ? "text-primary"
                            : "text-muted-foreground",
                        )}
                      />
                      <div className="text-center">
                        <span className="text-sm font-medium">Cloud</span>
                        <span className="text-[10px] text-muted-foreground block">
                          Faster for 4K
                        </span>
                      </div>
                    </CardContent>
                  </Card>
                </div>
              </div>

              {/* Orientation */}
              <div className="space-y-3">
                <Label className="text-muted-foreground">
                  Video Orientation
                </Label>
                <div className="grid grid-cols-2 gap-4">
                  <Card
                    className={cn(
                      "cursor-pointer transition-all border-white/10 hover:bg-white/5",
                      config.orientation === "landscape" &&
                      "border-primary bg-primary/10",
                    )}
                    onClick={() =>
                      setConfig({ ...config, orientation: "landscape" })
                    }
                  >
                    <CardContent className="flex flex-col items-center justify-center p-4 gap-2">
                      <Monitor
                        className={cn(
                          "h-6 w-6",
                          config.orientation === "landscape"
                            ? "text-primary"
                            : "text-muted-foreground",
                        )}
                      />
                      <span className="text-sm font-medium">
                        Landscape (16:9)
                      </span>
                    </CardContent>
                  </Card>
                  <Card
                    className={cn(
                      "cursor-pointer transition-all border-white/10 hover:bg-white/5",
                      config.orientation === "portrait" &&
                      "border-primary bg-primary/10",
                    )}
                    onClick={() =>
                      setConfig({ ...config, orientation: "portrait" })
                    }
                  >
                    <CardContent className="flex flex-col items-center justify-center p-4 gap-2">
                      <Smartphone
                        className={cn(
                          "h-6 w-6",
                          config.orientation === "portrait"
                            ? "text-primary"
                            : "text-muted-foreground",
                        )}
                      />
                      <span className="text-sm font-medium">
                        Portrait (9:16)
                      </span>
                    </CardContent>
                  </Card>
                </div>
              </div>

              {/* Visual Style */}
              <div className="flex items-center justify-between space-x-2 p-3 rounded-lg border border-white/10 bg-white/5">
                <div className="space-y-0.5">
                  <Label className="text-base text-foreground flex items-center gap-2">
                    <Settings className="w-4 h-4 text-primary" /> Cinematic
                    Effects
                  </Label>
                  <p className="text-xs text-muted-foreground">
                    Ken Burns zoom, smooth transitions, glow
                  </p>
                </div>
                <Switch
                  checked={config.useModernEffects}
                  onCheckedChange={(checked) =>
                    setConfig({ ...config, useModernEffects: checked })
                  }
                  className="data-[state=checked]:bg-primary"
                />
              </div>

              {/* Transition Type */}
              <div className="space-y-3">
                <Label className="text-muted-foreground">Scene Transitions</Label>
                <div className="grid grid-cols-5 gap-2">
                  {[
                    { value: "none", label: "Cut", icon: CircleSlash },
                    { value: "fade", label: "Fade", icon: Sparkles },
                    { value: "dissolve", label: "Dissolve", icon: Blend },
                    { value: "zoom", label: "Zoom", icon: ZoomIn },
                    { value: "slide", label: "Slide", icon: ArrowRightLeft },
                  ].map(({ value, label, icon: Icon }) => (
                    <button
                      key={value}
                      onClick={() => setConfig({ ...config, transitionType: value as TransitionType })}
                      className={cn(
                        "flex flex-col items-center gap-1 p-2 rounded-lg border transition-all text-xs",
                        config.transitionType === value
                          ? "border-primary bg-primary/10 text-primary"
                          : "border-border bg-card text-muted-foreground hover:border-primary/30"
                      )}
                    >
                      <Icon size={16} />
                      <span>{label}</span>
                    </button>
                  ))}
                </div>
              </div>

              <div className="space-y-4 pt-2 border-t border-border/50">
                <Label className="text-muted-foreground">Lyric Animation</Label>

                <div className="space-y-3">
                  <div className="flex justify-between">
                    <span className="text-xs text-muted-foreground">
                      Sync Offset
                    </span>
                    <span className="text-xs text-primary font-mono">
                      {config.syncOffsetMs}ms
                    </span>
                  </div>
                  <Slider
                    min={-200}
                    max={100}
                    step={10}
                    value={[config.syncOffsetMs]}
                    onValueChange={(vals) =>
                      setConfig({ ...config, syncOffsetMs: vals[0] ?? -50 })
                    }
                    className="py-2"
                  />
                  <p className="text-[10px] text-muted-foreground">
                    Negative values make lyrics appear earlier.
                  </p>
                </div>

                <div className="flex items-center justify-between space-x-2">
                  <div className="space-y-0.5">
                    <Label className="text-sm text-muted-foreground">
                      Word-by-Word Karaoke
                    </Label>
                    <p className="text-xs text-muted-foreground">
                      Highlight each word as it's sung
                    </p>
                  </div>
                  <Switch
                    checked={config.wordLevelHighlight}
                    onCheckedChange={(checked) =>
                      setConfig({ ...config, wordLevelHighlight: checked })
                    }
                  />
                </div>

                <div className="flex items-center justify-between space-x-2">
                  <div className="space-y-0.5">
                    <Label className="text-sm text-muted-foreground">
                      Fade Before Scene Change
                    </Label>
                    <p className="text-xs text-muted-foreground">
                      Lyrics fade out before transitions
                    </p>
                  </div>
                  <Switch
                    checked={config.fadeOutBeforeCut}
                    onCheckedChange={(checked) =>
                      setConfig({ ...config, fadeOutBeforeCut: checked })
                    }
                  />
                </div>

                {/* NEW: Visualizer Settings */}
                <div className="space-y-4 pt-4 border-t border-border/50">
                  <div className="flex items-center justify-between">
                    <Label className="text-muted-foreground flex items-center gap-2">
                      <Activity className="w-4 h-4" /> Audio Visualizer
                    </Label>
                    <Switch
                      checked={config.visualizerConfig?.enabled}
                      onCheckedChange={(checked) =>
                        setConfig({
                          ...config,
                          visualizerConfig: config.visualizerConfig ? {
                            ...config.visualizerConfig,
                            enabled: checked,
                          } : undefined,
                        })
                      }
                    />
                  </div>

                  {config.visualizerConfig?.enabled && (
                    <div className="space-y-4 pl-6">
                      <div className="space-y-2">
                        <div className="flex justify-between">
                          <span className="text-[10px] text-muted-foreground uppercase">Opacity</span>
                          <span className="text-[10px] font-mono">{Math.round((config.visualizerConfig?.opacity ?? 0.15) * 100)}%</span>
                        </div>
                        <Slider
                          min={0.05}
                          max={0.5}
                          step={0.01}
                          value={[config.visualizerConfig?.opacity ?? 0.15]}
                          onValueChange={([val]) =>
                            setConfig({
                              ...config,
                              visualizerConfig: config.visualizerConfig ? { ...config.visualizerConfig, opacity: val ?? 0.15 } : undefined,
                            })
                          }
                        />
                      </div>

                      <div className="space-y-2">
                        <div className="flex justify-between">
                          <span className="text-[10px] text-muted-foreground uppercase">Max Height</span>
                          <span className="text-[10px] font-mono">{Math.round((config.visualizerConfig?.maxHeightRatio ?? 0.25) * 100)}%</span>
                        </div>
                        <Slider
                          min={0.1}
                          max={0.5}
                          step={0.05}
                          value={[config.visualizerConfig?.maxHeightRatio ?? 0.25]}
                          onValueChange={([val]) =>
                            setConfig({
                              ...config,
                              visualizerConfig: config.visualizerConfig ? { ...config.visualizerConfig, maxHeightRatio: val ?? 0.25 } : undefined,
                            })
                          }
                        />
                      </div>

                      <div className="space-y-2">
                        <span className="text-[10px] text-muted-foreground uppercase block mb-1">Color Scheme</span>
                        <div className="grid grid-cols-3 gap-2">
                          {(["cyan-purple", "rainbow", "monochrome"] as const).map((scheme) => (
                            <Button
                              key={scheme}
                              variant="glass"
                              size="sm"
                              className={cn(
                                "text-[10px] h-7 px-2",
                                config.visualizerConfig?.colorScheme === scheme && "border-primary bg-primary/10"
                              )}
                              onClick={() =>
                                setConfig({
                                  ...config,
                                  visualizerConfig: config.visualizerConfig ? { ...config.visualizerConfig, colorScheme: scheme } : undefined,
                                })
                              }
                            >
                              {scheme.replace("-", " ")}
                            </Button>
                          ))}
                        </div>
                      </div>
                    </div>
                  )}
                </div>

                {/* NEW: Text Animation Settings */}
                <div className="space-y-4 pt-4 border-t border-border/50">
                  <Label className="text-muted-foreground flex items-center gap-2">
                    <Type className="w-4 h-4" /> Text Reveal Animation
                  </Label>

                  <div className="grid grid-cols-2 gap-4">
                    <div className="space-y-1.5">
                      <span className="text-[10px] text-muted-foreground uppercase">Direction</span>
                      <Select
                        value={config.textAnimationConfig?.revealDirection}
                        onValueChange={(val: "ltr" | "rtl" | "center-out" | "center-in") =>
                          setConfig({
                            ...config,
                            textAnimationConfig: config.textAnimationConfig ? { ...config.textAnimationConfig, revealDirection: val } : undefined,
                          })
                        }
                      >
                        <SelectTrigger className="h-8 text-xs">
                          <SelectValue />
                        </SelectTrigger>
                        <SelectContent>
                          <SelectItem value="ltr">Left to Right</SelectItem>
                          <SelectItem value="rtl">Right to Left</SelectItem>
                          <SelectItem value="center-out">Center Out</SelectItem>
                          <SelectItem value="center-in">Center In</SelectItem>
                        </SelectContent>
                      </Select>
                    </div>

                    <div className="space-y-1.5">
                      <span className="text-[10px] text-muted-foreground uppercase">Speed (s)</span>
                      <div className="flex items-center gap-2">
                        <Slider
                          min={0.1}
                          max={1.0}
                          step={0.1}
                          value={[config.textAnimationConfig?.revealDuration || 0.3]}
                          onValueChange={([val]) =>
                            setConfig({
                              ...config,
                              textAnimationConfig: config.textAnimationConfig ? { ...config.textAnimationConfig, revealDuration: val ?? 0.3 } : undefined,
                            })
                          }
                          className="flex-1"
                        />
                        <span className="text-[10px] font-mono w-6">
                          {config.textAnimationConfig?.revealDuration}s
                        </span>
                      </div>
                    </div>
                  </div>
                </div>

                {/* SFX Audio Mixing Settings */}
                {hasSFX && (
                  <div className="space-y-4 pt-4 border-t border-border/50">
                    <div className="flex items-center justify-between">
                      <Label className="text-muted-foreground flex items-center gap-2">
                        <Volume2 className="w-4 h-4" /> Ambient SFX
                      </Label>
                      <Switch
                        checked={enableSFX}
                        onCheckedChange={setEnableSFX}
                      />
                    </div>

                    {enableSFX && (
                      <div className="space-y-4 pl-6">
                        <div className="space-y-2">
                          <div className="flex justify-between">
                            <span className="text-[10px] text-muted-foreground uppercase">SFX Volume</span>
                            <span className="text-[10px] font-mono">{Math.round((config.sfxMasterVolume || 1.0) * 100)}%</span>
                          </div>
                          <Slider
                            min={0}
                            max={1}
                            step={0.05}
                            value={[config.sfxMasterVolume || 1.0]}
                            onValueChange={([val]) =>
                              setConfig({ ...config, sfxMasterVolume: val ?? 1.0 })
                            }
                          />
                        </div>

                        {sfxPlan?.backgroundMusic && (
                          <div className="space-y-2">
                            <div className="flex justify-between">
                              <span className="text-[10px] text-muted-foreground uppercase">Music Volume</span>
                              <span className="text-[10px] font-mono">{Math.round((config.musicMasterVolume || 0.5) * 100)}%</span>
                            </div>
                            <Slider
                              min={0}
                              max={1.0}
                              step={0.05}
                              value={[config.musicMasterVolume || 0.5]}
                              onValueChange={([val]) =>
                                setConfig({ ...config, musicMasterVolume: val ?? 0.5 })
                              }
                            />
                          </div>
                        )}

                        <p className="text-[10px] text-muted-foreground">
                          🎵 {sfxPlan?.scenes.filter(s => s.ambientTrack?.audioUrl).length || 0} scene sounds will be mixed
                          {sfxPlan?.backgroundMusic?.audioUrl && " + background music"}
                        </p>
                      </div>
                    )}
                  </div>
                )}
              </div>
            </div>
          )}
        </div>

        <DialogFooter className="sm:justify-between">
          {!isExporting && !isComplete && (
            <Button
              className="w-full bg-primary hover:bg-primary/90 text-primary-foreground font-bold border-0 h-11"
              onClick={startExport}
            >
              <Film className="mr-2 h-4 w-4" /> Start Export
            </Button>
          )}
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
};
````

## File: packages/frontend/components/VideoPreviewCard.tsx
````typescript
/**
 * VideoPreviewCard - Video preview with scene thumbnails
 *
 * Displays the current scene with playback controls and
 * a horizontal scrollable thumbnail strip for scene navigation.
 * Includes a full transport bar (seek, skip, volume, fullscreen)
 * and a "N / M Videos generated" counter badge.
 *
 * When `currentVisual` is a video, the internal <video> ref drives the
 * transport bar — callers get `currentTime` / `onSeek` / `onPlayPause`
 * callbacks but can also leave them unset and the card self-manages.
 */

import React, { useRef, useState, useEffect, useCallback } from 'react';
import { motion } from 'framer-motion';
import {
  Video, Play, Pause, Loader2, CheckCircle2,
  SkipBack, SkipForward, ChevronFirst, Volume2, VolumeX, Maximize,
} from 'lucide-react';
import { cn } from '@/lib/utils';
import type { Scene } from '@/types';

export interface VideoPreviewCardProps {
  /** List of scenes to display */
  scenes: Scene[];
  /** Map of scene ID to visual URL */
  visualsMap: Record<string, string | undefined>;
  /** Currently selected scene index */
  currentSceneIndex: number;
  /** Callback when scene is selected */
  onSceneSelect: (index: number) => void;
  /** Whether video is currently playing (controlled externally; ignored when self-managing) */
  isPlaying: boolean;
  /** Callback to toggle play/pause */
  onPlayPause: () => void;
  /** Whether video is ready for export */
  isReady?: boolean;
  /** Total duration in seconds */
  totalDuration?: number;
  /** Text for "scenes" label */
  scenesLabel?: string;
  /** Text for "done" status */
  doneLabel?: string;
  /** RTL layout */
  isRTL?: boolean;
  /** Additional class names */
  className?: string;

  // Transport bar — caller-controlled (optional; if omitted the card self-manages via videoRef)
  currentTime?: number;
  onSeek?: (time: number) => void;
  onSkipToStart?: () => void;
  onSkipPrev?: () => void;
  onSkipNext?: () => void;
  volume?: number;
  onVolumeChange?: (v: number) => void;
  onFullscreen?: () => void;

  // Generation counter + Generate Video button
  videosGeneratedCount?: number;
  totalVideos?: number;
  onGenerateVideo?: () => void;
}

function formatTime(secs: number): string {
  const s = Math.floor(secs);
  const m = Math.floor(s / 60);
  return `${m}:${String(s % 60).padStart(2, '0')}`;
}

export function VideoPreviewCard({
  scenes,
  visualsMap,
  currentSceneIndex,
  onSceneSelect,
  isPlaying,
  onPlayPause,
  isReady = false,
  totalDuration = 0,
  scenesLabel = 'Scenes',
  doneLabel = 'Ready',
  isRTL = false,
  className,
  currentTime: externalCurrentTime,
  onSeek: externalOnSeek,
  onSkipToStart,
  onSkipPrev,
  onSkipNext,
  volume: externalVolume,
  onVolumeChange: externalOnVolumeChange,
  onFullscreen,
  videosGeneratedCount,
  totalVideos,
  onGenerateVideo,
}: VideoPreviewCardProps) {
  const currentScene = scenes[currentSceneIndex];
  const currentVisual = currentScene ? visualsMap[currentScene.id] : undefined;
  const isVideoSrc = !!(
    currentVisual &&
    (currentVisual.match(/\.(mp4|webm)$/i) ||
      currentVisual.includes('generativelanguage.googleapis.com'))
  );

  // Internal state for self-managed playback (only used when currentVisual is a <video>)
  const videoRef = useRef<HTMLVideoElement>(null);
  const containerRef = useRef<HTMLDivElement>(null);
  const [internalTime, setInternalTime] = useState(0);
  const [internalDuration, setInternalDuration] = useState(0);
  const [internalVolume, setInternalVolume] = useState(1);
  const [internalPlaying, setInternalPlaying] = useState(false);

  // Whether we self-manage (no external seek wired)
  const selfManaged = isVideoSrc && !externalOnSeek;

  const currentTime = selfManaged ? internalTime : (externalCurrentTime ?? 0);
  const duration = selfManaged ? internalDuration : (totalDuration || 0);
  const volume = selfManaged ? internalVolume : (externalVolume ?? 1);
  const playing = selfManaged ? internalPlaying : isPlaying;

  // Sync video element play/pause with external isPlaying when not self-managed
  useEffect(() => {
    if (!videoRef.current || selfManaged) return;
    if (isPlaying) {
      videoRef.current.play().catch(() => {});
    } else {
      videoRef.current.pause();
    }
  }, [isPlaying, selfManaged]);

  // Sync volume to video element
  useEffect(() => {
    if (videoRef.current) {
      videoRef.current.volume = volume;
    }
  }, [volume]);

  // Reset internal state when scene changes
  useEffect(() => {
    setInternalTime(0);
    setInternalDuration(0);
    setInternalPlaying(false);
  }, [currentSceneIndex]);

  const handleVideoTimeUpdate = useCallback(() => {
    if (videoRef.current && selfManaged) {
      setInternalTime(videoRef.current.currentTime);
    }
  }, [selfManaged]);

  const handleVideoLoadedMetadata = useCallback(() => {
    if (videoRef.current) {
      setInternalDuration(videoRef.current.duration);
    }
  }, []);

  const handleVideoPlay = useCallback(() => setInternalPlaying(true), []);
  const handleVideoPause = useCallback(() => setInternalPlaying(false), []);
  const handleVideoEnded = useCallback(() => setInternalPlaying(false), []);

  const handlePlayPause = useCallback(() => {
    if (selfManaged && videoRef.current) {
      if (videoRef.current.paused) {
        videoRef.current.play().catch(() => {});
      } else {
        videoRef.current.pause();
      }
    } else {
      onPlayPause();
    }
  }, [selfManaged, onPlayPause]);

  const handleSeek = useCallback((time: number) => {
    if (selfManaged && videoRef.current) {
      videoRef.current.currentTime = time;
      setInternalTime(time);
    } else {
      externalOnSeek?.(time);
    }
  }, [selfManaged, externalOnSeek]);

  const handleSkipToStart = useCallback(() => {
    if (selfManaged && videoRef.current) {
      videoRef.current.currentTime = 0;
      setInternalTime(0);
    } else {
      onSkipToStart?.();
    }
  }, [selfManaged, onSkipToStart]);

  const handleVolumeChange = useCallback((v: number) => {
    if (selfManaged) {
      setInternalVolume(v);
      if (videoRef.current) videoRef.current.volume = v;
    } else {
      externalOnVolumeChange?.(v);
    }
  }, [selfManaged, externalOnVolumeChange]);

  const handleFullscreen = useCallback(() => {
    if (onFullscreen) {
      onFullscreen();
    } else if (containerRef.current) {
      if (!document.fullscreenElement) {
        containerRef.current.requestFullscreen?.();
      } else {
        document.exitFullscreen?.();
      }
    }
  }, [onFullscreen]);

  const hasTransportBar = isVideoSrc || !!(
    externalOnSeek || onSkipToStart || onSkipPrev || onSkipNext ||
    externalOnVolumeChange || onFullscreen
  );
  const hasGenerationCounter =
    videosGeneratedCount !== undefined && totalVideos !== undefined;

  if (scenes.length === 0) return null;

  return (
    <motion.div
      ref={containerRef}
      initial={{ opacity: 0, y: 20 }}
      animate={{ opacity: 1, y: 0 }}
      className={cn('', className)}
    >
      <div className="bg-white/5 border border-white/10 rounded-2xl overflow-hidden backdrop-blur-sm">
        {/* Main Preview Area */}
        <div className="relative aspect-video bg-black/40">
          {/* Current Visual */}
          {currentVisual ? (
            isVideoSrc ? (
              <video
                ref={videoRef}
                src={currentVisual}
                className="w-full h-full object-cover"
                controls={false}
                loop
                muted={volume === 0}
                playsInline
                crossOrigin="anonymous"
                onTimeUpdate={handleVideoTimeUpdate}
                onLoadedMetadata={handleVideoLoadedMetadata}
                onPlay={handleVideoPlay}
                onPause={handleVideoPause}
                onEnded={handleVideoEnded}
              />
            ) : (
              <img
                src={currentVisual}
                alt={currentScene?.name || 'Scene preview'}
                className="w-full h-full object-cover"
                crossOrigin="anonymous"
              />
            )
          ) : (
            <div className="w-full h-full flex items-center justify-center">
              <Loader2 className="w-8 h-8 text-white/30 animate-spin" aria-hidden="true" />
            </div>
          )}

          {/* Gradient Overlay */}
          <div className="absolute inset-0 bg-linear-to-t from-black/80 via-transparent to-black/40 pointer-events-none" />

          {/* Scene Counter Badge (top-left) */}
          <div className={cn('absolute top-4', isRTL ? 'right-4' : 'left-4')}>
            <span className="px-3 py-1 rounded-full bg-black/50 backdrop-blur text-xs text-white/80 border border-white/10">
              {scenesLabel} {currentSceneIndex + 1} / {scenes.length}
            </span>
          </div>

          {/* Generation Counter Badge (top-right) */}
          {hasGenerationCounter && (
            <div className={cn('absolute top-4', isRTL ? 'left-4' : 'right-4')}>
              <span className="px-3 py-1 rounded-full bg-black/50 backdrop-blur text-xs text-white/80 border border-white/10">
                {videosGeneratedCount} / {totalVideos} Videos generated
              </span>
            </div>
          )}

          {/* Generate Video overlay (centered, when no visual yet) */}
          {onGenerateVideo && !currentVisual && (
            <div className="absolute inset-0 flex items-center justify-center z-10">
              <button
                onClick={onGenerateVideo}
                className="px-6 py-3 rounded-xl bg-yellow-400 hover:bg-yellow-300 text-black font-bold text-sm shadow-2xl transition-colors flex items-center gap-2"
              >
                <Video className="w-4 h-4" aria-hidden="true" />
                Generate Video →
              </button>
            </div>
          )}

          {/* Play/Pause overlay button (center) */}
          {isReady && (
            <button
              onClick={handlePlayPause}
              className="absolute inset-0 flex items-center justify-center group"
              aria-label={playing ? 'Pause' : 'Play'}
            >
              <div
                className={cn(
                  'w-16 h-16 rounded-full bg-white/10 backdrop-blur-md border border-white/20 flex items-center justify-center transition-all',
                  'group-hover:scale-110 group-hover:bg-white/20',
                  playing ? 'opacity-0 group-hover:opacity-100' : 'opacity-100'
                )}
              >
                {playing ? (
                  <Pause className="w-6 h-6 text-white" aria-hidden="true" />
                ) : (
                  <Play className="w-6 h-6 text-white ms-1" aria-hidden="true" />
                )}
              </div>
            </button>
          )}

          {/* Scene Info Overlay */}
          <div
            className={cn(
              'absolute bottom-4 pointer-events-none',
              isRTL ? 'right-4 left-4 text-right' : 'left-4 right-4'
            )}
          >
            <h3 className="text-lg font-medium text-white mb-1">{currentScene?.name}</h3>
            <p className="text-sm text-white/60 line-clamp-2">{currentScene?.narrationScript}</p>
          </div>
        </div>

        {/* Transport Bar */}
        {hasTransportBar && (
          <div className="px-4 py-2 bg-black/30 border-t border-white/5 space-y-1.5">
            {/* Seek bar */}
            <input
              type="range"
              min={0}
              max={duration || 100}
              step={0.1}
              value={currentTime}
              onChange={e => handleSeek(parseFloat(e.target.value))}
              className="w-full h-1.5 rounded-full appearance-none cursor-pointer bg-white/10 accent-yellow-400"
              aria-label="Seek"
            />

            {/* Controls row */}
            <div className="flex items-center gap-1.5">
              {/* Skip to start */}
              <button
                onClick={handleSkipToStart}
                className="p-1 text-white/50 hover:text-white transition-colors"
                aria-label="Skip to start"
              >
                <ChevronFirst className="w-4 h-4" aria-hidden="true" />
              </button>

              {/* Previous scene */}
              <button
                onClick={() => {
                  if (onSkipPrev) onSkipPrev();
                  else if (currentSceneIndex > 0) onSceneSelect(currentSceneIndex - 1);
                }}
                disabled={currentSceneIndex === 0}
                className="p-1 text-white/50 hover:text-white disabled:text-white/20 transition-colors"
                aria-label="Previous scene"
              >
                <SkipBack className="w-4 h-4" aria-hidden="true" />
              </button>

              {/* Play / Pause */}
              <button
                onClick={handlePlayPause}
                className="p-1.5 rounded-full bg-white/10 hover:bg-white/20 text-white transition-colors"
                aria-label={playing ? 'Pause' : 'Play'}
              >
                {playing ? (
                  <Pause className="w-4 h-4" aria-hidden="true" />
                ) : (
                  <Play className="w-4 h-4 ms-0.5" aria-hidden="true" />
                )}
              </button>

              {/* Next scene */}
              <button
                onClick={() => {
                  if (onSkipNext) onSkipNext();
                  else if (currentSceneIndex < scenes.length - 1) onSceneSelect(currentSceneIndex + 1);
                }}
                disabled={currentSceneIndex >= scenes.length - 1}
                className="p-1 text-white/50 hover:text-white disabled:text-white/20 transition-colors"
                aria-label="Next scene"
              >
                <SkipForward className="w-4 h-4" aria-hidden="true" />
              </button>

              {/* Time display */}
              {duration > 0 && (
                <span className="text-xs text-white/40 font-mono tabular-nums ml-1">
                  {formatTime(currentTime)} / {formatTime(duration)}
                </span>
              )}

              <div className="flex-1" />

              {/* Volume */}
              <button
                onClick={() => handleVolumeChange(volume > 0 ? 0 : 1)}
                className="p-1 text-white/50 hover:text-white transition-colors"
                aria-label={volume > 0 ? 'Mute' : 'Unmute'}
              >
                {volume > 0 ? (
                  <Volume2 className="w-4 h-4" aria-hidden="true" />
                ) : (
                  <VolumeX className="w-4 h-4" aria-hidden="true" />
                )}
              </button>
              <input
                type="range"
                min={0}
                max={1}
                step={0.05}
                value={volume}
                onChange={e => handleVolumeChange(parseFloat(e.target.value))}
                className="w-16 h-1 rounded-full appearance-none cursor-pointer bg-white/10 accent-yellow-400"
                aria-label="Volume"
              />

              {/* Fullscreen */}
              <button
                onClick={handleFullscreen}
                className="p-1 text-white/50 hover:text-white transition-colors"
                aria-label="Fullscreen"
              >
                <Maximize className="w-4 h-4" aria-hidden="true" />
              </button>
            </div>
          </div>
        )}

        {/* Scene Thumbnails */}
        {scenes.length > 1 && (
          <div className="p-3 flex gap-2 overflow-x-auto bg-black/20" role="tablist">
            {scenes.map((scene, idx) => (
              <button
                key={scene.id}
                role="tab"
                aria-selected={idx === currentSceneIndex}
                aria-label={`Scene ${idx + 1}: ${scene.name}`}
                onClick={() => onSceneSelect(idx)}
                className={cn(
                  'shrink-0 w-20 h-12 rounded-lg overflow-hidden border-2 transition-all',
                  idx === currentSceneIndex
                    ? 'border-violet-500 ring-2 ring-violet-500/30'
                    : 'border-transparent opacity-60 hover:opacity-100'
                )}
              >
                {visualsMap[scene.id] ? (
                  (() => {
                    const url = visualsMap[scene.id]!;
                    const isVid = url.match(/\.(mp4|webm)$/i) ||
                      url.includes('generativelanguage.googleapis.com');
                    if (isVid) {
                      return (
                        <video
                          src={url}
                          className="w-full h-full object-cover"
                          muted
                          playsInline
                          crossOrigin="anonymous"
                          onMouseOver={e => e.currentTarget.play()}
                          onMouseOut={e => e.currentTarget.pause()}
                        />
                      );
                    }
                    return (
                      <img
                        src={url}
                        alt=""
                        className="w-full h-full object-cover"
                        crossOrigin="anonymous"
                      />
                    );
                  })()
                ) : (
                  <div className="w-full h-full bg-white/5 flex items-center justify-center text-xs text-white/30">
                    {idx + 1}
                  </div>
                )}
              </button>
            ))}
          </div>
        )}

        {/* Status Footer */}
        {isReady && (
          <div className="px-4 py-3 flex items-center justify-between border-t border-white/5 bg-black/20">
            <div className="flex items-center gap-4 text-xs text-white/40">
              <span className="flex items-center gap-1.5">
                <Video className="w-3.5 h-3.5" aria-hidden="true" />
                {scenes.length} {scenesLabel.toLowerCase()}
              </span>
              {totalDuration > 0 && <span>{Math.round(totalDuration)}s</span>}
            </div>
            <div className="flex items-center gap-1.5 text-emerald-400 text-xs">
              <CheckCircle2 className="w-3.5 h-3.5" aria-hidden="true" />
              {doneLabel}
            </div>
          </div>
        )}
      </div>
    </motion.div>
  );
}

export default VideoPreviewCard;
````

## File: packages/frontend/components/visualizer/AudioUploadForm.tsx
````typescript
/**
 * AudioUploadForm - Audio file upload with style/provider selection
 *
 * Extracted from VisualizerScreen for better maintainability.
 */

import React, { useCallback, useRef } from 'react';
import { Music, Sparkles, Loader2, CheckCircle2, X, Wand2 } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/i18n/useLanguage';
import { ART_STYLES } from '@/constants';
import { AppState } from '@/types';
import { LoadingState } from '@/components/ui/LoadingState';
import { ErrorState } from '@/components/ui/ErrorState';

export interface AudioUploadFormProps {
  /** Selected audio file */
  audioFile: File | null;
  /** Callback when audio file changes */
  onAudioFileChange: (file: File | null) => void;
  /** Selected visual style */
  selectedStyle: string;
  /** Callback when style changes */
  onStyleChange: (style: string) => void;
  /** Selected image provider */
  imageProvider: 'gemini' | 'deapi';
  /** Callback when provider changes */
  onProviderChange: (provider: 'gemini' | 'deapi') => void;
  /** Selected director mode (chain = faster, agent = smarter) */
  directorMode?: 'chain' | 'agent';
  /** Callback when director mode changes */
  onDirectorModeChange?: (mode: 'chain' | 'agent') => void;
  /** Global subject for visual consistency (e.g., "a bearded prophet in flowing robes") */
  globalSubject?: string;
  /** Callback when global subject changes */
  onGlobalSubjectChange?: (subject: string) => void;
  /** Current app state for processing status */
  appState: AppState;
  /** Error message to display */
  errorMsg?: string;
  /** Callback to start processing */
  onStartProcessing: () => void;
  /** Additional class names */
  className?: string;
}

/**
 * Audio upload form with drag & drop, style selection, and provider choice
 */
export function AudioUploadForm({
  audioFile,
  onAudioFileChange,
  selectedStyle,
  onStyleChange,
  imageProvider,
  onProviderChange,
  directorMode = 'chain',
  onDirectorModeChange,
  globalSubject = '',
  onGlobalSubjectChange,
  appState,
  errorMsg,
  onStartProcessing,
  className,
}: AudioUploadFormProps) {
  const { t, isRTL } = useLanguage();
  const audioInputRef = useRef<HTMLInputElement>(null);

  const isProcessing = appState === AppState.PROCESSING_AUDIO ||
    appState === AppState.TRANSCRIBING ||
    appState === AppState.ANALYZING_LYRICS ||
    appState === AppState.GENERATING_PROMPTS;

  const handleAudioSelect = useCallback((e: React.ChangeEvent<HTMLInputElement>) => {
    const file = e.target.files?.[0];
    if (file) {
      onAudioFileChange(file);
    }
  }, [onAudioFileChange]);

  const handleDrop = useCallback((e: React.DragEvent) => {
    e.preventDefault();
    const file = e.dataTransfer.files[0];
    if (file) {
      const isAudio = file.type.startsWith('audio/') ||
        file.name.endsWith('.mp3') ||
        file.name.endsWith('.wav');
      if (isAudio) {
        onAudioFileChange(file);
      }
    }
  }, [onAudioFileChange]);

  const handleDragOver = useCallback((e: React.DragEvent) => {
    e.preventDefault();
  }, []);

  const getProcessingMessage = () => {
    switch (appState) {
      case AppState.TRANSCRIBING:
        return 'Transcribing audio...';
      case AppState.ANALYZING_LYRICS:
        return 'Analyzing content...';
      case AppState.GENERATING_PROMPTS:
        return 'Generating visual prompts...';
      case AppState.PROCESSING_AUDIO:
        return t('common.loading');
      default:
        return t('common.loading');
    }
  };

  return (
    <div className={cn('max-w-2xl w-full', className)}>
      {/* Title */}
      <div className={cn('text-center mb-8', isRTL && 'rtl')}>
        <div className="w-16 h-16 mx-auto mb-6 rounded-2xl bg-gradient-to-br from-cyan-600/20 to-blue-600/20 border border-white/10 flex items-center justify-center">
          <Wand2 className="w-8 h-8 text-cyan-400" aria-hidden="true" />
        </div>
        <h1 className="text-3xl font-bold mb-3">{t('visualizer.title')}</h1>
        <p className="text-white/60">{t('visualizer.uploadAudio')}</p>
      </div>

      {/* Form Content */}
      <div className="space-y-4">
        {/* Audio Upload */}
        <div
          onClick={() => audioInputRef.current?.click()}
          onDrop={handleDrop}
          onDragOver={handleDragOver}
          className={cn(
            'relative p-8 rounded-2xl border-2 border-dashed transition-all cursor-pointer',
            audioFile
              ? 'border-cyan-500/50 bg-cyan-500/5'
              : 'border-white/20 hover:border-white/40 bg-white/5 hover:bg-white/10'
          )}
          role="button"
          tabIndex={0}
          aria-label={audioFile ? `Selected: ${audioFile.name}` : 'Click or drag to upload audio'}
          onKeyDown={(e) => {
            if (e.key === 'Enter' || e.key === ' ') {
              e.preventDefault();
              audioInputRef.current?.click();
            }
          }}
        >
          <input
            ref={audioInputRef}
            type="file"
            accept="audio/*,.mp3,.wav,.m4a,.ogg"
            onChange={handleAudioSelect}
            className="hidden"
            aria-hidden="true"
          />
          <div className={cn('flex flex-col items-center gap-4', isRTL && 'rtl')}>
            <div className={cn(
              'w-14 h-14 rounded-xl flex items-center justify-center',
              audioFile ? 'bg-cyan-500/20' : 'bg-white/10'
            )}>
              {audioFile ? (
                <CheckCircle2 className="w-7 h-7 text-cyan-400" aria-hidden="true" />
              ) : (
                <Music className="w-7 h-7 text-white/60" aria-hidden="true" />
              )}
            </div>
            <div className="text-center">
              <p className="font-medium mb-1">
                {audioFile ? audioFile.name : t('visualizer.dropAudio')}
              </p>
              <p className="text-sm text-white/40">
                MP3, WAV, M4A, OGG
              </p>
            </div>
          </div>
          {audioFile && (
            <button
              onClick={(e) => {
                e.stopPropagation();
                onAudioFileChange(null);
              }}
              className="absolute top-4 end-4 p-1 rounded-full bg-white/10 hover:bg-white/20 transition-colors"
              aria-label="Remove selected file"
            >
              <X className="w-4 h-4" aria-hidden="true" />
            </button>
          )}
        </div>

        {/* Style Selection */}
        <div className="bg-white/5 rounded-2xl p-6 border border-white/10">
          <label className="block text-sm font-medium text-white/80 mb-3">
            Visual Style
          </label>
          <div className="grid grid-cols-3 md:grid-cols-5 gap-2" role="radiogroup" aria-label="Visual style selection">
            {ART_STYLES.slice(0, 10).map((style) => (
              <button
                key={style}
                role="radio"
                aria-checked={selectedStyle === style}
                onClick={() => onStyleChange(style)}
                className={cn(
                  'px-3 py-2 rounded-lg text-sm font-medium transition-all',
                  selectedStyle === style
                    ? 'bg-cyan-500/20 border-2 border-cyan-500 text-cyan-300'
                    : 'bg-white/5 border border-white/10 text-white/70 hover:bg-white/10'
                )}
              >
                {style}
              </button>
            ))}
          </div>
        </div>

        {/* Image Provider Selection */}
        <div className="bg-white/5 rounded-2xl p-6 border border-white/10">
          <label className="block text-sm font-medium text-white/80 mb-3">
            Image Provider
          </label>
          <div className="grid grid-cols-2 gap-2" role="radiogroup" aria-label="Image provider selection">
            <button
              role="radio"
              aria-checked={imageProvider === 'gemini'}
              onClick={() => onProviderChange('gemini')}
              className={cn(
                'px-4 py-3 rounded-lg text-sm font-medium transition-all text-start',
                imageProvider === 'gemini'
                  ? 'bg-cyan-500/20 border-2 border-cyan-500 text-cyan-300'
                  : 'bg-white/5 border border-white/10 text-white/70 hover:bg-white/10'
              )}
            >
              <div className="flex flex-col items-start gap-1">
                <span className="font-semibold">Gemini Imagen</span>
                <span className="text-xs text-white/50">Google AI (default)</span>
              </div>
            </button>
            <button
              role="radio"
              aria-checked={imageProvider === 'deapi'}
              onClick={() => onProviderChange('deapi')}
              className={cn(
                'px-4 py-3 rounded-lg text-sm font-medium transition-all text-start',
                imageProvider === 'deapi'
                  ? 'bg-purple-500/20 border-2 border-purple-500 text-purple-300'
                  : 'bg-white/5 border border-white/10 text-white/70 hover:bg-white/10'
              )}
            >
              <div className="flex flex-col items-start gap-1">
                <span className="font-semibold">DeAPI FLUX</span>
                <span className="text-xs text-white/50">Fast, high-quality</span>
              </div>
            </button>
          </div>
        </div>

        {/* Director Mode Selection */}
        {onDirectorModeChange && (
          <div className="bg-white/5 rounded-2xl p-6 border border-white/10">
            <label className="block text-sm font-medium text-white/80 mb-3">
              AI Director Mode
            </label>
            <div className="grid grid-cols-2 gap-2" role="radiogroup" aria-label="Director mode selection">
              <button
                role="radio"
                aria-checked={directorMode === 'chain'}
                onClick={() => onDirectorModeChange('chain')}
                className={cn(
                  'px-4 py-3 rounded-lg text-sm font-medium transition-all text-start',
                  directorMode === 'chain'
                    ? 'bg-cyan-500/20 border-2 border-cyan-500 text-cyan-300'
                    : 'bg-white/5 border border-white/10 text-white/70 hover:bg-white/10'
                )}
              >
                <div className="flex flex-col items-start gap-1">
                  <span className="font-semibold">Chain Mode</span>
                  <span className="text-xs text-white/50">Faster, predictable</span>
                </div>
              </button>
              <button
                role="radio"
                aria-checked={directorMode === 'agent'}
                onClick={() => onDirectorModeChange('agent')}
                className={cn(
                  'px-4 py-3 rounded-lg text-sm font-medium transition-all text-start',
                  directorMode === 'agent'
                    ? 'bg-amber-500/20 border-2 border-amber-500 text-amber-300'
                    : 'bg-white/5 border border-white/10 text-white/70 hover:bg-white/10'
                )}
              >
                <div className="flex flex-col items-start gap-1">
                  <span className="font-semibold">Agent Mode</span>
                  <span className="text-xs text-white/50">Smarter, self-correcting</span>
                </div>
              </button>
            </div>
          </div>
        )}

        {/* Global Subject Input */}
        {onGlobalSubjectChange && (
          <div className="bg-white/5 rounded-2xl p-6 border border-white/10">
            <label className="block text-sm font-medium text-white/80 mb-2">
              Global Subject (Optional)
            </label>
            <p className="text-xs text-white/50 mb-3">
              Describe a consistent character/subject to appear in all scenes (e.g., &quot;a bearded prophet in flowing white robes&quot;)
            </p>
            <input
              type="text"
              value={globalSubject}
              onChange={(e) => onGlobalSubjectChange(e.target.value)}
              placeholder="e.g., a young woman with dark curly hair in a crimson dress"
              className="w-full px-4 py-3 rounded-lg bg-white/5 border border-white/10 text-white placeholder-white/30 focus:outline-none focus:border-cyan-500/50 focus:ring-1 focus:ring-cyan-500/25"
            />
          </div>
        )}

        {/* Error Message */}
        {errorMsg && (
          <ErrorState variant="inline" message={errorMsg} />
        )}

        {/* Processing Status */}
        {isProcessing && (
          <div className="p-4 rounded-xl bg-cyan-500/10 border border-cyan-500/20">
            <div className={cn('flex items-center gap-3', isRTL && 'flex-row-reverse')}>
              <Loader2 className="w-5 h-5 text-cyan-400 animate-spin" aria-hidden="true" />
              <span className="text-cyan-200">{getProcessingMessage()}</span>
            </div>
          </div>
        )}

        {/* Start Button */}
        <Button
          onClick={onStartProcessing}
          disabled={!audioFile || isProcessing}
          size="lg"
          className={cn(
            'w-full h-14 text-lg font-semibold rounded-xl',
            'bg-gradient-to-r from-cyan-500 to-blue-600 hover:from-cyan-600 hover:to-blue-700',
            'disabled:opacity-50 disabled:cursor-not-allowed'
          )}
        >
          {isProcessing ? (
            <>
              <Loader2 className="w-5 h-5 me-2 animate-spin" aria-hidden="true" />
              {t('studio.processing')}
            </>
          ) : (
            <>
              <Sparkles className="w-5 h-5 me-2" aria-hidden="true" />
              {t('visualizer.generate')}
            </>
          )}
        </Button>
      </div>
    </div>
  );
}

export default AudioUploadForm;
````

## File: packages/frontend/components/visualizer/index.ts
````typescript
/**
 * Visualizer Components Index
 *
 * Exports all visualizer-related components for easy importing.
 */

export { AudioUploadForm } from './AudioUploadForm';
export type { AudioUploadFormProps } from './AudioUploadForm';

export { VisualPreview } from './VisualPreview';
export type { VisualPreviewProps } from './VisualPreview';

export { SceneThumbnails } from './SceneThumbnails';
export type { SceneThumbnailsProps } from './SceneThumbnails';
````

## File: packages/frontend/components/visualizer/SceneThumbnails.tsx
````typescript
/**
 * SceneThumbnails - Horizontal scrollable scene thumbnail strip
 *
 * Extracted from VisualizerScreen for better maintainability.
 */

import React from 'react';
import { Video, Image as ImageIcon, Loader2 } from 'lucide-react';
import { cn } from '@/lib/utils';
import type { ImagePrompt, GeneratedImage } from '@/types';

export interface SceneThumbnailsProps {
  /** List of prompts/scenes */
  prompts: ImagePrompt[];
  /** Generated images for each prompt */
  generatedImages: GeneratedImage[];
  /** Currently active scene index */
  currentSceneIndex: number;
  /** Callback when scene is selected */
  onSceneSelect: (index: number, timestampSeconds?: number) => void;
  /** ID of prompt currently being animated */
  animatingPromptId: string | null;
  /** Additional class names */
  className?: string;
}

/**
 * Horizontal scrollable strip of scene thumbnails
 */
export function SceneThumbnails({
  prompts,
  generatedImages,
  currentSceneIndex,
  onSceneSelect,
  animatingPromptId,
  className,
}: SceneThumbnailsProps) {
  return (
    <div
      className={cn('flex gap-2 overflow-x-auto pb-2 scrollbar-thin scrollbar-thumb-white/20', className)}
      role="tablist"
      aria-label="Scene thumbnails"
    >
      {prompts.map((prompt, idx) => {
        const image = generatedImages.find(img => img.promptId === prompt.id);
        const isActive = idx === currentSceneIndex;
        const isAnimating = animatingPromptId === prompt.id;
        const isVideo = image?.type === 'video';

        return (
          <div key={prompt.id} className="relative shrink-0">
            <button
              role="tab"
              aria-selected={isActive}
              aria-label={`Scene ${idx + 1}${image ? (isVideo ? ' (video)' : ' (image)') : ''}`}
              onClick={() => onSceneSelect(idx, prompt.timestampSeconds)}
              className={cn(
                'w-24 h-14 rounded-lg overflow-hidden border-2 transition-all',
                isActive
                  ? 'border-cyan-500 ring-2 ring-cyan-500/30'
                  : 'border-transparent opacity-60 hover:opacity-100'
              )}
            >
              {image?.imageUrl ? (
                isVideo ? (
                  <video
                    src={image.imageUrl}
                    className="w-full h-full object-cover"
                    muted
                    playsInline
                  />
                ) : (
                  <img
                    src={image.imageUrl}
                    alt={`Scene ${idx + 1}`}
                    className="w-full h-full object-cover"
                  />
                )
              ) : (
                <div className="w-full h-full bg-white/5 flex items-center justify-center text-xs text-white/30">
                  {idx + 1}
                </div>
              )}

              {/* Video/Image indicator badge */}
              {image && (
                <div className="absolute top-0.5 end-0.5">
                  <span className={cn(
                    'flex items-center justify-center w-4 h-4 rounded-full text-[8px]',
                    isVideo ? 'bg-purple-500/80' : 'bg-cyan-500/80'
                  )}>
                    {isVideo ? (
                      <Video className="w-2.5 h-2.5" aria-hidden="true" />
                    ) : (
                      <ImageIcon className="w-2.5 h-2.5" aria-hidden="true" />
                    )}
                  </span>
                </div>
              )}

              {/* Animating overlay */}
              {isAnimating && (
                <div className="absolute inset-0 bg-black/60 flex items-center justify-center">
                  <Loader2 className="w-4 h-4 text-purple-400 animate-spin" aria-hidden="true" />
                </div>
              )}
            </button>
          </div>
        );
      })}
    </div>
  );
}

export default SceneThumbnails;
````

## File: packages/frontend/components/visualizer/VisualPreview.tsx
````typescript
/**
 * VisualPreview - Main visual preview with play/pause and animation controls
 *
 * Extracted from VisualizerScreen for better maintainability.
 */

import React from 'react';
import { Play, Pause, Video, Image as ImageIcon, Loader2 } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { cn } from '@/lib/utils';
import type { GeneratedImage, SubtitleItem } from '@/types';

export interface VisualPreviewProps {
  /** Current visual being displayed */
  currentVisual: GeneratedImage | null;
  /** Current scene index */
  currentSceneIndex: number;
  /** Total number of scenes */
  totalScenes: number;
  /** Whether video is playing */
  isPlaying: boolean;
  /** Callback to toggle play/pause */
  onPlayPause: () => void;
  /** Current playback time in seconds */
  currentTime: number;
  /** Subtitles for overlay */
  subtitles: SubtitleItem[];
  /** ID of prompt currently being animated */
  animatingPromptId: string | null;
  /** Callback to animate current image */
  onAnimateImage: (promptId: string) => void;
  /** RTL layout */
  isRTL?: boolean;
  /** Additional class names */
  className?: string;
}

/**
 * Visual preview with play/pause, subtitle overlay, and animation controls
 */
export function VisualPreview({
  currentVisual,
  currentSceneIndex,
  totalScenes,
  isPlaying,
  onPlayPause,
  currentTime,
  subtitles,
  animatingPromptId,
  onAnimateImage,
  isRTL = false,
  className,
}: VisualPreviewProps) {
  // Find current subtitle
  const currentSubtitle = subtitles.find(
    s => currentTime >= s.startTime && currentTime <= s.endTime
  );

  const isVideo = currentVisual?.type === 'video';
  const canAnimate = !!(currentVisual && !isVideo);
  const isAnimating = !!(currentVisual && animatingPromptId === currentVisual.promptId);

  return (
    <div className={cn('relative aspect-video bg-black/40 rounded-2xl overflow-hidden border border-white/10', className)}>
      {/* Visual Content */}
      {currentVisual ? (
        isVideo ? (
          <video
            src={currentVisual.imageUrl}
            className="w-full h-full object-cover"
            autoPlay
            muted
            playsInline
            controls
          />
        ) : (
          <img
            src={currentVisual.imageUrl}
            alt={`Scene ${currentSceneIndex + 1}`}
            className="w-full h-full object-cover"
          />
        )
      ) : (
        <div className="w-full h-full flex items-center justify-center">
          <div className="text-center text-white/40">
            <ImageIcon className="w-12 h-12 mx-auto mb-2 opacity-50" aria-hidden="true" />
            <p>No visual for this scene</p>
          </div>
        </div>
      )}

      {/* Overlay gradient */}
      <div className="absolute inset-0 bg-gradient-to-t from-black/60 via-transparent to-black/20 pointer-events-none" />

      {/* Scene indicator */}
      <div className={cn('absolute top-4 z-20', isRTL ? 'right-4' : 'left-4')}>
        <span className="px-3 py-1 rounded-full bg-black/50 backdrop-blur text-xs text-white/80 border border-white/10 flex items-center gap-1">
          {isVideo ? (
            <Video className="w-3 h-3" aria-hidden="true" />
          ) : (
            <ImageIcon className="w-3 h-3" aria-hidden="true" />
          )}
          Scene {currentSceneIndex + 1} / {totalScenes}
        </span>
      </div>

      {/* Animate button for current scene */}
      {canAnimate && (
        <div className={cn('absolute top-4 z-20', isRTL ? 'left-4' : 'right-4')}>
          <Button
            size="sm"
            onClick={(e) => {
              e.stopPropagation();
              onAnimateImage(currentVisual.promptId);
            }}
            disabled={isAnimating}
            className="bg-purple-600/80 hover:bg-purple-500 backdrop-blur text-xs"
          >
            {isAnimating ? (
              <>
                <Loader2 className="w-3 h-3 me-1 animate-spin" aria-hidden="true" />
                Animating...
              </>
            ) : (
              <>
                <Video className="w-3 h-3 me-1" aria-hidden="true" />
                Animate
              </>
            )}
          </Button>
        </div>
      )}

      {/* Play/Pause overlay */}
      <button
        onClick={onPlayPause}
        className="absolute inset-0 z-10 flex items-center justify-center group"
        aria-label={isPlaying ? 'Pause' : 'Play'}
      >
        <div className={cn(
          'w-16 h-16 rounded-full bg-white/10 backdrop-blur-md border border-white/20 flex items-center justify-center transition-all',
          'group-hover:scale-110 group-hover:bg-white/20',
          isPlaying ? 'opacity-0 group-hover:opacity-100' : 'opacity-100'
        )}>
          {isPlaying ? (
            <Pause className="w-6 h-6 text-white" aria-hidden="true" />
          ) : (
            <Play className="w-6 h-6 text-white ms-1" aria-hidden="true" />
          )}
        </div>
      </button>

      {/* Current subtitle */}
      {currentSubtitle && (
        <div className={cn('absolute bottom-4 z-20', isRTL ? 'right-4 left-4 text-right' : 'left-4 right-4')}>
          <p className="text-lg font-medium text-white text-center px-4 py-2 bg-black/60 rounded-lg backdrop-blur-sm">
            {currentSubtitle.text}
          </p>
        </div>
      )}
    </div>
  );
}

export default VisualPreview;
````

## File: packages/frontend/hooks/useAuth.ts
````typescript
/**
 * useAuth Hook
 *
 * React hook for Firebase authentication state.
 * Provides current user, loading state, and auth methods.
 * Syncs auth state with the global app store.
 */
import { useState, useEffect, useCallback } from 'react';
import {
  onAuthChange,
  signInWithGoogle,
  signInWithEmail,
  createAccount,
  signOut,
  getCurrentUser,
  isAuthAvailable,
  handleRedirectResult,
  type AuthUser,
} from '@/services/firebase';
import { useAppStore } from '@/stores/appStore';

interface UseAuthReturn {
  user: AuthUser | null;
  isLoading: boolean;
  isAuthenticated: boolean;
  isAuthAvailable: boolean;
  error: string | null;
  signInWithGoogle: () => Promise<void>;
  signInWithEmail: (email: string, password: string) => Promise<void>;
  createAccount: (email: string, password: string) => Promise<void>;
  signOut: () => Promise<void>;
  clearError: () => void;
}

export function useAuth(): UseAuthReturn {
  const [user, setUser] = useState<AuthUser | null>(() => getCurrentUser());
  const [isLoading, setIsLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);

  // Get app store actions for syncing auth state
  const setCurrentUser = useAppStore((state) => state.setCurrentUser);
  const clearCurrentUser = useAppStore((state) => state.clearCurrentUser);

  // Helper to sync auth user to app store
  const syncUserToStore = useCallback((authUser: AuthUser | null) => {
    if (authUser) {
      setCurrentUser({
        uid: authUser.uid,
        email: authUser.email,
        displayName: authUser.displayName,
        photoURL: authUser.photoURL,
        isAuthenticated: true,
      });
    } else {
      clearCurrentUser();
    }
  }, [setCurrentUser, clearCurrentUser]);

  // Subscribe to auth state changes and check for redirect result
  useEffect(() => {
    let mounted = true;

    console.log('[useAuth] Starting auth initialization...');

    // Subscribe to auth state changes immediately (don't gate behind redirect check)
    const unsubscribe = onAuthChange((authUser) => {
      if (!mounted) return;
      console.log('[useAuth] Auth state changed:', authUser?.email || 'signed out');
      setUser(authUser);
      syncUserToStore(authUser);
      setIsLoading(false);
    });

    // If Firebase not configured, stop loading immediately
    if (!unsubscribe) {
      setIsLoading(false);
    }

    // Check for redirect result separately (from Google sign-in redirect)
    handleRedirectResult()
      .then((redirectUser) => {
        if (!mounted) return;
        if (redirectUser) {
          console.log('[useAuth] Got redirect user:', redirectUser.email);
          setUser(redirectUser);
          syncUserToStore(redirectUser);
        }
      })
      .catch((error) => {
        console.error('[useAuth] Redirect result error:', error);
      });

    return () => {
      mounted = false;
      unsubscribe?.();
    };
  }, [syncUserToStore]);

  const handleSignInWithGoogle = useCallback(async () => {
    setError(null);
    setIsLoading(true);
    try {
      await signInWithGoogle();
    } catch (e) {
      const err = e as { message?: string; code?: string };
      setError(getAuthErrorMessage(err.code));
    } finally {
      setIsLoading(false);
    }
  }, []);

  const handleSignInWithEmail = useCallback(
    async (email: string, password: string) => {
      setError(null);
      setIsLoading(true);
      try {
        await signInWithEmail(email, password);
      } catch (e) {
        const err = e as { message?: string; code?: string };
        setError(getAuthErrorMessage(err.code));
      } finally {
        setIsLoading(false);
      }
    },
    []
  );

  const handleCreateAccount = useCallback(
    async (email: string, password: string) => {
      setError(null);
      setIsLoading(true);
      try {
        await createAccount(email, password);
      } catch (e) {
        const err = e as { message?: string; code?: string };
        setError(getAuthErrorMessage(err.code));
      } finally {
        setIsLoading(false);
      }
    },
    []
  );

  const handleSignOut = useCallback(async () => {
    setError(null);
    try {
      await signOut();
    } catch (e) {
      const err = e as { message?: string };
      setError(err.message || 'Sign out failed');
    }
  }, []);

  const clearError = useCallback(() => {
    setError(null);
  }, []);

  return {
    user,
    isLoading,
    isAuthenticated: user !== null,
    isAuthAvailable: isAuthAvailable(),
    error,
    signInWithGoogle: handleSignInWithGoogle,
    signInWithEmail: handleSignInWithEmail,
    createAccount: handleCreateAccount,
    signOut: handleSignOut,
    clearError,
  };
}

/**
 * Convert Firebase auth error codes to user-friendly messages
 */
function getAuthErrorMessage(code?: string): string {
  switch (code) {
    case 'auth/email-already-in-use':
      return 'This email is already registered. Try signing in instead.';
    case 'auth/invalid-email':
      return 'Please enter a valid email address.';
    case 'auth/operation-not-allowed':
      return 'This sign-in method is not enabled.';
    case 'auth/weak-password':
      return 'Password should be at least 6 characters.';
    case 'auth/user-disabled':
      return 'This account has been disabled.';
    case 'auth/user-not-found':
    case 'auth/wrong-password':
    case 'auth/invalid-credential':
      return 'Invalid email or password.';
    case 'auth/too-many-requests':
      return 'Too many attempts. Please try again later.';
    case 'auth/network-request-failed':
      return 'Network error. Check your connection.';
    case 'auth/popup-closed-by-user':
      return ''; // User cancelled, no error message needed
    default:
      return code ? `Authentication error: ${code}` : 'An error occurred.';
  }
}

export default useAuth;
````

## File: packages/frontend/hooks/useFocusTrap.ts
````typescript
/**
 * useFocusTrap Hook
 * Requirements: 9.4 - Trap focus in modals
 * 
 * This hook provides focus trapping functionality for modals and dialogs.
 * It ensures that keyboard focus stays within the modal when it's open.
 */

import { useEffect, useRef, useCallback } from 'react';

interface UseFocusTrapOptions {
  /** Whether the focus trap is active */
  isActive: boolean;
  /** Element to return focus to when trap is deactivated */
  returnFocusOnDeactivate?: boolean;
  /** Initial element to focus when trap is activated */
  initialFocusRef?: React.RefObject<HTMLElement>;
  /** Callback when escape key is pressed */
  onEscape?: () => void;
}

/**
 * Get all focusable elements within a container
 */
function getFocusableElements(container: HTMLElement): HTMLElement[] {
  const focusableSelectors = [
    'a[href]',
    'button:not([disabled])',
    'textarea:not([disabled])',
    'input:not([disabled])',
    'select:not([disabled])',
    '[tabindex]:not([tabindex="-1"])',
    '[contenteditable="true"]',
  ].join(', ');

  const elements = Array.from(
    container.querySelectorAll<HTMLElement>(focusableSelectors)
  );

  // Filter out elements that are not visible
  return elements.filter((el) => {
    const style = window.getComputedStyle(el);
    return (
      style.display !== 'none' &&
      style.visibility !== 'hidden' &&
      el.offsetParent !== null
    );
  });
}

/**
 * Hook to trap focus within a container element
 * 
 * @example
 * ```tsx
 * function Modal({ isOpen, onClose }) {
 *   const containerRef = useFocusTrap({
 *     isActive: isOpen,
 *     onEscape: onClose,
 *   });
 * 
 *   return (
 *     <div ref={containerRef} role="dialog" aria-modal="true">
 *       <button>First focusable</button>
 *       <button>Last focusable</button>
 *     </div>
 *   );
 * }
 * ```
 */
export function useFocusTrap<T extends HTMLElement = HTMLDivElement>({
  isActive,
  returnFocusOnDeactivate = true,
  initialFocusRef,
  onEscape,
}: UseFocusTrapOptions) {
  const containerRef = useRef<T>(null);
  const previousActiveElement = useRef<HTMLElement | null>(null);

  // Handle keyboard events for focus trapping
  const handleKeyDown = useCallback(
    (event: KeyboardEvent) => {
      if (!containerRef.current || !isActive) return;

      // Handle Escape key
      if (event.key === 'Escape' && onEscape) {
        event.preventDefault();
        onEscape();
        return;
      }

      // Handle Tab key for focus trapping
      if (event.key === 'Tab') {
        const focusableElements = getFocusableElements(containerRef.current);
        
        if (focusableElements.length === 0) {
          event.preventDefault();
          return;
        }

        const firstElement = focusableElements[0];
        const lastElement = focusableElements[focusableElements.length - 1];

        if (!firstElement || !lastElement) return;

        const activeElement = document.activeElement as HTMLElement;

        // Shift + Tab: Move focus backwards
        if (event.shiftKey) {
          if (activeElement === firstElement || !containerRef.current.contains(activeElement)) {
            event.preventDefault();
            lastElement.focus();
          }
        } 
        // Tab: Move focus forwards
        else {
          if (activeElement === lastElement || !containerRef.current.contains(activeElement)) {
            event.preventDefault();
            firstElement.focus();
          }
        }
      }
    },
    [isActive, onEscape]
  );

  // Set up focus trap when activated
  useEffect(() => {
    if (!isActive) return;

    // Store the currently focused element to restore later
    previousActiveElement.current = document.activeElement as HTMLElement;

    // Focus the initial element or the first focusable element
    const focusInitialElement = () => {
      if (initialFocusRef?.current) {
        initialFocusRef.current.focus();
      } else if (containerRef.current) {
        const focusableElements = getFocusableElements(containerRef.current);
        const firstFocusable = focusableElements[0];
        if (firstFocusable) {
          firstFocusable.focus();
        } else {
          // If no focusable elements, focus the container itself
          containerRef.current.setAttribute('tabindex', '-1');
          containerRef.current.focus();
        }
      }
    };

    // Small delay to ensure the modal is rendered
    const timeoutId = setTimeout(focusInitialElement, 10);

    // Add keyboard event listener
    document.addEventListener('keydown', handleKeyDown);

    return () => {
      clearTimeout(timeoutId);
      document.removeEventListener('keydown', handleKeyDown);

      // Return focus to the previously focused element
      if (returnFocusOnDeactivate && previousActiveElement.current) {
        previousActiveElement.current.focus();
      }
    };
  }, [isActive, handleKeyDown, initialFocusRef, returnFocusOnDeactivate]);

  return containerRef;
}

export default useFocusTrap;
````

## File: packages/frontend/hooks/useFormatPipeline.ts
````typescript
/**
 * useFormatPipeline Hook
 *
 * Orchestration hook bridging UI ↔ format-specific pipeline services.
 * Manages format selection, genre, idea, reference documents, pipeline execution,
 * checkpoint approval/rejection, and cancellation.
 *
 * movie-animation is excluded — it delegates to the existing useStoryGeneration hook.
 */

import { useState, useCallback, useRef } from 'react';
import type { VideoFormat, CheckpointState } from '@/types';
import type { IndexedDocument } from '@/services/documentParser';
import type { PipelineCallbacks, PipelineResult } from '@/services/formatRouter';
import { formatRouter } from '@/services/formatRouter';
import { formatRegistry } from '@/services/formatRegistry';
import type { CheckpointSystem } from '@/services/checkpointSystem';
import type { ExecutionProgress } from '@/services/parallelExecutionEngine';
import type { PipelineTask } from '@/components/PipelineProgress';

// Pipeline class imports (lazy-registered before each execute)
import { YouTubeNarratorPipeline } from '@/services/pipelines/youtubeNarrator';
import { AdvertisementPipeline } from '@/services/pipelines/advertisement';
import { EducationalPipeline } from '@/services/pipelines/educational';
import { ShortsPipeline } from '@/services/pipelines/shorts';
import { DocumentaryPipeline } from '@/services/pipelines/documentary';
import { MusicVideoPipeline } from '@/services/pipelines/musicVideo';
import { NewsPoliticsPipeline } from '@/services/pipelines/newsPolitics';

/**
 * Build a static task list from format metadata for progress display.
 */
function buildTaskList(formatId: VideoFormat): PipelineTask[] {
  const meta = formatRegistry.getFormat(formatId);
  if (!meta) return [];

  const tasks: PipelineTask[] = [];

  if (meta.requiresResearch) {
    tasks.push({ id: 'research', name: 'Research & Sources', type: 'research', status: 'queued' });
  }

  tasks.push({ id: 'script', name: 'Script Generation', type: 'script', status: 'queued' });
  tasks.push({ id: 'visual', name: 'Visual Generation', type: 'visual', status: 'queued' });
  tasks.push({ id: 'audio', name: 'Audio / Narration', type: 'audio', status: 'queued' });
  tasks.push({ id: 'assembly', name: 'Final Assembly', type: 'assembly', status: 'queued' });

  return tasks;
}

export interface UseFormatPipelineReturn {
  // Selection state
  selectedFormat: VideoFormat | null;
  selectedGenre: string | null;
  idea: string;
  referenceDocuments: IndexedDocument[];
  setFormat: (format: VideoFormat) => void;
  setGenre: (genre: string) => void;
  setIdea: (idea: string) => void;
  setReferenceDocuments: (docs: IndexedDocument[]) => void;

  // Execution state
  isRunning: boolean;
  isCancelling: boolean;
  currentPhase: string;
  executionProgress: ExecutionProgress | null;
  tasks: PipelineTask[];
  result: PipelineResult | null;
  error: string | null;

  // Checkpoint state
  activeCheckpoint: CheckpointState | null;

  // Actions
  execute: (userId: string, projectId: string) => Promise<void>;
  cancel: () => void;
  approveCheckpoint: () => void;
  rejectCheckpoint: (changeRequest?: string) => void;
  reset: () => void;
}

export function useFormatPipeline(): UseFormatPipelineReturn {
  // Selection state
  const [selectedFormat, setSelectedFormat] = useState<VideoFormat | null>(null);
  const [selectedGenre, setSelectedGenre] = useState<string | null>(null);
  const [idea, setIdea] = useState('');
  const [referenceDocuments, setReferenceDocuments] = useState<IndexedDocument[]>([]);

  // Execution state
  const [isRunning, setIsRunning] = useState(false);
  const [isCancelling, setIsCancelling] = useState(false);
  const [currentPhase, setCurrentPhase] = useState('');
  const [executionProgress, setExecutionProgress] = useState<ExecutionProgress | null>(null);
  const [tasks, setTasks] = useState<PipelineTask[]>([]);
  const [result, setResult] = useState<PipelineResult | null>(null);
  const [error, setError] = useState<string | null>(null);

  // Checkpoint state
  const [activeCheckpoint, setActiveCheckpoint] = useState<CheckpointState | null>(null);

  // Refs for bridging callbacks to React state
  const checkpointSystemRef = useRef<CheckpointSystem | null>(null);
  const cancelFnRef = useRef<(() => void) | null>(null);

  const setFormat = useCallback((format: VideoFormat) => {
    setSelectedFormat(format);
    setSelectedGenre(null); // Reset genre when format changes
    setResult(null);
    setError(null);
  }, []);

  const setGenre = useCallback((genre: string) => {
    setSelectedGenre(genre);
  }, []);

  /**
   * Register all 7 non-movie pipelines on the format router with fresh callback closures.
   * Called before each execute() to avoid stale React closures.
   */
  const registerPipelines = useCallback((callbacks: PipelineCallbacks) => {
    // We don't pass callbacks to constructors — callbacks are passed via execute()
    formatRouter.registerPipeline('youtube-narrator', new YouTubeNarratorPipeline());
    formatRouter.registerPipeline('advertisement', new AdvertisementPipeline());
    formatRouter.registerPipeline('educational', new EducationalPipeline());
    formatRouter.registerPipeline('shorts', new ShortsPipeline());
    formatRouter.registerPipeline('documentary', new DocumentaryPipeline());
    formatRouter.registerPipeline('music-video', new MusicVideoPipeline());
    formatRouter.registerPipeline('news-politics', new NewsPoliticsPipeline());
  }, []);

  /**
   * Update task statuses based on checkpoint phase.
   */
  const updateTaskFromPhase = useCallback((phase: string) => {
    setTasks(prev => {
      const updated = [...prev];
      // Simple heuristic: map checkpoint phases to task types
      if (phase.includes('research')) {
        const task = updated.find(t => t.id === 'research');
        if (task) task.status = 'completed';
        const scriptTask = updated.find(t => t.id === 'script');
        if (scriptTask && scriptTask.status === 'queued') scriptTask.status = 'in-progress';
      } else if (phase.includes('script') || phase.includes('cta')) {
        const researchTask = updated.find(t => t.id === 'research');
        if (researchTask && researchTask.status === 'queued') researchTask.status = 'completed';
        const scriptTask = updated.find(t => t.id === 'script');
        if (scriptTask) scriptTask.status = 'completed';
        const visualTask = updated.find(t => t.id === 'visual');
        if (visualTask && visualTask.status === 'queued') visualTask.status = 'in-progress';
      } else if (phase.includes('visual') || phase.includes('preview')) {
        const researchTask = updated.find(t => t.id === 'research');
        if (researchTask) researchTask.status = 'completed';
        const scriptTask = updated.find(t => t.id === 'script');
        if (scriptTask) scriptTask.status = 'completed';
        const visualTask = updated.find(t => t.id === 'visual');
        if (visualTask) visualTask.status = 'completed';
        const audioTask = updated.find(t => t.id === 'audio');
        if (audioTask && audioTask.status === 'queued') audioTask.status = 'in-progress';
      } else if (phase.includes('assembly') || phase.includes('final')) {
        for (const t of updated) {
          if (t.id !== 'assembly') t.status = 'completed';
        }
        const assemblyTask = updated.find(t => t.id === 'assembly');
        if (assemblyTask) assemblyTask.status = 'in-progress';
      }
      return updated;
    });
  }, []);

  const execute = useCallback(async (userId: string, projectId: string) => {
    if (!selectedFormat || selectedFormat === 'movie-animation') return;
    if (isRunning) return;

    setIsRunning(true);
    setIsCancelling(false);
    setError(null);
    setResult(null);
    setActiveCheckpoint(null);
    setCurrentPhase('Initializing...');

    // Build static task list from format metadata
    const taskList = buildTaskList(selectedFormat);
    setTasks(taskList);

    // Set first task as in-progress
    if (taskList.length > 0) {
      taskList[0]!.status = 'in-progress';
      setTasks([...taskList]);
    }

    // Build callbacks with fresh closures
    const callbacks: PipelineCallbacks = {
      onCheckpointCreated: (checkpoint: CheckpointState) => {
        setActiveCheckpoint(checkpoint);
        setCurrentPhase(`Checkpoint: ${checkpoint.phase}`);
        updateTaskFromPhase(checkpoint.phase);
      },
      onCheckpointSystemCreated: (system: CheckpointSystem) => {
        checkpointSystemRef.current = system;
      },
      onProgress: (progress: ExecutionProgress) => {
        setExecutionProgress(progress);
      },
      onCancelRequested: (cancelFn: () => void) => {
        cancelFnRef.current = cancelFn;
      },
    };

    // Register fresh pipeline instances
    registerPipelines(callbacks);

    try {
      const pipelineResult = await formatRouter.dispatch(
        {
          formatId: selectedFormat,
          idea,
          genre: selectedGenre ?? undefined,
          language: 'en', // TODO: detect from idea
          referenceDocuments: referenceDocuments.length > 0 ? referenceDocuments : undefined,
          userId,
          projectId,
        },
        callbacks,
      );

      setResult(pipelineResult);

      if (pipelineResult.success) {
        // Mark all tasks as completed
        setTasks(prev => prev.map(t => ({ ...t, status: 'completed' as const })));
        setCurrentPhase('Complete');
      } else {
        setError(pipelineResult.error ?? 'Pipeline failed');
        setCurrentPhase('Failed');
      }
    } catch (err) {
      const msg = err instanceof Error ? err.message : String(err);
      setError(msg);
      setCurrentPhase('Failed');
    } finally {
      setIsRunning(false);
      checkpointSystemRef.current = null;
      cancelFnRef.current = null;
    }
  }, [selectedFormat, selectedGenre, idea, referenceDocuments, isRunning, registerPipelines, updateTaskFromPhase]);

  const cancel = useCallback(() => {
    if (!isRunning) return;
    setIsCancelling(true);
    cancelFnRef.current?.();
    // Mark remaining tasks as cancelled
    setTasks(prev => prev.map(t =>
      t.status === 'queued' || t.status === 'in-progress'
        ? { ...t, status: 'cancelled' as const }
        : t
    ));
  }, [isRunning]);

  const approveCheckpoint = useCallback(() => {
    if (!activeCheckpoint || !checkpointSystemRef.current) return;
    checkpointSystemRef.current.approveCheckpoint(activeCheckpoint.checkpointId);
    setActiveCheckpoint(null);
  }, [activeCheckpoint]);

  const rejectCheckpoint = useCallback((changeRequest?: string) => {
    if (!activeCheckpoint || !checkpointSystemRef.current) return;
    checkpointSystemRef.current.rejectCheckpoint(activeCheckpoint.checkpointId, changeRequest);
    setActiveCheckpoint(null);
  }, [activeCheckpoint]);

  const reset = useCallback(() => {
    setSelectedFormat(null);
    setSelectedGenre(null);
    setIdea('');
    setReferenceDocuments([]);
    setIsRunning(false);
    setIsCancelling(false);
    setCurrentPhase('');
    setExecutionProgress(null);
    setTasks([]);
    setResult(null);
    setError(null);
    setActiveCheckpoint(null);
    checkpointSystemRef.current = null;
    cancelFnRef.current = null;
  }, []);

  return {
    selectedFormat,
    selectedGenre,
    idea,
    referenceDocuments,
    setFormat,
    setGenre,
    setIdea,
    setReferenceDocuments,

    isRunning,
    isCancelling,
    currentPhase,
    executionProgress,
    tasks,
    result,
    error,

    activeCheckpoint,

    execute,
    cancel,
    approveCheckpoint,
    rejectCheckpoint,
    reset,
  };
}
````

## File: packages/frontend/hooks/useLyricLens.ts
````typescript
import { useState } from "react";
import { AppState, SongData, GeneratedImage, AssetType, ImagePrompt, SubtitleItem } from "@/types";
import {
  transcribeAudioWithWordTiming,
  fileToGenerativePart,
  inferAudioMimeType,
  generateImageFromPrompt,
  generateVideoFromPrompt,
  refineImagePrompt,
  translateSubtitles,
  generateMotionPrompt,
  VideoPurpose,
} from "@/services/geminiService";
import { generatePromptsWithLangChain } from "@/services/directorService";
import { generatePromptsWithAgent } from "@/services/agentDirectorService";
import {
  animateImageWithDeApi,
  generateImageWithAspectRatio,
  generateImageBatch,
  BatchGenerationItem,
} from "@/services/deapiService";
import { subtitlesToSRT } from "@/utils/srtParser";
import { calculateOptimalAssets } from "@/services/assetCalculatorService";

export function useLyricLens() {
  const [appState, setAppState] = useState<AppState>(AppState.IDLE);
  const [songData, setSongData] = useState<SongData | null>(null);
  const [errorMsg, setErrorMsg] = useState<string | null>(null);
  const [isBulkGenerating, setIsBulkGenerating] = useState(false);
  const [contentType, setContentType] = useState<"music" | "story">("music");
  const [globalSubject, setGlobalSubject] = useState("");
  const [aspectRatio, setAspectRatio] = useState("16:9");
  const [videoPurpose, setVideoPurpose] = useState<VideoPurpose>("music_video");
  const [generationMode, setGenerationMode] = useState<"image" | "video">(
    "image",
  );
  const [videoProvider, setVideoProvider] = useState<"veo" | "deapi">("veo");
  const [imageProvider, setImageProvider] = useState<"gemini" | "deapi">("gemini");
  const [directorMode, setDirectorMode] = useState<"chain" | "agent">("chain");
  const [pendingFile, setPendingFile] = useState<File | null>(null);

  // Translation State
  const [isTranslating, setIsTranslating] = useState(false);

  // Store file for processing (kept for backward compatibility)
  const handleFileSelect = (file: File) => {
    setErrorMsg(null);
    setPendingFile(file);
    // Don't change state - let caller decide when to process
  };

  /**
   * Process a file directly without relying on pendingFile state.
   * This eliminates the race condition where state may not be updated
   * before processing begins.
   * 
   * @param file - The audio file to process
   * @param selectedStyle - The style to use for prompt generation
   */
  const processFile = async (file: File, selectedStyle: string) => {
    // Also update pendingFile for backward compatibility
    setPendingFile(file);

    try {
      setErrorMsg(null);
      setAppState(AppState.PROCESSING_AUDIO);

      // 1. Setup local preview
      const audioUrl = URL.createObjectURL(file);
      const partialData: SongData = {
        fileName: file.name,
        audioUrl,
        srtContent: "",
        parsedSubtitles: [],
        prompts: [],
        generatedImages: [],
      };
      setSongData(partialData);

      // 2. Convert to Base64
      const base64Audio = await fileToGenerativePart(file);

      // 3. Transcribe with word-level timing
      setAppState(AppState.TRANSCRIBING);
      const parsedSubs = await transcribeAudioWithWordTiming(
        base64Audio,
        inferAudioMimeType(file.name, file.type),
      );
      // Generate SRT string for backward compat (downloads, prompts)
      const srt = subtitlesToSRT(parsedSubs);

      partialData.srtContent = srt;
      partialData.parsedSubtitles = parsedSubs;
      setSongData({ ...partialData });

      // 4. Calculate optimal number of assets
      // Note: Analysis is now done inside generatePromptsWithLangChain/Agent
      // to avoid duplicate API calls. calculateOptimalAssets only uses duration.
      setAppState(AppState.ANALYZING_LYRICS);

      // Get audio duration from the audio buffer
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const arrayBuffer = await file.arrayBuffer();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      const audioDuration = audioBuffer.duration;

      // Calculate optimal asset count based on duration
      // Note: analysisOutput is not used by the current implementation
      const assetCalc = await calculateOptimalAssets({
        audioDuration,
        analysisOutput: {
          sections: [],
          emotionalArc: { opening: "", peak: "", resolution: "" },
          themes: [],
          motifs: [],
          visualScenes: [],
        },
        videoPurpose,
        contentType: contentType === "story" ? "story" : "lyrics",
      });

      console.log(`[useLyricLens] Dynamic asset calculation: ${assetCalc.optimalAssetCount} assets recommended`);
      console.log(`[useLyricLens] Reasoning: ${assetCalc.reasoning}`);

      // 6. Generate Prompts using selected Director mode with calculated asset count
      setAppState(AppState.GENERATING_PROMPTS);
      // "chain" = LangChain LCEL pipeline (faster, deterministic)
      // "agent" = LangChain Agent with tools (smarter, self-improving)
      let prompts;
      if (directorMode === "agent") {
        console.log("[useLyricLens] Using Agent Director mode");
        prompts = await generatePromptsWithAgent(
          srt,
          selectedStyle,
          contentType === "story" ? "story" : "lyrics",
          videoPurpose,
          globalSubject,
          { targetAssetCount: assetCalc.optimalAssetCount }
        );
      } else {
        console.log("[useLyricLens] Using Chain Director mode");
        prompts = await generatePromptsWithLangChain(
          srt,
          selectedStyle,
          contentType === "story" ? "story" : "lyrics",
          videoPurpose,
          globalSubject,
          { targetAssetCount: assetCalc.optimalAssetCount }
        );
      }

      partialData.prompts = prompts;
      setSongData({ ...partialData });

      setAppState(AppState.READY);
    } catch (e: any) {
      console.error(e);
      setErrorMsg(e.message || "An unexpected error occurred.");
      setAppState(AppState.ERROR);
    }
  };

  /**
   * Legacy function that uses pendingFile state.
   * @deprecated Use processFile instead to avoid race conditions.
   */
  const startProcessing = async (selectedStyle: string) => {
    if (!pendingFile) return;
    await processFile(pendingFile, selectedStyle);
  };

  const handleImageGenerated = (newImg: GeneratedImage) => {
    if (!songData) return;
    setSongData((prev: SongData | null) => {
      if (!prev) return null;
      // Remove existing if replacing
      const filtered = prev.generatedImages.filter(
        (img: GeneratedImage) => img.promptId !== newImg.promptId,
      );

      return {
        ...prev,
        generatedImages: [...filtered, newImg],
      };
    });
  };

  const handleGenerateAll = async (
    selectedStyle: string,
    selectedAspectRatio: "16:9" | "9:16",
  ) => {
    if (!songData || isBulkGenerating) return;
    setIsBulkGenerating(true);

    const pendingPrompts = songData.prompts.filter(
      (p: ImagePrompt) => !songData.generatedImages.some((img: GeneratedImage) => img.promptId === p.id),
    );

    // Track refined prompts for cross-scene deduplication
    const refinedPromptTexts: string[] = [];

    // Collect all existing prompt texts (from already-generated scenes) as initial context
    const existingPromptTexts = songData.prompts
      .filter((p: ImagePrompt) =>
        songData.generatedImages.some((img: GeneratedImage) => img.promptId === p.id),
      )
      .map((p: ImagePrompt) => p.text);

    refinedPromptTexts.push(...existingPromptTexts);

    // Determine if we can use parallel batch generation (DeAPI image-only mode)
    const canUseBatchGeneration = imageProvider === "deapi" && generationMode === "image";

    if (canUseBatchGeneration) {
      // ============================================================
      // PARALLEL BATCH GENERATION (DeAPI)
      // Dramatically faster - runs multiple requests concurrently
      // ============================================================
      console.log(`[useLyricLens] Using parallel batch generation for ${pendingPrompts.length} assets`);

      // First, refine all prompts (this is fast and can be done serially)
      const refinedPrompts: Array<{ prompt: ImagePrompt; refinedText: string }> = [];
      for (const prompt of pendingPrompts) {
        try {
          const { refinedPrompt } = await refineImagePrompt({
            promptText: prompt.text,
            style: selectedStyle,
            globalSubject,
            aspectRatio: selectedAspectRatio,
            intent: "auto",
            previousPrompts: refinedPromptTexts,
          });
          refinedPromptTexts.push(refinedPrompt);
          refinedPrompts.push({ prompt, refinedText: refinedPrompt });
        } catch (e) {
          console.error(`Failed to refine prompt ${prompt.id}`, e);
          // Use original prompt if refinement fails
          refinedPrompts.push({ prompt, refinedText: prompt.text });
        }
      }

      // Prepare batch items for parallel generation
      const batchItems: BatchGenerationItem[] = refinedPrompts.map(({ prompt, refinedText }) => ({
        id: prompt.id,
        prompt: refinedText,
        aspectRatio: selectedAspectRatio as "16:9" | "9:16" | "1:1",
        model: "Flux1schnell" as const,
        negativePrompt: "blur, darkness, noise, low quality, text, watermark, logo",
      }));

      // Run parallel batch generation with concurrency limit of 5
      const batchResults = await generateImageBatch(
        batchItems,
        5, // Concurrency limit - adjust based on API rate limits
        (progress) => {
          console.log(`[useLyricLens] Batch progress: ${progress.completed}/${progress.total}`);
        }
      );

      // Process results and update state
      for (const result of batchResults) {
        if (result.success && result.imageUrl) {
          handleImageGenerated({
            promptId: result.id,
            imageUrl: result.imageUrl,
            type: "image",
          });
        } else {
          console.error(`Failed to generate image for prompt ${result.id}:`, result.error);
        }
      }
    } else {
      // ============================================================
      // SEQUENTIAL GENERATION (Gemini, Video, or mixed modes)
      // Used when parallel generation isn't suitable
      // ============================================================
      console.log(`[useLyricLens] Using sequential generation for ${pendingPrompts.length} assets`);

      for (const prompt of pendingPrompts) {
        try {
          // First, refine the prompt with cross-scene awareness
          const { refinedPrompt } = await refineImagePrompt({
            promptText: prompt.text,
            style: selectedStyle,
            globalSubject,
            aspectRatio: selectedAspectRatio,
            intent: "auto",
            previousPrompts: refinedPromptTexts,
          });

          // Track the refined prompt for subsequent scenes
          refinedPromptTexts.push(refinedPrompt);

          // Determine asset type: use per-card setting if available, otherwise fall back to global
          const getAssetTypeForPrompt = (): AssetType => {
            if (prompt.assetType) return prompt.assetType;
            if (generationMode === "video") {
              return videoProvider === "deapi" ? "video_with_image" : "video";
            }
            return "image";
          };

          const assetType = getAssetTypeForPrompt();
          let base64: string;
          let baseImageUrl: string | undefined;
          let resultType: "image" | "video" = "image";

          if (assetType === "video") {
            // Direct video generation (Veo)
            base64 = await generateVideoFromPrompt(
              refinedPrompt,
              selectedStyle,
              globalSubject,
              selectedAspectRatio,
            );
            resultType = "video";
          } else if (assetType === "video_with_image") {
            // Two-step: Image first, then animate (DeAPI)
            // 1. Generate Image (Gemini)
            const imgBase64 = await generateImageFromPrompt(
              refinedPrompt,
              selectedStyle,
              globalSubject,
              selectedAspectRatio,
              true,
            );
            baseImageUrl = imgBase64;

            // 2. Generate motion-optimized prompt for animation
            const motionResult = await generateMotionPrompt(
              refinedPrompt,
              prompt.mood || "cinematic",
              globalSubject,
            );

            // 3. Animate with motion-focused prompt (DeAPI)
            base64 = await animateImageWithDeApi(
              imgBase64,
              motionResult.combined,
              selectedAspectRatio as "16:9" | "9:16" | "1:1",
            );
            resultType = "video";
          } else {
            // Standard Image Generation
            // Choose provider: Gemini (default) or DeAPI
            if (imageProvider === "deapi") {
              // Use DeAPI for image generation (FLUX.1-schnell or Z-Image-Turbo)
              base64 = await generateImageWithAspectRatio(
                refinedPrompt,
                selectedAspectRatio as "16:9" | "9:16" | "1:1",
                "Flux1schnell", // Fast, high-quality model
              );
            } else {
              // Use Gemini Imagen (default)
              base64 = await generateImageFromPrompt(
                refinedPrompt,
                selectedStyle,
                globalSubject,
                selectedAspectRatio,
                true, // skipRefine - already refined above with previousPrompts
              );
            }
            resultType = "image";
          }

          handleImageGenerated({
            promptId: prompt.id,
            imageUrl: base64,
            type: resultType,
            baseImageUrl,
          });
        } catch (e) {
          console.error(`Failed to generate asset for prompt ${prompt.id}`, e);
        }
      }
    }

    setIsBulkGenerating(false);
  };

  const handleTranslate = async (targetLang: string) => {
    if (!songData || isTranslating) return;
    setIsTranslating(true);
    try {
      const translations = await translateSubtitles(
        songData.parsedSubtitles,
        targetLang,
      );

      // Merge translations
      const updatedSubs = songData.parsedSubtitles.map((sub: SubtitleItem) => {
        const trans = translations.find((t: { id: number; translation: string }) => t.id === sub.id);
        return trans ? { ...sub, translation: trans.translation } : sub;
      });

      setSongData((prev: SongData | null) =>
        prev ? { ...prev, parsedSubtitles: updatedSubs } : null,
      );
    } catch (e) {
      console.error("Translation failed", e);
      alert("Translation failed. Please try again.");
    } finally {
      setIsTranslating(false);
    }
  };

  const loadTestData = async () => {
    try {
      setErrorMsg(null);
      setAppState(AppState.PROCESSING_AUDIO);

      // Use browser-compatible test data
      const { createTestSongData } = await import("@/utils/testData");
      const testData = createTestSongData();

      setSongData(testData);
      setAppState(AppState.READY);

      console.log("✅ Test data loaded successfully");
    } catch (e: any) {
      console.error("Failed to load test data:", e);
      setErrorMsg("Failed to load test data: " + e.message);
      setAppState(AppState.ERROR);
    }
  };

  const resetApp = () => {
    setAppState(AppState.IDLE);
    setSongData(null);
    setPendingFile(null);
    setErrorMsg(null);
    setIsBulkGenerating(false);
    setIsTranslating(false);
    setGlobalSubject("");
    setAspectRatio("16:9");
    setGenerationMode("image");
    setVideoProvider("veo");
    setVideoPurpose("music_video");
  };

  return {
    appState,
    songData,
    setSongData,
    errorMsg,
    isBulkGenerating,
    contentType,
    isTranslating,
    globalSubject,
    aspectRatio,
    generationMode,
    videoPurpose,
    setGenerationMode,
    videoProvider,
    setVideoProvider,
    imageProvider,
    setImageProvider,
    // Expose director mode for UI toggle (chain vs agent)
    directorMode,
    setDirectorMode,
    setAspectRatio,
    setGlobalSubject,
    setContentType,
    setVideoPurpose,
    handleFileSelect,
    startProcessing,
    processFile,
    // pendingFile removed from exports - use processFile instead
    handleImageGenerated,
    handleGenerateAll,
    handleTranslate,
    loadTestData,
    resetApp,
  };
}
````

## File: packages/frontend/hooks/useMediaPlayback.ts
````typescript
/**
 * useMediaPlayback - Shared media playback state and controls
 *
 * Provides unified playback controls for audio/video across components.
 * Used by StudioScreen, VisualizerScreen, and timeline components.
 */

import { useState, useCallback, useRef, useEffect, useMemo } from 'react';

export interface PlaybackState {
  isPlaying: boolean;
  currentTime: number;
  duration: number;
  volume: number;
  isMuted: boolean;
  playbackRate: number;
}

export interface UseMediaPlaybackOptions {
  /** Initial volume (0-1) */
  initialVolume?: number;
  /** Auto-play on load */
  autoPlay?: boolean;
  /** Loop playback */
  loop?: boolean;
  /** Callback when playback ends */
  onEnded?: () => void;
  /** Callback when time updates */
  onTimeUpdate?: (time: number) => void;
}

export interface UseMediaPlaybackReturn {
  // State
  isPlaying: boolean;
  currentTime: number;
  duration: number;
  volume: number;
  isMuted: boolean;
  playbackRate: number;

  // Ref to attach to audio/video element
  mediaRef: React.RefObject<HTMLAudioElement | HTMLVideoElement>;

  // Controls
  play: () => void;
  pause: () => void;
  togglePlayPause: () => void;
  seek: (time: number) => void;
  seekRelative: (delta: number) => void;
  setVolume: (volume: number) => void;
  toggleMute: () => void;
  setPlaybackRate: (rate: number) => void;
  reset: () => void;

  // Utilities
  formatTime: (seconds: number) => string;
  getProgress: () => number;
}

/**
 * Hook for managing audio/video playback state
 *
 * @example
 * ```tsx
 * const {
 *   mediaRef,
 *   isPlaying,
 *   togglePlayPause,
 *   seek,
 *   formatTime
 * } = useMediaPlayback({ onEnded: () => console.log('done') });
 *
 * return (
 *   <>
 *     <audio ref={mediaRef} src={audioUrl} />
 *     <button onClick={togglePlayPause}>
 *       {isPlaying ? 'Pause' : 'Play'}
 *     </button>
 *     <span>{formatTime(currentTime)} / {formatTime(duration)}</span>
 *   </>
 * );
 * ```
 */
export function useMediaPlayback(options: UseMediaPlaybackOptions = {}): UseMediaPlaybackReturn {
  const {
    initialVolume = 1,
    autoPlay = false,
    loop = false,
    onEnded,
    onTimeUpdate,
  } = options;

  const [state, setState] = useState<PlaybackState>({
    isPlaying: false,
    currentTime: 0,
    duration: 0,
    volume: initialVolume,
    isMuted: false,
    playbackRate: 1,
  });

  const mediaRef = useRef<HTMLAudioElement | HTMLVideoElement>(null);
  const onEndedRef = useRef(onEnded);
  const onTimeUpdateRef = useRef(onTimeUpdate);

  // Keep callbacks fresh
  useEffect(() => {
    onEndedRef.current = onEnded;
    onTimeUpdateRef.current = onTimeUpdate;
  }, [onEnded, onTimeUpdate]);

  // Set up media element event listeners
  useEffect(() => {
    const media = mediaRef.current;
    if (!media) return;

    const handleTimeUpdate = () => {
      const time = media.currentTime;
      setState((prev) => ({ ...prev, currentTime: time }));
      onTimeUpdateRef.current?.(time);
    };

    const handleDurationChange = () => {
      setState((prev) => ({ ...prev, duration: media.duration || 0 }));
    };

    const handlePlay = () => {
      setState((prev) => ({ ...prev, isPlaying: true }));
    };

    const handlePause = () => {
      setState((prev) => ({ ...prev, isPlaying: false }));
    };

    const handleEnded = () => {
      setState((prev) => ({ ...prev, isPlaying: false, currentTime: 0 }));
      onEndedRef.current?.();
    };

    const handleVolumeChange = () => {
      setState((prev) => ({
        ...prev,
        volume: media.volume,
        isMuted: media.muted,
      }));
    };

    const handleLoadedMetadata = () => {
      setState((prev) => ({ ...prev, duration: media.duration || 0 }));
      media.volume = initialVolume;
      media.loop = loop;
      if (autoPlay) {
        media.play().catch(() => {
          // Autoplay blocked by browser
        });
      }
    };

    media.addEventListener('timeupdate', handleTimeUpdate);
    media.addEventListener('durationchange', handleDurationChange);
    media.addEventListener('play', handlePlay);
    media.addEventListener('pause', handlePause);
    media.addEventListener('ended', handleEnded);
    media.addEventListener('volumechange', handleVolumeChange);
    media.addEventListener('loadedmetadata', handleLoadedMetadata);

    return () => {
      media.removeEventListener('timeupdate', handleTimeUpdate);
      media.removeEventListener('durationchange', handleDurationChange);
      media.removeEventListener('play', handlePlay);
      media.removeEventListener('pause', handlePause);
      media.removeEventListener('ended', handleEnded);
      media.removeEventListener('volumechange', handleVolumeChange);
      media.removeEventListener('loadedmetadata', handleLoadedMetadata);
    };
  }, [autoPlay, loop, initialVolume]);

  const play = useCallback(() => {
    mediaRef.current?.play().catch(() => {
      // Play was prevented
    });
  }, []);

  const pause = useCallback(() => {
    mediaRef.current?.pause();
  }, []);

  const togglePlayPause = useCallback(() => {
    const media = mediaRef.current;
    if (!media) return;

    if (media.paused) {
      media.play().catch(() => {});
    } else {
      media.pause();
    }
  }, []);

  const seek = useCallback((time: number) => {
    const media = mediaRef.current;
    if (!media) return;

    const clampedTime = Math.max(0, Math.min(time, media.duration || 0));
    media.currentTime = clampedTime;
    setState((prev) => ({ ...prev, currentTime: clampedTime }));
  }, []);

  const seekRelative = useCallback((delta: number) => {
    const media = mediaRef.current;
    if (!media) return;

    const newTime = media.currentTime + delta;
    seek(newTime);
  }, [seek]);

  const setVolume = useCallback((volume: number) => {
    const media = mediaRef.current;
    if (!media) return;

    const clampedVolume = Math.max(0, Math.min(1, volume));
    media.volume = clampedVolume;
    if (clampedVolume > 0 && media.muted) {
      media.muted = false;
    }
  }, []);

  const toggleMute = useCallback(() => {
    const media = mediaRef.current;
    if (!media) return;

    media.muted = !media.muted;
  }, []);

  const setPlaybackRate = useCallback((rate: number) => {
    const media = mediaRef.current;
    if (!media) return;

    const clampedRate = Math.max(0.25, Math.min(4, rate));
    media.playbackRate = clampedRate;
    setState((prev) => ({ ...prev, playbackRate: clampedRate }));
  }, []);

  const reset = useCallback(() => {
    const media = mediaRef.current;
    if (media) {
      media.pause();
      media.currentTime = 0;
    }
    setState((prev) => ({
      ...prev,
      isPlaying: false,
      currentTime: 0,
    }));
  }, []);

  const formatTime = useCallback((seconds: number): string => {
    if (!isFinite(seconds) || isNaN(seconds)) return '0:00';

    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  }, []);

  const getProgress = useCallback((): number => {
    if (state.duration === 0) return 0;
    return (state.currentTime / state.duration) * 100;
  }, [state.currentTime, state.duration]);

  return useMemo(() => ({
    // State
    isPlaying: state.isPlaying,
    currentTime: state.currentTime,
    duration: state.duration,
    volume: state.volume,
    isMuted: state.isMuted,
    playbackRate: state.playbackRate,

    // Ref
    mediaRef: mediaRef as React.RefObject<HTMLAudioElement | HTMLVideoElement>,

    // Controls
    play,
    pause,
    togglePlayPause,
    seek,
    seekRelative,
    setVolume,
    toggleMute,
    setPlaybackRate,
    reset,

    // Utilities
    formatTime,
    getProgress,
  }), [
    state,
    play,
    pause,
    togglePlayPause,
    seek,
    seekRelative,
    setVolume,
    toggleMute,
    setPlaybackRate,
    reset,
    formatTime,
    getProgress,
  ]);
}

export default useMediaPlayback;
````

## File: packages/frontend/hooks/useModalState.ts
````typescript
/**
 * useModalState - Unified modal state management
 *
 * Replaces scattered boolean state flags with a single state machine
 * for managing modals and side panels.
 */

import { useState, useCallback, useMemo } from 'react';

export type ModalType =
  | 'export'
  | 'quality'
  | 'sceneEditor'
  | 'music'
  | 'settings'
  | 'timeline'
  | null;

export interface ModalOptions {
  /** Data to pass to the modal */
  data?: Record<string, unknown>;
  /** Callback when modal closes */
  onClose?: () => void;
}

export interface ModalState {
  activeModal: ModalType;
  modalData: Record<string, unknown>;
}

export interface UseModalStateReturn {
  /** Currently active modal (null if none) */
  activeModal: ModalType;
  /** Data passed to the active modal */
  modalData: Record<string, unknown>;
  /** Open a specific modal */
  openModal: (modal: ModalType, options?: ModalOptions) => void;
  /** Close the currently active modal */
  closeModal: () => void;
  /** Toggle a modal open/closed */
  toggleModal: (modal: ModalType, options?: ModalOptions) => void;
  /** Check if a specific modal is open */
  isOpen: (modal: ModalType) => boolean;
  /** Close all modals */
  closeAll: () => void;

  // Convenience boolean getters for common use cases
  showExport: boolean;
  showQuality: boolean;
  showSceneEditor: boolean;
  showMusic: boolean;
  showSettings: boolean;
  showTimeline: boolean;

  // Convenience setters for backwards compatibility
  setShowExport: (show: boolean) => void;
  setShowQuality: (show: boolean) => void;
  setShowSceneEditor: (show: boolean) => void;
  setShowMusic: (show: boolean) => void;
  setShowSettings: (show: boolean) => void;
  setShowTimeline: (show: boolean) => void;
}

/**
 * Hook for managing modal/panel visibility state
 *
 * @example
 * ```tsx
 * const { activeModal, openModal, closeModal, showExport } = useModalState();
 *
 * // Open with data
 * openModal('export', { data: { videoTitle: 'My Video' } });
 *
 * // Use convenience boolean
 * if (showExport) { ... }
 *
 * // Toggle
 * toggleModal('sceneEditor');
 * ```
 */
export function useModalState(initialModal: ModalType = null): UseModalStateReturn {
  const [state, setState] = useState<ModalState>({
    activeModal: initialModal,
    modalData: {},
  });

  const [closeCallback, setCloseCallback] = useState<(() => void) | null>(null);

  const openModal = useCallback((modal: ModalType, options?: ModalOptions) => {
    setState({
      activeModal: modal,
      modalData: options?.data || {},
    });
    if (options?.onClose) {
      setCloseCallback(() => options.onClose);
    }
  }, []);

  const closeModal = useCallback(() => {
    if (closeCallback) {
      closeCallback();
      setCloseCallback(null);
    }
    setState({
      activeModal: null,
      modalData: {},
    });
  }, [closeCallback]);

  const toggleModal = useCallback((modal: ModalType, options?: ModalOptions) => {
    setState((prev) => {
      if (prev.activeModal === modal) {
        if (closeCallback) {
          closeCallback();
          setCloseCallback(null);
        }
        return { activeModal: null, modalData: {} };
      }
      if (options?.onClose) {
        setCloseCallback(() => options.onClose);
      }
      return {
        activeModal: modal,
        modalData: options?.data || {},
      };
    });
  }, [closeCallback]);

  const isOpen = useCallback((modal: ModalType): boolean => {
    return state.activeModal === modal;
  }, [state.activeModal]);

  const closeAll = useCallback(() => {
    if (closeCallback) {
      closeCallback();
      setCloseCallback(null);
    }
    setState({ activeModal: null, modalData: {} });
  }, [closeCallback]);

  // Convenience booleans
  const showExport = state.activeModal === 'export';
  const showQuality = state.activeModal === 'quality';
  const showSceneEditor = state.activeModal === 'sceneEditor';
  const showMusic = state.activeModal === 'music';
  const showSettings = state.activeModal === 'settings';
  const showTimeline = state.activeModal === 'timeline';

  // Convenience setters for backwards compatibility
  const setShowExport = useCallback((show: boolean) => {
    if (show) openModal('export');
    else if (state.activeModal === 'export') closeModal();
  }, [openModal, closeModal, state.activeModal]);

  const setShowQuality = useCallback((show: boolean) => {
    if (show) openModal('quality');
    else if (state.activeModal === 'quality') closeModal();
  }, [openModal, closeModal, state.activeModal]);

  const setShowSceneEditor = useCallback((show: boolean) => {
    if (show) openModal('sceneEditor');
    else if (state.activeModal === 'sceneEditor') closeModal();
  }, [openModal, closeModal, state.activeModal]);

  const setShowMusic = useCallback((show: boolean) => {
    if (show) openModal('music');
    else if (state.activeModal === 'music') closeModal();
  }, [openModal, closeModal, state.activeModal]);

  const setShowSettings = useCallback((show: boolean) => {
    if (show) openModal('settings');
    else if (state.activeModal === 'settings') closeModal();
  }, [openModal, closeModal, state.activeModal]);

  const setShowTimeline = useCallback((show: boolean) => {
    if (show) openModal('timeline');
    else if (state.activeModal === 'timeline') closeModal();
  }, [openModal, closeModal, state.activeModal]);

  return useMemo(() => ({
    activeModal: state.activeModal,
    modalData: state.modalData,
    openModal,
    closeModal,
    toggleModal,
    isOpen,
    closeAll,
    showExport,
    showQuality,
    showSceneEditor,
    showMusic,
    showSettings,
    showTimeline,
    setShowExport,
    setShowQuality,
    setShowSceneEditor,
    setShowMusic,
    setShowSettings,
    setShowTimeline,
  }), [
    state,
    openModal,
    closeModal,
    toggleModal,
    isOpen,
    closeAll,
    showExport,
    showQuality,
    showSceneEditor,
    showMusic,
    showSettings,
    showTimeline,
    setShowExport,
    setShowQuality,
    setShowSceneEditor,
    setShowMusic,
    setShowSettings,
    setShowTimeline,
  ]);
}

export default useModalState;
````

## File: packages/frontend/hooks/useProjectSession.ts
````typescript
/**
 * useProjectSession Hook
 *
 * Manages the connection between Project (Firestore) and Production Session (IndexedDB).
 * Handles loading, restoring, and syncing project state.
 */

import { useState, useEffect, useCallback, useRef } from 'react';
import { useAppStore } from '@/stores';
import {
  getProject,
  updateProject,
  markProjectAccessed,
  type Project,
  type UpdateProjectInput,
} from '@/services/projectService';
import {
  restoreProductionSession,
  initializeProductionSession,
  flushPendingPersistence,
} from '@/services/ai/production/store';
import { cloudAutosave } from '@/services/cloudStorageService';
import type { ProductionState } from '@/services/ai/production/types';

export interface UseProjectSessionResult {
  project: Project | null;
  sessionId: string | null;
  isLoading: boolean;
  error: string | null;
  restoredState: ProductionState | null;
  syncProjectMetadata: (updates: Partial<UpdateProjectInput>) => void;
  flushSession: () => Promise<void>;
}

const SYNC_DEBOUNCE_MS = 2000;

export function useProjectSession(projectId: string | undefined): UseProjectSessionResult {
  const [project, setProject] = useState<Project | null>(null);
  const [sessionId, setSessionId] = useState<string | null>(null);
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [restoredState, setRestoredState] = useState<ProductionState | null>(null);

  const setCurrentProjectId = useAppStore((s) => s.setCurrentProjectId);
  const syncTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const pendingUpdatesRef = useRef<Partial<UpdateProjectInput>>({});

  // Load project and restore session on mount or projectId change
  useEffect(() => {
    if (!projectId) {
      // No project - clear state
      setProject(null);
      setSessionId(null);
      setRestoredState(null);
      setCurrentProjectId(null);
      setError(null);
      return;
    }

    // Capture projectId to help TypeScript narrow the type
    const currentProjectId = projectId;
    let cancelled = false;

    async function loadAndRestore() {
      setIsLoading(true);
      setError(null);

      try {
        // 1. Load project from Firestore
        const loadedProject = await getProject(currentProjectId);

        if (cancelled) return;

        if (!loadedProject) {
          throw new Error(`Project "${currentProjectId}" not found or access denied`);
        }

        setProject(loadedProject);
        setCurrentProjectId(currentProjectId);

        // Mark as accessed (fire-and-forget)
        markProjectAccessed(currentProjectId);

        // 2. Get session ID from project
        const cloudSessionId = loadedProject.cloudSessionId;
        setSessionId(cloudSessionId);

        // 3. Try to restore production state from IndexedDB
        const restored = await restoreProductionSession(cloudSessionId);

        if (cancelled) return;

        if (restored) {
          console.log(
            `[useProjectSession] Restored session ${cloudSessionId} with ${restored.contentPlan?.scenes?.length || 0} scenes`
          );
          setRestoredState(restored);
        } else {
          // No existing session - initialize new one
          console.log(
            `[useProjectSession] No existing session, initializing ${cloudSessionId}`
          );
          await initializeProductionSession(cloudSessionId, {});

          // Initialize cloud autosave (fire-and-forget)
          cloudAutosave.initSession(cloudSessionId).catch((err) => {
            console.warn('[useProjectSession] Cloud autosave init failed:', err);
          });
        }
      } catch (err) {
        if (cancelled) return;
        console.error('[useProjectSession] Failed to load project:', err);
        setError(err instanceof Error ? err.message : 'Failed to load project');
      } finally {
        if (!cancelled) {
          setIsLoading(false);
        }
      }
    }

    loadAndRestore();

    // Cleanup on unmount or projectId change
    return () => {
      cancelled = true;
      if (syncTimeoutRef.current) {
        clearTimeout(syncTimeoutRef.current);
        // Flush any pending updates before unmount
        if (Object.keys(pendingUpdatesRef.current).length > 0) {
          updateProject(currentProjectId, pendingUpdatesRef.current).catch(() => {});
          pendingUpdatesRef.current = {};
        }
      }
    };
  }, [projectId, setCurrentProjectId]);

  // Sync project metadata to Firestore (debounced)
  const syncProjectMetadata = useCallback(
    (updates: Partial<UpdateProjectInput>) => {
      if (!projectId || !project) return;

      // Merge updates with pending
      pendingUpdatesRef.current = {
        ...pendingUpdatesRef.current,
        ...updates,
      };

      // Debounce sync calls
      if (syncTimeoutRef.current) {
        clearTimeout(syncTimeoutRef.current);
      }

      syncTimeoutRef.current = setTimeout(async () => {
        const toSync = { ...pendingUpdatesRef.current };
        pendingUpdatesRef.current = {};

        try {
          await updateProject(projectId, toSync);
          console.log('[useProjectSession] Synced project metadata:', Object.keys(toSync));
        } catch (err) {
          console.warn('[useProjectSession] Failed to sync project metadata:', err);
        }
      }, SYNC_DEBOUNCE_MS);
    },
    [projectId, project]
  );

  // Flush session to IndexedDB
  const flushSession = useCallback(async () => {
    if (!sessionId) return;

    // Flush IndexedDB persistence
    await flushPendingPersistence(sessionId);

    // Also flush any pending Firestore updates
    if (syncTimeoutRef.current) {
      clearTimeout(syncTimeoutRef.current);
      syncTimeoutRef.current = null;
    }

    if (projectId && Object.keys(pendingUpdatesRef.current).length > 0) {
      const toSync = { ...pendingUpdatesRef.current };
      pendingUpdatesRef.current = {};
      await updateProject(projectId, toSync);
    }
  }, [sessionId, projectId]);

  return {
    project,
    sessionId,
    isLoading,
    error,
    restoredState,
    syncProjectMetadata,
    flushSession,
  };
}
````

## File: packages/frontend/hooks/useStoryGeneration.ts
````typescript
/**
 * useStoryGeneration Hook
 *
 * Manages the state and transition logic for the Story Mode workflow.
 * Workflow: Idea (Topic) → Breakdown → Screenplay → Characters → Shotlist → Narration → Animation → Export
 */

import { useState, useCallback, useEffect } from 'react';
import type {
    StoryStep,
    StoryState,
    ScreenplayScene,
    CharacterProfile,
    ShotlistEntry,
    ConsistencyReport,
    StoryShot,
    Scene
} from '@/types';
import { runProductionAgent } from '@/services/ai/productionAgent';
import { breakAllScenesIntoShots } from '@/services/ai/shotBreakdownAgent';
import { storyModeStore } from '@/services/ai/production/store';
import type { StoryModeState } from '@/services/ai/production/types';
import { narrateScene, narrateAllShots, createAudioUrl, type NarratorConfig } from '@/services/narratorService';
import { generateVideoFromPrompt } from '@/services/videoService';
import { animateImageWithDeApi, generateVideoWithDeApi, isDeApiConfigured, generateImageWithAspectRatio, generateImageBatch } from '@/services/deapiService';
import { exportVideoWithFFmpeg } from '@/services/ffmpeg/exporters';
import { generateCharacterReference, enrichCharactersWithCoreAnchors } from '@/services/characterService';
import { cloudAutosave } from '@/services/cloudStorageService';
import { createCombinedNarrationAudio } from '@/services/audioConcatService';
import {
    debouncedSaveToCloud,
    loadStoryFromCloud,
    isSyncAvailable,
    flushPendingSave,
    getCurrentUser,
    onAuthChange,
} from '@/services/firebase';
import {
    fromShotBreakdown,
    serializeStyleGuideAsText,
    type ExtractedStyleOverride,
} from '@/services/prompt/imageStyleGuide';
import { getSystemPersona } from '@/services/prompt/personaData';
import type { VideoPurpose } from '@/constants';
import {
    extractVisualStyle,
    type VisualStyle,
} from '@/services/visualConsistencyService';
import { getCharacterSeed } from '@/services/imageService';
import { cleanForTTS, cleanForSubtitles } from '@/services/textSanitizer';
import { generateVoiceoverScripts } from '@/services/ai/storyPipeline';
import { detectLanguage } from '@/services/languageDetector';

/**
 * Generate anti-style negative prompts based on chosen visual style.
 * Prevents style contamination (e.g., cinematic shots appearing as 3D renders).
 */
function generateNegativePromptsForStyle(style: string): string[] {
    const lower = style.toLowerCase();
    const base = ["watermark", "text overlay", "UI elements", "blurry", "low resolution"];

    const styleNegatives: Record<string, string[]> = {
        cinematic: ["3D render", "stock photo", "cartoon", "flat lighting", "anime", "pixel art"],
        "anime / manga": ["photorealistic", "3D render", "stock photo", "film grain"],
        cyberpunk: ["pastoral", "bright daylight", "cartoon", "watercolor"],
        watercolor: ["photorealistic", "3D render", "sharp edges", "neon"],
        "oil painting": ["photorealistic", "digital art", "flat colors", "anime"],
        "pixel art": ["photorealistic", "smooth gradients", "film grain"],
        photorealistic: ["cartoon", "anime", "pixel art", "painting", "illustration"],
        "dark fantasy": ["bright colors", "cartoon", "modern setting", "clean"],
        "comic book": ["photorealistic", "film grain", "watercolor", "muted colors"],
    };

    return [...base, ...(styleNegatives[lower] || styleNegatives["cinematic"]!)];
}

/** Motion strength configuration for DeAPI animation (Issue 4) */
type MotionStrength = 'subtle' | 'moderate' | 'dynamic';

const MOTION_CONFIGS: Record<MotionStrength, { frames: number; promptPrefix: string }> = {
    subtle: { frames: 60, promptPrefix: "Slow gentle camera movement. Minimal subject motion." },
    moderate: { frames: 90, promptPrefix: "Smooth camera movement. Subtle subject motion." },
    dynamic: { frames: 120, promptPrefix: "Dynamic camera movement." },
};

/** Auto-select motion strength based on shot type and camera movement (Issue 4) */
function selectMotionStrength(shotType: string, movement: string): MotionStrength {
    const type = shotType.toLowerCase();
    const mov = movement.toLowerCase();

    // Close-ups use subtle to prevent face distortion
    if (type.includes('close-up') || type.includes('extreme close')) return 'subtle';
    // Static shots use subtle
    if (mov === 'static') return 'subtle';
    // Tracking/handheld use dynamic
    if (mov === 'tracking' || mov === 'handheld') return 'dynamic';
    // Pan/tilt/dolly/zoom use moderate
    return 'moderate';
}

/** Build camera-focused animation prompt instead of raw narrative description (Issue 4) */
function buildAnimationPrompt(movement: string, description: string): string {
    const movLower = movement.toLowerCase();
    let cameraDirection = '';
    if (movLower === 'pan') cameraDirection = 'slow horizontal pan';
    else if (movLower === 'tilt') cameraDirection = 'gentle vertical tilt';
    else if (movLower === 'zoom') cameraDirection = 'slow zoom in';
    else if (movLower === 'dolly') cameraDirection = 'smooth dolly forward';
    else if (movLower === 'tracking') cameraDirection = 'tracking camera movement';
    else if (movLower === 'handheld') cameraDirection = 'subtle handheld sway';
    else cameraDirection = 'slow gentle camera drift';

    // Truncate description to 200 chars to leave room for camera instruction
    const shortDesc = description.length > 200 ? description.substring(0, 197) + '...' : description;
    return `${cameraDirection}. ${shortDesc}. Atmospheric, minimal character motion.`;
}

const STORAGE_KEY = 'ai_soul_studio_story_state';
const SESSION_KEY = 'ai_soul_studio_story_session';
const USER_ID_KEY = 'ai_soul_studio_story_user_id';
const PROJECT_ID_KEY = 'ai_soul_studio_story_project_id';

/**
 * Strip markdown and metadata artifacts from narration text.
 * Delegates to the extracted textSanitizer service for comprehensive cleaning.
 */
function cleanNarrationText(text: string): string {
    return cleanForTTS(text);
}

/**
 * Infer emotional tone and instruction triplet for a scene based on its content
 * and position in the narrative arc. Replaces hardcoded 'dramatic' for all scenes.
 */
function inferSceneEmotion(scene: ScreenplayScene, index: number, total: number): {
    emotionalTone: 'professional' | 'dramatic' | 'friendly' | 'urgent' | 'calm';
    instructionTriplet: { primaryEmotion: string; cinematicDirection: string; environmentalAtmosphere: string };
} {
    const text = `${scene.heading} ${scene.action}`.toLowerCase();

    // Keyword-based emotion detection (English + Arabic)
    const urgentWords = /\b(run|escape|chase|hurry|danger|attack|fight|scream|crash|explode|fire|flood|storm)\b|يركض|يهرب|خطر|هجوم|يهاجم|يصرخ|صراخ|ينفجر|حريق|فيضان|عاصفة|خنق|يخنق|رعب|فزع|هلع|يطارد/;
    const calmWords = /\b(peace|serene|quiet|gentle|soft|still|dawn|morning|garden|rest|sleep|dream)\b|سلام|هدوء|سكون|صباح|حديقة|راحة|نوم|حلم|فجر|طمأنينة|أمان/;
    const friendlyWords = /\b(smile|laugh|friend|welcome|warm|celebrate|joy|happy|festival|feast|gift)\b|ابتسامة|ضحك|صديق|ترحيب|فرح|احتفال|سعادة|عيد|هدية/;
    const dramaticWords = /\b(reveal|secret|truth|betray|lost|dark|shadow|death|ancient|fate|destiny|mystery)\b|سر|حقيقة|خيانة|ظلام|ظل|موت|قديم|مصير|قدر|غموض|لغز|شبح|جن|لعنة|مهجور|مخيف|غامض/;

    // Narrative arc position
    const position = total > 1 ? index / (total - 1) : 0.5;
    const isOpening = index === 0;
    const isClimax = position >= 0.6 && position <= 0.8;
    const isEnding = index === total - 1;

    let tone: 'professional' | 'dramatic' | 'friendly' | 'urgent' | 'calm' = 'dramatic';
    let emotion = 'cinematic-wonder';
    let cinematic = 'slow-push-in';
    let atmosphere = 'golden-hour-decay';

    if (urgentWords.test(text)) {
        tone = 'urgent';
        emotion = 'visceral-dread';
        cinematic = 'handheld-float';
        atmosphere = 'tension-drone';
    } else if (calmWords.test(text)) {
        tone = 'calm';
        emotion = 'nostalgic-warmth';
        cinematic = 'slow-pull-back';
        atmosphere = 'golden-hour-decay';
    } else if (friendlyWords.test(text)) {
        tone = 'friendly';
        emotion = 'bittersweet-longing';
        cinematic = 'tracking-shot';
        atmosphere = 'hopeful-pad';
    } else if (dramaticWords.test(text)) {
        tone = 'dramatic';
        emotion = 'visceral-dread';
        cinematic = 'dutch-angle';
        atmosphere = 'foggy-ruins';
    }

    // Narrative arc overrides
    if (isOpening) {
        cinematic = 'slow-push-in';
        if (tone === 'dramatic') atmosphere = 'foggy-ruins';
    }
    if (isClimax) {
        tone = urgentWords.test(text) ? 'urgent' : 'dramatic';
        cinematic = 'dutch-angle';
        emotion = 'visceral-dread';
    }
    if (isEnding) {
        cinematic = 'slow-pull-back';
        if (!urgentWords.test(text)) {
            tone = 'calm';
            emotion = 'nostalgic-warmth';
            atmosphere = 'golden-hour-decay';
        }
    }

    return {
        emotionalTone: tone,
        instructionTriplet: {
            primaryEmotion: emotion,
            cinematicDirection: cinematic,
            environmentalAtmosphere: atmosphere,
        },
    };
}

/**
 * Strip base64 image/audio/video data from state before saving to localStorage.
 * Media is saved to cloud storage separately, so we only need metadata for recovery.
 * This prevents QuotaExceededError when state contains many generated assets.
 */
function stripImageDataForStorage(state: StoryState): StoryState {
    const isBase64 = (url?: string) => url?.startsWith('data:');
    const isBlobUrl = (url?: string) => url?.startsWith('blob:');
    const shouldStrip = (url?: string) => isBase64(url) || isBlobUrl(url);

    return {
        ...state,
        characters: state.characters.map(char => ({
            ...char,
            // Keep URL if it's a cloud/remote URL, remove if base64/blob
            referenceImageUrl: shouldStrip(char.referenceImageUrl) ? undefined : char.referenceImageUrl,
        })),
        shotlist: state.shotlist.map(shot => ({
            ...shot,
            imageUrl: shouldStrip(shot.imageUrl) ? undefined : shot.imageUrl,
        })),
        shots: state.shots?.map(shot => ({
            ...shot,
            imageUrl: shouldStrip(shot.imageUrl) ? undefined : shot.imageUrl,
        })),
        // Strip blob URLs from narration (audio)
        narrationSegments: state.narrationSegments?.map(seg => ({
            ...seg,
            audioUrl: shouldStrip(seg.audioUrl) ? '' : seg.audioUrl,
        })),
        // Strip blob URLs from per-shot narration segments
        shotNarrationSegments: state.shotNarrationSegments?.map(seg => ({
            ...seg,
            audioUrl: shouldStrip(seg.audioUrl) ? '' : seg.audioUrl,
        })),
        // Strip blob URLs from animated shots (video)
        animatedShots: state.animatedShots?.map(shot => ({
            ...shot,
            videoUrl: shouldStrip(shot.videoUrl) ? '' : shot.videoUrl,
            thumbnailUrl: shouldStrip(shot.thumbnailUrl) ? undefined : shot.thumbnailUrl,
        })),
        // Don't save final video URL (too large)
        finalVideoUrl: undefined,
    };
}

/** Matches ASCII digits (0-9), Arabic-Indic (٠-٩), and Extended Arabic-Indic (۰-۹) */
const DIGITS = '(?:[0-9\u0660-\u0669\u06F0-\u06F9])';

/**
 * Strip LLM preamble text that appears before the first scene/act/chapter marker.
 * LLMs often prepend conversational text like "Here is a narrative breakdown..."
 * or Arabic equivalents like "إليك تفصيل سردي..." which pollutes scene data.
 */
function stripLLMPreamble(text: string): string {
    // Find the first occurrence of a scene/act/chapter marker
    const markerPattern = new RegExp(`(?:Act|Chapter|Scene|Part|فصل|مشهد)\\s*${DIGITS}+`, 'i');
    const match = text.match(markerPattern);

    if (match && match.index !== undefined && match.index > 0) {
        const preamble = text.substring(0, match.index).trim();
        // Only strip if the preamble looks like conversational text (not actual content)
        // Heuristic: preamble is short-ish (< 300 chars) and doesn't contain multiple newlines
        // (which would suggest it's actual structured content)
        const newlineCount = (preamble.match(/\n/g) || []).length;
        if (preamble.length < 300 || newlineCount < 3) {
            console.log(`[parseBreakdown] Stripped LLM preamble (${preamble.length} chars): "${preamble.substring(0, 80)}..."`);
            return text.substring(match.index);
        }
    }

    // Also try numbered list markers (e.g., "1." or "1)" or "١.")
    const numberedMatch = text.match(new RegExp(`^\\s*${DIGITS}[.)]\\s`, 'm'));
    if (numberedMatch && numberedMatch.index !== undefined && numberedMatch.index > 0) {
        const preamble = text.substring(0, numberedMatch.index).trim();
        const newlineCount = (preamble.match(/\n/g) || []).length;
        if (preamble.length < 300 || newlineCount < 3) {
            console.log(`[parseBreakdown] Stripped LLM preamble before numbered list (${preamble.length} chars)`);
            return text.substring(numberedMatch.index);
        }
    }

    return text;
}

/**
 * Parse AI-generated breakdown text into structured ScreenplayScene objects.
 * Handles various formats: "Act 1:", "Chapter 1:", "Scene 1:", numbered lists, etc.
 */
function parseBreakdownToScenes(breakdownText: string, topic: string): ScreenplayScene[] {
    const scenes: ScreenplayScene[] = [];

    // Strip LLM preamble before parsing to prevent it from becoming scene_0
    const cleanedText = stripLLMPreamble(breakdownText);

    // Try to split by common patterns: Act, Chapter, Scene, or numbered sections
    // Supports ASCII digits (0-9), Arabic-Indic (٠-٩), and Extended Arabic-Indic (۰-۹)
    const patterns = [
        new RegExp(`(?:Act|Chapter|Scene|Part|فصل|مشهد|المشهد)\\s*${DIGITS}+[:.]?\\s*`, 'gi'),
        new RegExp(`(?:^${DIGITS}+[.)]\\s*)`, 'gm'),
        /(?:\n\n+)/g, // Double newlines as fallback
    ];

    let sections: string[] = [];

    // Try each pattern until we get reasonable sections
    for (const pattern of patterns) {
        sections = cleanedText.split(pattern).filter(s => s.trim().length > 20);
        if (sections.length >= 2 && sections.length <= 10) break;
    }

    // If no good split found, treat whole text as one section
    if (sections.length < 2) {
        sections = [cleanedText];
    }

    sections.forEach((section, index) => {
        const lines = section.trim().split('\n').filter(l => l.trim());
        if (lines.length === 0) return;

        // Extract title from first line or generate one
        let title = lines[0]?.replace(/^[*\-#\d.)]+\s*/, '').trim() || `Scene ${index + 1}`;
        // Clean up title - remove markdown, limit length
        title = title.replace(/[*_#]/g, '').substring(0, 100);

        // Rest of lines become the action/description
        // Strip trailing scene number artifacts (e.g., "٢. **" or "3. **") that leak
        // from the next section's numbering during split, and clean leftover markdown.
        const actionLines = lines.slice(1).join(' ').trim()
            .replace(/\s*[0-9\u0660-\u0669\u06F0-\u06F9]+\.\s*\*{0,2}\s*$/, '')
            .replace(/\*{2,}/g, '')
            .trim();

        scenes.push({
            id: `scene_${index}`,
            sceneNumber: index + 1,
            heading: title,
            action: actionLines || `Scene from: ${topic}`,
            dialogue: [],
            charactersPresent: [],
        });
    });

    // Ensure we have at least one scene
    if (scenes.length === 0) {
        scenes.push({
            id: 'scene_0',
            sceneNumber: 1,
            heading: 'Opening',
            action: breakdownText.substring(0, 500),
            dialogue: [],
            charactersPresent: [],
        });
    }

    return scenes;
}

interface StoryAgentResult {
    sessionId?: string;
    scenes?: ScreenplayScene[];
    screenplay?: { title: string; scenes: ScreenplayScene[] };
    characters?: CharacterProfile[];
    shots?: ShotlistEntry[];
    report?: ConsistencyReport;
    [key: string]: unknown;
}

export function useStoryGeneration(projectId?: string | null) {
    const initialState: StoryState = {
        currentStep: 'idea',
        breakdown: [],
        script: null,
        characters: [],
        shotlist: [],
    };

    const [state, setState] = useState<StoryState>(initialState);

    const [sessionId, setSessionId] = useState<string | null>(null);
    const [topic, setTopic] = useState<string | null>(null);
    const [isProcessing, setIsProcessing] = useState(false);
    const [error, setError] = useState<string | null>(null);
    const [progress, setProgress] = useState<{ message: string; percent: number }>({
        message: '',
        percent: 0
    });

    // History for Undo/Redo
    const [past, setPast] = useState<StoryState[]>([]);
    const [future, setFuture] = useState<StoryState[]>([]);

    /**
     * Helper to push a new state to history
     */
    const pushState = useCallback((newState: StoryState) => {
        setPast(prev => {
            // Limit history size to 50
            const nextPast = [...prev, state];
            if (nextPast.length > 50) return nextPast.slice(nextPast.length - 50);
            return nextPast;
        });
        setFuture([]); // Clear redo stack on new action
        setState(newState);
    }, [state]);

    const undo = useCallback(() => {
        if (past.length === 0) return;

        const previous = past[past.length - 1];
        if (!previous) return;

        const newPast = past.slice(0, past.length - 1);

        setPast(newPast);
        setFuture(prev => [state, ...prev]);
        setState(previous);
    }, [past, state]);

    const redo = useCallback(() => {
        if (future.length === 0) return;

        const next = future[0];
        if (!next) return;

        const newFuture = future.slice(1);

        setFuture(newFuture);
        setPast(prev => [...prev, state]);
        setState(next);
    }, [future, state]);

    // Load state from localStorage on mount / project change (with ownership validation)
    useEffect(() => {
        const savedState = localStorage.getItem(STORAGE_KEY);
        const savedSession = localStorage.getItem(SESSION_KEY);
        const savedUserId = localStorage.getItem(USER_ID_KEY);
        const savedProjectId = localStorage.getItem(PROJECT_ID_KEY);

        // Get current user to validate ownership
        const currentUser = getCurrentUser();

        // Clear stale session if user mismatch (prevents Firebase permission errors)
        if (savedUserId && currentUser && savedUserId !== currentUser.uid) {
            console.log('[useStoryGeneration] Session belongs to different user, clearing');
            localStorage.removeItem(STORAGE_KEY);
            localStorage.removeItem(SESSION_KEY);
            localStorage.removeItem(USER_ID_KEY);
            localStorage.removeItem(PROJECT_ID_KEY);
            return;
        }

        // If a projectId is provided and it differs from the saved one,
        // this is a different/new project — start fresh instead of loading old state
        if (projectId && savedProjectId && projectId !== savedProjectId) {
            console.log('[useStoryGeneration] Different project detected, resetting state', {
                current: projectId,
                saved: savedProjectId,
            });
            setState(initialState);
            setSessionId(null);
            setTopic(null);
            setPast([]);
            setFuture([]);
            localStorage.removeItem(STORAGE_KEY);
            localStorage.removeItem(SESSION_KEY);
            // Update the stored projectId to the new one
            localStorage.setItem(PROJECT_ID_KEY, projectId);
            return;
        }

        // If projectId is provided but nothing was saved yet, store it
        if (projectId && !savedProjectId) {
            localStorage.setItem(PROJECT_ID_KEY, projectId);
        }

        if (savedState) {
            try {
                const parsed = JSON.parse(savedState);
                setState(parsed);
                console.log('[useStoryGeneration] Recovered story state');
            } catch (e) {
                console.error('Failed to parse saved story state', e);
            }
        }

        if (savedSession) {
            setSessionId(savedSession);
            // Re-initialize cloud storage for the restored session
            cloudAutosave.initSession(savedSession).then(success => {
                if (success) {
                    console.log('[useStoryGeneration] Cloud storage re-initialized for restored session');
                }
            });

            // Re-populate storyModeStore with restored state so tools can find the session
            if (savedState) {
                try {
                    const parsed = JSON.parse(savedState);
                    // Convert React state format to StoryModeState format
                    const storyModeState = {
                        id: savedSession,
                        topic: parsed.breakdown?.[0]?.heading || 'Restored Story',
                        breakdown: parsed.breakdown?.map((s: ScreenplayScene) =>
                            `${s.heading}: ${s.action}`
                        ).join('\n') || '',
                        screenplay: parsed.script?.scenes || [],
                        characters: parsed.characters || [],
                        shotlist: parsed.shotlist || [],
                        currentStep: parsed.currentStep === 'script' ? 'screenplay' : parsed.currentStep,
                        updatedAt: Date.now(),
                    };
                    storyModeStore.set(savedSession, storyModeState);
                    console.log('[useStoryGeneration] Restored storyModeStore for session:', savedSession);
                } catch (e) {
                    console.error('[useStoryGeneration] Failed to restore storyModeStore:', e);
                }
            }
        }
    }, [projectId]);

    // Save state to localStorage and Firestore on change
    useEffect(() => {
        if (state.currentStep !== 'idea') {
            try {
                const stateForStorage = stripImageDataForStorage(state);
                localStorage.setItem(STORAGE_KEY, JSON.stringify(stateForStorage));

                // Also sync to Firestore if user is authenticated
                if (sessionId && isSyncAvailable()) {
                    debouncedSaveToCloud(sessionId, state, topic || undefined);
                }
            } catch (err) {
                // QuotaExceededError - log but don't crash
                console.warn('[useStoryGeneration] Failed to save state to localStorage:', err);
            }
        }
        if (sessionId) {
            localStorage.setItem(SESSION_KEY, sessionId);
        }
        // Keep projectId in sync
        if (projectId) {
            localStorage.setItem(PROJECT_ID_KEY, projectId);
        }
    }, [state, sessionId, topic, projectId]);

    // Clear stale sessions on auth state change (sign-out or user switch)
    useEffect(() => {
        const unsubscribe = onAuthChange((user) => {
            const savedUserId = localStorage.getItem(USER_ID_KEY);

            if (!user) {
                // User signed out - clear session data
                console.log('[useStoryGeneration] User signed out, clearing session');
                localStorage.removeItem(STORAGE_KEY);
                localStorage.removeItem(SESSION_KEY);
                localStorage.removeItem(USER_ID_KEY);
                localStorage.removeItem(PROJECT_ID_KEY);
            } else if (savedUserId && savedUserId !== user.uid) {
                // Different user signed in - clear stale session
                console.log('[useStoryGeneration] Different user signed in, clearing stale session');
                localStorage.removeItem(STORAGE_KEY);
                localStorage.removeItem(SESSION_KEY);
                localStorage.removeItem(USER_ID_KEY);
                localStorage.removeItem(PROJECT_ID_KEY);
            }
        });

        return () => {
            if (unsubscribe) unsubscribe();
        };
    }, []);

    /**
     * Step 1: Generate Breakdown
     */
    const generateBreakdown = useCallback(async (inputTopic: string, genre: string) => {
        setIsProcessing(true);
        setError(null);
        setTopic(inputTopic);
        setProgress({ message: 'Generating story breakdown...', percent: 20 });

        try {
            const prompt = `Use the generate_breakdown tool to create a ${genre} story about ${inputTopic}. Return 3-5 scenes.`;
            let capturedSessionId: string | null = null;
            
            await runProductionAgent(prompt, (progress) => {
                setProgress({ message: progress.message, percent: progress.isComplete ? 100 : 50 });
                // Capture sessionId from the progress callback
                if (progress.sessionId) {
                    capturedSessionId = progress.sessionId;
                }
            });

            // Use the captured sessionId, or fall back to searching storyModeStore
            let foundSessionId: string | null = capturedSessionId;
            let foundState = null;

            if (!foundSessionId) {
                // Fallback: Find the best matching story session
                // Priority: exact topic match > most recent story session
                let bestMatch: { sid: string; state: StoryModeState; updatedAt: number } | null = null;
                for (const [sid, storyState] of storyModeStore.entries()) {
                    if (storyState.topic === inputTopic) {
                        // Exact topic match — use immediately
                        foundSessionId = sid;
                        foundState = storyState;
                        break;
                    }
                    if (sid.startsWith('story_')) {
                        const ts = storyState.updatedAt || 0;
                        if (!bestMatch || ts > bestMatch.updatedAt) {
                            bestMatch = { sid, state: storyState, updatedAt: ts };
                        }
                    }
                }
                if (!foundSessionId && bestMatch) {
                    foundSessionId = bestMatch.sid;
                    foundState = bestMatch.state;
                }
            } else {
                // Get the state from storyModeStore using the captured sessionId
                foundState = storyModeStore.get(foundSessionId);
            }

            if (foundSessionId && foundState && foundState.breakdown) {
                setSessionId(foundSessionId);

                // Save userId with session to prevent cross-user sync issues
                const user = getCurrentUser();
                if (user) {
                    localStorage.setItem(USER_ID_KEY, user.uid);
                }

                // Initialize cloud storage session for media persistence
                cloudAutosave.initSession(foundSessionId).then(success => {
                    if (success) {
                        console.log('[useStoryGeneration] Cloud storage initialized for session');
                    } else {
                        console.warn('[useStoryGeneration] Cloud storage unavailable, using local storage only');
                    }
                });

                // Parse the breakdown text into scenes
                const breakdownText = foundState.breakdown;
                const scenes: ScreenplayScene[] = parseBreakdownToScenes(breakdownText, inputTopic);

                console.log('[useStoryGeneration] Breakdown parsed into scenes:', scenes.length);

                setState(prev => ({
                    ...prev,
                    currentStep: 'breakdown',
                    breakdown: scenes,
                    genre,
                }));
            } else {
                setError('Story breakdown was generated but could not be retrieved. Please try again.');
            }
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, []);

    /**
     * Step 1.5: Regenerate Specific Scene
     */
    const regenerateScene = useCallback(async (sceneNumber: number, feedback: string) => {
        if (!sessionId) return;
        setIsProcessing(true);
        setError(null);
        setProgress({ message: `Regenerating scene ${sceneNumber}...`, percent: 30 });

        try {
            const prompt = `Using sessionId ${sessionId}, call regenerate_scene_breakdown for scene ${sceneNumber} with feedback: ${feedback}`;
            const result = await runProductionAgent(prompt, (p) => {
                setProgress({ message: p.message, percent: p.isComplete ? 100 : 50 });
            });

            if (result && (result as unknown as StoryAgentResult).scenes) {
                setState(prev => ({
                    ...prev,
                    breakdown: (result as unknown as StoryAgentResult).scenes || prev.breakdown
                }));
            }
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, [sessionId]);

    /**
     * Step 2: Create Screenplay
     */
    const generateScreenplay = useCallback(async () => {
        if (!sessionId) return;
        setIsProcessing(true);
        setError(null);
        setProgress({ message: 'Expanding breakdown into full screenplay...', percent: 40 });

        try {
            const prompt = `Using sessionId ${sessionId}, call create_screenplay with the current breakdown.`;
            await runProductionAgent(prompt, (progress) => {
                setProgress({ message: progress.message, percent: progress.isComplete ? 100 : 60 });
            });

            // Fetch the screenplay from storyModeStore
            const storyState = storyModeStore.get(sessionId);
            if (storyState && storyState.screenplay && storyState.screenplay.length > 0) {
                console.log('[useStoryGeneration] Screenplay retrieved:', storyState.screenplay.length, 'scenes');

                // Reconcile scene count: if screenplay has fewer scenes than breakdown,
                // align breakdown to match screenplay to prevent downstream misalignment
                // (e.g., narration iterating over more scenes than the screenplay covers).
                const screenplayScenes = storyState.screenplay;
                let reconciledBreakdown = state.breakdown;

                if (screenplayScenes.length !== state.breakdown.length) {
                    console.warn(
                        `[useStoryGeneration] Scene count mismatch: breakdown=${state.breakdown.length}, screenplay=${screenplayScenes.length}. Reconciling...`
                    );
                    // Use screenplay as source of truth — trim or pad breakdown to match
                    reconciledBreakdown = screenplayScenes.map((sp, idx) => {
                        // Try to match with existing breakdown scene by index
                        const existing = state.breakdown[idx];
                        return {
                            ...sp,
                            // Preserve breakdown's id scheme for consistency
                            id: existing?.id || `scene_${idx}`,
                            sceneNumber: idx + 1,
                            // Use screenplay's richer action text if available
                            action: sp.action || existing?.action || '',
                            heading: sp.heading || existing?.heading || `Scene ${idx + 1}`,
                        };
                    });
                    console.log(`[useStoryGeneration] Reconciled to ${reconciledBreakdown.length} scenes`);
                }

                // Build script object from screenplay scenes
                const script = {
                    title: reconciledBreakdown[0]?.heading || 'Untitled Story',
                    scenes: screenplayScenes,
                };

                setState(prev => ({
                    ...prev,
                    currentStep: 'script',
                    script,
                    breakdown: reconciledBreakdown,
                }));
            } else {
                setError('Screenplay was generated but could not be retrieved. Please try again.');
            }
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, [sessionId, state.breakdown]);

    /**
     * Step 3: Extract Characters
     */
    const generateCharacters = useCallback(async () => {
        if (!sessionId) return;
        setIsProcessing(true);
        setError(null);
        setProgress({ message: 'Extracting and visualizing characters...', percent: 60 });

        try {
            const prompt = `Using sessionId ${sessionId}, call generate_characters for the current script.`;
            await runProductionAgent(prompt, (progress) => {
                setProgress({ message: progress.message, percent: progress.isComplete ? 100 : 75 });
            });

            // Fetch the characters from storyModeStore
            const storyState = storyModeStore.get(sessionId);
            if (storyState && storyState.characters && storyState.characters.length > 0) {
                console.log('[useStoryGeneration] Characters retrieved:', storyState.characters.length);

                // Enrich with coreAnchors for stronger prompt anchoring in image generation
                const enrichedCharacters = enrichCharactersWithCoreAnchors(
                    storyState.characters,
                    state.visualStyle || 'Cinematic'
                );

                setState(prev => ({
                    ...prev,
                    currentStep: 'characters',
                    characters: enrichedCharacters,
                }));
            } else {
                setError('Characters were generated but could not be retrieved. Please try again.');
            }
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, [sessionId]);

    /**
     * Generate (or regenerate) a single character's reference image via DeAPI.
     */
    const generateCharacterImage = useCallback(async (characterId: string) => {
        if (!sessionId) return;

        const char = state.characters.find(c => c.id === characterId);
        if (!char) {
            setError(`Character not found: ${characterId}`);
            return;
        }

        setIsProcessing(true);
        setError(null);
        setProgress({ message: `Generating portrait for ${char.name}...`, percent: 50 });

        try {
            const referenceUrl = await generateCharacterReference(
                char.name,
                char.visualDescription,
                sessionId,
                state.visualStyle || 'Cinematic',
            );

            setState(prev => ({
                ...prev,
                characters: prev.characters.map(c =>
                    c.id === characterId
                        ? { ...c, referenceImageUrl: referenceUrl }
                        : c
                ),
            }));

            setProgress({ message: `Portrait for ${char.name} ready!`, percent: 100 });
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, [sessionId, state.characters]);

    /**
     * Step 4: Generate Shotlist
     */
    const generateShotlist = useCallback(async () => {
        if (!sessionId) return;
        setIsProcessing(true);
        setError(null);
        setProgress({ message: 'Creating technical shotlist/storyboard...', percent: 80 });

        try {
            const prompt = `Using sessionId ${sessionId}, call generate_shotlist for the current screenplay.`;
            await runProductionAgent(prompt, (progress) => {
                setProgress({ message: progress.message, percent: progress.isComplete ? 100 : 90 });
            });

            // Fetch the shotlist from storyModeStore
            const storyState = storyModeStore.get(sessionId);
            if (storyState && storyState.shotlist && storyState.shotlist.length > 0) {
                console.log('[useStoryGeneration] Shotlist retrieved:', storyState.shotlist.length, 'shots');

                setState(prev => ({
                    ...prev,
                    currentStep: 'storyboard',
                    shotlist: storyState.shotlist,
                }));
            } else {
                setError('Shotlist was generated but could not be retrieved. Please try again.');
            }
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, [sessionId]);

    /**
     * Navigation actions
     */
    const setStep = (step: StoryStep) => {
        setState(prev => ({ ...prev, currentStep: step }));
    };

    const updateBreakdown = (scenes: ScreenplayScene[]) => {
        pushState({ ...state, breakdown: scenes });
    };

    const updateScript = (script: { title: string; scenes: ScreenplayScene[] }) => {
        pushState({ ...state, script });
    };

    /**
     * Update a single shot's metadata (from Shot Editor Modal saves).
     * Merges `updates` into the matching ShotlistEntry without regenerating visuals.
     */
    const updateShot = useCallback((shotId: string, updates: Partial<ShotlistEntry>) => {
        const updatedShotlist = state.shotlist.map(s =>
            s.id === shotId ? { ...s, ...updates } : s
        );
        pushState({ ...state, shotlist: updatedShotlist });
    }, [state, pushState]);

    const resetStory = useCallback(() => {
        setState(initialState);
        setSessionId(null);
        setTopic(null);
        setPast([]);
        setFuture([]);
        localStorage.removeItem(STORAGE_KEY);
        localStorage.removeItem(SESSION_KEY);
        localStorage.removeItem(PROJECT_ID_KEY);
    }, []);

    const exportScreenplay = useCallback((format: 'txt' | 'pdf' = 'txt') => {
        if (!state.script) return;

        if (format === 'pdf') {
            // PDF export using browser print API (industry-standard screenplay format)
            const printWindow = window.open('', '_blank');
            if (!printWindow) {
                setError('Please allow popups to export PDF');
                return;
            }

            // Build HTML with proper screenplay formatting (Courier 12pt, specific margins)
            let html = `<!DOCTYPE html>
<html>
<head>
    <title>${state.script.title} - Screenplay</title>
    <style>
        @page { size: letter; margin: 1in 1.5in 1in 1.5in; }
        body { font-family: 'Courier New', Courier, monospace; font-size: 12pt; line-height: 1; }
        .title { text-align: center; margin-bottom: 3in; margin-top: 2in; }
        .title h1 { font-size: 12pt; text-transform: uppercase; }
        .scene-heading { text-transform: uppercase; margin-top: 24pt; margin-bottom: 12pt; }
        .action { margin-bottom: 12pt; }
        .character { text-transform: uppercase; margin-left: 2.2in; margin-bottom: 0; }
        .dialogue { margin-left: 1in; margin-right: 1.5in; margin-bottom: 12pt; }
        .parenthetical { margin-left: 1.6in; margin-right: 2in; font-style: italic; }
        .transition { text-align: right; text-transform: uppercase; margin-top: 12pt; }
        .page-break { page-break-after: always; }
    </style>
</head>
<body>
    <div class="title">
        <h1>${state.script.title}</h1>
        <p>Written by AI Soul Studio</p>
    </div>
    <div class="page-break"></div>
`;

            state.script.scenes.forEach((scene: ScreenplayScene) => {
                html += `<div class="scene-heading">${scene.heading}</div>\n`;
                html += `<div class="action">${scene.action}</div>\n`;

                scene.dialogue.forEach((line) => {
                    html += `<div class="character">${line.speaker}</div>\n`;
                    html += `<div class="dialogue">${line.text}</div>\n`;
                });
            });

            html += `</body></html>`;

            printWindow.document.write(html);
            printWindow.document.close();
            printWindow.onload = () => {
                printWindow.print();
            };
            return;
        }

        // Text export (original behavior)
        let content = `${state.script.title.toUpperCase()}\n\n`;

        state.script.scenes.forEach((scene: ScreenplayScene) => {
            content += `SCENE ${scene.sceneNumber}: ${scene.heading.toUpperCase()}\n\n`;
            content += `${scene.action.toUpperCase()}\n\n`;

            scene.dialogue.forEach((line) => {
                content += `\t\t${line.speaker.toUpperCase()}\n`;
                content += `\t${line.text}\n\n`;
            });

            content += `\n${'-'.repeat(40)}\n\n`;
        });

        const blob = new Blob([content], { type: 'text/plain' });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `${state.script.title.replace(/\s+/g, '_')}_Screenplay.txt`;
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
        URL.revokeObjectURL(url);
    }, [state.script]);

    /**
     * Step 5: Verify Character Consistency
     */
    const verifyConsistency = useCallback(async (characterName: string) => {
        if (!sessionId) return;
        setIsProcessing(true);
        setError(null);
        setProgress({ message: `Verifying consistency for ${characterName}...`, percent: 90 });

        try {
            const prompt = `Using sessionId ${sessionId}, verify consistency for character ${characterName}`;
            const result = await runProductionAgent(prompt, (p) => {
                setProgress({ message: p.message, percent: p.isComplete ? 100 : 95 });
            });

            if (result && (result as any).report) {
                const report = (result as any).report as ConsistencyReport;
                setState(prev => ({
                    ...prev,
                    consistencyReports: {
                        ...(prev.consistencyReports || {}),
                        [characterName]: report
                    }
                }));
            }
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, [sessionId]);

    /**
     * Lock Story - Only locks the screenplay for editing, does NOT auto-generate shots.
     * Shot generation is now a separate step triggered by generateShots().
     */
    const lockStory = useCallback(() => {
        if (state.isLocked) return;

        // Only lock the story - no async generation here
        const lockedState: StoryState = {
            ...state,
            isLocked: true,
            lockedAt: new Date().toISOString(),
            version: 'locked_v1' as const,
        };

        pushState(lockedState);
    }, [state, pushState]);

    /**
     * Generate shot breakdown for all scenes (or a specific scene).
     * This is now a separate step from locking.
     *
     * @param sceneIndex - Optional specific scene to generate shots for (for per-scene control)
     */
    const generateShots = useCallback(async (sceneIndex?: number) => {
        if (!state.isLocked) {
            setError('Story must be locked before generating shots');
            return;
        }

        setIsProcessing(true);
        setError(null);

        try {
            // Get scenes to process, filtering out undefined
            const scenesToProcess: ScreenplayScene[] = sceneIndex !== undefined
                ? [state.breakdown[sceneIndex]].filter((s): s is ScreenplayScene => s !== undefined)
                : state.breakdown;

            if (scenesToProcess.length === 0) {
                setError('No scenes to process');
                setIsProcessing(false);
                return;
            }

            const genre = state.genre || 'Drama';
            const isPerScene = sceneIndex !== undefined;

            setProgress({
                message: isPerScene
                    ? `Generating shots for scene ${sceneIndex + 1}...`
                    : 'Generating shot breakdown...',
                percent: 10
            });

            const newShots = await breakAllScenesIntoShots(
                scenesToProcess,
                genre,
                (sceneIdx, totalScenes) => {
                    const percent = 10 + ((sceneIdx + 1) / totalScenes) * 80;
                    setProgress({
                        message: `Processing scene ${sceneIdx + 1} of ${totalScenes}...`,
                        percent
                    });
                },
                sessionId || undefined,
            );

            // Convert Shot[] to StoryShot[]
            const storyShots: StoryShot[] = newShots.map(shot => ({
                ...shot,
            }));

            setProgress({ message: 'Finalizing...', percent: 95 });

            // If generating for a specific scene, merge with existing shots
            if (isPerScene && state.shots) {
                const existingShotsFromOtherScenes = state.shots.filter(
                    s => s.sceneId !== state.breakdown[sceneIndex]?.id
                );
                const sceneId = state.breakdown[sceneIndex]?.id;
                pushState({
                    ...state,
                    shots: [...existingShotsFromOtherScenes, ...storyShots],
                    currentStep: 'shots',
                    // Track which scenes have shots generated
                    scenesWithShots: [
                        ...(state.scenesWithShots || []),
                        ...(sceneId ? [sceneId] : [])
                    ].filter((v, i, a) => a.indexOf(v) === i), // dedupe
                });
            } else {
                // Generating all shots
                pushState({
                    ...state,
                    shots: storyShots,
                    currentStep: 'shots',
                    scenesWithShots: state.breakdown.map(s => s.id),
                });
            }

            setProgress({ message: 'Complete!', percent: 100 });
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, [state, pushState]);

    /**
     * Update Visual Style
     */
    const updateVisualStyle = useCallback((style: string) => {
        setState(prev => ({
            ...prev,
            visualStyle: style,
        }));
    }, []);

    /**
     * Update Aspect Ratio
     */
    const updateAspectRatio = useCallback((ratio: string) => {
        setState(prev => ({
            ...prev,
            aspectRatio: ratio,
        }));
    }, []);

    /**
     * Update Genre
     */
    const updateGenre = useCallback((genre: string) => {
        setState(prev => ({
            ...prev,
            genre,
        }));
    }, []);

    /**
     * Update Image Provider (gemini or deapi)
     */
    const updateImageProvider = useCallback((provider: 'gemini' | 'deapi') => {
        setState(prev => ({
            ...prev,
            imageProvider: provider,
        }));
    }, []);

    /**
     * Generate storyboard visuals for all scenes or a specific scene.
     * This enables per-scene control over visual generation.
     *
     * @param sceneIndex - Optional specific scene to generate visuals for
     */
    const generateVisuals = useCallback(async (sceneIndex?: number) => {
        if (!state.shots || state.shots.length === 0) {
            setError('Shots must be generated before creating visuals');
            return;
        }

        setIsProcessing(true);
        setError(null);

        const isPerScene = sceneIndex !== undefined;
        const targetSceneId = isPerScene ? state.breakdown[sceneIndex]?.id : null;

        // Filter shots for the target scene(s)
        const shotsToProcess = isPerScene
            ? state.shots.filter(s => s.sceneId === targetSceneId)
            : state.shots;

        if (shotsToProcess.length === 0) {
            setError('No shots to process for this scene');
            setIsProcessing(false);
            return;
        }

        setProgress({
            message: isPerScene
                ? `Generating visuals for scene ${sceneIndex + 1}...`
                : 'Generating storyboard visuals...',
            percent: 10
        });

        try {
            // Import the image generation service dynamically
            const { generateImageFromPrompt } = await import('@/services/imageService');

            const updatedShotlist: ShotlistEntry[] = [...state.shotlist];
            const style = state.visualStyle || 'Cinematic';

            // Build character input list for structured prompt builder
            const characterInputs = state.characters
                .filter(c => c.visualDescription)
                .map(c => ({ name: c.name, visualDescription: c.visualDescription, facialTags: c.facialTags }));

            // Extract visual style from first generated shot for consistency (Issue 3)
            let extractedStyleOverride: ExtractedStyleOverride | undefined;

            // Resolve persona from story genre for persona-aware negative injection
            const genreToPurpose: Record<string, VideoPurpose> = {
                'Drama': 'story_drama',
                'Comedy': 'story_comedy',
                'Thriller': 'story_thriller',
                'Sci-Fi': 'story_scifi',
                'Action': 'story_action',
                'Fantasy': 'story_fantasy',
                'Romance': 'story_romance',
                'Historical': 'story_historical',
                'Animation': 'story_animation',
            };
            const storyPurpose: VideoPurpose = genreToPurpose[state.genre || ''] ?? 'storytelling';
            const storyPersona = getSystemPersona(storyPurpose);

            // Helper: build structured prompt for a shot (Issues 1, 2, 3)
            const buildShotPrompt = (shot: NonNullable<typeof shotsToProcess[0]>) => {
                const guide = fromShotBreakdown(
                    {
                        description: shot.description,
                        shotType: shot.shotType,
                        cameraAngle: shot.cameraAngle,
                        movement: shot.movement,
                        lighting: shot.lighting,
                        emotion: shot.emotion,
                    },
                    characterInputs,
                    style,
                    extractedStyleOverride,
                    storyPersona,
                );
                const serialized = serializeStyleGuideAsText(guide);

                // Inject CHARACTERS IN FRAME section from coreAnchors for stronger consistency
                const shotDescLower = shot.description.toLowerCase();
                const presentChars = state.characters.filter(c =>
                    shotDescLower.includes(c.name.toLowerCase()) && c.coreAnchors
                );
                if (presentChars.length > 0) {
                    const charSection = "CHARACTERS IN FRAME:\n" +
                        presentChars.map(c => `- ${c.coreAnchors}`).join('\n');
                    return `${charSection}\n\nSCENE:\n${serialized}`;
                }
                return serialized;
            };

            // Helper: get seed for the primary character in a shot (Issue 1)
            const getShotSeed = (shot: NonNullable<typeof shotsToProcess[0]>): number | undefined => {
                const shotDescLower = shot.description.toLowerCase();
                const primaryChar = characterInputs.find(c =>
                    shotDescLower.includes(c.name.toLowerCase())
                );
                if (primaryChar) {
                    return getCharacterSeed(primaryChar.visualDescription);
                }
                return undefined;
            };

            // Helper: upload to cloud and create shotlist entry
            const processShotResult = async (shot: NonNullable<typeof shotsToProcess[0]>, imageUrl: string) => {
                let finalUrl = imageUrl;
                if (sessionId && finalUrl && (finalUrl.startsWith('data:') || finalUrl.startsWith('blob:'))) {
                    const cloudUrl = await cloudAutosave.saveImageWithUrl(sessionId, finalUrl, shot.id);
                    if (cloudUrl) {
                        console.log(`[useStoryGeneration] Image uploaded to cloud: ${shot.id}`);
                        finalUrl = cloudUrl;
                    }
                }
                const existingIdx = updatedShotlist.findIndex(s => s.id === shot.id);
                const shotlistEntry: ShotlistEntry = {
                    id: shot.id,
                    sceneId: shot.sceneId,
                    shotNumber: shot.shotNumber,
                    description: shot.description,
                    cameraAngle: shot.cameraAngle,
                    movement: shot.movement,
                    lighting: shot.lighting,
                    dialogue: '',
                    imageUrl: finalUrl,
                };
                if (existingIdx >= 0) {
                    updatedShotlist[existingIdx] = shotlistEntry;
                } else {
                    updatedShotlist.push(shotlistEntry);
                }
            };

            // --- Resume logic: build storyboard status and filter already-done shots ---
            const alreadyDoneShots = state.shotlist.filter(s => s.imageUrl);
            const alreadyDoneStatus: Record<string, 'pending' | 'success' | 'failed'> = {};
            alreadyDoneShots.forEach(s => { alreadyDoneStatus[s.id] = 'success'; });
            const storyboardStatus: Record<string, 'pending' | 'success' | 'failed'> = {
                ...(state.storyboardStatus || {}),
                ...alreadyDoneStatus,
            };
            // Only process shots that don't already have an imageUrl
            const shotsNeedingVisuals = shotsToProcess.filter(s => !s?.imageUrl);

            if (state.imageProvider === 'deapi') {
                // Sequential-first for shot #1 (extract style), then parallel for rest
                const validShots = shotsNeedingVisuals.filter((s): s is NonNullable<typeof s> => s != null);

                // Generate shot #1 first to extract visual style (Issue 3)
                if (validShots.length > 0) {
                    const firstShot = validShots[0]!;
                    setProgress({ message: 'Generating reference image (shot 1)...', percent: 12 });
                    const firstSeed = getShotSeed(firstShot);
                    const firstResult = await generateImageWithAspectRatio(
                        buildShotPrompt(firstShot),
                        (state.aspectRatio || '16:9') as '16:9' | '9:16' | '1:1',
                        'Flux_2_Klein_4B_BF16',
                        undefined, // negativePrompt — style guide handles avoid
                    );
                    await processShotResult(firstShot, firstResult);

                    // Extract visual DNA from first image for consistency
                    try {
                        const visualStyle = await extractVisualStyle(firstResult, sessionId || undefined);
                        extractedStyleOverride = {
                            colorPalette: visualStyle.colorPalette,
                            lighting: visualStyle.lighting,
                            texture: visualStyle.texture,
                            moodKeywords: visualStyle.moodKeywords,
                            negativePrompts: generateNegativePromptsForStyle(style),
                        };
                        // Store master style on state for persistence
                        console.log('[useStoryGeneration] Extracted master style:', extractedStyleOverride.colorPalette?.join(', '));
                    } catch (styleErr) {
                        console.warn('[useStoryGeneration] Style extraction failed, continuing without:', styleErr);
                    }
                }

                // Generate remaining shots in parallel with extracted style
                const remainingShots = validShots.slice(1);
                if (remainingShots.length > 0) {
                    const batchItems = remainingShots.map((shot) => ({
                        id: shot.id,
                        prompt: buildShotPrompt(shot), // Now includes extractedStyleOverride
                        aspectRatio: (state.aspectRatio || '16:9') as '16:9' | '9:16' | '1:1',
                        model: 'Flux_2_Klein_4B_BF16' as const,
                        seed: getShotSeed(shot),
                    }));

                    const batchResults = await generateImageBatch(
                        batchItems,
                        5, // concurrency
                        (prog) => {
                            const percent = 20 + (prog.completed / prog.total) * 70;
                            setProgress({
                                message: `Generating visuals ${prog.completed + 1}/${validShots.length} (parallel)...`,
                                percent,
                            });
                        },
                    );

                    for (const result of batchResults) {
                        const shot = remainingShots.find(s => s.id === result.id);
                        if (!shot) continue;
                        if (result.success && result.imageUrl) {
                            await processShotResult(shot, result.imageUrl);
                        } else {
                            console.error(`Failed to generate visual for shot ${shot.shotNumber}:`, result.error);
                        }
                    }
                }
            } else {
                // Parallel generation via Gemini Imagen using ParallelExecutionEngine
                const { ParallelExecutionEngine } = await import('@/services/parallelExecutionEngine');
                const engine = new ParallelExecutionEngine();

                // If we have existing images, extract style from the first one for consistency
                const firstExistingImage = state.shotlist.find(s => s.imageUrl)?.imageUrl;
                if (firstExistingImage && !extractedStyleOverride) {
                    try {
                        const visualStyleData = await extractVisualStyle(firstExistingImage, sessionId || undefined);
                        extractedStyleOverride = {
                            colorPalette: visualStyleData.colorPalette,
                            lighting: visualStyleData.lighting,
                            texture: visualStyleData.texture,
                            moodKeywords: visualStyleData.moodKeywords,
                            negativePrompts: generateNegativePromptsForStyle(style),
                        };
                    } catch (e) { /* non-fatal */ }
                }

                // Style extraction guard — only the first completed task triggers it
                let styleExtractionDone = !!extractedStyleOverride;

                const tasks = shotsNeedingVisuals
                    .filter((s): s is NonNullable<typeof s> => s != null)
                    .map((shot, idx) => ({
                        id: shot.id,
                        type: 'visual' as const,
                        priority: shot.shotNumber,
                        retryable: true,
                        timeout: 90_000,
                        execute: async () => {
                            const prompt = buildShotPrompt(shot);
                            const imageUrl = await generateImageFromPrompt(
                                prompt,
                                style,
                                '',
                                state.aspectRatio || '16:9',
                                false,
                                undefined,
                                sessionId || undefined,
                                idx
                            );
                            return { shotId: shot.id, imageUrl };
                        },
                    }));

                let completedCount = alreadyDoneShots.length;
                const geminiResults = await engine.execute(tasks, {
                    concurrencyLimit: 4,
                    retryAttempts: 2,
                    retryDelay: 3000,
                    exponentialBackoff: true,
                    onProgress: (p) => {
                        completedCount = alreadyDoneShots.length + p.completedTasks;
                        setProgress({
                            message: `Generating visual ${completedCount + 1}/${shotsToProcess.length}...`,
                            percent: 10 + (completedCount / shotsToProcess.length) * 80,
                        });
                    },
                    onTaskFail: (taskId, error) => {
                        console.error(`[generateVisuals] Shot ${taskId} failed:`, error.message);
                        storyboardStatus[taskId] = 'failed';
                    },
                    onTaskComplete: (taskId) => {
                        storyboardStatus[taskId] = 'success';
                    },
                });

                // Process results in post-execution loop (cloud upload + shotlist mutation NOT inside execute())
                for (const result of geminiResults) {
                    if (!result.success || !result.data) continue;
                    const shot = shotsNeedingVisuals.find(s => s?.id === result.taskId);
                    if (!shot) continue;
                    await processShotResult(shot, result.data.imageUrl);
                    storyboardStatus[result.taskId] = 'success';

                    // Extract visual style from first successfully generated image
                    if (!styleExtractionDone) {
                        styleExtractionDone = true;
                        try {
                            const vs = await extractVisualStyle(result.data.imageUrl, sessionId || undefined);
                            extractedStyleOverride = {
                                colorPalette: vs.colorPalette,
                                lighting: vs.lighting,
                                texture: vs.texture,
                                moodKeywords: vs.moodKeywords,
                                negativePrompts: generateNegativePromptsForStyle(style),
                            };
                        } catch (styleErr) {
                            console.warn('[useStoryGeneration] Style extraction failed:', styleErr);
                        }
                    }
                }
            }

            setProgress({ message: 'Finalizing...', percent: 95 });

            // Update scenes with visuals tracking
            const newScenesWithVisuals = isPerScene && targetSceneId
                ? [...(state.scenesWithVisuals || []), targetSceneId].filter((v, i, a) => a.indexOf(v) === i)
                : state.breakdown.map(s => s.id);

            pushState({
                ...state,
                shotlist: updatedShotlist,
                currentStep: 'storyboard',
                scenesWithVisuals: newScenesWithVisuals,
                storyboardStatus,
            });

            setProgress({ message: 'Complete!', percent: 100 });
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, [state, sessionId, pushState]);

    /**
     * Regenerate visual for a single shot (Storyboarder.ai-style per-shot refresh)
     * Allows users to regenerate any individual shot without affecting others.
     * 
     * @param shotId - The ID of the shot to regenerate
     * @param customPrompt - Optional custom prompt override (for user edits)
     */
    const regenerateShotVisual = useCallback(async (shotId: string, customPrompt?: string) => {
        const shot = state.shots?.find(s => s.id === shotId);
        const existingEntry = state.shotlist.find(s => s.id === shotId);

        if (!shot && !existingEntry) {
            setError(`Shot ${shotId} not found`);
            return;
        }

        setIsProcessing(true);
        setError(null);
        setProgress({ message: `Regenerating shot...`, percent: 20 });

        try {
            const { generateImageFromPrompt } = await import('@/services/imageService');
            const style = state.visualStyle || 'Cinematic';

            // Build prompt from shot details or use custom prompt
            const baseDescription = customPrompt || shot?.description || existingEntry?.description || '';
            const shotType = shot?.shotType || existingEntry?.cameraAngle || 'Medium';
            const cameraAngle = shot?.cameraAngle || 'Eye-level';
            const lighting = shot?.lighting || existingEntry?.lighting || 'Natural';
            const emotion = shot?.emotion || 'neutral';

            setProgress({ message: 'Generating new image...', percent: 50 });

            // Build a structured style guide for consistent prompts across providers
            const { buildImageStyleGuide, serializeStyleGuideAsText } = await import('@/services/prompt/imageStyleGuide');
            const regenGenreToPurpose: Record<string, VideoPurpose> = {
                'Drama': 'story_drama',
                'Comedy': 'story_comedy',
                'Thriller': 'story_thriller',
                'Sci-Fi': 'story_scifi',
                'Action': 'story_action',
                'Fantasy': 'story_fantasy',
                'Romance': 'story_romance',
                'Historical': 'story_historical',
                'Animation': 'story_animation',
            };
            const regenPurpose: VideoPurpose = regenGenreToPurpose[state.genre || ''] ?? 'storytelling';
            const regenPersona = getSystemPersona(regenPurpose);
            const guide = buildImageStyleGuide({
                scene: customPrompt || baseDescription,
                style,
                mood: emotion,
                composition: { shot_type: shotType, camera_angle: cameraAngle, framing: 'rule of thirds' },
                lighting: { source: lighting, quality: 'natural' },
                personaNegatives: regenPersona.negative_constraints,
            });

            let imageUrl: string;
            if (state.imageProvider === 'deapi') {
                const guidePrompt = serializeStyleGuideAsText(guide);
                const negativePrompt = guide.avoid.map(item => `no ${item}`).join(', ');
                imageUrl = await generateImageWithAspectRatio(
                    guidePrompt,
                    (state.aspectRatio || '16:9') as '16:9' | '9:16' | '1:1',
                    'Flux_2_Klein_4B_BF16',
                    negativePrompt,
                );
            } else {
                imageUrl = await generateImageFromPrompt(
                    baseDescription,
                    style,
                    '',
                    state.aspectRatio || '16:9',
                    true,            // skipRefine — guide is already complete
                    undefined,       // New seed for variation
                    sessionId || undefined,
                    undefined,
                    guide,           // prebuiltGuide
                );
            }

            // Upload to cloud storage for persistence
            if (sessionId && imageUrl && (imageUrl.startsWith('data:') || imageUrl.startsWith('blob:'))) {
                const cloudUrl = await cloudAutosave.saveImageWithUrl(sessionId, imageUrl, shotId);
                if (cloudUrl) {
                    console.log(`[useStoryGeneration] Regenerated image uploaded to cloud: ${shotId}`);
                    imageUrl = cloudUrl;
                }
            }

            setProgress({ message: 'Updating storyboard...', percent: 90 });

            // Update the shotlist with new image
            const updatedShotlist = state.shotlist.map(entry =>
                entry.id === shotId
                    ? { ...entry, imageUrl, description: customPrompt || entry.description }
                    : entry
            );

            // If shot wasn't in shotlist yet, add it
            if (!state.shotlist.find(s => s.id === shotId) && shot) {
                updatedShotlist.push({
                    id: shot.id,
                    sceneId: shot.sceneId,
                    shotNumber: shot.shotNumber,
                    description: customPrompt || shot.description,
                    cameraAngle: shot.cameraAngle,
                    movement: shot.movement,
                    lighting: shot.lighting,
                    dialogue: '',
                    imageUrl,
                });
            }

            pushState({
                ...state,
                shotlist: updatedShotlist,
            });

            setProgress({ message: 'Shot regenerated!', percent: 100 });
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, [state, sessionId, pushState]);

    /**
     * Check if all scenes have shots generated
     */
    const allScenesHaveShots = useCallback(() => {
        if (!state.scenesWithShots) return false;
        return state.breakdown.every(s => state.scenesWithShots?.includes(s.id));
    }, [state.breakdown, state.scenesWithShots]);

    /**
     * Check if all scenes have visuals generated
     */
    const allScenesHaveVisuals = useCallback(() => {
        if (!state.scenesWithVisuals) return false;
        return state.breakdown.every(s => state.scenesWithVisuals?.includes(s.id));
    }, [state.breakdown, state.scenesWithVisuals]);

    /**
     * Get progress info for current stage
     */
    const getStageProgress = useCallback(() => {
        const totalScenes = state.breakdown.length;
        const scenesWithShots = state.scenesWithShots?.length || 0;
        const scenesWithVisuals = state.scenesWithVisuals?.length || 0;

        return {
            totalScenes,
            scenesWithShots,
            scenesWithVisuals,
            shotsComplete: scenesWithShots >= totalScenes,
            visualsComplete: scenesWithVisuals >= totalScenes,
        };
    }, [state.breakdown, state.scenesWithShots, state.scenesWithVisuals]);

    /**
     * Clear any error state
     */
    const clearError = useCallback(() => {
        setError(null);
    }, []);

    /**
     * Check if we have a recovered session (from page refresh)
     */
    const hasRecoveredSession = useCallback(() => {
        return state.currentStep !== 'idea' && state.breakdown.length > 0;
    }, [state.currentStep, state.breakdown.length]);

    /**
     * Retry the last failed operation based on current step
     */
    const retryLastOperation = useCallback(async () => {
        clearError();
        switch (state.currentStep) {
            case 'breakdown':
                // Cannot retry breakdown without topic
                break;
            case 'script':
                await generateScreenplay();
                break;
            case 'characters':
                await generateCharacters();
                break;
            case 'shots':
                await generateShots();
                break;
            case 'storyboard':
                await generateVisuals();
                break;
            default:
                break;
        }
    }, [state.currentStep, clearError, generateScreenplay, generateCharacters, generateShots, generateVisuals]);

    /**
     * Step 6: Generate per-shot narration (TTS).
     * Uses narrateAllShots() for per-shot audio segments, with resume logic to skip
     * already-narrated shots. Falls back to voiceover scripts → scene action → description.
     */
    const generateNarration = useCallback(async () => {
        if (!state.shotlist || state.shotlist.length === 0) {
            setError('Visuals must be generated before creating narration');
            return;
        }

        setIsProcessing(true);
        setError(null);
        setProgress({ message: 'Rewriting scripts for voiceover...', percent: 5 });

        try {
            const screenplayScenes = state.script?.scenes || [];

            // Step A: Generate voiceover scripts from screenplay action text (delivery markers)
            const breakdownHooks = state.breakdown.map(s => {
                const action = s.action || '';
                return action.length > 100 ? action.slice(0, 100) : action;
            });
            const voiceoverMap = await generateVoiceoverScripts(screenplayScenes, breakdownHooks);

            // Detect language for voice selection
            const sampleSources = [
                state.breakdown[0]?.action || '',
                state.breakdown[0]?.heading || '',
                screenplayScenes[0]?.action || '',
                screenplayScenes[0]?.heading || '',
            ].join(' ');
            const detectedLang = detectLanguage(sampleSources);
            const narratorConfig: NarratorConfig = {
                videoPurpose: 'storytelling',
                ...(detectedLang === 'ar' ? { language: 'ar' as const } : {}),
            };

            // Resume logic: identify shots already narrated
            const existingNarrations = state.shotNarrationSegments || [];
            const narrationStatus: Record<string, 'pending' | 'success' | 'failed'> = {
                ...(state.narrationStatus || {}),
            };
            const successfulIds = new Set(
                existingNarrations.filter(s => s.audioUrl).map(s => s.shotId)
            );
            const shotsToNarrate = state.shotlist.filter(s => !successfulIds.has(s.id));

            setProgress({ message: 'Generating narration...', percent: 15 });

            // Call narrateAllShots for parallel per-shot TTS (serialized via acquireTtsSlot mutex)
            const newSegments = await narrateAllShots(
                shotsToNarrate,
                screenplayScenes,
                narratorConfig,
                (completed, total) => {
                    setProgress({
                        message: `Narrating shot ${completed}/${total}...`,
                        percent: 15 + (total > 0 ? (completed / total) * 70 : 0),
                    });
                },
                sessionId || undefined,
                state.narrationStatus,
                existingNarrations,
            );

            setProgress({ message: 'Uploading audio...', percent: 88 });

            // Process results: create object URLs, optionally upload to cloud
            const updatedShotNarrations: NonNullable<StoryState['shotNarrationSegments']> = [];
            for (const seg of newSegments) {
                let audioUrl = URL.createObjectURL(seg.audioBlob);
                if (sessionId) {
                    const cloudUrl = await cloudAutosave.saveNarrationWithUrl(
                        sessionId,
                        seg.audioBlob,
                        seg.shotId
                    );
                    if (cloudUrl) {
                        console.log(`[useStoryGeneration] Shot narration uploaded to cloud: ${seg.shotId}`);
                        audioUrl = cloudUrl;
                    }
                }
                narrationStatus[seg.shotId] = 'success';
                updatedShotNarrations.push({
                    shotId: seg.shotId,
                    sceneId: seg.sceneId,
                    audioUrl,
                    duration: seg.duration,
                    text: seg.text,
                });
            }

            // Mark failed shots
            const newShotIds = new Set(updatedShotNarrations.map(s => s.shotId));
            for (const shot of shotsToNarrate) {
                if (!newShotIds.has(shot.id)) {
                    narrationStatus[shot.id] = 'failed';
                }
            }

            // Merge with existing (keep previously narrated, replace updated ones)
            const allShotNarrations: NonNullable<StoryState['shotNarrationSegments']> = [
                ...existingNarrations.filter(n => !newShotIds.has(n.shotId)),
                ...updatedShotNarrations,
            ];

            // Build legacy scene-level narrationSegments for backward compat with animateShots()
            const sceneNarrationMap = new Map<string, { audioUrl: string; duration: number; text: string }>();
            for (const seg of allShotNarrations) {
                const existing = sceneNarrationMap.get(seg.sceneId);
                if (existing) {
                    existing.duration += seg.duration;
                    existing.text += ' ' + seg.text;
                } else {
                    sceneNarrationMap.set(seg.sceneId, {
                        audioUrl: seg.audioUrl, // Use first shot's audio for legacy compat
                        duration: seg.duration,
                        text: seg.text,
                    });
                }
            }
            const narrationSegments: NonNullable<StoryState['narrationSegments']> = Array.from(
                sceneNarrationMap.entries()
            ).map(([sceneId, data]) => ({ sceneId, ...data }));

            setProgress({ message: 'Finalizing narration...', percent: 95 });

            pushState({
                ...state,
                shotNarrationSegments: allShotNarrations,
                narrationStatus,
                narrationSegments, // Legacy: for animateShots() fallback
                scenesWithNarration: [...new Set(allShotNarrations.map(n => n.sceneId))],
                currentStep: 'narration',
            });

            setProgress({ message: 'Narration complete!', percent: 100 });
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, [state, sessionId, pushState]);

    /**
     * Step 7: Animate shots using Veo or DeAPI
     * Converts static storyboard images into animated video clips
     */
    const animateShots = useCallback(async (shotIndex?: number) => {
        if (!state.shotlist || state.shotlist.length === 0) {
            setError('Visuals must be generated before animation');
            return;
        }

        setIsProcessing(true);
        setError(null);

        const isPerShot = shotIndex !== undefined;
        const shotsToAnimate = isPerShot
            ? [state.shotlist[shotIndex]].filter(Boolean)
            : state.shotlist.filter(s => s.imageUrl);

        if (shotsToAnimate.length === 0) {
            setError('No shots with images to animate');
            setIsProcessing(false);
            return;
        }

        setProgress({
            message: isPerShot
                ? `Animating shot ${shotIndex + 1}...`
                : 'Animating storyboard shots...',
            percent: 10
        });

        try {
            const animatedShots: StoryState['animatedShots'] = [
                ...(state.animatedShots || [])
            ];

            const useDeApi = isDeApiConfigured();

            // Pre-compute per-shot target durations from narration
            const shotTargetDurations = new Map<string, number>();
            if (state.shotNarrationSegments && state.shotNarrationSegments.length > 0) {
                // Exact per-shot durations from per-shot narration (new system)
                state.shotNarrationSegments.forEach(seg => {
                    shotTargetDurations.set(seg.shotId, seg.duration);
                });
            } else if (state.narrationSegments && state.narrationSegments.length > 0) {
                // Legacy fallback: divide scene duration evenly across shots in that scene
                const sceneIds = [...new Set(state.shotlist.map(s => s.sceneId))];
                for (const sceneId of sceneIds) {
                    const sceneShotIds = state.shotlist.filter(s => s.sceneId === sceneId).map(s => s.id);
                    const sceneNarration = state.narrationSegments.find(n => n.sceneId === sceneId);
                    const sceneDur = sceneNarration?.duration || 5;
                    const perShot = sceneDur / Math.max(sceneShotIds.length, 1);
                    for (const sid of sceneShotIds) {
                        shotTargetDurations.set(sid, perShot);
                    }
                }
            }

            for (let i = 0; i < shotsToAnimate.length; i++) {
                const shot = shotsToAnimate[i];
                if (!shot || !shot.imageUrl) continue;

                const percent = 10 + ((i + 1) / shotsToAnimate.length) * 80;
                setProgress({
                    message: `Animating shot ${i + 1}/${shotsToAnimate.length}...`,
                    percent
                });

                try {
                    let videoUrl: string;

                    // Normalize aspect ratio to valid values
                    const aspectRatio = (state.aspectRatio === '9:16' ? '9:16' : '16:9') as '16:9' | '9:16';
                    const deapiAspectRatio = (state.aspectRatio === '9:16' ? '9:16' : state.aspectRatio === '1:1' ? '1:1' : '16:9') as '16:9' | '9:16' | '1:1';

                    // Get the StoryShot data for motion strength selection (Issue 4)
                    const storyShot = state.shots?.find(s => s.id === shot.id);
                    const shotType = storyShot?.shotType || '';
                    const movement = storyShot?.movement || shot.movement || 'Static';

                    // Auto-select motion strength based on shot type (Issue 4)
                    const motionStrength = selectMotionStrength(shotType, movement);
                    const motionConfig = MOTION_CONFIGS[motionStrength];

                    // Build camera-focused animation prompt (Issue 4)
                    const animationPrompt = buildAnimationPrompt(movement, shot.description);

                    if (useDeApi && shot.imageUrl) {
                        // img2video requires a data: URL — convert remote URLs
                        let imageDataUrl = shot.imageUrl;
                        if (!imageDataUrl.startsWith('data:')) {
                            try {
                                const resp = await fetch(imageDataUrl);
                                const blob = await resp.blob();
                                imageDataUrl = await new Promise<string>((resolve, reject) => {
                                    const reader = new FileReader();
                                    reader.onloadend = () => resolve(reader.result as string);
                                    reader.onerror = reject;
                                    reader.readAsDataURL(blob);
                                });
                            } catch (fetchErr) {
                                console.warn(`[useStoryGeneration] Failed to fetch image for img2video, falling back to txt2video:`, fetchErr);
                                imageDataUrl = '';
                            }
                        }

                        if (imageDataUrl.startsWith('data:')) {
                            videoUrl = await animateImageWithDeApi(
                                imageDataUrl,
                                animationPrompt,
                                deapiAspectRatio,
                                sessionId || undefined,
                                i,
                                { motionStrength },
                            );
                        } else {
                            // Fallback to txt2video only if image conversion failed
                            videoUrl = await generateVideoWithDeApi(
                                {
                                    prompt: animationPrompt,
                                    frames: motionConfig.frames,
                                },
                                deapiAspectRatio,
                                sessionId || undefined,
                                i
                            );
                        }
                    } else {
                        videoUrl = await generateVideoFromPrompt(
                            animationPrompt,
                            state.visualStyle || 'Cinematic',
                            '',
                            aspectRatio,
                            6,
                            true,
                            undefined,
                            sessionId || undefined,
                            i
                        );
                    }

                    // Upload to cloud storage for persistence
                    if (sessionId && videoUrl && (videoUrl.startsWith('data:') || videoUrl.startsWith('blob:'))) {
                        const cloudUrl = await cloudAutosave.saveAnimatedVideoWithUrl(sessionId, videoUrl, shot.id);
                        if (cloudUrl) {
                            console.log(`[useStoryGeneration] Animated video uploaded to cloud: ${shot.id}`);
                            videoUrl = cloudUrl;
                        }
                    }

                    // Store with target duration from narration (Issue 6)
                    const targetDuration = shotTargetDurations.get(shot.id) || motionConfig.frames / 30;
                    const existingIdx = animatedShots.findIndex(a => a.shotId === shot.id);
                    const animatedShot = {
                        shotId: shot.id,
                        videoUrl,
                        duration: targetDuration,
                    };

                    if (existingIdx >= 0) {
                        animatedShots[existingIdx] = animatedShot;
                    } else {
                        animatedShots.push(animatedShot);
                    }
                } catch (err) {
                    console.error(`Failed to animate shot ${shot.id}:`, err);
                }
            }

            setProgress({ message: 'Finalizing animations...', percent: 95 });

            pushState({
                ...state,
                animatedShots,
                shotsWithAnimation: animatedShots.map(s => s.shotId),
                currentStep: 'animation',
            });

            setProgress({ message: 'Animation complete!', percent: 100 });
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
        } finally {
            setIsProcessing(false);
        }
    }, [state, sessionId, pushState]);

    /**
     * Step 8: Export final video using FFmpeg
     * Combines narration audio, animated shots, and renders final MP4
     */
    const exportFinalVideo = useCallback(async () => {
        if (!state.narrationSegments || state.narrationSegments.length === 0) {
            setError('Narration must be generated before export');
            return;
        }

        if (!state.animatedShots || state.animatedShots.length === 0) {
            // Fall back to static images if no animations
            console.warn('[useStoryGeneration] No animations, will use static images for export');
        }

        setIsProcessing(true);
        setError(null);
        setProgress({ message: 'Preparing video export...', percent: 5 });

        try {
            // Build SongData structure for FFmpeg exporter
            // Safely get narration segments (already validated above)
            const narrationSegs = state.narrationSegments || [];

            // Calculate total duration with fallback for undefined/NaN values
            const totalDuration = narrationSegs.reduce((sum, s) => sum + (s.duration || 0), 0);

            console.log('[useStoryGeneration] Export stats:', {
                narrationSegments: narrationSegs.length,
                totalDuration,
                shotlistLength: state.shotlist.length,
                segmentDurations: narrationSegs.map(s => s.duration),
            });

            // Combine all narration audio segments into a single audio track
            let combinedAudioUrl: string;

            try {
                combinedAudioUrl = await createCombinedNarrationAudio(
                    narrationSegs,
                    (message, percent) => setProgress({ message, percent })
                );
                console.log('[useStoryGeneration] Combined', narrationSegs.length, 'narration segments');
            } catch (audioErr) {
                console.warn('[useStoryGeneration] Failed to combine audio, using first segment:', audioErr);
                combinedAudioUrl = narrationSegs[0]?.audioUrl || '';
            }

            // Narration-aware timestamps: distribute shots proportionally across scenes
            // Each scene's narration duration determines its shots' time slots
            const effectiveDuration = totalDuration > 0 ? totalDuration : state.shotlist.length * 5;

            // Build scene → shots mapping and scene → narration duration mapping
            const sceneIds = [...new Set(state.shotlist.map(s => s.sceneId))];
            const prompts: Array<{
                id: string;
                text: string;
                mood: string;
                timestamp: string;
                timestampSeconds: number;
            }> = [];
            let accumulatedTime = 0;

            for (const sceneId of sceneIds) {
                const sceneShotlist = state.shotlist.filter(s => s.sceneId === sceneId);
                // Find narration segment for this scene
                const sceneNarration = narrationSegs.find(n => n.sceneId === sceneId);
                const sceneDuration = sceneNarration?.duration || (effectiveDuration / sceneIds.length);
                const perShotDuration = sceneDuration / Math.max(sceneShotlist.length, 1);

                for (let i = 0; i < sceneShotlist.length; i++) {
                    const shot = sceneShotlist[i]!;
                    const shotTimestamp = accumulatedTime + i * perShotDuration;
                    prompts.push({
                        id: shot.id,
                        text: shot.description,
                        mood: 'cinematic',
                        timestamp: `${Math.floor(shotTimestamp / 60)}:${Math.floor(shotTimestamp % 60).toString().padStart(2, '0')}`,
                        timestampSeconds: shotTimestamp,
                    });
                }
                accumulatedTime += sceneDuration;
            }

            console.log('[useStoryGeneration] Narration-aware shot timing:', {
                effectiveDuration,
                sceneCount: sceneIds.length,
                totalShots: state.shotlist.length,
                firstTs: prompts[0]?.timestampSeconds,
                lastTs: prompts[prompts.length - 1]?.timestampSeconds,
            });

            // Build generatedImages array
            const generatedImages = state.shotlist.map((shot) => {
                const animated = state.animatedShots?.find(a => a.shotId === shot.id);
                return {
                    promptId: shot.id,
                    imageUrl: animated?.videoUrl || shot.imageUrl || '',
                    type: (animated ? 'video' : 'image') as 'video' | 'image',
                };
            });

            // Validate generatedImages have URLs
            const validImages = generatedImages.filter(g => g.imageUrl);
            console.log('[useStoryGeneration] Generated images:', {
                total: generatedImages.length,
                withUrl: validImages.length,
                sample: generatedImages.slice(0, 3).map(g => ({ id: g.promptId, hasUrl: !!g.imageUrl, type: g.type })),
            });

            // Build subtitle items from narration using textSanitizer service
            const parsedSubtitles: { id: string; text: string; startTime: number; endTime: number }[] = [];

            for (let idx = 0; idx < narrationSegs.length; idx++) {
                const seg = narrationSegs[idx]!;
                const segStart = narrationSegs.slice(0, idx).reduce((sum, s) => sum + (s.duration || 0), 0);
                const segDuration = seg.duration || 0;

                const { chunks, minDisplayTime } = cleanForSubtitles(seg.text || '', 80);
                if (chunks.length === 0) continue;

                // Distribute segment duration across chunks, respecting minimum display time
                const rawChunkDuration = chunks.length > 0 ? segDuration / chunks.length : segDuration;
                const chunkDuration = Math.max(rawChunkDuration, minDisplayTime);

                for (let c = 0; c < chunks.length; c++) {
                    parsedSubtitles.push({
                        id: `sub_${idx}_${c}`,
                        text: chunks[c]!,
                        startTime: segStart + c * chunkDuration,
                        endTime: segStart + (c + 1) * chunkDuration,
                    });
                }
            }

            setProgress({ message: 'Building video timeline...', percent: 20 });

            // Build SongData structure matching the expected interface
            const songData = {
                fileName: `${state.script?.title || 'story'}.mp4`,
                audioUrl: combinedAudioUrl,
                srtContent: '', // Not used for story mode
                parsedSubtitles,
                prompts,
                generatedImages,
                durationSeconds: effectiveDuration,
            };

            console.log('[useStoryGeneration] SongData built:', {
                promptCount: prompts.length,
                generatedImagesCount: generatedImages.length,
                durationSeconds: effectiveDuration,
                firstPromptTs: prompts[0]?.timestampSeconds,
                lastPromptTs: prompts[prompts.length - 1]?.timestampSeconds,
            });

            const exportConfig = {
                orientation: (state.aspectRatio === '9:16' ? 'portrait' : 'landscape') as 'portrait' | 'landscape',
                subtitlePosition: 'bottom' as const,
                subtitleSize: 'medium' as const,
                contentMode: 'story' as const,
            };

            const exportResult = await exportVideoWithFFmpeg(
                songData as any,
                (progress) => {
                    setProgress({
                        message: progress.message || 'Rendering video...',
                        percent: progress.progress || 50
                    });
                },
                exportConfig,
                { cloudSessionId: sessionId || undefined }
            );

            // Create URL for the final video (use cloud URL if available, otherwise create blob URL)
            const finalVideoUrl = exportResult.cloudUrl || URL.createObjectURL(exportResult.blob);

            pushState({
                ...state,
                finalVideoUrl,
                currentStep: 'export',
            });

            setProgress({ message: 'Export complete!', percent: 100 });

            // Return the blob for download
            return exportResult.blob;
        } catch (err) {
            setError(err instanceof Error ? err.message : String(err));
            return null;
        } finally {
            setIsProcessing(false);
        }
    }, [state, sessionId, pushState]);

    /**
     * Download the final exported video
     */
    const downloadVideo = useCallback(() => {
        if (!state.finalVideoUrl) {
            setError('No video to download. Please export first.');
            return;
        }

        const a = document.createElement('a');
        a.href = state.finalVideoUrl;
        a.download = `${state.script?.title || 'story'}_video.mp4`;
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
    }, [state.finalVideoUrl, state.script?.title]);

    /**
     * Check if all scenes have narration
     */
    const allScenesHaveNarration = useCallback(() => {
        if (!state.scenesWithNarration) return false;
        return state.breakdown.every(s => state.scenesWithNarration?.includes(s.id));
    }, [state.breakdown, state.scenesWithNarration]);

    /**
     * Check if all shots have animation
     */
    const allShotsHaveAnimation = useCallback(() => {
        if (!state.shotsWithAnimation || !state.shotlist) return false;
        return state.shotlist.every(s => state.shotsWithAnimation?.includes(s.id));
    }, [state.shotlist, state.shotsWithAnimation]);

    /**
     * Load a story from Firestore by session ID
     */
    const loadFromCloud = useCallback(async (cloudSessionId: string): Promise<boolean> => {
        setIsProcessing(true);
        setProgress({ message: 'Loading story from cloud...', percent: 50 });

        try {
            const cloudState = await loadStoryFromCloud(cloudSessionId);
            if (!cloudState) {
                setError('Story not found in cloud');
                return false;
            }

            // Initialize cloud storage for media
            setSessionId(cloudSessionId);
            await cloudAutosave.initSession(cloudSessionId);

            // Apply the loaded state
            pushState(cloudState);
            setProgress({ message: 'Story loaded', percent: 100 });
            console.log(`[useStoryGeneration] Loaded story ${cloudSessionId} from cloud`);
            return true;
        } catch (err) {
            console.error('[useStoryGeneration] Failed to load from cloud:', err);
            setError('Failed to load story from cloud');
            return false;
        } finally {
            setIsProcessing(false);
        }
    }, [pushState]);

    /**
     * Save current state to cloud immediately (flush pending debounced saves)
     */
    const saveToCloud = useCallback(async (): Promise<boolean> => {
        if (!sessionId || !isSyncAvailable()) {
            return false;
        }
        await flushPendingSave(sessionId, state, topic || undefined);
        return true;
    }, [sessionId, state, topic]);

    /**
     * Apply a template to the current story state
     */
    const applyTemplate = useCallback((templateState: Partial<StoryState>) => {
        console.log('[useStoryGeneration] Applying template:', templateState);
        pushState({
            ...state,
            ...templateState,
        });
    }, [state, pushState]);

    /**
     * Import a complete project state (e.g., from JSON file or version history)
     */
    const importProject = useCallback((importedState: StoryState) => {
        console.log('[useStoryGeneration] Importing project state');
        pushState(importedState);
    }, [pushState]);

    return {
        state,
        sessionId,
        isProcessing,
        error,
        progress,
        generateBreakdown,
        generateScreenplay,
        generateCharacters,
        generateCharacterImage,
        generateShotlist,
        verifyConsistency,
        regenerateScene,
        setStep,
        updateBreakdown,
        updateScript,
        resetStory,
        exportScreenplay,
        undo,
        redo,
        canUndo: past.length > 0,
        canRedo: future.length > 0,
        // Storyboarder.ai-style workflow functions
        lockStory,
        updateVisualStyle,
        updateAspectRatio,
        updateGenre,
        updateImageProvider,
        // New step-by-step generation methods
        generateShots,
        generateVisuals,
        regenerateShotVisual, // Storyboarder.ai-style per-shot refresh
        updateShot,           // Merge metadata edits from Shot Editor Modal
        // Narration, Animation, and Export methods
        generateNarration,
        animateShots,
        exportFinalVideo,
        downloadVideo,
        // Progress tracking helpers
        allScenesHaveShots,
        allScenesHaveVisuals,
        allScenesHaveNarration,
        allShotsHaveAnimation,
        getStageProgress,
        // Error handling
        clearError,
        retryLastOperation,
        hasRecoveredSession,
        // Cloud sync
        loadFromCloud,
        saveToCloud,
        isSyncAvailable: isSyncAvailable(),
        // Template and project management
        applyTemplate,
        importProject,
    };
}
````

## File: packages/frontend/hooks/useStoryProject.ts
````typescript
/**
 * useStoryProject Hook
 *
 * Manages UI state for the Story Workspace: tab navigation, step completion,
 * keyboard shortcuts, and auto-save. Decouples UI orchestration from the
 * StoryWorkspace component so it receives data via props/context.
 */

import { useState, useEffect, useCallback, useRef } from 'react';
import type { StoryState, StoryStep } from '@/types';

// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------

export type MainStep = 'idea' | 'breakdown' | 'storyboard';
export type StepStatus = 'completed' | 'active' | 'pending' | 'processing';

export interface UseStoryProjectOptions {
  storyState: StoryState;
  isProcessing: boolean;
  onUndo?: () => void;
  onRedo?: () => void;
  canUndo?: boolean;
  canRedo?: boolean;
  onGenerateScreenplay?: () => void;
  onGenerateCharacters?: () => void;
  onAutoSave?: (state: StoryState) => void;
}

export interface UseStoryProjectReturn {
  // Tab state
  activeMainTab: MainStep;
  setActiveMainTab: (tab: MainStep) => void;
  subTab: StoryStep;
  setSubTab: (tab: StoryStep) => void;

  // Dialog state
  showLockDialog: boolean;
  setShowLockDialog: (show: boolean) => void;
  showVersionHistory: boolean;
  setShowVersionHistory: (show: boolean) => void;

  // Helpers
  getHighLevelStep: (step: StoryStep) => MainStep;
  getStepCompletionStatus: (stepId: StoryStep) => StepStatus;
  handleTabNavigation: (tabId: StoryStep) => void;
  handleMainTabClick: (tabId: MainStep) => void;

  // Derived state
  isBreakdownProcessing: boolean;
  isStoryboardProcessing: boolean;
  currentStepIndex: number;
}

// ---------------------------------------------------------------------------
// Helpers
// ---------------------------------------------------------------------------

export function getHighLevelStep(step: StoryStep): MainStep {
  if (step === 'idea') return 'idea';
  if (['breakdown', 'script', 'characters'].includes(step)) return 'breakdown';
  return 'storyboard';
}

const AUTOSAVE_DELAY = 2000;

// ---------------------------------------------------------------------------
// Hook
// ---------------------------------------------------------------------------

export function useStoryProject({
  storyState,
  isProcessing,
  onUndo,
  onRedo,
  canUndo,
  canRedo,
  onGenerateScreenplay,
  onGenerateCharacters,
  onAutoSave,
}: UseStoryProjectOptions): UseStoryProjectReturn {
  // --- Tab state ---
  const [activeMainTab, setActiveMainTab] = useState<MainStep>(
    getHighLevelStep(storyState.currentStep),
  );
  const [subTab, setSubTab] = useState<StoryStep>(storyState.currentStep);

  // --- Dialog state ---
  const [showLockDialog, setShowLockDialog] = useState(false);
  const [showVersionHistory, setShowVersionHistory] = useState(false);

  // Sync tabs when storyState.currentStep changes externally
  useEffect(() => {
    const newMain = getHighLevelStep(storyState.currentStep);
    setActiveMainTab(newMain);
    setSubTab(storyState.currentStep);
  }, [storyState.currentStep]);

  // --- Keyboard shortcuts (Ctrl+Z / Ctrl+Y) ---
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if (!(e.ctrlKey || e.metaKey)) return;
      if (e.key === 'z') {
        if (e.shiftKey) {
          if (canRedo && onRedo) { e.preventDefault(); onRedo(); }
        } else {
          if (canUndo && onUndo) { e.preventDefault(); onUndo(); }
        }
      } else if (e.key === 'y') {
        if (canRedo && onRedo) { e.preventDefault(); onRedo(); }
      }
    };
    window.addEventListener('keydown', handleKeyDown);
    return () => window.removeEventListener('keydown', handleKeyDown);
  }, [onUndo, onRedo, canUndo, canRedo]);

  // --- Auto-save (debounced 2 s) ---
  const autoSaveTimer = useRef<ReturnType<typeof setTimeout> | null>(null);
  const prevStateRef = useRef(storyState);

  useEffect(() => {
    if (!onAutoSave) return;
    // Don't auto-save the initial "idea" step
    if (storyState.currentStep === 'idea') return;
    // Don't auto-save if nothing changed
    if (prevStateRef.current === storyState) return;
    prevStateRef.current = storyState;

    if (autoSaveTimer.current) clearTimeout(autoSaveTimer.current);
    autoSaveTimer.current = setTimeout(() => {
      onAutoSave(storyState);
    }, AUTOSAVE_DELAY);

    return () => {
      if (autoSaveTimer.current) clearTimeout(autoSaveTimer.current);
    };
  }, [storyState, onAutoSave]);

  // --- Tab navigation with auto-generation ---
  const handleTabNavigation = useCallback(
    (tabId: StoryStep) => {
      if (tabId === 'script' && !storyState.script && !isProcessing) {
        onGenerateScreenplay?.();
      } else if (
        tabId === 'characters' &&
        storyState.characters.length === 0 &&
        !isProcessing
      ) {
        onGenerateCharacters?.();
      }
      setSubTab(tabId);
    },
    [storyState.script, storyState.characters.length, isProcessing, onGenerateScreenplay, onGenerateCharacters],
  );

  const handleMainTabClick = useCallback(
    (tabId: MainStep) => {
      const stepOrder: MainStep[] = ['idea', 'breakdown', 'storyboard'];
      const currentIdx = stepOrder.indexOf(getHighLevelStep(storyState.currentStep));
      const tabIdx = stepOrder.indexOf(tabId);
      if (tabIdx > currentIdx) return; // not accessible

      setActiveMainTab(tabId);
      if (tabId === 'idea') setSubTab('idea');
      if (tabId === 'breakdown') setSubTab('breakdown');
      if (tabId === 'storyboard') setSubTab('shots');
    },
    [storyState.currentStep],
  );

  // --- Step completion status ---
  const getStepCompletionStatus = useCallback(
    (stepId: StoryStep): StepStatus => {
      const storyboardOrder: StoryStep[] = ['shots', 'style', 'storyboard', 'narration', 'animation', 'export'];
      const breakdownOrder: StoryStep[] = ['breakdown', 'script', 'characters'];
      const currentOrder = activeMainTab === 'storyboard' ? storyboardOrder : breakdownOrder;
      const currentIndex = currentOrder.indexOf(subTab);
      const stepIndex = currentOrder.indexOf(stepId);

      if (stepId === subTab) return isProcessing ? 'processing' : 'active';

      if (activeMainTab === 'storyboard') {
        switch (stepId) {
          case 'shots': return (storyState.shots?.length ?? 0) > 0 ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
          case 'style': return storyState.visualStyle ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
          case 'storyboard': return (storyState.scenesWithVisuals?.length ?? 0) > 0 ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
          case 'narration': return (storyState.narrationSegments?.length ?? 0) > 0 ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
          case 'animation': return (storyState.animatedShots?.length ?? 0) > 0 ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
          case 'export': return storyState.finalVideoUrl ? 'completed' : 'pending';
        }
      } else {
        switch (stepId) {
          case 'breakdown': return storyState.breakdown.length > 0 ? 'completed' : 'pending';
          case 'script': return storyState.script ? 'completed' : stepIndex < currentIndex ? 'completed' : 'pending';
          case 'characters': return storyState.characters.length > 0 ? 'completed' : 'pending';
        }
      }

      return stepIndex < currentIndex ? 'completed' : 'pending';
    },
    [activeMainTab, subTab, isProcessing, storyState],
  );

  // --- Derived state ---
  const isBreakdownProcessing =
    isProcessing && activeMainTab === 'breakdown' && storyState.breakdown.length === 0;
  const isStoryboardProcessing =
    isProcessing && activeMainTab === 'storyboard' && (!storyState.shots || storyState.shots.length === 0);

  const stepOrder: MainStep[] = ['idea', 'breakdown', 'storyboard'];
  const currentStepIndex = stepOrder.indexOf(getHighLevelStep(storyState.currentStep));

  return {
    activeMainTab,
    setActiveMainTab,
    subTab,
    setSubTab,
    showLockDialog,
    setShowLockDialog,
    showVersionHistory,
    setShowVersionHistory,
    getHighLevelStep,
    getStepCompletionStatus,
    handleTabNavigation,
    handleMainTabClick,
    isBreakdownProcessing,
    isStoryboardProcessing,
    currentStepIndex,
  };
}
````

## File: packages/frontend/hooks/useSunoMusic.ts
````typescript
/**
 * useSunoMusic Hook
 *
 * Manages AI music generation using the Suno API.
 * Extracted from useVideoProduction to adhere to Single Responsibility Principle.
 *
 * ROBUST PATTERNS IMPLEMENTED:
 * - Timeout protection for all polling loops
 * - AbortController support for cancellation
 * - Safe state updates that check mount status
 * - Comprehensive error handling
 */

import { useState, useCallback, useRef, useEffect } from "react";
import {
    SunoTaskStatus,
    SunoGeneratedTrack,
    SunoGenerationConfig,
    SunoExtendConfig,
    SunoUploadConfig,
    SunoPersonaConfig,
    SunoStemSeparationResult,
    isSunoConfigured,
    generateMusic as sunoGenerateMusic,
    waitForCompletion,
    generateLyrics as sunoGenerateLyrics,
    getLyricsStatus,
    getCredits,
    extendMusic as sunoExtendMusic,
    uploadAndExtend as sunoUploadAndExtend,
    generatePersona as sunoGeneratePersona,
    convertToWav as sunoConvertToWav,
    separateVocals as sunoSeparateVocals,
    waitForStemSeparation,
    createMusicVideo,
    generateCover,
    addVocals,
    addInstrumental,
    uploadAndCover,
    uploadAudioFile
} from "@/services/sunoService";

/**
 * Music generation state for Suno API integration.
 * Tracks the status of AI music generation requests.
 */
export interface MusicGenerationState {
    /** Whether a generation is currently in progress */
    isGenerating: boolean;
    /** Whether an extend operation is in progress */
    isExtending: boolean;
    /** Whether a stem separation is in progress */
    isSeparating: boolean;
    /** Whether a WAV conversion is in progress */
    isConverting: boolean;
    /** Whether a persona generation is in progress */
    isGeneratingPersona: boolean;
    /** Current task ID from Suno API */
    taskId: string | null;
    /** Current generation status */
    status: SunoTaskStatus | null;
    /** Progress percentage (0-100) */
    progress: number;
    /** Generated tracks (Suno returns 2 variations) */
    generatedTracks: SunoGeneratedTrack[];
    /** ID of the selected track */
    selectedTrackId: string | null;
    /** Generated or custom lyrics */
    lyrics: string | null;
    /** Lyrics generation task ID */
    lyricsTaskId: string | null;
    /** Remaining API credits */
    credits: number | null;
    /** Error message if generation failed */
    error: string | null;
    /** Result of stem separation (vocals and instrumental URLs) */
    stemSeparationResult: SunoStemSeparationResult | null;
    /** URL of converted WAV file */
    convertedWavUrl: string | null;
    /** Generated persona ID */
    personaId: string | null;
}

const initialMusicState: MusicGenerationState = {
    isGenerating: false,
    isExtending: false,
    isSeparating: false,
    isConverting: false,
    isGeneratingPersona: false,
    taskId: null,
    status: null,
    progress: 0,
    generatedTracks: [],
    selectedTrackId: null,
    lyrics: null,
    lyricsTaskId: null,
    credits: null,
    error: null,
    stemSeparationResult: null,
    convertedWavUrl: null,
    personaId: null,
};

// Default timeout for long-running operations (5 minutes)
const DEFAULT_TIMEOUT_MS = 5 * 60 * 1000;
// Poll interval for status checks
const POLL_INTERVAL_MS = 5000;

export function useSunoMusic() {
    const [musicState, setMusicState] = useState<MusicGenerationState>(initialMusicState);

    // Track if component is mounted
    const isMountedRef = useRef(true);

    // AbortController for cancelling operations
    const abortControllerRef = useRef<AbortController | null>(null);

    // Cleanup on unmount
    useEffect(() => {
        isMountedRef.current = true;
        return () => {
            isMountedRef.current = false;
            // Cancel any in-progress operations
            if (abortControllerRef.current) {
                abortControllerRef.current.abort();
                abortControllerRef.current = null;
            }
        };
    }, []);

    // Safe state updater
    const safeSetState = useCallback((
        updater: React.SetStateAction<MusicGenerationState>
    ) => {
        if (isMountedRef.current) {
            setMusicState(updater);
        }
    }, []);

    /**
     * Cancel any in-progress music generation operations.
     */
    const cancelGeneration = useCallback(() => {
        if (abortControllerRef.current) {
            console.log("[useSunoMusic] Cancelling in-progress operation");
            abortControllerRef.current.abort();
            abortControllerRef.current = null;
            safeSetState(prev => ({
                ...prev,
                isGenerating: false,
                isExtending: false,
                isSeparating: false,
                isConverting: false,
                isGeneratingPersona: false,
                status: null,
                progress: 0,
                error: "Operation was cancelled",
            }));
            return true;
        }
        return false;
    }, [safeSetState]);

    /**
     * Generate AI music using Suno API.
     * Includes timeout protection and cancellation support.
     */
    const generateMusic = useCallback(async (config: Partial<SunoGenerationConfig> & { prompt: string }) => {
        if (!isSunoConfigured()) {
            safeSetState(prev => ({
                ...prev,
                error: "Suno API key not configured. Add VITE_SUNO_API_KEY to .env.local",
            }));
            return;
        }

        // Cancel any existing operation
        if (abortControllerRef.current) {
            abortControllerRef.current.abort();
        }
        abortControllerRef.current = new AbortController();
        const signal = abortControllerRef.current.signal;

        safeSetState(prev => ({
            ...prev,
            isGenerating: true,
            taskId: null,
            status: "PENDING",
            progress: 0,
            generatedTracks: [],
            error: null,
        }));

        try {
            // Check for abort
            if (signal.aborted) throw new Error("Operation was cancelled");

            console.log("[useSunoMusic] Starting music generation...");
            const taskId = await sunoGenerateMusic(config);

            if (signal.aborted) throw new Error("Operation was cancelled");

            safeSetState(prev => ({
                ...prev,
                taskId,
                status: "PROCESSING",
                progress: 25,
            }));

            // Wait with timeout protection
            const startTime = Date.now();
            const tracks = await Promise.race([
                waitForCompletion(taskId),
                new Promise<never>((_, reject) => {
                    const checkAbort = setInterval(() => {
                        if (signal.aborted) {
                            clearInterval(checkAbort);
                            reject(new Error("Operation was cancelled"));
                        }
                        if (Date.now() - startTime > DEFAULT_TIMEOUT_MS) {
                            clearInterval(checkAbort);
                            reject(new Error(`Music generation timed out after ${DEFAULT_TIMEOUT_MS / 1000}s`));
                        }
                    }, 1000);
                }),
            ]);

            if (signal.aborted) throw new Error("Operation was cancelled");

            console.log(`[useSunoMusic] Music generation complete: ${tracks.length} tracks`);

            safeSetState(prev => ({
                ...prev,
                isGenerating: false,
                status: "SUCCESS",
                progress: 100,
                generatedTracks: tracks,
                selectedTrackId: tracks[0]?.id || null,
            }));

            abortControllerRef.current = null;
        } catch (err) {
            if (signal.aborted) {
                console.log("[useSunoMusic] Music generation was cancelled");
                return;
            }
            console.error("[useSunoMusic] Music generation failed:", err);
            safeSetState(prev => ({
                ...prev,
                isGenerating: false,
                status: "FAILED",
                progress: 0,
                error: err instanceof Error ? err.message : String(err),
            }));
            abortControllerRef.current = null;
        }
    }, [safeSetState]);

    /**
     * Generate lyrics using Suno API.
     * Includes timeout protection and cancellation support.
     */
    const generateLyrics = useCallback(async (prompt: string) => {
        if (!isSunoConfigured()) {
            safeSetState(prev => ({
                ...prev,
                error: "Suno API key not configured. Add VITE_SUNO_API_KEY to .env.local",
            }));
            return;
        }

        // Cancel any existing operation
        if (abortControllerRef.current) {
            abortControllerRef.current.abort();
        }
        abortControllerRef.current = new AbortController();
        const signal = abortControllerRef.current.signal;

        safeSetState(prev => ({
            ...prev,
            lyricsTaskId: null,
            lyrics: null,
            error: null,
        }));

        try {
            if (signal.aborted) throw new Error("Operation was cancelled");

            console.log("[useSunoMusic] Starting lyrics generation...");
            const taskId = await sunoGenerateLyrics(prompt);

            if (signal.aborted) throw new Error("Operation was cancelled");

            safeSetState(prev => ({
                ...prev,
                lyricsTaskId: taskId,
            }));

            const maxWaitMs = 2 * 60 * 1000;
            const startTime = Date.now();

            while (Date.now() - startTime < maxWaitMs) {
                // Check for cancellation at each iteration
                if (signal.aborted) throw new Error("Operation was cancelled");

                const result = await getLyricsStatus(taskId);

                if (result.status === "SUCCESS" && result.text) {
                    console.log("[useSunoMusic] Lyrics generation complete");
                    safeSetState(prev => ({
                        ...prev,
                        lyrics: result.text || null,
                    }));
                    abortControllerRef.current = null;
                    return;
                }

                if (result.status === "FAILED") {
                    throw new Error(result.errorMessage || "Lyrics generation failed");
                }

                // Wait with abort check
                await new Promise<void>((resolve, reject) => {
                    const timeoutId = setTimeout(resolve, POLL_INTERVAL_MS);
                    signal.addEventListener('abort', () => {
                        clearTimeout(timeoutId);
                        reject(new Error("Operation was cancelled"));
                    }, { once: true });
                });
            }

            throw new Error("Lyrics generation timed out. Please try again.");
        } catch (err) {
            if (signal.aborted) {
                console.log("[useSunoMusic] Lyrics generation was cancelled");
                return;
            }
            console.error("[useSunoMusic] Lyrics generation failed:", err);
            safeSetState(prev => ({
                ...prev,
                error: err instanceof Error ? err.message : String(err),
            }));
            abortControllerRef.current = null;
        }
    }, [safeSetState]);

    /**
     * Select a generated track for use.
     */
    const selectTrack = useCallback((trackId: string) => {
        setMusicState(prev => ({
            ...prev,
            selectedTrackId: trackId,
        }));
    }, []);

    /**
     * Refresh the Suno API credits balance.
     */
    const refreshCredits = useCallback(async () => {
        if (!isSunoConfigured()) {
            setMusicState(prev => ({
                ...prev,
                credits: null,
                error: "Suno API key not configured",
            }));
            return;
        }

        try {
            console.log("[useSunoMusic] Fetching Suno credits...");
            const result = await getCredits();

            setMusicState(prev => ({
                ...prev,
                credits: result.credits,
            }));

            console.log(`[useSunoMusic] Suno credits: ${result.credits}`);
        } catch (err) {
            console.error("[useSunoMusic] Failed to fetch credits:", err);
            setMusicState(prev => ({
                ...prev,
                credits: null,
            }));
        }
    }, []);

    /**
     * Reset music generation state.
     */
    const resetMusicState = useCallback(() => {
        setMusicState(initialMusicState);
    }, []);

    /**
     * Get the currently selected track.
     */
    const getSelectedTrack = useCallback((): SunoGeneratedTrack | null => {
        const { selectedTrackId, generatedTracks } = musicState;
        if (!selectedTrackId) return null;
        return generatedTracks.find(t => t.id === selectedTrackId) || null;
    }, [musicState]);

    /**
     * Extend an existing music track.
     * Wraps the service function with state management.
     */
    const extendMusic = useCallback(async (config: SunoExtendConfig) => {
        if (!isSunoConfigured()) {
            setMusicState(prev => ({
                ...prev,
                error: "Suno API key not configured. Add VITE_SUNO_API_KEY to .env.local",
            }));
            return;
        }

        setMusicState(prev => ({
            ...prev,
            isExtending: true,
            taskId: null,
            status: "PENDING",
            progress: 0,
            error: null,
        }));

        try {
            console.log("[useSunoMusic] Starting music extension...");
            const taskId = await sunoExtendMusic(config);

            setMusicState(prev => ({
                ...prev,
                taskId,
                status: "PROCESSING",
                progress: 25,
            }));

            const tracks = await waitForCompletion(taskId);

            console.log(`[useSunoMusic] Music extension complete: ${tracks.length} tracks`);

            setMusicState(prev => ({
                ...prev,
                isExtending: false,
                status: "SUCCESS",
                progress: 100,
                generatedTracks: [...prev.generatedTracks, ...tracks],
                selectedTrackId: tracks[0]?.id || prev.selectedTrackId,
            }));

            return tracks;
        } catch (err) {
            console.error("[useSunoMusic] Music extension failed:", err);
            setMusicState(prev => ({
                ...prev,
                isExtending: false,
                status: "FAILED",
                progress: 0,
                error: err instanceof Error ? err.message : String(err),
            }));
            return null;
        }
    }, []);

    /**
     * Upload and extend audio with new content.
     * Wraps the service function with state management.
     */
    const uploadAndExtend = useCallback(async (config: SunoUploadConfig) => {
        if (!isSunoConfigured()) {
            setMusicState(prev => ({
                ...prev,
                error: "Suno API key not configured. Add VITE_SUNO_API_KEY to .env.local",
            }));
            return null;
        }

        setMusicState(prev => ({
            ...prev,
            isExtending: true,
            taskId: null,
            status: "PENDING",
            progress: 0,
            error: null,
        }));

        try {
            console.log("[useSunoMusic] Starting upload and extend...");
            const taskId = await sunoUploadAndExtend(config);

            setMusicState(prev => ({
                ...prev,
                taskId,
                status: "PROCESSING",
                progress: 25,
            }));

            const tracks = await waitForCompletion(taskId);

            console.log(`[useSunoMusic] Upload and extend complete: ${tracks.length} tracks`);

            setMusicState(prev => ({
                ...prev,
                isExtending: false,
                status: "SUCCESS",
                progress: 100,
                generatedTracks: [...prev.generatedTracks, ...tracks],
                selectedTrackId: tracks[0]?.id || prev.selectedTrackId,
            }));

            return tracks;
        } catch (err) {
            console.error("[useSunoMusic] Upload and extend failed:", err);
            setMusicState(prev => ({
                ...prev,
                isExtending: false,
                status: "FAILED",
                progress: 0,
                error: err instanceof Error ? err.message : String(err),
            }));
            return null;
        }
    }, []);

    /**
     * Convert a generated track to WAV format.
     * Wraps the service function with state management.
     */
    const convertToWav = useCallback(async (taskId: string, audioId: string) => {
        if (!isSunoConfigured()) {
            setMusicState(prev => ({
                ...prev,
                error: "Suno API key not configured. Add VITE_SUNO_API_KEY to .env.local",
            }));
            return null;
        }

        setMusicState(prev => ({
            ...prev,
            isConverting: true,
            convertedWavUrl: null,
            error: null,
        }));

        try {
            console.log("[useSunoMusic] Starting WAV conversion...");
            const conversionTaskId = await sunoConvertToWav(taskId, audioId);

            // Poll for completion - WAV conversion uses the same task status endpoint
            const tracks = await waitForCompletion(conversionTaskId);

            // The converted WAV URL should be in the first track's audio_url
            const wavUrl = tracks.length > 0 ? tracks[0]?.audio_url || null : null;

            console.log(`[useSunoMusic] WAV conversion complete: ${wavUrl}`);

            setMusicState(prev => ({
                ...prev,
                isConverting: false,
                convertedWavUrl: wavUrl,
            }));

            return wavUrl;
        } catch (err) {
            console.error("[useSunoMusic] WAV conversion failed:", err);
            setMusicState(prev => ({
                ...prev,
                isConverting: false,
                error: err instanceof Error ? err.message : String(err),
            }));
            return null;
        }
    }, []);

    /**
     * Separate vocals from instrumental in a track.
     * Wraps the service function with state management.
     */
    const separateVocals = useCallback(async (taskId: string, audioId: string) => {
        if (!isSunoConfigured()) {
            setMusicState(prev => ({
                ...prev,
                error: "Suno API key not configured. Add VITE_SUNO_API_KEY to .env.local",
            }));
            return null;
        }

        setMusicState(prev => ({
            ...prev,
            isSeparating: true,
            stemSeparationResult: null,
            error: null,
        }));

        try {
            console.log("[useSunoMusic] Starting vocal separation...");
            const separationTaskId = await sunoSeparateVocals(taskId, audioId);

            // Wait for stem separation to complete
            const result = await waitForStemSeparation(separationTaskId);

            console.log(`[useSunoMusic] Vocal separation complete:`, result);

            setMusicState(prev => ({
                ...prev,
                isSeparating: false,
                stemSeparationResult: result,
            }));

            return result;
        } catch (err) {
            console.error("[useSunoMusic] Vocal separation failed:", err);
            setMusicState(prev => ({
                ...prev,
                isSeparating: false,
                error: err instanceof Error ? err.message : String(err),
            }));
            return null;
        }
    }, []);

    /**
     * Generate a personalized music style/persona.
     * Wraps the service function with state management.
     */
    const generatePersona = useCallback(async (config: SunoPersonaConfig) => {
        if (!isSunoConfigured()) {
            setMusicState(prev => ({
                ...prev,
                error: "Suno API key not configured. Add VITE_SUNO_API_KEY to .env.local",
            }));
            return null;
        }

        setMusicState(prev => ({
            ...prev,
            isGeneratingPersona: true,
            personaId: null,
            error: null,
        }));

        try {
            console.log("[useSunoMusic] Starting persona generation...");
            const personaTaskId = await sunoGeneratePersona(config);

            // Poll for completion - persona generation uses the same task status endpoint
            await waitForCompletion(personaTaskId);

            // The persona ID should be returned in the response
            // For now, we use the task ID as the persona ID
            const personaId = personaTaskId;

            console.log(`[useSunoMusic] Persona generation complete: ${personaId}`);

            setMusicState(prev => ({
                ...prev,
                isGeneratingPersona: false,
                personaId,
            }));

            return personaId;
        } catch (err) {
            console.error("[useSunoMusic] Persona generation failed:", err);
            setMusicState(prev => ({
                ...prev,
                isGeneratingPersona: false,
                error: err instanceof Error ? err.message : String(err),
            }));
            return null;
        }
    }, []);

    return {
        // State
        musicState,

        // Actions
        generateMusic,
        generateLyrics,
        selectTrack,
        refreshCredits,
        resetMusicState,
        cancelGeneration,

        // New Actions with State Management
        extendMusic,
        uploadAndExtend,
        convertToWav,
        separateVocals,
        generatePersona,

        // Helpers
        getSelectedTrack,

        // Advanced Actions (Static imports - no dynamic loading)
        createMusicVideo: async (taskId: string, audioId: string) => {
            return createMusicVideo(taskId, audioId);
        },
        generateCover: async (taskId: string) => {
            return generateCover(taskId);
        },
        addVocals: async (config: any) => {
            setMusicState(prev => ({
                ...prev,
                isGenerating: true,
                taskId: null,
                status: "PENDING",
                progress: 0,
                generatedTracks: [],
                error: null,
            }));

            try {
                console.log("[useSunoMusic] Starting add vocals...");
                const taskId = await addVocals(config);

                setMusicState(prev => ({
                    ...prev,
                    taskId,
                    status: "PROCESSING",
                    progress: 25,
                }));

                const tracks = await waitForCompletion(taskId);

                console.log(`[useSunoMusic] Add vocals complete: ${tracks.length} tracks`);

                setMusicState(prev => ({
                    ...prev,
                    isGenerating: false,
                    status: "SUCCESS",
                    progress: 100,
                    generatedTracks: tracks,
                    selectedTrackId: tracks[0]?.id || null,
                }));

                return taskId;
            } catch (err) {
                console.error("[useSunoMusic] Add vocals failed:", err);
                setMusicState(prev => ({
                    ...prev,
                    isGenerating: false,
                    status: "FAILED",
                    progress: 0,
                    error: err instanceof Error ? err.message : String(err),
                }));
                throw err;
            }
        },
        addInstrumental: async (config: any) => {
            setMusicState(prev => ({
                ...prev,
                isGenerating: true,
                taskId: null,
                status: "PENDING",
                progress: 0,
                generatedTracks: [],
                error: null,
            }));

            try {
                console.log("[useSunoMusic] Starting add instrumental...");
                const taskId = await addInstrumental(config);

                setMusicState(prev => ({
                    ...prev,
                    taskId,
                    status: "PROCESSING",
                    progress: 25,
                }));

                const tracks = await waitForCompletion(taskId);

                console.log(`[useSunoMusic] Add instrumental complete: ${tracks.length} tracks`);

                setMusicState(prev => ({
                    ...prev,
                    isGenerating: false,
                    status: "SUCCESS",
                    progress: 100,
                    generatedTracks: tracks,
                    selectedTrackId: tracks[0]?.id || null,
                }));

                return taskId;
            } catch (err) {
                console.error("[useSunoMusic] Add instrumental failed:", err);
                setMusicState(prev => ({
                    ...prev,
                    isGenerating: false,
                    status: "FAILED",
                    progress: 0,
                    error: err instanceof Error ? err.message : String(err),
                }));
                throw err;
            }
        },
        uploadAndCover: async (config: any) => {
            setMusicState(prev => ({
                ...prev,
                isGenerating: true,
                taskId: null,
                status: "PENDING",
                progress: 0,
                generatedTracks: [],
                error: null,
            }));

            try {
                console.log("[useSunoMusic] Starting upload and cover...");
                const taskId = await uploadAndCover(config);

                setMusicState(prev => ({
                    ...prev,
                    taskId,
                    status: "PROCESSING",
                    progress: 25,
                }));

                const tracks = await waitForCompletion(taskId);

                console.log(`[useSunoMusic] Upload and cover complete: ${tracks.length} tracks`);

                setMusicState(prev => ({
                    ...prev,
                    isGenerating: false,
                    status: "SUCCESS",
                    progress: 100,
                    generatedTracks: tracks,
                    selectedTrackId: tracks[0]?.id || null,
                }));

                return taskId;
            } catch (err) {
                console.error("[useSunoMusic] Upload and cover failed:", err);
                setMusicState(prev => ({
                    ...prev,
                    isGenerating: false,
                    status: "FAILED",
                    progress: 0,
                    error: err instanceof Error ? err.message : String(err),
                }));
                throw err;
            }
        },
        uploadAudio: async (file: File) => {
            return uploadAudioFile(file);
        }
    };
}
````

## File: packages/frontend/hooks/useTimelineAdapter.ts
````typescript
/**
 * useTimelineAdapter Hook
 *
 * Custom React hook that converts external props (Scene, NarrationSegment, VideoSFXPlan)
 * to the internal data model (Track, AudioClip, VideoClip) used by AudioTimelineEditor.
 *
 * This adapter layer enables backward compatibility, allowing the new timeline component
 * to be used as a drop-in replacement without requiring changes to the rest of the application.
 *
 * Key responsibilities:
 * 1. Convert scenes → videoClips using scenesToVideoTrack
 * 2. Convert narrationSegments → audioClips using narrationToAudioTrack
 * 3. Convert sfxPlan → tracks/clips using sfxPlanToTracks
 * 4. Sync selectedSceneId ↔ selectedClipId using clipIdToSceneId/sceneIdToClipId
 * 5. Wrap onSceneSelect to convert clip IDs back to scene IDs
 * 6. Wrap onDeleteClip similarly
 *
 * @see .kiro/specs/timeline-editor-replacement/design.md for architecture details
 * @requirements 9.1-9.6, 10.1, 10.3, 10.4
 */

import { useMemo, useCallback } from "react";
import type { Scene, NarrationSegment, VideoSFXPlan } from "@/types";
import type {
  Track,
  AudioClip,
  VideoClip,
  ImageClip,
  SubtitleCue,
} from "@/types/audio-editor";
import {
  scenesToVideoTrack,
  narrationToAudioTrack,
  sfxPlanToTracks,
  clipIdToSceneId,
  sceneIdToClipId,
} from "@/components/TimelineEditor/timelineAdapter";

/**
 * Props interface matching AudioTimelineEditorProps from design.md
 * These are the external props that the adapter converts from.
 */
export interface UseTimelineAdapterProps {
  /** Array of Scene objects from ContentPlan */
  scenes: Scene[];
  /** Map of scene IDs to thumbnail URLs */
  visuals?: Record<string, string>;
  /** Array of NarrationSegment objects */
  narrationSegments?: NarrationSegment[];
  /** Sound effects plan */
  sfxPlan?: VideoSFXPlan | null;
  /** Currently selected scene ID (external state) */
  selectedSceneId?: string | null;
  /** Callback when a scene is selected */
  onSceneSelect?: (sceneId: string) => void;
  /** Callback when a clip is deleted */
  onDeleteClip?: (clipId: string) => void;
}

/**
 * Return type for the useTimelineAdapter hook.
 * Contains the converted internal data model and wrapped callbacks.
 */
export interface UseTimelineAdapterReturn {
  /** All tracks (video, narrator, sfx, etc.) */
  tracks: Track[];
  /** Audio clips (narrator and sfx) */
  audioClips: AudioClip[];
  /** Video clips (from scenes) */
  videoClips: VideoClip[];
  /** Image clips (currently empty, for future use) */
  imageClips: ImageClip[];
  /** Subtitle cues (currently empty, for future use) */
  subtitles: SubtitleCue[];
  /** Currently selected clip ID (converted from selectedSceneId) */
  selectedClipId: string | null;
  /** Handler for clip selection - converts clip ID to scene ID and calls onSceneSelect */
  handleClipSelect: (clipId: string | null) => void;
  /** Handler for clip deletion - converts clip ID to scene ID and calls onDeleteClip */
  handleDeleteClip: (clipId: string) => void;
}

/**
 * Hook for adapting external timeline props to internal data model.
 *
 * @param props - External props from AudioTimelineEditor
 * @returns Converted internal data model and wrapped callbacks
 *
 * @example
 * ```tsx
 * const {
 *   tracks,
 *   audioClips,
 *   videoClips,
 *   selectedClipId,
 *   handleClipSelect,
 *   handleDeleteClip,
 * } = useTimelineAdapter({
 *   scenes,
 *   visuals,
 *   narrationSegments,
 *   sfxPlan,
 *   selectedSceneId,
 *   onSceneSelect,
 *   onDeleteClip,
 * });
 * ```
 *
 * @requirements 9.1-9.6, 10.1, 10.3, 10.4
 */
export function useTimelineAdapter(
  props: UseTimelineAdapterProps
): UseTimelineAdapterReturn {
  const {
    scenes,
    visuals = {},
    narrationSegments = [],
    sfxPlan = null,
    selectedSceneId = null,
    onSceneSelect,
    onDeleteClip,
  } = props;

  /**
   * Convert scenes to video track and clips.
   * Memoized to prevent unnecessary recalculations.
   *
   * @requirements 9.1, 9.5
   * @validates Requirements 1.1, 1.2, 1.3, 1.4
   */
  const { track: videoTrack, clips: videoClips } = useMemo(
    () => scenesToVideoTrack(scenes, visuals),
    [scenes, visuals]
  );

  /**
   * Convert narration segments to narrator track and audio clips.
   * Memoized to prevent unnecessary recalculations.
   *
   * @requirements 9.2, 9.5
   * @validates Requirements 1.5
   */
  const { track: narratorTrack, clips: narratorClips } = useMemo(
    () => narrationToAudioTrack(narrationSegments, scenes),
    [narrationSegments, scenes]
  );

  /**
   * Convert SFX plan to SFX tracks and audio clips.
   * Memoized to prevent unnecessary recalculations.
   *
   * @requirements 9.3, 9.5
   * @validates Requirements 1.6
   */
  const { tracks: sfxTracks, clips: sfxClips } = useMemo(
    () => sfxPlanToTracks(sfxPlan, scenes, narrationSegments),
    [sfxPlan, scenes, narrationSegments]
  );

  /**
   * Combine all tracks into a single array.
   * Order: video, narrator, sfx tracks
   */
  const tracks = useMemo<Track[]>(
    () => [videoTrack, narratorTrack, ...sfxTracks],
    [videoTrack, narratorTrack, sfxTracks]
  );

  /**
   * Combine all audio clips (narrator + sfx).
   */
  const audioClips = useMemo<AudioClip[]>(
    () => [...narratorClips, ...sfxClips],
    [narratorClips, sfxClips]
  );

  /**
   * Image clips - currently empty, for future media import functionality.
   * @requirements 9.1
   */
  const imageClips = useMemo<ImageClip[]>(() => [], []);

  /**
   * Subtitle cues - currently empty, for future subtitle import functionality.
   * @requirements 9.3
   */
  const subtitles = useMemo<SubtitleCue[]>(() => [], []);

  /**
   * Convert selectedSceneId to selectedClipId.
   * Video clips use scene ID directly, so we can use it as-is.
   *
   * @requirements 9.4, 10.4
   * @validates Requirements 3.5
   */
  const selectedClipId = useMemo<string | null>(() => {
    if (!selectedSceneId) {
      return null;
    }
    // Video clips use scene ID directly (no prefix)
    // This allows selection to work with video track clips
    return sceneIdToClipId(selectedSceneId, "video");
  }, [selectedSceneId]);

  /**
   * Handle clip selection - converts clip ID back to scene ID and calls onSceneSelect.
   * This wraps the external callback to maintain backward compatibility.
   *
   * @requirements 9.4, 10.3, 10.4
   * @validates Requirements 3.2
   */
  const handleClipSelect = useCallback(
    (clipId: string | null) => {
      if (!onSceneSelect) {
        return;
      }

      if (clipId === null) {
        // Selection cleared - some implementations may want to handle this
        // For now, we don't call onSceneSelect with null since the original
        // GraphiteTimeline didn't support clearing selection via callback
        return;
      }

      // Convert clip ID back to scene ID
      const sceneId = clipIdToSceneId(clipId);
      onSceneSelect(sceneId);
    },
    [onSceneSelect]
  );

  /**
   * Handle clip deletion - converts clip ID back to scene ID and calls onDeleteClip.
   * This wraps the external callback to maintain backward compatibility.
   *
   * @requirements 9.4, 10.3
   * @validates Requirements 3.4
   */
  const handleDeleteClip = useCallback(
    (clipId: string) => {
      if (!onDeleteClip) {
        return;
      }

      // Convert clip ID back to scene ID
      const sceneId = clipIdToSceneId(clipId);
      onDeleteClip(sceneId);
    },
    [onDeleteClip]
  );

  // Return memoized result to prevent unnecessary re-renders
  return useMemo(
    () => ({
      tracks,
      audioClips,
      videoClips,
      imageClips,
      subtitles,
      selectedClipId,
      handleClipSelect,
      handleDeleteClip,
    }),
    [
      tracks,
      audioClips,
      videoClips,
      imageClips,
      subtitles,
      selectedClipId,
      handleClipSelect,
      handleDeleteClip,
    ]
  );
}

export default useTimelineAdapter;
````

## File: packages/frontend/hooks/useTimelineKeyboard.ts
````typescript
/**
 * useTimelineKeyboard Hook
 * 
 * Custom React hook for comprehensive keyboard navigation in the Graphite Timeline.
 * Implements professional video editor-style keyboard shortcuts for:
 * - Playback control (Space/K for play/pause)
 * - Time navigation (Arrow keys, J/K/L, Home/End)
 * - Clip navigation and selection (Tab, Enter, Escape, Delete)
 * - Frame-by-frame navigation (Ctrl+Arrow)
 * 
 * Requirements: Accessibility - keyboard-only navigation support
 */

import { useCallback, useEffect, useRef } from 'react';

// --- Types ---

export interface UseTimelineKeyboardOptions {
  /** Whether the timeline component is currently focused/active */
  isActive: boolean;
  /** Total duration of the timeline in seconds */
  duration: number;
  /** Current playhead time in seconds */
  currentTime: number;
  /** Whether playback is currently active */
  isPlaying: boolean;
  /** Index of the currently selected clip, or null if none selected */
  selectedClipIndex: number | null;
  /** Total number of clips in the timeline */
  clipCount: number;
  /** Callback when time should change */
  onTimeChange: (time: number) => void;
  /** Callback for play/pause toggle */
  onPlayPause: () => void;
  /** Callback for selecting a clip by index */
  onSelectClip: (index: number | null) => void;
  /** Callback for deleting the selected clip */
  onDeleteClip?: (index: number) => void;
  /** Callback to navigate to next clip */
  onNextClip: () => void;
  /** Callback to navigate to previous clip */
  onPrevClip: () => void;
  /** Callback to jump to start of timeline */
  onJumpToStart: () => void;
  /** Callback to jump to end of timeline */
  onJumpToEnd: () => void;
  /** Frames per second for frame-by-frame navigation (default: 30) */
  fps?: number;
  /** Amount to skip for small time jumps in seconds (default: 1) */
  smallSkip?: number;
  /** Amount to skip for large time jumps in seconds (default: 5) */
  largeSkip?: number;
}

export interface UseTimelineKeyboardReturn {
  /** Map of keyboard shortcut descriptions for displaying help */
  shortcuts: Record<string, string>;
}

// --- Constants ---

const DEFAULT_FPS = 30;
const DEFAULT_SMALL_SKIP = 1; // 1 second
const DEFAULT_LARGE_SKIP = 5; // 5 seconds

/**
 * Keyboard shortcut reference map for UI display
 */
const SHORTCUTS: Record<string, string> = {
  'Space / K': 'Play/Pause',
  '← / J': 'Rewind 1 second',
  '→ / L': 'Forward 1 second',
  'Shift + ←/→': 'Move 5 seconds',
  'Ctrl + ←/→': 'Move 1 frame',
  'Home': 'Jump to start',
  'End': 'Jump to end',
  'Tab': 'Next clip',
  'Shift + Tab': 'Previous clip',
  'Delete / Backspace': 'Remove selected clip',
  'Escape': 'Deselect clip',
};

// --- Hook Implementation ---

/**
 * Custom hook for keyboard navigation in the timeline editor.
 * 
 * Implements industry-standard keyboard shortcuts similar to professional
 * video editing software (Premiere Pro, Final Cut, DaVinci Resolve).
 * 
 * @param options - Configuration options and callbacks
 * @returns Object with keyboard shortcut map for UI display
 * 
 * @example
 * ```tsx
 * const { shortcuts } = useTimelineKeyboard({
 *   isActive: isFocused,
 *   duration: 120,
 *   currentTime,
 *   isPlaying,
 *   selectedClipIndex,
 *   clipCount: clips.length,
 *   onTimeChange: setCurrentTime,
 *   onPlayPause: togglePlay,
 *   onSelectClip: setSelectedClipIndex,
 *   onNextClip: () => selectNext(),
 *   onPrevClip: () => selectPrev(),
 *   onJumpToStart: () => setCurrentTime(0),
 *   onJumpToEnd: () => setCurrentTime(duration),
 * });
 * ```
 */
export function useTimelineKeyboard({
  isActive,
  duration,
  currentTime,
  isPlaying,
  selectedClipIndex,
  clipCount,
  onTimeChange,
  onPlayPause,
  onSelectClip,
  onDeleteClip,
  onNextClip,
  onPrevClip,
  onJumpToStart,
  onJumpToEnd,
  fps = DEFAULT_FPS,
  smallSkip = DEFAULT_SMALL_SKIP,
  largeSkip = DEFAULT_LARGE_SKIP,
}: UseTimelineKeyboardOptions): UseTimelineKeyboardReturn {
  
  // Track last announced time for screen readers to avoid excessive announcements
  const lastAnnouncedTimeRef = useRef<number>(currentTime);
  
  /**
   * Main keyboard event handler.
   * Processes keyboard events when the timeline has focus.
   */
  const handleKeyDown = useCallback((e: KeyboardEvent) => {
    // Only handle events when the timeline is active/focused
    if (!isActive) return;
    
    // Don't interfere with input elements
    const target = e.target as HTMLElement;
    if (target.tagName === 'INPUT' || target.tagName === 'TEXTAREA' || target.isContentEditable) {
      return;
    }
    
    // Calculate frame duration for frame-by-frame navigation
    const frameDuration = 1 / fps;
    
    switch (e.code) {
      // ==================== PLAYBACK CONTROLS ====================
      
      // Play/Pause - Space or K (standard video editor shortcut)
      case 'Space':
      case 'KeyK':
        e.preventDefault();
        onPlayPause();
        break;
        
      // ==================== TIME NAVIGATION ====================
      
      // Move playhead left - ArrowLeft or J
      case 'ArrowLeft':
      case 'KeyJ':
        e.preventDefault();
        if (e.ctrlKey || e.metaKey) {
          // Frame-by-frame backward (Ctrl/Cmd + Left)
          onTimeChange(Math.max(0, currentTime - frameDuration));
        } else if (e.shiftKey) {
          // Large skip backward (Shift + Left)
          onTimeChange(Math.max(0, currentTime - largeSkip));
        } else {
          // Small skip backward
          onTimeChange(Math.max(0, currentTime - smallSkip));
        }
        break;
        
      // Move playhead right - ArrowRight or L  
      case 'ArrowRight':
      case 'KeyL':
        e.preventDefault();
        if (e.ctrlKey || e.metaKey) {
          // Frame-by-frame forward (Ctrl/Cmd + Right)
          onTimeChange(Math.min(duration, currentTime + frameDuration));
        } else if (e.shiftKey) {
          // Large skip forward (Shift + Right)
          onTimeChange(Math.min(duration, currentTime + largeSkip));
        } else {
          // Small skip forward
          onTimeChange(Math.min(duration, currentTime + smallSkip));
        }
        break;
        
      // Jump to start
      case 'Home':
        e.preventDefault();
        onJumpToStart();
        break;
        
      // Jump to end
      case 'End':
        e.preventDefault();
        onJumpToEnd();
        break;
        
      // ==================== CLIP SELECTION ====================
      
      // Navigate between clips with Tab
      case 'Tab':
        if (clipCount > 0) {
          e.preventDefault();
          if (e.shiftKey) {
            onPrevClip();
          } else {
            onNextClip();
          }
        }
        break;
        
      // Deselect current selection
      case 'Escape':
        e.preventDefault();
        onSelectClip(null);
        break;
        
      // ==================== CLIP ACTIONS ====================
      
      // Delete selected clip
      case 'Delete':
      case 'Backspace':
        if (selectedClipIndex !== null && onDeleteClip) {
          e.preventDefault();
          onDeleteClip(selectedClipIndex);
        }
        break;
        
      // Select/confirm clip (could be extended for clip editing)
      case 'Enter':
        e.preventDefault();
        // Currently just ensures clip is selected
        // Could be extended to open clip editor in the future
        break;
        
      // ==================== ADDITIONAL SHORTCUTS ====================
      
      // Quick jump shortcuts using number keys (0-9)
      // Jump to percentage of timeline: 0=0%, 1=10%, ... 9=90%
      case 'Digit0':
      case 'Digit1':
      case 'Digit2':
      case 'Digit3':
      case 'Digit4':
      case 'Digit5':
      case 'Digit6':
      case 'Digit7':
      case 'Digit8':
      case 'Digit9':
        if (!e.ctrlKey && !e.metaKey && !e.altKey && !e.shiftKey) {
          e.preventDefault();
          const digit = parseInt(e.code.replace('Digit', ''), 10);
          const targetTime = (digit / 10) * duration;
          onTimeChange(targetTime);
        }
        break;
    }
  }, [
    isActive,
    duration,
    currentTime,
    isPlaying, // Added missing dependency
    selectedClipIndex,
    clipCount,
    onTimeChange,
    onPlayPause,
    onSelectClip,
    onDeleteClip,
    onNextClip,
    onPrevClip,
    onJumpToStart,
    onJumpToEnd,
    fps,
    smallSkip,
    largeSkip,
  ]);
  
  /**
   * Attach keyboard event listener to window.
   * Uses capture phase to handle events before bubbling.
   */
  useEffect(() => {
    window.addEventListener('keydown', handleKeyDown);
    
    return () => {
      window.removeEventListener('keydown', handleKeyDown);
    };
  }, [handleKeyDown]);
  
  return {
    shortcuts: SHORTCUTS,
  };
}

export default useTimelineKeyboard;
````

## File: packages/frontend/hooks/useTimelineSelection.ts
````typescript
/**
 * useTimelineSelection Hook
 * 
 * Custom React hook for managing timeline clip selection state.
 * Implements single-selection logic with support for:
 * - Selecting clips on click (Requirement 10.1)
 * - Clearing selection on click outside clips (Requirement 10.2)
 * - Notifying parent components via callback (Requirement 10.3)
 * - Single-selection only - no multi-select (Requirement 10.4)
 * 
 * Requirements: 10.1, 10.2, 10.3, 10.4
 */

import { useState, useCallback, useMemo, useEffect } from "react";
import {
  SelectionState,
  createInitialSelectionState,
  handleClipClick,
  handleOutsideClick,
  isClipSelected,
} from "@/components/TimelineEditor/graphite-timeline-utils";
import { clipIdToSceneId } from "@/components/TimelineEditor/timelineAdapter";

export interface UseTimelineSelectionOptions {
  /** Initial selected clip ID */
  initialSelectedId?: string | null;
  /** Callback when a scene is selected (Requirement 10.3) */
  onSceneSelect?: (sceneId: string) => void;
  /** Callback when selection is cleared */
  onSelectionClear?: () => void;
}

export interface UseTimelineSelectionReturn {
  /** Currently selected clip ID, or null if none */
  selectedClipId: string | null;
  /** Handler for clip click events - selects the clip */
  handleSelectClip: (clipId: string) => void;
  /** Handler for clicking outside clips - clears selection */
  handleClearSelection: () => void;
  /** Check if a specific clip is selected */
  isSelected: (clipId: string) => boolean;
  /** Programmatically set the selected clip ID */
  setSelectedClipId: (clipId: string | null) => void;
}

/**
 * Hook for managing timeline selection state.
 * 
 * @param options - Configuration options
 * @returns Selection state and handlers
 * 
 * @example
 * ```tsx
 * const { selectedClipId, handleSelectClip, handleClearSelection, isSelected } = 
 *   useTimelineSelection({
 *     onSceneSelect: (sceneId) => console.log("Selected:", sceneId),
 *   });
 * 
 * // In TrackLane:
 * <TrackLane
 *   selectedClipId={selectedClipId}
 *   onClipSelect={handleSelectClip}
 *   onLaneClick={handleClearSelection}
 * />
 * ```
 */
export function useTimelineSelection(
  options: UseTimelineSelectionOptions = {}
): UseTimelineSelectionReturn {
  const { initialSelectedId, onSceneSelect, onSelectionClear } = options;

  // Internal selection state
  const [selectionState, setSelectionState] = useState<SelectionState>(() =>
    createInitialSelectionState(initialSelectedId)
  );

  /**
   * Handle clip click - selects the clip and notifies parent.
   * Implements Requirement 10.1 (mark as selected) and 10.3 (notify via callback).
   */
  const handleSelectClip = useCallback(
    (clipId: string) => {
      setSelectionState((currentState) => {
        const { newState, sceneId } = handleClipClick(currentState, clipId);
        return newState;
      });
    },
    []
  );

  // Use useEffect to handle scene selection callback instead of setTimeout
  useEffect(() => {
    if (selectionState.selectedClipId && onSceneSelect) {
      const sceneId = clipIdToSceneId(selectionState.selectedClipId);
      onSceneSelect(sceneId);
    }
  }, [selectionState.selectedClipId, onSceneSelect]);

  /**
   * Handle click outside clips - clears selection.
   * Implements Requirement 10.2.
   */
  const handleClearSelection = useCallback(() => {
    setSelectionState((currentState) => {
      const newState = handleOutsideClick(currentState);
      
      // Call the onSelectionClear callback if selection was cleared
      if (currentState.selectedClipId !== null && onSelectionClear) {
        setTimeout(() => onSelectionClear(), 0);
      }
      
      return newState;
    });
  }, [onSelectionClear]);

  /**
   * Check if a specific clip is selected.
   */
  const isSelected = useCallback(
    (clipId: string) => isClipSelected(selectionState, clipId),
    [selectionState]
  );

  /**
   * Programmatically set the selected clip ID.
   * Useful for external control of selection state.
   */
  const setSelectedClipId = useCallback(
    (clipId: string | null) => {
      setSelectionState({ selectedClipId: clipId });
    },
    []
  );

  // Memoize the return value to prevent unnecessary re-renders
  return useMemo(
    () => ({
      selectedClipId: selectionState.selectedClipId,
      handleSelectClip,
      handleClearSelection,
      isSelected,
      setSelectedClipId,
    }),
    [
      selectionState.selectedClipId,
      handleSelectClip,
      handleClearSelection,
      isSelected,
      setSelectedClipId,
    ]
  );
}

export default useTimelineSelection;
````

## File: packages/frontend/hooks/useVideoNarration.ts
````typescript
/**
 * useVideoNarration Hook
 * 
 * Handles narration generation and audio playback for video production.
 * Manages audio URLs and cleanup to prevent memory leaks.
 */

import { useState, useCallback, useRef, useEffect } from "react";
import { ContentPlan, NarrationSegment } from "@/types";
import { narrateScene, createAudioUrl, revokeAudioUrl, NarratorConfig } from "@/services/narratorService";
import { syncDurationsToNarration } from "@/services/editorService";
import { VideoPurpose } from "@/constants";
import { ProductionProgress } from "@/services/agentOrchestrator";

export interface VideoNarrationState {
    narrationSegments: NarrationSegment[];
    playingSceneId: string | null;
}

export function useVideoNarration(
    contentPlan: ContentPlan | null,
    videoPurpose: VideoPurpose,
    onProgress?: (progress: ProductionProgress) => void,
    onError?: (error: string) => void,
    onContentPlanUpdate?: (plan: ContentPlan) => void
) {
    const [narrationSegments, setNarrationSegments] = useState<NarrationSegment[]>([]);
    const [playingSceneId, setPlayingSceneId] = useState<string | null>(null);
    
    const audioUrlsRef = useRef<Map<string, string>>(new Map());
    const audioRef = useRef<HTMLAudioElement | null>(null);

    // Cleanup effect for audio URLs to prevent memory leaks
    useEffect(() => {
        return () => {
            // Cleanup all blob URLs on unmount
            audioUrlsRef.current.forEach(url => revokeAudioUrl(url));
            audioUrlsRef.current.clear();
            
            // Stop and cleanup audio element
            if (audioRef.current) {
                audioRef.current.pause();
                audioRef.current = null;
            }
        };
    }, []);

    /**
     * Generate narration for all scenes
     */
    const generateNarration = useCallback(async () => {
        if (!contentPlan) {
            onError?.("No content plan to narrate");
            return;
        }

        const segments: NarrationSegment[] = [];

        for (let i = 0; i < contentPlan.scenes.length; i++) {
            const scene = contentPlan.scenes[i];
            
            // TypeScript strict mode: ensure scene exists
            if (!scene) {
                console.error(`[useVideoNarration] Scene at index ${i} is undefined`);
                continue;
            }

            onProgress?.({
                stage: "narrating",
                progress: Math.round((i / contentPlan.scenes.length) * 100),
                message: `Narrating: ${scene.name}`,
                currentScene: i + 1,
                totalScenes: contentPlan.scenes.length,
            });

            try {
                const narratorConfig: NarratorConfig = { videoPurpose };
                const segment = await narrateScene(scene, narratorConfig);
                segments.push(segment);

                // Create audio URL for playback
                const url = createAudioUrl(segment);
                audioUrlsRef.current.set(scene.id, url);
            } catch (err) {
                console.error(`[useVideoNarration] Narration failed for scene ${scene.id}:`, err);
                onError?.(`Narration failed for "${scene.name}"`);
                return;
            }
        }

        setNarrationSegments(segments);

        // Sync durations
        if (contentPlan && onContentPlanUpdate) {
            const synced = syncDurationsToNarration(contentPlan, segments);
            onContentPlanUpdate(synced);
        }

        onProgress?.({
            stage: "narrating",
            progress: 100,
            message: `${segments.length} narrations complete`,
        });
    }, [contentPlan, videoPurpose, onProgress, onError, onContentPlanUpdate]);

    /**
     * Regenerate narration for a single scene after script edit
     */
    const regenerateSceneNarration = useCallback(async (sceneId: string) => {
        if (!contentPlan) {
            onError?.("No content plan available");
            return;
        }

        const scene = contentPlan.scenes.find(s => s.id === sceneId);
        if (!scene) {
            onError?.(`Scene ${sceneId} not found`);
            return;
        }

        // TypeScript type narrowing: scene is guaranteed to be defined here
        const currentScene = scene;

        onProgress?.({
            stage: "narrating",
            progress: 0,
            message: `Regenerating narration for: ${currentScene.name}`,
            currentScene: 1,
            totalScenes: 1,
        });

        try {
            // Revoke old audio URL if exists
            const oldUrl = audioUrlsRef.current.get(sceneId);
            if (oldUrl) {
                revokeAudioUrl(oldUrl);
                audioUrlsRef.current.delete(sceneId);
            }

            // Generate new narration
            const narratorConfig: NarratorConfig = { videoPurpose };
            const segment = await narrateScene(currentScene, narratorConfig);

            // Create new audio URL
            const url = createAudioUrl(segment);
            audioUrlsRef.current.set(sceneId, url);

            // Update narration segments
            setNarrationSegments(prev => {
                const existing = prev.findIndex(s => s.sceneId === sceneId);
                if (existing >= 0) {
                    const updated = [...prev];
                    updated[existing] = segment;
                    return updated;
                }
                return [...prev, segment];
            });

            // Sync duration for this scene
            if (contentPlan && onContentPlanUpdate) {
                const updatedScenes = contentPlan.scenes.map(s =>
                    s.id === sceneId ? { ...s, duration: Math.ceil(segment.audioDuration) } : s
                );
                const totalDuration = updatedScenes.reduce((sum, s) => sum + s.duration, 0);
                onContentPlanUpdate({
                    ...contentPlan,
                    scenes: updatedScenes,
                    totalDuration,
                });
            }

            onProgress?.({
                stage: "narrating",
                progress: 100,
                message: `Narration updated for: ${currentScene.name}`,
            });

            console.log(`[useVideoNarration] Regenerated narration for scene ${sceneId}, duration: ${segment.audioDuration}s`);
        } catch (err) {
            console.error(`[useVideoNarration] Failed to regenerate narration for scene ${sceneId}:`, err);
            onError?.(err instanceof Error ? err.message : String(err));
        }
    }, [contentPlan, videoPurpose, onProgress, onError, onContentPlanUpdate]);

    /**
     * Play narration for a scene
     */
    const playNarration = useCallback((sceneId: string) => {
        // Stop current playback
        if (audioRef.current) {
            audioRef.current.pause();
            audioRef.current = null;
        }

        if (playingSceneId === sceneId) {
            setPlayingSceneId(null);
            return;
        }

        const url = audioUrlsRef.current.get(sceneId);
        if (!url) {
            console.warn(`No audio URL for scene ${sceneId}`);
            return;
        }

        const audio = new Audio(url);
        audio.onended = () => setPlayingSceneId(null);
        audio.play();
        audioRef.current = audio;
        setPlayingSceneId(sceneId);
    }, [playingSceneId]);

    /**
     * Get audio URLs map for SceneEditor
     */
    const getAudioUrlMap = useCallback((): Record<string, string> => {
        const map: Record<string, string> = {};
        audioUrlsRef.current.forEach((url, sceneId) => {
            map[sceneId] = url;
        });
        return map;
    }, []);

    /**
     * Reset narration state
     */
    const resetNarration = useCallback(() => {
        // Cleanup audio URLs
        audioUrlsRef.current.forEach((url) => revokeAudioUrl(url));
        audioUrlsRef.current.clear();

        if (audioRef.current) {
            audioRef.current.pause();
            audioRef.current = null;
        }

        setNarrationSegments([]);
        setPlayingSceneId(null);
    }, []);

    return {
        // State
        narrationSegments,
        playingSceneId,

        // Actions
        generateNarration,
        regenerateSceneNarration,
        playNarration,
        getAudioUrlMap,
        resetNarration,
        setNarrationSegments,
    };
}
````

## File: packages/frontend/hooks/useVideoProductionCore.ts
````typescript
/**
 * useVideoProductionCore Hook
 * 
 * Core pipeline state and configuration for video production.
 * Handles the main workflow orchestration and state management.
 */

import { useState, useCallback } from "react";
import { AppState, ContentPlan, Scene, ValidationResult } from "@/types";
import { ProductionProgress, ProductionConfig } from "@/services/agentOrchestrator";
import { VideoPurpose, LanguageCode } from "@/constants";

export interface VideoProductionCoreState {
    // Core state
    appState: AppState;
    topic: string;
    contentPlan: ContentPlan | null;
    validation: ValidationResult | null;
    progress: ProductionProgress | null;
    error: string | null;

    // Config
    targetDuration: number;
    targetAudience: string;
    videoPurpose: VideoPurpose;
    visualStyle: string;
    language: LanguageCode;
    useAgentMode: boolean;
    veoVideoCount: number;
}

export function useVideoProductionCore() {
    // Core state
    const [appState, setAppState] = useState<AppState>(AppState.IDLE);
    const [topic, setTopic] = useState("");
    const [contentPlan, setContentPlan] = useState<ContentPlan | null>(null);
    const [validation, setValidation] = useState<ValidationResult | null>(null);
    const [progress, setProgress] = useState<ProductionProgress | null>(null);
    const [error, setError] = useState<string | null>(null);

    // Config state
    const [targetDuration, setTargetDuration] = useState(60);
    const [targetAudience, setTargetAudience] = useState("General audience");
    const [videoPurpose, setVideoPurpose] = useState<VideoPurpose>("documentary");
    const [visualStyle, setVisualStyle] = useState("Cinematic");
    const [language, setLanguage] = useState<LanguageCode>("auto");
    const [useAgentMode, setUseAgentMode] = useState(true);
    const [veoVideoCount, setVeoVideoCount] = useState(1);

    /**
     * Update scenes (from SceneEditor)
     */
    const updateScenes = useCallback((scenes: Scene[]) => {
        if (!contentPlan) return;

        const totalDuration = scenes.reduce((sum, s) => sum + s.duration, 0);
        setContentPlan({
            ...contentPlan,
            scenes,
            totalDuration,
        });
    }, [contentPlan]);

    /**
     * Reset core state
     */
    const resetCore = useCallback(() => {
        setAppState(AppState.IDLE);
        setTopic("");
        setContentPlan(null);
        setValidation(null);
        setProgress(null);
        setError(null);
    }, []);

    return {
        // State
        appState,
        topic,
        contentPlan,
        validation,
        progress,
        error,
        targetDuration,
        targetAudience,
        videoPurpose,
        visualStyle,
        language,
        useAgentMode,
        veoVideoCount,

        // Setters
        setAppState,
        setTopic,
        setContentPlan,
        setValidation,
        setProgress,
        setError,
        setTargetDuration,
        setTargetAudience,
        setVideoPurpose,
        setVisualStyle,
        setLanguage,
        setUseAgentMode,
        setVeoVideoCount,

        // Actions
        updateScenes,
        resetCore,
    };
}
````

## File: packages/frontend/hooks/useVideoProductionRefactored.ts
````typescript
/**
 * useVideoProductionRefactored Hook
 * 
 * Refactored version of useVideoProduction that combines focused hooks.
 * This replaces the massive 987-line hook with a clean composition pattern.
 * 
 * Flow: Topic Input → ContentPlanner → Narrator → Visuals → SFX → Editor → Export
 */

import { useCallback } from "react";
import { AppState } from "@/types";
import {
    runProductionPipeline,
    ProductionConfig,
    stageToAppState
} from "@/services/agentOrchestrator";
import {
    runProductionAgent,
    runProductionAgentWithSubagents,
    ProductionProgress as AgentProgress,
} from "@/services/ai/productionAgent";
import { generateContentPlan, ContentPlannerConfig } from "@/services/contentPlannerService";
import { NarratorConfig, createAudioUrl } from "@/services/narratorService";

// Import focused hooks
import { useVideoProductionCore } from "./useVideoProductionCore";
import { useVideoNarration } from "./useVideoNarration";
import { useVideoVisuals } from "./useVideoVisuals";
import { useVideoQuality } from "./useVideoQuality";
import { useVideoSFX } from "./useVideoSFX";
import { useVideoPromptTools } from "./useVideoPromptTools";
import { useSunoMusic } from "./useSunoMusic";

// Toggle between monolithic and multi-agent system
const USE_MULTI_AGENT = import.meta.env.VITE_USE_MULTI_AGENT !== 'false';

export function useVideoProductionRefactored() {
    // Core state and configuration
    const coreHook = useVideoProductionCore();

    // Narration management with proper callbacks
    const narrationHook = useVideoNarration(
        coreHook.contentPlan,
        coreHook.videoPurpose,
        coreHook.setProgress,
        coreHook.setError,
        coreHook.setContentPlan
    );

    // Visual generation and management
    const visualsHook = useVideoVisuals();

    // Quality monitoring with proper callbacks
    const qualityHook = useVideoQuality(
        coreHook.setProgress,
        coreHook.setError
    );

    // SFX and audio mixing with proper callbacks
    const sfxHook = useVideoSFX(
        coreHook.setProgress,
        coreHook.setError
    );

    // Prompt quality tools
    const promptToolsHook = useVideoPromptTools(
        coreHook.contentPlan,
        coreHook.visualStyle,
        coreHook.topic
    );

    // Music generation (Suno API)
    const musicHook = useSunoMusic();

    /**
     * Start the full production pipeline
     */
    const startProduction = useCallback(async (config?: ProductionConfig, topicOverride?: string) => {
        const effectiveTopic = topicOverride || coreHook.topic;

        if (!effectiveTopic.trim()) {
            coreHook.setError("Please enter a topic");
            return;
        }

        // Update topic state if override provided
        if (topicOverride) {
            coreHook.setTopic(topicOverride);
        }

        coreHook.setError(null);
        coreHook.setProgress(null);

        // Calculate scene count from duration (1 scene per ~12 seconds, min 3)
        const effectiveDuration = config?.targetDuration ?? coreHook.targetDuration;
        const calculatedSceneCount = Math.max(3, Math.floor(effectiveDuration / 12));

        try {
            // Check if we should use AI Agent mode (default for complex automation)
            if (coreHook.useAgentMode) {
                console.log(`[useVideoProduction] Using AI Agent mode for ${effectiveDuration}s video`);
                coreHook.setAppState(AppState.CONTENT_PLANNING);

                // Build user request for the agent
                const userRequest = `Create a ${effectiveDuration} second ${coreHook.videoPurpose} video about: ${effectiveTopic}. 
Style: ${coreHook.visualStyle}. Language: ${coreHook.language === 'auto' ? 'detect from topic' : coreHook.language}.
Target audience: ${coreHook.targetAudience}.
${effectiveDuration > 300 ? 'This is a long video, use appropriate number of scenes.' : ''}
${config?.animateVisuals ? 'IMPORTANT: The user wants VIDEO, so you MUST use the animate_image tool for every scene.' : ''}
${coreHook.veoVideoCount > 0 ? `IMPORTANT: Use generate_visuals with veoVideoCount=${coreHook.veoVideoCount} to generate professional videos for the first ${coreHook.veoVideoCount} scenes.` : ''}`;

                // Choose which agent system to use
                const productionFunction = USE_MULTI_AGENT
                    ? runProductionAgentWithSubagents
                    : runProductionAgent;

                console.log(`[useVideoProduction] Using ${USE_MULTI_AGENT ? 'MULTI-AGENT' : 'MONOLITHIC'} system`);

                const agentResult = await productionFunction(
                    userRequest,
                    (agentProg: AgentProgress) => {
                        // Map agent progress to our progress format
                        coreHook.setProgress({
                            stage: agentProg.stage as any,
                            progress: agentProg.isComplete ? 100 : 50,
                            message: agentProg.message,
                        });

                        // Update app state based on tool being called
                        if (agentProg.tool === 'plan_video') {
                            coreHook.setAppState(AppState.CONTENT_PLANNING);
                        } else if (agentProg.tool === 'narrate_scenes') {
                            coreHook.setAppState(AppState.NARRATING);
                        } else if (agentProg.tool === 'generate_visuals' || agentProg.tool === 'animate_image') {
                            coreHook.setAppState(AppState.GENERATING_PROMPTS);
                        } else if (agentProg.tool === 'validate_plan') {
                            coreHook.setAppState(AppState.VALIDATING);
                        }
                    }
                );

                if (agentResult) {
                    coreHook.setContentPlan(agentResult.contentPlan);
                    narrationHook.setNarrationSegments(agentResult.narrationSegments);
                    visualsHook.setVisuals(agentResult.visuals);
                    sfxHook.setSfxPlan(agentResult.sfxPlan);

                    // Create audio URLs for playback from narration segments
                    agentResult.narrationSegments.forEach((segment) => {
                        if (segment.audioBlob) {
                            const url = createAudioUrl(segment);
                            // Note: This would need to be handled by the narration hook
                        }
                    });

                    // Generate quality report if we have a content plan
                    if (agentResult.contentPlan) {
                        // Convert ToolError[] to validation issues format
                        const errorMessages = (agentResult.errors || []).map(err => {
                            if (typeof err === 'string') {
                                return { scene: 'general', type: 'error' as const, message: err };
                            }
                            const sceneInfo = err.sceneIndex !== undefined ? `Scene ${err.sceneIndex}` : err.tool;
                            return {
                                scene: sceneInfo,
                                type: 'error' as const,
                                message: `${err.tool}: ${err.error}${err.fallbackApplied ? ` (fallback: ${err.fallbackApplied})` : ''}`
                            };
                        });

                        const partialReport = agentResult.partialSuccessReport;
                        const hasErrors = errorMessages.length > 0;
                        const score = partialReport?.isUsable
                            ? (partialReport.fallbackApplied > 0 ? 75 : 85)
                            : (hasErrors ? 60 : 85);

                        const validation = {
                            approved: !hasErrors || (partialReport?.isUsable ?? true),
                            score,
                            issues: errorMessages,
                            suggestions: partialReport ? [partialReport.summary] : []
                        };
                        coreHook.setValidation(validation);

                        const qualityReport = qualityHook.generateAndSaveQualityReport(
                            agentResult.contentPlan as any,
                            agentResult.narrationSegments,
                            agentResult.sfxPlan,
                            validation,
                            coreHook.videoPurpose
                        );
                        console.log(`[useVideoProduction] Agent Mode Quality Report: ${qualityReport.overallScore}/100`);
                    }

                    coreHook.setAppState(AppState.READY);
                } else {
                    throw new Error("Agent returned no result");
                }
            } else {
                // Fast mode - use direct orchestrator pipeline
                console.log(`[useVideoProduction] Using Fast mode (orchestrator) for ${effectiveDuration}s video`);

                const result = await runProductionPipeline(
                    effectiveTopic,
                    {
                        targetDuration: effectiveDuration,
                        sceneCount: calculatedSceneCount,
                        targetAudience: coreHook.targetAudience,
                        visualStyle: coreHook.visualStyle,
                        contentPlannerConfig: {
                            videoPurpose: coreHook.videoPurpose,
                            visualStyle: coreHook.visualStyle,
                            language: coreHook.language,
                        },
                        narratorConfig: {
                            videoPurpose: coreHook.videoPurpose,
                            language: coreHook.language,
                        },
                        veoVideoCount: coreHook.veoVideoCount,
                        ...config,
                    },
                    (prog) => {
                        coreHook.setProgress(prog);
                        coreHook.setAppState(stageToAppState(prog.stage));
                    }
                );

                coreHook.setContentPlan(result.contentPlan);
                narrationHook.setNarrationSegments(result.narrationSegments);
                visualsHook.setVisuals(result.visuals);
                sfxHook.setSfxPlan(result.sfxPlan);
                coreHook.setValidation(result.validation);

                // Generate quality report
                const report = qualityHook.generateAndSaveQualityReport(
                    result.contentPlan as any,
                    result.narrationSegments,
                    result.sfxPlan,
                    result.validation,
                    coreHook.videoPurpose
                );
                console.log(`[useVideoProduction] Fast Mode Quality Report: ${report.overallScore}/100`);

                if (!result.success) {
                    coreHook.setError(`Production completed with issues (score: ${result.validation.score})`);
                }

                coreHook.setAppState(AppState.READY);
            }
        } catch (err) {
            console.error("[useVideoProduction] Pipeline failed:", err);
            coreHook.setError(err instanceof Error ? err.message : String(err));
            coreHook.setAppState(AppState.ERROR);
        }
    }, [coreHook, narrationHook, visualsHook, sfxHook, qualityHook]);

    /**
     * Generate content plan only (without narration)
     */
    const generatePlan = useCallback(async (config?: Partial<ContentPlannerConfig>) => {
        if (!coreHook.topic.trim()) {
            coreHook.setError("Please enter a topic");
            return;
        }

        coreHook.setError(null);
        coreHook.setAppState(AppState.CONTENT_PLANNING);
        coreHook.setProgress({
            stage: "content_planning",
            progress: 0,
            message: "Generating video plan...",
        });

        try {
            const effectiveDuration = config?.targetDuration ?? coreHook.targetDuration;
            const plan = await generateContentPlan(coreHook.topic, {
                targetDuration: effectiveDuration,
                sceneCount: Math.max(3, Math.floor(effectiveDuration / 12)),
                targetAudience: coreHook.targetAudience,
                config: {
                    videoPurpose: coreHook.videoPurpose,
                    visualStyle: coreHook.visualStyle,
                },
            });

            coreHook.setContentPlan(plan);
            coreHook.setProgress({
                stage: "content_planning",
                progress: 100,
                message: `Created ${plan.scenes.length} scenes`,
            });
            coreHook.setAppState(AppState.READY);
        } catch (err) {
            console.error("[useVideoProduction] Plan generation failed:", err);
            coreHook.setError(err instanceof Error ? err.message : String(err));
            coreHook.setAppState(AppState.ERROR);
        }
    }, [coreHook]);

    /**
     * Add the selected music track to the timeline
     */
    const addMusicToTimeline = useCallback(() => {
        const selectedTrack = musicHook.getSelectedTrack();

        if (!selectedTrack) {
            console.warn("[useVideoProduction] No track selected to add to timeline");
            return;
        }

        console.log(`[useVideoProduction] Adding track "${selectedTrack.title}" to timeline`);

        sfxHook.setSfxPlan(prev => {
            const basePlan = prev || {
                scenes: [],
                backgroundMusic: null,
                masterVolume: 1.0,
            };

            return {
                ...basePlan,
                generatedMusic: {
                    trackId: selectedTrack.id,
                    audioUrl: selectedTrack.audio_url,
                    duration: selectedTrack.duration,
                    title: selectedTrack.title,
                },
            };
        });
    }, [musicHook.getSelectedTrack, sfxHook.setSfxPlan]);

    /**
     * Reset all state
     */
    const reset = useCallback(() => {
        coreHook.resetCore();
        narrationHook.resetNarration();
        visualsHook.resetVisuals();
        qualityHook.resetQuality();
        sfxHook.resetSFX();
        musicHook.resetMusicState();
    }, [coreHook, narrationHook, visualsHook, qualityHook, sfxHook, musicHook]);

    return {
        // Music generation state (Prioritized)
        musicState: musicHook.musicState,
        generateMusic: musicHook.generateMusic,
        generateLyrics: musicHook.generateLyrics,
        selectTrack: musicHook.selectTrack,
        refreshCredits: musicHook.refreshCredits,
        createMusicVideo: musicHook.createMusicVideo,
        generateCover: musicHook.generateCover,
        addVocals: musicHook.addVocals,
        addInstrumental: musicHook.addInstrumental,
        uploadAndCover: musicHook.uploadAndCover,
        uploadAudio: musicHook.uploadAudio,

        // Core State
        appState: coreHook.appState,
        topic: coreHook.topic,
        contentPlan: coreHook.contentPlan,
        narrationSegments: narrationHook.narrationSegments,
        visuals: visualsHook.visuals,
        sfxPlan: sfxHook.sfxPlan,
        validation: coreHook.validation,
        qualityReport: qualityHook.qualityReport,
        progress: coreHook.progress,
        error: coreHook.error,
        playingSceneId: narrationHook.playingSceneId,

        // Config
        targetDuration: coreHook.targetDuration,
        targetAudience: coreHook.targetAudience,
        videoPurpose: coreHook.videoPurpose,
        visualStyle: coreHook.visualStyle,
        language: coreHook.language,
        useAgentMode: coreHook.useAgentMode,
        setTargetDuration: coreHook.setTargetDuration,
        setTargetAudience: coreHook.setTargetAudience,
        setVideoPurpose: coreHook.setVideoPurpose,
        setVisualStyle: coreHook.setVisualStyle,
        setLanguage: coreHook.setLanguage,
        setUseAgentMode: coreHook.setUseAgentMode,
        veoVideoCount: coreHook.veoVideoCount,
        setVeoVideoCount: coreHook.setVeoVideoCount,

        // Actions
        setTopic: coreHook.setTopic,
        startProduction,
        generatePlan,
        generateNarration: narrationHook.generateNarration,
        regenerateSceneNarration: narrationHook.regenerateSceneNarration,
        runValidation: () => qualityHook.runValidation(
            coreHook.contentPlan as any,
            narrationHook.narrationSegments,
            visualsHook.visuals
        ),
        addMusicToTimeline,
        updateScenes: coreHook.updateScenes,
        playNarration: narrationHook.playNarration,
        reset,

        // Utilities
        getAudioUrlMap: narrationHook.getAudioUrlMap,
        getVisualsMap: visualsHook.getVisualsMap,
        visualsMap: visualsHook.visualsMap,

        // Test/Debug setters (for loading saved sessions)
        setVisuals: visualsHook.setVisuals,
        setContentPlan: coreHook.setContentPlan,
        setNarrationSegments: narrationHook.setNarrationSegments,
        setAppState: coreHook.setAppState,

        // SFX & Freesound
        browseSfx: sfxHook.browseSfx,
        getSfxCategories: sfxHook.getSfxCategories,
        previewSfx: sfxHook.previewSfx,
        isSfxAvailable: sfxHook.isSfxAvailable,
        mixAudio: sfxHook.mixAudio,

        // Prompt Quality Tools
        checkPromptQuality: promptToolsHook.checkPromptQuality,
        improvePrompt: promptToolsHook.improvePrompt,
        getCameraAngles: visualsHook.getCameraAngles,
        getLightingMoods: visualsHook.getLightingMoods,
        setPreferredCameraAngle: visualsHook.setPreferredCameraAngle,
        setPreferredLightingMood: visualsHook.setPreferredLightingMood,

        // Quality Functions
        getQualityHistoryData: qualityHook.getQualityHistoryData,
        getQualityTrend: qualityHook.getQualityTrend,
        exportQualityReport: qualityHook.exportQualityReport,
        getQualitySummaryText: qualityHook.getQualitySummaryText,

        // Camera & Lighting Preferences
        preferredCameraAngle: visualsHook.preferredCameraAngle,
        preferredLightingMood: visualsHook.preferredLightingMood,
    };
}

export default useVideoProductionRefactored;
````

## File: packages/frontend/hooks/useVideoPromptTools.ts
````typescript
/**
 * useVideoPromptTools Hook
 * 
 * Handles prompt quality checking and improvement tools for video production.
 * Provides AI-powered prompt refinement and quality analysis.
 */

import { useCallback } from "react";
import { ContentPlan } from "@/types";
import {
    lintPrompt,
    refineImagePrompt,
    type PromptLintIssue
} from "@/services/promptService";

export function useVideoPromptTools(
    contentPlan: ContentPlan | null,
    visualStyle: string,
    topic: string
) {
    /**
     * Lint a prompt for quality issues
     */
    const checkPromptQuality = useCallback((promptText: string, globalSubject?: string): PromptLintIssue[] => {
        return lintPrompt({
            promptText,
            globalSubject,
            previousPrompts: contentPlan?.scenes.map(s => s.visualDescription) || []
        });
    }, [contentPlan]);

    /**
     * Refine a prompt using AI
     */
    const improvePrompt = useCallback(async (
        promptText: string,
        intent: "auto" | "more_detailed" | "more_cinematic" | "shorten" = "auto"
    ): Promise<{ refinedPrompt: string; issues: PromptLintIssue[] }> => {
        return refineImagePrompt({
            promptText,
            style: visualStyle,
            globalSubject: topic,
            intent,
            previousPrompts: contentPlan?.scenes.map(s => s.visualDescription) || []
        });
    }, [visualStyle, topic, contentPlan]);

    return {
        // Actions
        checkPromptQuality,
        improvePrompt,
    };
}
````

## File: packages/frontend/hooks/useVideoQuality.ts
````typescript
/**
 * useVideoQuality Hook
 * 
 * Handles quality monitoring and reporting for video production.
 * Manages quality reports, history, and validation.
 */

import { useState, useCallback } from "react";
import { ContentPlan, NarrationSegment, ValidationResult } from "@/types";
import { VideoSFXPlan } from "@/services/sfxService";
import { VideoPurpose } from "@/constants";
import {
    generateQualityReport,
    saveReportToHistory,
    getQualityHistory,
    getHistoricalAverages,
    getQualitySummary,
    exportReportAsJson,
    ProductionQualityReport
} from "@/services/qualityMonitorService";
import { validateContentPlan } from "@/services/editorService";
import { ProductionProgress } from "@/services/agentOrchestrator";

export interface VideoQualityState {
    qualityReport: ProductionQualityReport | null;
}

export function useVideoQuality(
    onProgress?: (progress: ProductionProgress) => void,
    onError?: (error: string) => void
) {
    const [qualityReport, setQualityReport] = useState<ProductionQualityReport | null>(null);

    /**
     * Generate and save quality report
     */
    const generateAndSaveQualityReport = useCallback((
        contentPlan: ContentPlan,
        narrationSegments: NarrationSegment[],
        sfxPlan: VideoSFXPlan | null,
        validation: ValidationResult,
        videoPurpose: VideoPurpose
    ) => {
        const report = generateQualityReport(
            contentPlan,
            narrationSegments,
            sfxPlan,
            validation,
            videoPurpose
        );
        setQualityReport(report);
        saveReportToHistory(report);
        console.log(`[useVideoQuality] Quality Report: ${report.overallScore}/100`);
        return report;
    }, []);

    /**
     * Validate the current plan
     */
    const runValidation = useCallback(async (
        contentPlan: ContentPlan,
        narrationSegments: NarrationSegment[],
        visuals: any[]
    ): Promise<ValidationResult | null> => {
        if (!contentPlan) {
            onError?.("No content plan to validate");
            return null;
        }

        onProgress?.({
            stage: "validating",
            progress: 0,
            message: "Validating production...",
        });

        try {
            const result = await validateContentPlan(contentPlan, {
                narrationSegments,
                visuals,
                useAICritique: true,
            });

            onProgress?.({
                stage: "validating",
                progress: 100,
                message: `Validation score: ${result.score}/100`,
            });

            return result;
        } catch (err) {
            console.error("[useVideoQuality] Validation failed:", err);
            onError?.(err instanceof Error ? err.message : String(err));
            return null;
        }
    }, [onProgress, onError]);

    /**
     * Get quality history from localStorage
     */
    const getQualityHistoryData = useCallback(() => {
        return getQualityHistory();
    }, []);

    /**
     * Get historical quality averages and trend
     */
    const getQualityTrend = useCallback(() => {
        return getHistoricalAverages();
    }, []);

    /**
     * Export current quality report as JSON
     */
    const exportQualityReport = useCallback(() => {
        if (!qualityReport) return null;
        return exportReportAsJson(qualityReport);
    }, [qualityReport]);

    /**
     * Get quality summary string
     */
    const getQualitySummaryText = useCallback(() => {
        if (!qualityReport) return null;
        return getQualitySummary(qualityReport);
    }, [qualityReport]);

    /**
     * Reset quality state
     */
    const resetQuality = useCallback(() => {
        setQualityReport(null);
    }, []);

    return {
        // State
        qualityReport,

        // Actions
        generateAndSaveQualityReport,
        runValidation,
        getQualityHistoryData,
        getQualityTrend,
        exportQualityReport,
        getQualitySummaryText,
        resetQuality,
        setQualityReport,
    };
}
````

## File: packages/frontend/hooks/useVideoSFX.ts
````typescript
/**
 * useVideoSFX Hook
 * 
 * Handles sound effects and audio mixing for video production.
 * Manages SFX browsing, preview, and audio mixing capabilities.
 */

import { useState, useCallback } from "react";
import { ContentPlan, NarrationSegment } from "@/types";
import { VideoSFXPlan } from "@/services/sfxService";
import {
    searchAmbientSound,
    getPreviewUrl,
    isFreesoundConfigured,
    AMBIENT_SEARCH_QUERIES,
    type FreesoundSound
} from "@/services/freesoundService";
import {
    mixAudioWithSFX,
    canMixSFX,
    type MixConfig
} from "@/services/audioMixerService";
import { ProductionProgress } from "@/services/agentOrchestrator";

export interface VideoSFXState {
    sfxPlan: VideoSFXPlan | null;
}

export function useVideoSFX(
    onProgress?: (progress: ProductionProgress) => void,
    onError?: (error: string) => void
) {
    const [sfxPlan, setSfxPlan] = useState<VideoSFXPlan | null>(null);

    /**
     * Browse SFX from Freesound library
     */
    const browseSfx = useCallback(async (category: string): Promise<FreesoundSound | null> => {
        if (!isFreesoundConfigured()) {
            console.warn("[useVideoSFX] Freesound API not configured");
            onError?.("Freesound API key not configured. Add VITE_FREESOUND_API_KEY to .env.local");
            return null;
        }

        try {
            const sound = await searchAmbientSound(category);
            if (sound) {
                console.log(`[useVideoSFX] Found SFX: ${sound.name} (${sound.duration.toFixed(1)}s)`);
            }
            return sound;
        } catch (err) {
            console.error("[useVideoSFX] SFX browse failed:", err);
            onError?.(err instanceof Error ? err.message : String(err));
            return null;
        }
    }, [onError]);

    /**
     * Get available SFX categories
     */
    const getSfxCategories = useCallback(() => {
        return Object.keys(AMBIENT_SEARCH_QUERIES);
    }, []);

    /**
     * Preview an SFX sound
     */
    const previewSfx = useCallback((sound: FreesoundSound) => {
        const url = getPreviewUrl(sound);
        const audio = new Audio(url);
        audio.volume = 0.5;
        audio.play();
        return () => audio.pause();
    }, []);

    /**
     * Mix audio with SFX and background music
     */
    const mixAudio = useCallback(async (
        contentPlan: ContentPlan,
        narrationSegments: NarrationSegment[],
        options: {
            includeSfx?: boolean;
            includeMusic?: boolean;
        } = {}
    ): Promise<Blob | null> => {
        const { includeSfx = true, includeMusic = true } = options;

        if (!contentPlan || narrationSegments.length === 0) {
            onError?.("No content to mix - generate narration first");
            return null;
        }

        // Build merged narration URL first
        const orderedBlobs: Blob[] = [];
        for (const scene of contentPlan.scenes) {
            const narration = narrationSegments.find(n => n.sceneId === scene.id);
            if (narration?.audioBlob) orderedBlobs.push(narration.audioBlob);
        }

        if (orderedBlobs.length === 0) {
            onError?.("No narration audio available");
            return null;
        }

        try {
            // Merge narration blobs first
            const { mergeConsecutiveAudioBlobs } = await import("@/services/audioMixerService");
            const mergedNarration = await mergeConsecutiveAudioBlobs(orderedBlobs);
            const narrationUrl = URL.createObjectURL(mergedNarration);

            // Build scene timing info
            let currentTime = 0;
            const scenes = contentPlan.scenes.map(scene => {
                const narration = narrationSegments.find(n => n.sceneId === scene.id);
                const duration = narration?.audioDuration || scene.duration;
                const info = {
                    sceneId: scene.id,
                    startTime: currentTime,
                    duration
                };
                currentTime += duration;
                return info;
            });

            // Prepare mix config
            const mixConfig: MixConfig = {
                narrationUrl,
                sfxPlan: includeSfx ? sfxPlan : null,
                scenes,
                sfxMasterVolume: 0.3,
                musicMasterVolume: includeMusic ? 0.5 : 0,
            };

            // Check if we can actually mix SFX
            if (includeSfx && !canMixSFX(sfxPlan)) {
                console.warn("[useVideoSFX] SFX plan has no audio URLs - mixing narration only");
            }

            onProgress?.({
                stage: "validating",
                progress: 50,
                message: "Mixing audio tracks...",
            });

            const mixedBlob = await mixAudioWithSFX(mixConfig);

            // Cleanup temp URL
            URL.revokeObjectURL(narrationUrl);

            onProgress?.({
                stage: "validating",
                progress: 100,
                message: "Audio mix complete!",
            });

            return mixedBlob;
        } catch (err) {
            console.error("[useVideoSFX] Audio mixing failed:", err);
            onError?.(err instanceof Error ? err.message : String(err));
            return null;
        }
    }, [sfxPlan, onProgress, onError]);

    /**
     * Check if Freesound is configured
     */
    const isSfxAvailable = useCallback(() => isFreesoundConfigured(), []);

    /**
     * Reset SFX state
     */
    const resetSFX = useCallback(() => {
        setSfxPlan(null);
    }, []);

    return {
        // State
        sfxPlan,

        // Setters
        setSfxPlan,

        // Actions
        browseSfx,
        getSfxCategories,
        previewSfx,
        mixAudio,
        isSfxAvailable,
        resetSFX,
    };
}
````

## File: packages/frontend/hooks/useVideoVisuals.ts
````typescript
/**
 * useVideoVisuals Hook
 * 
 * Handles visual generation and management for video production.
 * Manages generated images and visual preferences.
 */

import { useState, useCallback, useMemo } from "react";
import { GeneratedImage } from "@/types";
import { CAMERA_ANGLES, LIGHTING_MOODS } from "@/constants/video";

export interface VideoVisualsState {
    visuals: GeneratedImage[];
    preferredCameraAngle: string | null;
    preferredLightingMood: string | null;
}

export function useVideoVisuals() {
    const [visuals, setVisuals] = useState<GeneratedImage[]>([]);
    const [preferredCameraAngle, setPreferredCameraAngle] = useState<string | null>(null);
    const [preferredLightingMood, setPreferredLightingMood] = useState<string | null>(null);

    /**
     * Get visuals map for SceneEditor (sceneId -> imageUrl)
     * Memoized to prevent unnecessary re-renders in consuming components
     */
    const visualsMap = useMemo((): Record<string, string> => {
        const map: Record<string, string> = {};
        visuals.forEach((visual) => {
            if (visual.imageUrl) {
                map[visual.promptId] = visual.imageUrl;
            }
        });
        return map;
    }, [visuals]);

    /**
     * Legacy getter for backward compatibility
     */
    const getVisualsMap = useCallback(() => visualsMap, [visualsMap]);

    /**
     * Get available camera angles
     */
    const getCameraAngles = useCallback(() => [...CAMERA_ANGLES], []);

    /**
     * Get available lighting moods
     */
    const getLightingMoods = useCallback(() => [...LIGHTING_MOODS], []);

    /**
     * Reset visuals state
     */
    const resetVisuals = useCallback(() => {
        setVisuals([]);
        setPreferredCameraAngle(null);
        setPreferredLightingMood(null);
    }, []);

    return {
        // State
        visuals,
        preferredCameraAngle,
        preferredLightingMood,
        visualsMap,

        // Setters
        setVisuals,
        setPreferredCameraAngle,
        setPreferredLightingMood,

        // Actions
        getVisualsMap,
        getCameraAngles,
        getLightingMoods,
        resetVisuals,
    };
}
````

## File: packages/frontend/i18n/index.ts
````typescript
import i18n from 'i18next';
import { initReactI18next } from 'react-i18next';
import LanguageDetector from 'i18next-browser-languagedetector';

import en from './locales/en.json';
import ar from './locales/ar.json';

export const supportedLanguages = ['en', 'ar'] as const;
export type SupportedLanguage = (typeof supportedLanguages)[number];

export const languageConfig = {
  en: { name: 'English', dir: 'ltr' as const, flag: '🇺🇸' },
  ar: { name: 'العربية', dir: 'rtl' as const, flag: '🇸🇦' },
};

i18n
  .use(LanguageDetector)
  .use(initReactI18next)
  .init({
    resources: {
      en: { translation: en },
      ar: { translation: ar },
    },
    fallbackLng: 'en',
    supportedLngs: supportedLanguages,
    interpolation: {
      escapeValue: false, // React already escapes values
    },
    detection: {
      order: ['localStorage', 'navigator'],
      caches: ['localStorage'],
      lookupLocalStorage: 'lyriclens-language',
    },
  });

export default i18n;
````

## File: packages/frontend/i18n/locales/ar.json
````json
{
  "nav": {
    "home": "الرئيسية",
    "studio": "الاستوديو",
    "visualizer": "المُصوِّر",
    "back": "رجوع",
    "settings": "الإعدادات",
    "projects": "المشاريع",
    "quickCreate": "إنشاء سريع",
    "gradientGenerator": "مولد التدرجات",
    "help": "المساعدة"
  },
  "home": {
    "title": "ليريك لينز",
    "subtitle": "إنتاج فيديو بالذكاء الاصطناعي",
    "createVideo": "إنشاء فيديو",
    "createVideoDesc": "أنشئ فيديوهات مذهلة من النص أو المواضيع",
    "createMusic": "إنشاء موسيقى",
    "createMusicDesc": "أنشئ مقاطع موسيقية بالذكاء الاصطناعي",
    "visualizer": "المُصوِّر",
    "visualizerDesc": "أنشئ فيديوهات كلمات من ملفات صوتية",
    "recentProjects": "المشاريع الأخيرة",
    "noProjects": "لا توجد مشاريع حديثة",
    "features": {
      "video": ["السرد بالذكاء الاصطناعي", "توليد المرئيات", "موسيقى خلفية"],
      "music": ["أغاني كاملة", "موسيقى آلية", "كلمات مخصصة"],
      "visualizer": ["مزامنة الكلمات", "مؤثرات بصرية", "توقيت مخصص"]
    }
  },
  "studio": {
    "title": "الاستوديو",
    "placeholder": "صِف ما تريد إنشاءه...",
    "send": "إرسال",
    "export": "تصدير",
    "edit": "تعديل",
    "timeline": "الجدول الزمني",
    "quality": "الجودة",
    "newProject": "مشروع جديد",
    "simpleMode": "بسيط",
    "advancedMode": "متقدم",
    "scenes": "المشاهد",
    "preview": "معاينة",
    "generating": "جارٍ الإنشاء...",
    "processing": "جارٍ المعالجة..."
  },
  "visualizer": {
    "title": "المُصوِّر",
    "uploadAudio": "رفع ملف صوتي",
    "uploadSubtitles": "رفع ملف ترجمة",
    "dropAudio": "اسحب الملف الصوتي هنا أو انقر للتصفح",
    "dropSubtitles": "اسحب ملف SRT هنا أو انقر للتصفح",
    "generate": "إنشاء الفيديو"
  },
  "common": {
    "loading": "جارٍ التحميل...",
    "error": "خطأ",
    "retry": "إعادة المحاولة",
    "cancel": "إلغاء",
    "save": "حفظ",
    "delete": "حذف",
    "confirm": "تأكيد",
    "close": "إغلاق",
    "yes": "نعم",
    "no": "لا",
    "ok": "موافق",
    "download": "تحميل",
    "upload": "رفع",
    "search": "بحث",
    "clear": "مسح",
    "reset": "إعادة تعيين",
    "apply": "تطبيق",
    "done": "تم",
    "next": "التالي",
    "previous": "السابق",
    "comingSoon": "قريباً",
    "unsavedChanges": "تغييرات غير محفوظة",
    "unsavedChangesMessage": "لديك تغييرات غير محفوظة. هل أنت متأكد أنك تريد المغادرة؟ ستفقد تغييراتك.",
    "stay": "البقاء",
    "leaveAnyway": "المغادرة على أي حال",
    "items_zero": "لا عناصر",
    "items_one": "عنصر واحد",
    "items_two": "عنصران",
    "items_few": "{{count}} عناصر",
    "items_many": "{{count}} عنصراً",
    "items_other": "{{count}} عنصر",
    "somethingWentWrong": "حدث خطأ ما",
    "import": "استيراد",
    "all": "الكل"
  },
  "a11y": {
    "languageSwitch": "تغيير اللغة",
    "mainNav": "التنقل الرئيسي",
    "skipToContent": "انتقل إلى المحتوى الرئيسي",
    "currentLanguage": "اللغة الحالية: {{language}}",
    "openMenu": "فتح القائمة",
    "closeMenu": "إغلاق القائمة"
  },
  "errors": {
    "generic": "حدث خطأ ما. يرجى المحاولة مرة أخرى.",
    "network": "خطأ في الشبكة. يرجى التحقق من اتصالك.",
    "notFound": "الصفحة غير موجودة",
    "unauthorized": "غير مصرح لك بعرض هذه الصفحة"
  },
  "story": {
    "charactersEmpty": "لم يتم استخراج الشخصيات بعد.",
    "continuity": "الاستمرارية",
    "visualDescription": "الوصف البصري",
    "issuesFound": "المشاكل المكتشفة",
    "suggestions": "الاقتراحات",
    "verifyContinuity": "تحقق من الاستمرارية",
    "verifying": "جاري التحقق من الاستمرارية...",
    "storyIdea": "فكرة القصة",
    "whatsYourStory": "ما هي قصتك؟",
    "describeYourConcept": "صِف فكرتك وسنحوّلها إلى مشاهد وشخصيات ولوحة قصصية بصرية.",
    "placeholderStory": "محقق يكتشف أن جريمة القتل التي يحقق فيها ارتكبها هو نفسه من المستقبل...",
    "tryTemplate": "جرّب قالباً",
    "browseAll": "تصفح الكل",
    "genre": "النوع",
    "allCount": "الكل {{count}}",
    "buildingStory": "جارٍ بناء قصتك...",
    "beginStory": "ابدأ القصة",
    "breakdown": "التقسيم",
    "storyboard": "لوحة القصة",
    "sceneBreakdown": "تقسيم المشاهد",
    "script": "السيناريو",
    "cast": "فريق التمثيل",
    "shotList": "قائمة اللقطات",
    "visualStyle": "النمط البصري",
    "narration": "السرد",
    "animation": "الرسوم المتحركة",
    "export": "التصدير",
    "shotBreakdown": "تفصيل اللقطات",
    "locked": "مُقفَل",
    "generateShots": "إنشاء اللقطات",
    "noShotsGenerated": "لم يتم إنشاء لقطات",
    "generateNarration": "إنشاء السرد",
    "regenerateNarration": "إعادة إنشاء السرد",
    "narrationDescription": "إنشاء تعليق صوتي بالذكاء الاصطناعي لكل مشهد باستخدام Gemini TTS.",
    "noNarrationYet": "لم يتم إنشاء سرد بعد",
    "clickGenerateNarration": "انقر \"إنشاء السرد\" لإنشاء التعليق الصوتي",
    "regenerateAll": "إعادة إنشاء الكل",
    "animateAllShots": "تحريك كل اللقطات",
    "animationDescription": "تحويل صور لوحة القصة إلى مقاطع فيديو متحركة باستخدام DeAPI أو Veo.",
    "animated": "متحرك",
    "noShotsToAnimate": "لا توجد لقطات للتحريك",
    "generateStoryboardFirst": "أنشئ الرسومات البصرية أولاً",
    "exportVideo": "تصدير الفيديو",
    "scenes": "المشاهد",
    "shots": "اللقطات",
    "duration": "المدة",
    "renderingVideo": "جارٍ معالجة الفيديو...",
    "downloadVideo": "تحميل الفيديو",
    "generateNarrationBeforeExport": "أنشئ السرد قبل التصدير",
    "createScript": "إنشاء السيناريو",
    "continueToCast": "الاستمرار إلى فريق التمثيل",
    "lockScript": "قفل السيناريو",
    "createShotList": "إنشاء قائمة اللقطات",
    "selectStyle": "اختيار النمط",
    "generateStoryboard": "إنشاء لوحة القصة",
    "addNarration": "إضافة السرد",
    "animateShots": "تحريك اللقطات",
    "genres": {
      "Drama": "دراما",
      "Comedy": "كوميديا",
      "Thriller": "إثارة",
      "Sci-Fi": "خيال علمي",
      "Mystery": "غموض",
      "Action": "أكشن",
      "Horror": "رعب",
      "Fantasy": "فانتازيا",
      "Romance": "رومانسي",
      "Historical": "تاريخي",
      "Documentary": "وثائقي",
      "Animation": "رسوم متحركة"
    },
    "templates": {
      "title": "القوالب",
      "searchPlaceholder": "البحث في القوالب...",
      "noTemplatesFound": "لم يتم العثور على قوالب",
      "clearSearch": "مسح البحث",
      "scenes": "المشاهد",
      "visualStyles": "الأنماط البصرية",
      "useTemplate": "استخدام القالب",
      "duration": "المدة",
      "style": "النمط",
      "ratio": "النسبة",
      "difficulty": {
        "beginner": "مبتدئ",
        "intermediate": "متوسط",
        "advanced": "متقدم"
      },
      "categories": {
        "narrative": "سردي",
        "commercial": "تجاري",
        "educational": "تعليمي",
        "social": "وسائل التواصل",
        "experimental": "تجريبي"
      },
      "genres": {
        "Drama": "دراما",
        "Horror": "رعب",
        "Comedy": "كوميديا",
        "Science Fiction": "خيال علمي",
        "Fantasy": "فانتازيا",
        "Commercial": "تجاري",
        "Social": "اجتماعي",
        "Educational": "تعليمي",
        "Music Video": "فيديو موسيقي",
        "Documentary": "وثائقي"
      },
      "tags": {
        "drama": "دراما",
        "emotional": "عاطفي",
        "character-driven": "قائم على الشخصيات",
        "cinematic": "سينمائي",
        "horror": "رعب",
        "suspense": "تشويق",
        "atmospheric": "جو مميز",
        "thriller": "إثارة",
        "comedy": "كوميديا",
        "humor": "فكاهة",
        "sketch": "اسكتش",
        "funny": "مضحك",
        "sci-fi": "خيال علمي",
        "futuristic": "مستقبلي",
        "technology": "تقنية",
        "visionary": "رؤيوي",
        "fantasy": "فانتازيا",
        "adventure": "مغامرة",
        "magic": "سحر",
        "epic": "ملحمي",
        "product": "منتج",
        "commercial": "تجاري",
        "marketing": "تسويق",
        "showcase": "عرض",
        "brand": "علامة تجارية",
        "story": "قصة",
        "values": "قيم",
        "tiktok": "تيك توك",
        "reels": "ريلز",
        "social": "اجتماعي",
        "viral": "فيروسي",
        "short-form": "قصير",
        "educational": "تعليمي",
        "explainer": "شرح",
        "tutorial": "درس",
        "informative": "معلوماتي",
        "music": "موسيقى",
        "artistic": "فني",
        "visual": "بصري",
        "experimental": "تجريبي",
        "documentary": "وثائقي",
        "real": "واقعي",
        "interview": "مقابلة",
        "investigative": "استقصائي"
      },
      "styles": {
        "Cinematic": "سينمائي",
        "Film Noir": "فيلم نوار",
        "Golden Hour": "الساعة الذهبية",
        "Moody": "درامي",
        "Dark Cinematic": "سينمائي داكن",
        "Desaturated": "ألوان باهتة",
        "High Contrast": "تباين عالي",
        "Gothic": "قوطي",
        "Bright": "مشرق",
        "Colorful": "ملوّن",
        "Sitcom": "مسلسل كوميدي",
        "Casual": "عفوي",
        "Cyberpunk": "سايبربانك",
        "Clean Futuristic": "مستقبلي نظيف",
        "Neon Noir": "نيون نوار",
        "Blade Runner": "بليد رنر",
        "Epic Fantasy": "فانتازيا ملحمية",
        "Painterly": "لوحة فنية",
        "Magical Realism": "واقعية سحرية",
        "Lord of the Rings": "سيد الخواتم",
        "Clean Minimal": "بسيط ونظيف",
        "Premium": "فاخر",
        "Tech Modern": "تقني حديث",
        "Lifestyle": "نمط حياة",
        "Documentary": "وثائقي",
        "Warm": "دافئ",
        "Authentic": "أصيل",
        "Trendy": "عصري",
        "Bold": "جريء",
        "High Energy": "طاقة عالية",
        "Aesthetic": "جمالي",
        "Clean": "نظيف",
        "Illustrated": "مرسوم",
        "Infographic": "إنفوجرافيك",
        "Modern": "حديث",
        "Artistic": "فني",
        "Abstract": "تجريدي",
        "Neon": "نيون",
        "Dreamlike": "حالم",
        "Raw": "خام",
        "Journalistic": "صحفي"
      },
      "items": {
        "short-film-drama": {
          "name": "فيلم قصير - دراما",
          "description": "فيلم درامي قصير مدته ٣-٥ دقائق مع قوس عاطفي وتطور شخصيات ومشاهد سينمائية.",
          "scene1_heading": "داخلي. موقع - نهار",
          "scene1_action": "مشهد افتتاحي يُعرّف بالبطل وعالمه.",
          "scene2_heading": "خارجي. موقع - نهار",
          "scene2_action": "الحدث المحرك الذي يقلب حياة البطل.",
          "scene3_heading": "داخلي. موقع - ليل",
          "scene3_action": "تصاعد التوتر والمواجهة العاطفية.",
          "scene4_heading": "خارجي. موقع - نهار",
          "scene4_action": "الحل وتحول الشخصية."
        },
        "horror-short": {
          "name": "فيلم رعب قصير",
          "description": "فيلم رعب مشوق مدته ٢-٣ دقائق مع توتر جوي ومفاجآت مرعبة وصور مقلقة.",
          "scene1_heading": "داخلي. موقع مظلم - ليل",
          "scene1_action": "تأسيس أجواء مقلقة. شيء ما ليس على ما يرام.",
          "scene2_heading": "داخلي. موقع مظلم - ليل",
          "scene2_action": "بداية الأحداث الغريبة. يتصاعد التوتر.",
          "scene3_heading": "داخلي. موقع مظلم - ليل",
          "scene3_action": "يكشف الرعب عن نفسه. ذروة الخوف.",
          "scene4_heading": "داخلي/خارجي. موقع - ليل",
          "scene4_action": "نهاية غامضة تترك المشاهد في حالة قلق."
        },
        "comedy-sketch": {
          "name": "اسكتش كوميدي",
          "description": "اسكتش كوميدي سريع مدته ١-٢ دقيقة مع تمهيد وتصعيد ونهاية مضحكة.",
          "scene1_heading": "داخلي. موقع يومي - نهار",
          "scene1_action": "التمهيد: تأسيس الموقف الطبيعي والشخصيات.",
          "scene2_heading": "داخلي. نفس الموقع - نهار",
          "scene2_action": "التعقيد: يحدث شيء سخيف أو يُكشف عنه.",
          "scene3_heading": "داخلي. نفس الموقع - نهار",
          "scene3_action": "النهاية المضحكة: القفلة الكوميدية."
        },
        "sci-fi-concept": {
          "name": "مفهوم خيال علمي",
          "description": "فيلم خيال علمي مذهل بصرياً مدته ٢-٤ دقائق يستكشف مواضيع مستقبلية.",
          "scene1_heading": "خارجي. مدينة مستقبلية - ليل",
          "scene1_action": "تأسيس العالم وتقنياته المتقدمة.",
          "scene2_heading": "داخلي. منشأة عالية التقنية - نهار",
          "scene2_action": "تقديم المفهوم المركزي أو الصراع.",
          "scene3_heading": "خارجي/داخلي. متنوع - نهار/ليل",
          "scene3_action": "تتكشف تداعيات التقنية.",
          "scene4_heading": "خارجي. منظر مستقبلي - فجر",
          "scene4_action": "خاتمة فلسفية عن الإنسانية والتقنية."
        },
        "fantasy-adventure": {
          "name": "مغامرة فانتازيا",
          "description": "رحلة فانتازيا ملحمية مدتها ٣-٥ دقائق مع عناصر سحرية ولحظات بطولية.",
          "scene1_heading": "خارجي. عالم سحري - نهار",
          "scene1_action": "تأسيس العالم السحري وجماله.",
          "scene2_heading": "داخلي. بناء قديم - نهار",
          "scene2_action": "البطل يتلقى مهمته أو نداءه.",
          "scene3_heading": "خارجي. أرض محفوفة بالمخاطر - نهار",
          "scene3_action": "البطل يواجه التحديات ويُظهر شجاعته.",
          "scene4_heading": "خارجي. موقع الانتصار - غروب",
          "scene4_action": "النصر وتحول البطل."
        },
        "product-showcase": {
          "name": "عرض منتج",
          "description": "فيديو منتج أنيق مدته ٣٠-٦٠ ثانية يسلط الضوء على المميزات والفوائد.",
          "scene1_heading": "كشف المنتج",
          "scene1_action": "كشف درامي للمنتج من الظلام إلى النور.",
          "scene2_heading": "أبرز المميزات",
          "scene2_action": "عرض المميزات الرئيسية بحركات كاميرا ديناميكية.",
          "scene3_heading": "قيد الاستخدام",
          "scene3_action": "المنتج في الاستخدام الواقعي لإظهار الفوائد.",
          "scene4_heading": "دعوة للعمل",
          "scene4_action": "لقطة نهائية للمنتج مع العلامة التجارية."
        },
        "brand-story": {
          "name": "قصة العلامة التجارية",
          "description": "فيديو علامة تجارية عاطفي مدته ١-٢ دقيقة يتواصل مع قيم الجمهور.",
          "scene1_heading": "المشكلة",
          "scene1_action": "عرض التحدي أو الحاجة التي يواجهها الجمهور.",
          "scene2_heading": "الرحلة",
          "scene2_action": "قصة العلامة التجارية وشغفها في حل المشكلة.",
          "scene3_heading": "الحل",
          "scene3_action": "كيف تقدم العلامة التجارية القيمة وتغير الحياة.",
          "scene4_heading": "الرؤية",
          "scene4_action": "المستقبل الذي تبنيه العلامة التجارية مع عملائها."
        },
        "tiktok-reel": {
          "name": "فيديو تيك توك/ريلز",
          "description": "فيديو عمودي سريع مدته ١٥-٦٠ ثانية محسّن للتفاعل الاجتماعي.",
          "scene1_heading": "الجذب",
          "scene1_action": "جذب الانتباه في أول ٣ ثوانٍ.",
          "scene2_heading": "المحتوى",
          "scene2_action": "تقديم الرسالة الرئيسية أو الترفيه.",
          "scene3_heading": "المكافأة",
          "scene3_action": "نهاية مُرضية تشجع على المشاركة."
        },
        "explainer-video": {
          "name": "فيديو توضيحي",
          "description": "فيديو تعليمي واضح مدته ٢-٣ دقائق يشرح المواضيع المعقدة.",
          "scene1_heading": "المقدمة",
          "scene1_action": "تقديم الموضوع وأهميته.",
          "scene2_heading": "المشكلة",
          "scene2_action": "شرح التحدي أو السؤال المطروح.",
          "scene3_heading": "الشرح",
          "scene3_action": "تفكيك المفهوم خطوة بخطوة.",
          "scene4_heading": "النقاط الرئيسية",
          "scene4_action": "تلخيص النقاط الرئيسية والخطوات التالية."
        },
        "music-video-concept": {
          "name": "مفهوم فيديو موسيقي",
          "description": "سرد بصري فني مدته ٣-٤ دقائق مصمم لمرافقة الموسيقى.",
          "scene1_heading": "المقدمة/المقطع الأول",
          "scene1_action": "تحديد المزاج وتقديم المواضيع البصرية.",
          "scene2_heading": "اللازمة الأولى",
          "scene2_action": "انفجار بصري يطابق الذروة الموسيقية.",
          "scene3_heading": "المقطع الثاني/الجسر",
          "scene3_action": "تطوير السرد البصري والمواضيع.",
          "scene4_heading": "اللازمة الأخيرة/الخاتمة",
          "scene4_action": "ذروة بصرية وخاتمة مُرضية."
        },
        "documentary-mini": {
          "name": "فيلم وثائقي قصير",
          "description": "فيلم وثائقي متأنٍ مدته ٣-٥ دقائق يستكشف موضوعاً بعمق.",
          "scene1_heading": "الافتتاحية الباردة",
          "scene1_action": "جذب المشاهد بلقطات أو تصريح مثير للاهتمام.",
          "scene2_heading": "السياق",
          "scene2_action": "تقديم الخلفية والسياق للموضوع.",
          "scene3_heading": "التعمق",
          "scene3_action": "استكشاف الموضوع من زوايا متعددة.",
          "scene4_heading": "الخاتمة",
          "scene4_action": "ترك المشاهد مع شيء للتفكير فيه."
        }
      }
    },
    "breakdown_progress": {
      "developing": "جارٍ التطوير...",
      "craftedFrameByFrame": "يتم صياغة قصتك إطاراً بإطار",
      "readingIdea": "قراءة فكرة القصة",
      "aligningGenre": "المواءمة مع النوع",
      "aligningWith": "المواءمة مع {{genre}}",
      "identifyingCharacters": "تحديد الشخصيات",
      "creatingBreakdown": "إنشاء تقسيم المشاهد",
      "processing": "جارٍ المعالجة"
    },
    "storyboard_progress": {
      "developingVision": "جارٍ تطوير رؤيتك",
      "renderedFrameByFrame": "يتم عرض لوحة القصة إطاراً بإطار...",
      "generatingShotList": "إنشاء قائمة اللقطات",
      "breakingScenes": "تقسيم المشاهد إلى لقطات",
      "preparingCast": "تجهيز فريق التمثيل",
      "loadingProfiles": "تحميل ملفات الشخصيات",
      "renderingStoryboard": "عرض لوحة القصة",
      "creatingFrames": "إنشاء الإطارات البصرية",
      "visualMasterpiece": "تحفتك البصرية تتشكل..."
    },
    "lock_dialog": {
      "lockTheScript": "قفل السيناريو",
      "scriptFinalized": "سيتم اعتماد السيناريو وبدء الإنتاج...",
      "productionEstimate": "تقدير الإنتاج",
      "estimatedShots": "اللقطات المقدّرة",
      "totalBudget": "الميزانية الإجمالية",
      "shotBreakdownInfo": "سيتم إنشاء تفاصيل اللقطات وتجهيز العناصر البصرية للإنتاج. قد تختلف التكاليف النهائية حسب تعقيد اللقطات.",
      "backToEdit": "العودة للتعديل",
      "lockAndBegin": "قفل وبدء"
    },
    "export_panel": {
      "exportOptions": "خيارات التصدير",
      "videoFormats": "صيغ الفيديو",
      "subtitles": "الترجمات",
      "project": "المشروع",
      "mp4Video": "فيديو MP4",
      "mp4Desc": "صيغة فيديو قياسية لجميع الأجهزة",
      "webmVideo": "فيديو WebM",
      "webmDesc": "صيغة محسّنة للويب",
      "srtSubtitles": "ترجمات SRT",
      "srtDesc": "صيغة ترجمة قياسية",
      "webvttSubtitles": "ترجمات WebVTT",
      "webvttDesc": "ترجمات HTML5 الأصلية",
      "projectFile": "ملف المشروع",
      "projectFileDesc": "نسخة احتياطية كاملة (.json)",
      "generateVideoFirst": "أنشئ الفيديو أولاً",
      "generateShotsFirst": "أنشئ اللقطات أولاً",
      "importProject": "استيراد مشروع",
      "importProjectDesc": "اختر ملف مشروع (.json) للاستيراد. سيحل محل مشروعك الحالي.",
      "clickToSelect": "انقر لاختيار ملف",
      "orDragDrop": "أو اسحب وأفلت"
    }
  },
  "projects": {
    "title": "مشاريعي",
    "loading": "جارٍ تحميل المشاريع...",
    "loadError": "فشل تحميل المشاريع",
    "createError": "فشل إنشاء المشروع",
    "create": "إنشاء جديد",
    "createVideo": "مشروع فيديو",
    "createStory": "مشروع قصة",
    "createVisualizer": "مشروع مُصوِّر",
    "favorites": "المفضلة",
    "recent": "الأخيرة",
    "allProjects": "جميع المشاريع",
    "search": "البحث في المشاريع...",
    "empty": "لا توجد مشاريع بعد",
    "emptyHint": "أنشئ مشروعك الأول للبدء",
    "noResults": "لم يتم العثور على مشاريع",
    "tryDifferentSearch": "جرب بحثاً أو فلتراً مختلفاً",
    "open": "فتح",
    "export": "تصدير",
    "delete": "حذف",
    "favorite": "إضافة للمفضلة",
    "unfavorite": "إزالة من المفضلة",
    "confirmDelete": "هل أنت متأكد من حذف هذا المشروع؟ لا يمكن التراجع عن هذا الإجراء."
  }
}
````

## File: packages/frontend/i18n/locales/en.json
````json
{
  "nav": {
    "home": "Home",
    "studio": "Studio",
    "visualizer": "Visualizer",
    "back": "Back",
    "settings": "Settings",
    "projects": "Projects",
    "quickCreate": "Quick Create",
    "gradientGenerator": "Gradient Generator",
    "help": "Help"
  },
  "home": {
    "title": "LyricLens",
    "subtitle": "AI-Powered Video Production",
    "createVideo": "Create Video",
    "createVideoDesc": "Generate stunning videos from text or topics",
    "createMusic": "Generate Music",
    "createMusicDesc": "Create AI-powered music tracks",
    "visualizer": "Visualizer",
    "visualizerDesc": "Create lyric videos from audio files",
    "recentProjects": "Recent Projects",
    "noProjects": "No recent projects",
    "features": {
      "video": ["AI Narration", "Visual Generation", "Background Music"],
      "music": ["Full Songs", "Instrumentals", "Custom Lyrics"],
      "visualizer": ["Lyric Sync", "Visual Effects", "Custom Timing"]
    }
  },
  "studio": {
    "title": "Studio",
    "placeholder": "Describe what you want to create...",
    "send": "Send",
    "export": "Export",
    "edit": "Edit",
    "timeline": "Timeline",
    "quality": "Quality",
    "newProject": "New Project",
    "simpleMode": "Simple",
    "advancedMode": "Advanced",
    "scenes": "Scenes",
    "preview": "Preview",
    "generating": "Generating...",
    "processing": "Processing..."
  },
  "visualizer": {
    "title": "Visualizer",
    "uploadAudio": "Upload Audio",
    "uploadSubtitles": "Upload Subtitles",
    "dropAudio": "Drop audio file here or click to browse",
    "dropSubtitles": "Drop SRT file here or click to browse",
    "generate": "Generate Video"
  },
  "common": {
    "loading": "Loading...",
    "error": "Error",
    "retry": "Retry",
    "cancel": "Cancel",
    "save": "Save",
    "delete": "Delete",
    "confirm": "Confirm",
    "close": "Close",
    "yes": "Yes",
    "no": "No",
    "ok": "OK",
    "download": "Download",
    "upload": "Upload",
    "search": "Search",
    "clear": "Clear",
    "reset": "Reset",
    "apply": "Apply",
    "done": "Done",
    "next": "Next",
    "previous": "Previous",
    "comingSoon": "Coming Soon",
    "unsavedChanges": "Unsaved Changes",
    "unsavedChangesMessage": "You have unsaved changes. Are you sure you want to leave? Your changes will be lost.",
    "stay": "Stay",
    "leaveAnyway": "Leave Anyway",
    "items_one": "{{count}} item",
    "items_other": "{{count}} items",
    "somethingWentWrong": "Something went wrong",
    "import": "Import",
    "all": "All"
  },
  "a11y": {
    "languageSwitch": "Switch language",
    "mainNav": "Main navigation",
    "skipToContent": "Skip to main content",
    "currentLanguage": "Current language: {{language}}",
    "openMenu": "Open menu",
    "closeMenu": "Close menu"
  },
  "errors": {
    "generic": "Something went wrong. Please try again.",
    "network": "Network error. Please check your connection.",
    "notFound": "Page not found",
    "unauthorized": "You are not authorized to view this page"
  },
  "story": {
    "charactersEmpty": "Characters haven't been extracted yet.",
    "continuity": "Continuity",
    "visualDescription": "Visual Description",
    "issuesFound": "Issues Found",
    "suggestions": "Suggestions",
    "verifyContinuity": "Verify Continuity",
    "verifying": "Verifying continuity...",
    "storyIdea": "Story Idea",
    "whatsYourStory": "What's your story?",
    "describeYourConcept": "Describe your concept and we'll shape it into scenes, characters, and a visual storyboard.",
    "placeholderStory": "A detective discovers that the murder they're investigating was committed by their own future self...",
    "tryTemplate": "Try a template",
    "browseAll": "Browse all",
    "genre": "Genre",
    "allCount": "All {{count}}",
    "buildingStory": "Building your story...",
    "beginStory": "Begin Story",
    "breakdown": "Breakdown",
    "storyboard": "Storyboard",
    "sceneBreakdown": "Scene Breakdown",
    "script": "Script",
    "cast": "Cast",
    "shotList": "Shot List",
    "visualStyle": "Visual Style",
    "narration": "Narration",
    "animation": "Animation",
    "export": "Export",
    "shotBreakdown": "Shot Breakdown",
    "locked": "LOCKED",
    "generateShots": "Generate Shots",
    "noShotsGenerated": "No shots generated",
    "generateNarration": "Generate Narration",
    "regenerateNarration": "Regenerate Narration",
    "narrationDescription": "Generate AI voiceover for each scene using Gemini TTS.",
    "noNarrationYet": "No narration generated yet",
    "clickGenerateNarration": "Click \"Generate Narration\" to create voiceover",
    "regenerateAll": "Regenerate All",
    "animateAllShots": "Animate All Shots",
    "animationDescription": "Convert storyboard images to animated video clips using DeAPI or Veo.",
    "animated": "Animated",
    "noShotsToAnimate": "No shots to animate",
    "generateStoryboardFirst": "Generate storyboard visuals first",
    "exportVideo": "Export Video",
    "scenes": "Scenes",
    "shots": "Shots",
    "duration": "Duration",
    "renderingVideo": "Rendering Video...",
    "downloadVideo": "Download Video",
    "generateNarrationBeforeExport": "Generate narration before exporting",
    "createScript": "Create Script",
    "continueToCast": "Continue to Cast",
    "lockScript": "Lock Script",
    "createShotList": "Create Shot List",
    "selectStyle": "Select Style",
    "generateStoryboard": "Generate Storyboard",
    "addNarration": "Add Narration",
    "animateShots": "Animate Shots",
    "genres": {
      "Drama": "Drama",
      "Comedy": "Comedy",
      "Thriller": "Thriller",
      "Sci-Fi": "Sci-Fi",
      "Mystery": "Mystery",
      "Action": "Action",
      "Horror": "Horror",
      "Fantasy": "Fantasy",
      "Romance": "Romance",
      "Historical": "Historical",
      "Documentary": "Doc",
      "Animation": "Animation"
    },
    "templates": {
      "title": "Templates",
      "searchPlaceholder": "Search templates...",
      "noTemplatesFound": "No templates found",
      "clearSearch": "Clear search",
      "scenes": "Scenes",
      "visualStyles": "Visual Styles",
      "useTemplate": "Use Template",
      "duration": "Duration",
      "style": "Style",
      "ratio": "Ratio",
      "difficulty": {
        "beginner": "Beginner",
        "intermediate": "Intermediate",
        "advanced": "Advanced"
      },
      "categories": {
        "narrative": "Narrative",
        "commercial": "Commercial",
        "educational": "Educational",
        "social": "Social Media",
        "experimental": "Experimental"
      },
      "genres": {
        "Drama": "Drama",
        "Horror": "Horror",
        "Comedy": "Comedy",
        "Science Fiction": "Science Fiction",
        "Fantasy": "Fantasy",
        "Commercial": "Commercial",
        "Social": "Social",
        "Educational": "Educational",
        "Music Video": "Music Video",
        "Documentary": "Documentary"
      },
      "tags": {
        "drama": "drama",
        "emotional": "emotional",
        "character-driven": "character-driven",
        "cinematic": "cinematic",
        "horror": "horror",
        "suspense": "suspense",
        "atmospheric": "atmospheric",
        "thriller": "thriller",
        "comedy": "comedy",
        "humor": "humor",
        "sketch": "sketch",
        "funny": "funny",
        "sci-fi": "sci-fi",
        "futuristic": "futuristic",
        "technology": "technology",
        "visionary": "visionary",
        "fantasy": "fantasy",
        "adventure": "adventure",
        "magic": "magic",
        "epic": "epic",
        "product": "product",
        "commercial": "commercial",
        "marketing": "marketing",
        "showcase": "showcase",
        "brand": "brand",
        "story": "story",
        "values": "values",
        "tiktok": "tiktok",
        "reels": "reels",
        "social": "social",
        "viral": "viral",
        "short-form": "short-form",
        "educational": "educational",
        "explainer": "explainer",
        "tutorial": "tutorial",
        "informative": "informative",
        "music": "music",
        "artistic": "artistic",
        "visual": "visual",
        "experimental": "experimental",
        "documentary": "documentary",
        "real": "real",
        "interview": "interview",
        "investigative": "investigative"
      },
      "styles": {
        "Cinematic": "Cinematic",
        "Film Noir": "Film Noir",
        "Golden Hour": "Golden Hour",
        "Moody": "Moody",
        "Dark Cinematic": "Dark Cinematic",
        "Desaturated": "Desaturated",
        "High Contrast": "High Contrast",
        "Gothic": "Gothic",
        "Bright": "Bright",
        "Colorful": "Colorful",
        "Sitcom": "Sitcom",
        "Casual": "Casual",
        "Cyberpunk": "Cyberpunk",
        "Clean Futuristic": "Clean Futuristic",
        "Neon Noir": "Neon Noir",
        "Blade Runner": "Blade Runner",
        "Epic Fantasy": "Epic Fantasy",
        "Painterly": "Painterly",
        "Magical Realism": "Magical Realism",
        "Lord of the Rings": "Lord of the Rings",
        "Clean Minimal": "Clean Minimal",
        "Premium": "Premium",
        "Tech Modern": "Tech Modern",
        "Lifestyle": "Lifestyle",
        "Documentary": "Documentary",
        "Warm": "Warm",
        "Authentic": "Authentic",
        "Trendy": "Trendy",
        "Bold": "Bold",
        "High Energy": "High Energy",
        "Aesthetic": "Aesthetic",
        "Clean": "Clean",
        "Illustrated": "Illustrated",
        "Infographic": "Infographic",
        "Modern": "Modern",
        "Artistic": "Artistic",
        "Abstract": "Abstract",
        "Neon": "Neon",
        "Dreamlike": "Dreamlike",
        "Raw": "Raw",
        "Journalistic": "Journalistic"
      },
      "items": {
        "short-film-drama": {
          "name": "Short Film - Drama",
          "description": "A compelling 3-5 minute dramatic short with emotional arc, character development, and cinematic visuals.",
          "scene1_heading": "INT. LOCATION - DAY",
          "scene1_action": "Opening scene establishing the protagonist and their world.",
          "scene2_heading": "EXT. LOCATION - DAY",
          "scene2_action": "The inciting incident that disrupts the protagonist's life.",
          "scene3_heading": "INT. LOCATION - NIGHT",
          "scene3_action": "Rising tension and emotional confrontation.",
          "scene4_heading": "EXT. LOCATION - DAY",
          "scene4_action": "Resolution and character transformation."
        },
        "horror-short": {
          "name": "Horror Short",
          "description": "A tense 2-3 minute horror piece with atmospheric tension, jump scares, and unsettling imagery.",
          "scene1_heading": "INT. DARK LOCATION - NIGHT",
          "scene1_action": "Establishing an unsettling atmosphere. Something feels wrong.",
          "scene2_heading": "INT. DARK LOCATION - NIGHT",
          "scene2_action": "Strange occurrences begin. Tension builds.",
          "scene3_heading": "INT. DARK LOCATION - NIGHT",
          "scene3_action": "The horror reveals itself. Peak terror moment.",
          "scene4_heading": "INT/EXT. LOCATION - NIGHT",
          "scene4_action": "Ambiguous ending leaving audience unsettled."
        },
        "comedy-sketch": {
          "name": "Comedy Sketch",
          "description": "A punchy 1-2 minute comedy sketch with setup, escalation, and punchline.",
          "scene1_heading": "INT. EVERYDAY LOCATION - DAY",
          "scene1_action": "Setup: Establish the normal situation and characters.",
          "scene2_heading": "INT. SAME LOCATION - DAY",
          "scene2_action": "Complication: Something absurd happens or is revealed.",
          "scene3_heading": "INT. SAME LOCATION - DAY",
          "scene3_action": "Punchline: The comedic payoff that lands the joke."
        },
        "sci-fi-concept": {
          "name": "Sci-Fi Concept",
          "description": "A visually stunning 2-4 minute science fiction piece exploring futuristic themes.",
          "scene1_heading": "EXT. FUTURISTIC CITY - NIGHT",
          "scene1_action": "Establishing the world and its advanced technology.",
          "scene2_heading": "INT. HIGH-TECH FACILITY - DAY",
          "scene2_action": "Introduction of the central concept or conflict.",
          "scene3_heading": "EXT/INT. VARIOUS - DAY/NIGHT",
          "scene3_action": "The implications of the technology unfold.",
          "scene4_heading": "EXT. FUTURISTIC VISTA - DAWN",
          "scene4_action": "Philosophical conclusion about humanity and technology."
        },
        "fantasy-adventure": {
          "name": "Fantasy Adventure",
          "description": "An epic 3-5 minute fantasy journey with magical elements and heroic moments.",
          "scene1_heading": "EXT. MAGICAL REALM - DAY",
          "scene1_action": "Establishing the magical world and its beauty.",
          "scene2_heading": "INT. ANCIENT STRUCTURE - DAY",
          "scene2_action": "The hero receives their quest or calling.",
          "scene3_heading": "EXT. PERILOUS TERRAIN - DAY",
          "scene3_action": "The hero faces trials and demonstrates courage.",
          "scene4_heading": "EXT. TRIUMPHANT LOCATION - SUNSET",
          "scene4_action": "Victory and transformation of the hero."
        },
        "product-showcase": {
          "name": "Product Showcase",
          "description": "A sleek 30-60 second product video highlighting features and benefits.",
          "scene1_heading": "PRODUCT REVEAL",
          "scene1_action": "Dramatic reveal of the product from darkness to light.",
          "scene2_heading": "FEATURE HIGHLIGHTS",
          "scene2_action": "Showcase key features with dynamic camera movements.",
          "scene3_heading": "IN USE",
          "scene3_action": "Product in real-world use showing benefits.",
          "scene4_heading": "CALL TO ACTION",
          "scene4_action": "Final product shot with branding and CTA."
        },
        "brand-story": {
          "name": "Brand Story",
          "description": "An emotional 1-2 minute brand video connecting with audience values.",
          "scene1_heading": "THE PROBLEM",
          "scene1_action": "Showing the challenge or need the audience faces.",
          "scene2_heading": "THE JOURNEY",
          "scene2_action": "The brand's story and passion for solving the problem.",
          "scene3_heading": "THE SOLUTION",
          "scene3_action": "How the brand delivers value and changes lives.",
          "scene4_heading": "THE VISION",
          "scene4_action": "The future the brand is building with its customers."
        },
        "tiktok-reel": {
          "name": "TikTok/Reels Video",
          "description": "A fast-paced 15-60 second vertical video optimized for social engagement.",
          "scene1_heading": "HOOK",
          "scene1_action": "Grab attention in the first 3 seconds.",
          "scene2_heading": "CONTENT",
          "scene2_action": "Deliver the main message or entertainment.",
          "scene3_heading": "PAYOFF",
          "scene3_action": "Satisfying ending that encourages sharing."
        },
        "explainer-video": {
          "name": "Explainer Video",
          "description": "A clear 2-3 minute educational video breaking down complex topics.",
          "scene1_heading": "INTRODUCTION",
          "scene1_action": "Introduce the topic and why it matters.",
          "scene2_heading": "THE PROBLEM",
          "scene2_action": "Explain the challenge or question being addressed.",
          "scene3_heading": "THE EXPLANATION",
          "scene3_action": "Break down the concept step by step.",
          "scene4_heading": "KEY TAKEAWAYS",
          "scene4_action": "Recap the main points and next steps."
        },
        "music-video-concept": {
          "name": "Music Video Concept",
          "description": "An artistic 3-4 minute visual narrative designed to accompany music.",
          "scene1_heading": "INTRO/VERSE 1",
          "scene1_action": "Set the mood and introduce visual themes.",
          "scene2_heading": "CHORUS 1",
          "scene2_action": "Visual explosion matching musical peak.",
          "scene3_heading": "VERSE 2/BRIDGE",
          "scene3_action": "Development of visual narrative and themes.",
          "scene4_heading": "FINAL CHORUS/OUTRO",
          "scene4_action": "Peak visual moment and satisfying conclusion."
        },
        "documentary-mini": {
          "name": "Mini Documentary",
          "description": "A thoughtful 3-5 minute documentary piece exploring a subject in depth.",
          "scene1_heading": "COLD OPEN",
          "scene1_action": "Hook the viewer with intriguing footage or statement.",
          "scene2_heading": "CONTEXT",
          "scene2_action": "Provide background and context for the subject.",
          "scene3_heading": "DEEP DIVE",
          "scene3_action": "Explore the subject through multiple perspectives.",
          "scene4_heading": "CONCLUSION",
          "scene4_action": "Leave the viewer with something to think about."
        }
      }
    },
    "breakdown_progress": {
      "developing": "DEVELOPING...",
      "craftedFrameByFrame": "Your story is being crafted frame by frame",
      "readingIdea": "Reading your story idea",
      "aligningGenre": "Aligning with genre",
      "aligningWith": "Aligning with {{genre}}",
      "identifyingCharacters": "Identifying characters",
      "creatingBreakdown": "Creating scene breakdown",
      "processing": "Processing"
    },
    "storyboard_progress": {
      "developingVision": "DEVELOPING YOUR VISION",
      "renderedFrameByFrame": "Your storyboard is being rendered frame by frame...",
      "generatingShotList": "Generating Shot List",
      "breakingScenes": "Breaking down scenes into shots",
      "preparingCast": "Preparing Cast",
      "loadingProfiles": "Loading character profiles",
      "renderingStoryboard": "Rendering Storyboard",
      "creatingFrames": "Creating visual frames",
      "visualMasterpiece": "Your visual masterpiece is taking shape..."
    },
    "lock_dialog": {
      "lockTheScript": "LOCK THE SCRIPT",
      "scriptFinalized": "Your screenplay will be finalized and production begins...",
      "productionEstimate": "Production Estimate",
      "estimatedShots": "Estimated Shots",
      "totalBudget": "Total Budget",
      "shotBreakdownInfo": "Shot breakdowns will be generated and visuals prepared for production. Final costs may vary based on shot complexity.",
      "backToEdit": "Back to Edit",
      "lockAndBegin": "Lock & Begin"
    },
    "export_panel": {
      "exportOptions": "Export Options",
      "videoFormats": "Video Formats",
      "subtitles": "Subtitles",
      "project": "Project",
      "mp4Video": "MP4 Video",
      "mp4Desc": "Standard video format for all devices",
      "webmVideo": "WebM Video",
      "webmDesc": "Web-optimized format for embedding",
      "srtSubtitles": "SRT Subtitles",
      "srtDesc": "Standard subtitle format",
      "webvttSubtitles": "WebVTT Subtitles",
      "webvttDesc": "HTML5 native subtitles",
      "projectFile": "Project File",
      "projectFileDesc": "Full project backup (.json)",
      "generateVideoFirst": "Generate video first",
      "generateShotsFirst": "Generate shots first",
      "importProject": "Import Project",
      "importProjectDesc": "Select a project file (.json) to import. This will replace your current project.",
      "clickToSelect": "Click to select a file",
      "orDragDrop": "or drag and drop"
    }
  },
  "projects": {
    "title": "My Projects",
    "loading": "Loading projects...",
    "loadError": "Failed to load projects",
    "createError": "Failed to create project",
    "create": "Create New",
    "createVideo": "Video Project",
    "createStory": "Story Project",
    "createVisualizer": "Visualizer Project",
    "favorites": "Favorites",
    "recent": "Recent",
    "allProjects": "All Projects",
    "search": "Search projects...",
    "empty": "No projects yet",
    "emptyHint": "Create your first project to get started",
    "noResults": "No projects found",
    "tryDifferentSearch": "Try a different search or filter",
    "open": "Open",
    "export": "Export",
    "delete": "Delete",
    "favorite": "Add to favorites",
    "unfavorite": "Remove from favorites",
    "confirmDelete": "Are you sure you want to delete this project? This cannot be undone."
  }
}
````

## File: packages/frontend/i18n/useLanguage.ts
````typescript
import { useCallback, useEffect } from 'react';
import { useTranslation } from 'react-i18next';
import { languageConfig, SupportedLanguage, supportedLanguages } from './index';

export interface UseLanguageReturn {
  language: SupportedLanguage;
  direction: 'ltr' | 'rtl';
  isRTL: boolean;
  setLanguage: (lang: SupportedLanguage) => void;
  t: ReturnType<typeof useTranslation>['t'];
  languageConfig: typeof languageConfig;
  supportedLanguages: typeof supportedLanguages;
}

export function useLanguage(): UseLanguageReturn {
  const { t, i18n } = useTranslation();

  const language = (supportedLanguages.includes(i18n.language as SupportedLanguage)
    ? i18n.language
    : 'en') as SupportedLanguage;

  const direction = languageConfig[language].dir;
  const isRTL = direction === 'rtl';

  const setLanguage = useCallback(
    (lang: SupportedLanguage) => {
      i18n.changeLanguage(lang);
    },
    [i18n]
  );

  // Update HTML attributes when language changes
  useEffect(() => {
    const html = document.documentElement;
    html.setAttribute('lang', language);
    html.setAttribute('dir', direction);
  }, [language, direction]);

  return {
    language,
    direction,
    isRTL,
    setLanguage,
    t,
    languageConfig,
    supportedLanguages,
  };
}

export default useLanguage;
````

## File: packages/frontend/index.tsx
````typescript
import React from 'react';
import ReactDOM from 'react-dom/client';
import './index.css';
import App from './App';

// Expose test functions only in development
if (import.meta.env.DEV) {
  import('@/services/freesoundService').then(({ testFreesoundAPI }) => {
    if (typeof window !== "undefined") {
      (window as any).testFreesoundAPI = testFreesoundAPI;
    }
  });

  // Test Firebase configuration at startup
  import('@/services/firebase/config').then(({ isFirebaseConfigured, getFirebaseApp, getFirebaseAuth }) => {
    console.log('=== FIREBASE STARTUP TEST ===');
    console.log('[Firebase] Is configured?', isFirebaseConfigured());
    console.log('[Firebase] Environment variables:', {
      apiKey: import.meta.env.VITE_FIREBASE_API_KEY ? '✓ Set' : '✗ Missing',
      authDomain: import.meta.env.VITE_FIREBASE_AUTH_DOMAIN ? '✓ Set' : '✗ Missing',
      projectId: import.meta.env.VITE_FIREBASE_PROJECT_ID ? '✓ Set' : '✗ Missing',
      // storageBucket not used - GCS handled by cloudStorageService.ts
      messagingSenderId: import.meta.env.VITE_FIREBASE_MESSAGING_SENDER_ID ? '✓ Set' : '✗ Missing',
      appId: import.meta.env.VITE_FIREBASE_APP_ID ? '✓ Set' : '✗ Missing',
    });

    const app = getFirebaseApp();
    console.log('[Firebase] App initialized?', !!app);

    if (app) {
      const auth = getFirebaseAuth();
      console.log('[Firebase] Auth initialized?', !!auth);
      if (auth) {
        console.log('[Firebase] Auth config:', {
          appName: auth.app.name,
          apiKey: auth.config.apiKey.substring(0, 10) + '...',
          authDomain: auth.config.authDomain
        });
      }
    }
    console.log('=== END FIREBASE TEST ===');
  });
}

const rootElement = document.getElementById('root');
if (!rootElement) {
  throw new Error("Could not find root element to mount to");
}

const root = ReactDOM.createRoot(rootElement);
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);
````

## File: packages/frontend/router/guards/index.ts
````typescript
/**
 * Router guards exports
 */

export { UnsavedChangesGuard, useUnsavedChanges } from './UnsavedChangesGuard';
````

## File: packages/frontend/router/guards/UnsavedChangesGuard.tsx
````typescript
/**
 * Unsaved Changes Guard
 * Requirements: 10.3 - Warn users before navigating away from unsaved work
 * 
 * This component provides:
 * 1. A hook to track unsaved changes
 * 2. Browser beforeunload event handling
 * 3. React Router navigation blocking with confirmation dialog
 */

import React, { useEffect, useCallback, useState } from 'react';
import { useBlocker, useLocation } from 'react-router-dom';
import { useTranslation } from 'react-i18next';
import { useAppStore } from '@/stores';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import { AlertTriangle } from 'lucide-react';

interface UnsavedChangesGuardProps {
  /** Whether there are unsaved changes to protect */
  hasUnsavedChanges?: boolean;
  /** Custom message to show in the dialog */
  message?: string;
  /** Callback when user confirms leaving */
  onConfirmLeave?: () => void;
  /** Children to render */
  children?: React.ReactNode;
}

/**
 * Guard component that prevents navigation when there are unsaved changes
 */
export function UnsavedChangesGuard({
  hasUnsavedChanges: propHasUnsavedChanges,
  message,
  onConfirmLeave,
  children,
}: UnsavedChangesGuardProps) {
  const { t } = useTranslation();
  const location = useLocation();
  
  // Get unsaved changes state from store if not provided via props
  const storeHasUnsavedChanges = useAppStore((s) => s.navigationState.hasUnsavedChanges);
  const hasUnsavedChanges = propHasUnsavedChanges ?? storeHasUnsavedChanges;
  
  const setHasUnsavedChanges = useAppStore((s) => s.setHasUnsavedChanges);
  const setLastRoute = useAppStore((s) => s.setLastRoute);

  // Track current route
  useEffect(() => {
    setLastRoute(location.pathname);
  }, [location.pathname, setLastRoute]);

  // Block navigation when there are unsaved changes
  const blocker = useBlocker(
    ({ currentLocation, nextLocation }) =>
      hasUnsavedChanges && currentLocation.pathname !== nextLocation.pathname
  );

  // Handle browser beforeunload event (refresh, close tab)
  useEffect(() => {
    const handleBeforeUnload = (e: BeforeUnloadEvent) => {
      if (hasUnsavedChanges) {
        e.preventDefault();
        // Modern browsers require returnValue to be set
        e.returnValue = '';
        return '';
      }
      return undefined;
    };

    window.addEventListener('beforeunload', handleBeforeUnload);
    return () => window.removeEventListener('beforeunload', handleBeforeUnload);
  }, [hasUnsavedChanges]);

  // Handle confirm leave
  const handleConfirmLeave = useCallback(() => {
    setHasUnsavedChanges(false);
    onConfirmLeave?.();
    if (blocker.state === 'blocked') {
      blocker.proceed();
    }
  }, [blocker, setHasUnsavedChanges, onConfirmLeave]);

  // Handle cancel (stay on page)
  const handleCancel = useCallback(() => {
    if (blocker.state === 'blocked') {
      blocker.reset();
    }
  }, [blocker]);

  const defaultMessage = t('common.unsavedChangesMessage', {
    defaultValue: 'You have unsaved changes. Are you sure you want to leave? Your changes will be lost.',
  });

  return (
    <>
      {children}
      
      {/* Confirmation Dialog */}
      <Dialog open={blocker.state === 'blocked'} onOpenChange={(open) => !open && handleCancel()}>
        <DialogContent className="sm:max-w-md bg-zinc-900 border-zinc-800">
          <DialogHeader>
            <DialogTitle className="flex items-center gap-2 text-white">
              <AlertTriangle className="w-5 h-5 text-amber-500" />
              {t('common.unsavedChanges', { defaultValue: 'Unsaved Changes' })}
            </DialogTitle>
            <DialogDescription className="text-zinc-400">
              {message || defaultMessage}
            </DialogDescription>
          </DialogHeader>
          <DialogFooter className="flex gap-2 sm:gap-0">
            <Button
              variant="ghost"
              onClick={handleCancel}
              className="text-zinc-400 hover:text-white hover:bg-zinc-800"
            >
              {t('common.stay', { defaultValue: 'Stay' })}
            </Button>
            <Button
              variant="destructive"
              onClick={handleConfirmLeave}
              className="bg-red-600 hover:bg-red-700"
            >
              {t('common.leaveAnyway', { defaultValue: 'Leave Anyway' })}
            </Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>
    </>
  );
}

/**
 * Hook to manage unsaved changes state
 * Use this in components that need to track unsaved changes
 */
export function useUnsavedChanges() {
  const hasUnsavedChanges = useAppStore((s) => s.navigationState.hasUnsavedChanges);
  const setHasUnsavedChanges = useAppStore((s) => s.setHasUnsavedChanges);

  const markAsUnsaved = useCallback(() => {
    setHasUnsavedChanges(true);
  }, [setHasUnsavedChanges]);

  const markAsSaved = useCallback(() => {
    setHasUnsavedChanges(false);
  }, [setHasUnsavedChanges]);

  return {
    hasUnsavedChanges,
    markAsUnsaved,
    markAsSaved,
    setHasUnsavedChanges,
  };
}

export default UnsavedChangesGuard;
````

## File: packages/frontend/router/index.tsx
````typescript
/**
 * Router configuration for LyricLens
 * Requirements: 2.1 - Use React Router for all navigation
 * Requirements: 2.2 - Support routes: / (Home), /studio (Studio), /visualizer (Visualizer)
 * Requirements: 5.1 - Show NotFound page for invalid routes
 */

import { Suspense, lazy } from 'react';
import {
  BrowserRouter,
  Routes,
  Route,
} from 'react-router-dom';
import { RouteLayout } from './RouteLayout';

// Lazy load screen components for code splitting
const HomeScreen = lazy(() => import('../screens/HomeScreen'));
const StudioScreen = lazy(() => import('../screens/StudioScreen'));
const VisualizerScreen = lazy(() => import('../screens/VisualizerScreen'));
const ProjectsScreen = lazy(() => import('../screens/ProjectsScreen'));
const GradientGeneratorScreen = lazy(() => import('../screens/GradientGeneratorScreen'));
const SettingsScreen = lazy(() => import('../screens/SettingsScreen'));
const SignInScreen = lazy(() => import('../screens/SignInScreen'));
const NotFoundScreen = lazy(() => import('../screens/NotFoundScreen'));

// Loading fallback component
function LoadingFallback() {
  return (
    <div className="flex items-center justify-center min-h-screen">
      <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-primary" />
    </div>
  );
}

/**
 * Main router component with all route definitions
 */
export function AppRouter() {
  return (
    <BrowserRouter>
      <Suspense fallback={<LoadingFallback />}>
        <Routes>
          <Route element={<RouteLayout />}>
            {/* Home route - default landing page */}
            <Route path="/" element={<HomeScreen />} />

            {/* Projects route - user's project dashboard */}
            <Route path="/projects" element={<ProjectsScreen />} />

            {/* Studio route - unified creation workspace */}
            <Route path="/studio" element={<StudioScreen />} />

            {/* Visualizer route - audio-first lyric videos */}
            <Route path="/visualizer" element={<VisualizerScreen />} />

            {/* Gradient Generator route - CSS gradient creation tool */}
            <Route path="/gradient-generator" element={<GradientGeneratorScreen />} />

            {/* Settings route - API key management */}
            <Route path="/settings" element={<SettingsScreen />} />

            {/* Sign-in route - authentication page */}
            <Route path="/signin" element={<SignInScreen />} />

            {/* Catch-all: show 404 page for invalid routes (Requirement 5.1) */}
            <Route path="*" element={<NotFoundScreen />} />
          </Route>
        </Routes>
      </Suspense>
    </BrowserRouter>
  );
}

export { routes, getRouteByPath, isValidRoute } from './routes';
export { UnsavedChangesGuard, useUnsavedChanges } from './guards';
````

## File: packages/frontend/router/RouteLayout.tsx
````typescript
/**
 * Route layout wrapper component
 * Requirements: 2.2 - Handle document title updates per route
 */

import React, { useEffect } from 'react';
import { Outlet, useLocation } from 'react-router-dom';
import { useTranslation } from 'react-i18next';
import { getRouteByPath } from './routes';
import { Sidebar } from '@/components/layout/Sidebar';
import { useLanguage } from '@/i18n/useLanguage';
import { cn } from '@/lib/utils';

/**
 * Layout wrapper that applies to all routes
 * - Updates document title based on current route
 * - Provides consistent layout structure with Sidebar
 */
export function RouteLayout() {
  const location = useLocation();
  const { t } = useTranslation();
  const { isRTL } = useLanguage();

  // Update document title when route changes
  useEffect(() => {
    const routeConfig = getRouteByPath(location.pathname);
    if (routeConfig) {
      const translatedTitle = t(routeConfig.title);
      document.title = `${translatedTitle} | LyricLens`;
    } else {
      document.title = 'LyricLens';
    }
  }, [location.pathname, t]);

  return (
    <div className={cn('flex min-h-screen', isRTL && 'flex-row-reverse')}>
      {/* Sidebar Navigation */}
      <aside
        className="w-16 shrink-0 bg-[#0a0a0f]/80 backdrop-blur-xl border-white/5 z-30"
        style={{ borderInlineEnd: '1px solid rgba(255,255,255,0.05)' }}
      >
        <div className="sticky top-0 h-screen py-4 px-2">
          <Sidebar />
        </div>
      </aside>

      {/* Main Content */}
      <main className="flex-1 min-h-screen overflow-x-hidden">
        <Outlet />
      </main>
    </div>
  );
}
````

## File: packages/frontend/router/routes.ts
````typescript
/**
 * Route definitions for LyricLens application
 * Requirements: 2.2 - Support routes: / (Home), /studio (Studio), /visualizer (Visualizer)
 */

export interface RouteConfig {
  path: string;
  title: string; // i18n key for document title
  meta?: {
    requiresAuth?: boolean;
    preserveState?: boolean;
  };
}

export const routes: RouteConfig[] = [
  {
    path: '/',
    title: 'nav.home',
  },
  {
    path: '/projects',
    title: 'nav.projects',
    meta: { requiresAuth: true },
  },
  {
    path: '/studio',
    title: 'nav.studio',
    meta: { preserveState: true },
  },
  {
    path: '/visualizer',
    title: 'nav.visualizer',
    meta: { preserveState: true },
  },
  {
    path: '/gradient-generator',
    title: 'nav.gradientGenerator',
  },
  {
    path: '/settings',
    title: 'nav.settings',
  },
];

/**
 * Get route config by path
 */
export function getRouteByPath(path: string): RouteConfig | undefined {
  return routes.find((route) => route.path === path);
}

/**
 * Check if a path is a valid route
 */
export function isValidRoute(path: string): boolean {
  return routes.some((route) => route.path === path);
}
````

## File: packages/frontend/screens/GradientGeneratorScreen.tsx
````typescript
/**
 * Gradient Generator Screen - CSS gradient creation tool
 * Provides a full-screen interface for creating and exporting CSS gradients
 */

import React from 'react';
import { useNavigate } from 'react-router-dom';
import { motion } from 'framer-motion';
import { Palette, Sparkles } from 'lucide-react';
import { useLanguage } from '@/i18n/useLanguage';
import { cn } from '@/lib/utils';
import { ScreenLayout } from '@/components/layout/ScreenLayout';
import { GradientGenerator } from '@/components/gradient-generator';

export default function GradientGeneratorScreen() {
  const { t, isRTL } = useLanguage();
  const navigate = useNavigate();

  return (
    <ScreenLayout
      title={t('nav.gradientGenerator') || 'Gradient Generator'}
      showBackButton
      onBack={() => navigate('/')}
      maxWidth="full"
      contentClassName="py-6"
    >
      {/* Header Section */}
      <motion.div
        initial={{ opacity: 0, y: 20 }}
        animate={{ opacity: 1, y: 0 }}
        className="mb-8 text-center"
      >
        <div className={cn('flex items-center justify-center gap-3 mb-4', isRTL && 'flex-row-reverse')}>
          <div className="w-14 h-14 rounded-xl bg-gradient-to-br from-violet-500 to-purple-600 flex items-center justify-center shadow-lg shadow-violet-500/30">
            <Palette className="w-7 h-7 text-white" aria-hidden="true" />
          </div>
          <div className={cn('text-left', isRTL && 'text-right')}>
            <h1 className="text-2xl md:text-3xl font-bold text-white">
              {t('nav.gradientGenerator') || 'Gradient Generator'}
            </h1>
            <p className="text-sm text-white/60">
              Create beautiful CSS gradients with ease
            </p>
          </div>
        </div>
      </motion.div>

      {/* Gradient Generator Component */}
      <motion.div
        initial={{ opacity: 0, y: 20 }}
        animate={{ opacity: 1, y: 0 }}
        transition={{ delay: 0.1 }}
      >
        <GradientGenerator
          showPresets={true}
          showExportPanel={true}
          maxColorStops={10}
          minColorStops={2}
          enableAnimation={false}
        />
      </motion.div>

      {/* Footer Info */}
      <motion.div
        initial={{ opacity: 0 }}
        animate={{ opacity: 1 }}
        transition={{ delay: 0.2 }}
        className="mt-8 text-center text-sm text-white/40"
      >
        <div className={cn('flex items-center justify-center gap-2', isRTL && 'flex-row-reverse')}>
          <Sparkles className="w-4 h-4" />
          <span>Create, customize, and export CSS gradients for your projects</span>
        </div>
      </motion.div>
    </ScreenLayout>
  );
}
````

## File: packages/frontend/screens/HomeScreen.tsx
````typescript
/**
 * Home Screen - Landing page with creation mode selection
 * Requirements: 1.1, 1.2 - Display Home screen as default with 3 main screens
 * Requirements: 7.1, 7.2 - Display max 3 creation mode cards and navigate on selection
 * Requirements: 9.1 - Use semantic HTML elements (nav, main, header, footer)
 */

import React, { useEffect, useRef } from 'react';
import { useNavigate, useLocation } from 'react-router-dom';
import { motion } from 'framer-motion';
import { Video, Music, AudioWaveform, Film } from 'lucide-react';
import { useLanguage } from '@/i18n/useLanguage';
import { cn } from '@/lib/utils';
import { Header } from '@/components/layout/Header';
import { ForwardChevron } from '@/components/layout/DirectionalIcon';
import {
  staggerContainer,
  staggerItem,
} from '@/lib/cinematicMotion';

// Creation mode card data with cinematic palette
const CREATION_MODES = [
  {
    id: 'video' as const,
    titleKey: 'home.createVideo',
    descKey: 'home.createVideoDesc',
    featuresKey: 'home.features.video',
    icon: Video,
    accentColor: 'var(--cinema-spotlight)',
    accentGlow: 'var(--glow-spotlight)',
    route: '/studio?mode=video',
  },
  {
    id: 'music' as const,
    titleKey: 'home.createMusic',
    descKey: 'home.createMusicDesc',
    featuresKey: 'home.features.music',
    icon: Music,
    accentColor: 'var(--cinema-editorial)',
    accentGlow: 'var(--glow-velvet)',
    route: '/studio?mode=music',
  },
  {
    id: 'visualizer' as const,
    titleKey: 'home.visualizer',
    descKey: 'home.visualizerDesc',
    featuresKey: 'home.features.visualizer',
    icon: AudioWaveform,
    accentColor: 'var(--primary)',
    accentGlow: 'var(--glow-primary)',
    route: '/visualizer',
  },
];

export default function HomeScreen() {
  const { t, isRTL } = useLanguage();
  const navigate = useNavigate();
  const location = useLocation();
  const mainContentRef = useRef<HTMLElement>(null);

  // Focus main content on navigation (Requirement 9.4)
  useEffect(() => {
    const timer = setTimeout(() => {
      mainContentRef.current?.focus();
    }, 100);
    return () => clearTimeout(timer);
  }, [location.pathname]);

  const handleModeSelect = (route: string) => {
    navigate(route);
  };

  return (
    <div className="min-h-screen bg-background text-foreground overflow-hidden flex flex-col">
      {/* Cinematic background */}
      <div className="fixed inset-0 pointer-events-none" aria-hidden="true">
        <div
          className="absolute top-[-20%] left-[10%] w-[600px] h-[600px] rounded-full blur-[180px] opacity-20"
          style={{ background: 'var(--cinema-spotlight)' }}
        />
        <div
          className="absolute bottom-[-10%] right-[15%] w-[500px] h-[500px] rounded-full blur-[160px] opacity-10"
          style={{ background: 'var(--primary)' }}
        />
        <div
          className="absolute top-[40%] right-[5%] w-[300px] h-[300px] rounded-full blur-[120px] opacity-8"
          style={{ background: 'var(--cinema-velvet)' }}
        />
      </div>

      {/* Content */}
      <div className="relative z-10 flex flex-col flex-1">
        {/* Header */}
        <div className="p-4 md:p-6">
          <Header />
        </div>

        {/* Main Content */}
        <main
          id="main-content"
          ref={mainContentRef}
          className="flex-1 flex items-center justify-center p-4 md:p-6"
          tabIndex={-1}
          aria-label={t('home.title')}
        >
          <motion.div
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            transition={{ duration: 0.8, ease: [0.22, 1, 0.36, 1] }}
            className="max-w-5xl w-full"
          >
            {/* Title Block */}
            <div className={cn('text-center mb-12 md:mb-16', isRTL && 'rtl')}>
              <motion.div
                initial={{ opacity: 0, y: 16 }}
                animate={{ opacity: 1, y: 0 }}
                transition={{ delay: 0.1, duration: 0.6, ease: [0.22, 1, 0.36, 1] }}
                className="flex items-center justify-center gap-3 mb-6"
              >
                <div
                  className="w-12 h-12 md:w-14 md:h-14 rounded-xl flex items-center justify-center"
                  style={{
                    background: 'linear-gradient(135deg, var(--cinema-spotlight), oklch(0.65 0.12 70))',
                    boxShadow: '0 4px 24px var(--glow-spotlight)',
                  }}
                >
                  <Film className="w-6 h-6 md:w-7 md:h-7 text-[var(--cinema-void)]" aria-hidden="true" />
                </div>
              </motion.div>

              <motion.h1
                initial={{ opacity: 0, y: 12 }}
                animate={{ opacity: 1, y: 0 }}
                transition={{ delay: 0.2, duration: 0.6, ease: [0.22, 1, 0.36, 1] }}
                className="heading-hero mb-4"
              >
                {t('home.title')}
              </motion.h1>

              <motion.p
                initial={{ opacity: 0, y: 8 }}
                animate={{ opacity: 1, y: 0 }}
                transition={{ delay: 0.3, duration: 0.5, ease: [0.22, 1, 0.36, 1] }}
                className="text-body-editorial max-w-2xl mx-auto"
              >
                {t('home.subtitle')}
              </motion.p>
            </div>

            {/* Mode Cards */}
            <nav aria-label={t('a11y.mainNav')}>
              <motion.div
                variants={staggerContainer}
                initial="initial"
                animate="animate"
                className="grid grid-cols-1 md:grid-cols-3 gap-5 md:gap-6"
                role="list"
              >
                {CREATION_MODES.map((mode) => {
                  const Icon = mode.icon;
                  const features = t(mode.featuresKey, { returnObjects: true }) as string[];
                  return (
                    <motion.button
                      key={mode.id}
                      variants={staggerItem}
                      onClick={() => handleModeSelect(mode.route)}
                      whileHover={{ y: -6, transition: { duration: 0.3, ease: [0.22, 1, 0.36, 1] } }}
                      whileTap={{ scale: 0.98 }}
                      className={cn(
                        'group relative surface-card p-6 md:p-7 text-start',
                        'focus:outline-none focus-visible:ring-2 focus-visible:ring-[var(--cinema-spotlight)]/50 focus-visible:ring-offset-2 focus-visible:ring-offset-background',
                        isRTL && 'text-right'
                      )}
                      style={{
                        ['--card-accent' as string]: mode.accentColor,
                      }}
                      aria-label={`${t(mode.titleKey)} - ${t(mode.descKey)}`}
                      role="listitem"
                    >
                      {/* Top accent line */}
                      <div
                        className="absolute top-0 left-4 right-4 h-px opacity-0 group-hover:opacity-100 transition-opacity duration-500"
                        style={{ background: `linear-gradient(90deg, transparent, ${mode.accentColor}, transparent)` }}
                      />

                      {/* Icon */}
                      <div
                        className="w-11 h-11 rounded-lg flex items-center justify-center mb-5 transition-all duration-300 group-hover:shadow-lg"
                        style={{
                          background: `color-mix(in oklch, ${mode.accentColor}, transparent 88%)`,
                          border: `1px solid color-mix(in oklch, ${mode.accentColor}, transparent 75%)`,
                        }}
                        aria-hidden="true"
                      >
                        <Icon
                          className="w-5 h-5 transition-colors duration-300"
                          style={{ color: mode.accentColor }}
                        />
                      </div>

                      {/* Title & Description */}
                      <h3 className="heading-card mb-2 transition-colors duration-300">
                        {t(mode.titleKey)}
                      </h3>
                      <p className="text-body-editorial text-sm mb-5 leading-relaxed">
                        {t(mode.descKey)}
                      </p>

                      {/* Features */}
                      <div
                        className={cn('flex flex-wrap gap-2', isRTL && 'justify-end')}
                        aria-label="Features"
                      >
                        {Array.isArray(features) &&
                          features.map((feature: string, i: number) => (
                            <span
                              key={i}
                              className="px-2.5 py-1 text-[11px] font-editorial font-medium rounded-md transition-colors duration-200"
                              style={{
                                background: 'rgba(255,255,255,0.04)',
                                border: '1px solid rgba(255,255,255,0.06)',
                                color: 'oklch(0.70 0.02 60)',
                              }}
                            >
                              {feature}
                            </span>
                          ))}
                      </div>

                      {/* Arrow indicator */}
                      <div
                        className={cn(
                          'absolute top-7 opacity-0 group-hover:opacity-100 transition-all duration-300 group-hover:translate-x-1',
                          isRTL ? 'left-6 group-hover:-translate-x-1' : 'right-6'
                        )}
                        aria-hidden="true"
                      >
                        <ForwardChevron size={18} className="text-[var(--cinema-silver)]/60" />
                      </div>
                    </motion.button>
                  );
                })}
              </motion.div>
            </nav>
          </motion.div>
        </main>

        {/* Footer */}
        <footer className="p-4 md:p-6 text-center">
          <span className="text-caption-mono">
            Powered by Gemini AI & Suno
          </span>
        </footer>
      </div>
    </div>
  );
}
````

## File: packages/frontend/screens/index.ts
````typescript
/**
 * Screen components barrel export
 */

export { default as HomeScreen } from './HomeScreen';
export { default as StudioScreen, parseStudioParams } from './StudioScreen';
export type { StudioParams } from './StudioScreen';
export { default as VisualizerScreen } from './VisualizerScreen';
export { default as SettingsScreen } from './SettingsScreen';
export { default as GradientGeneratorScreen } from './GradientGeneratorScreen';
export { default as NotFoundScreen } from './NotFoundScreen';
````

## File: packages/frontend/screens/NotFoundScreen.tsx
````typescript
/**
 * NotFound Screen - 404 error page for invalid routes
 * Requirements: 5.1 - Display NotFound page for invalid routes
 * Requirements: 5.2, 5.3, 5.4 - Display 404 message, home button, and use design system
 * Requirements: 5.5 - Support i18n translations
 */

import React from 'react';
import { useNavigate } from 'react-router-dom';
import { motion } from 'framer-motion';
import { Home, AlertCircle } from 'lucide-react';
import { useLanguage } from '@/i18n/useLanguage';
import { Button } from '@/components/ui/button';
import { cn } from '@/lib/utils';

export default function NotFoundScreen() {
  const { t, isRTL } = useLanguage();
  const navigate = useNavigate();

  const handleGoHome = () => {
    navigate('/');
  };

  return (
    <div className="min-h-screen bg-[#0a0a0f] text-white overflow-hidden flex items-center justify-center">
      {/* Background gradient */}
      <div className="fixed inset-0 pointer-events-none" aria-hidden="true">
        <div className="absolute top-1/4 left-1/4 w-96 h-96 bg-red-500/10 rounded-full blur-[128px]" />
        <div className="absolute bottom-1/4 right-1/4 w-96 h-96 bg-orange-500/10 rounded-full blur-[128px]" />
      </div>

      {/* Content */}
      <div className="relative z-10 p-4 md:p-6">
        <motion.div
          initial={{ opacity: 0, y: 20 }}
          animate={{ opacity: 1, y: 0 }}
          className={cn(
            "max-w-md w-full text-center",
            isRTL && "rtl"
          )}
        >
          {/* Glass panel container */}
          <div className="p-8 md:p-12 rounded-2xl bg-white/5 backdrop-blur-sm border border-white/10">
            {/* Icon */}
            <motion.div
              initial={{ scale: 0 }}
              animate={{ scale: 1 }}
              transition={{ delay: 0.2, type: "spring", stiffness: 200 }}
              className="flex justify-center mb-6"
            >
              <div className="w-20 h-20 rounded-full bg-gradient-to-br from-red-500/20 to-orange-500/20 flex items-center justify-center border border-red-500/30">
                <AlertCircle className="w-10 h-10 text-red-400" aria-hidden="true" />
              </div>
            </motion.div>

            {/* 404 Heading */}
            <motion.h1
              initial={{ opacity: 0 }}
              animate={{ opacity: 1 }}
              transition={{ delay: 0.3 }}
              className="text-6xl md:text-7xl font-bold mb-4 bg-gradient-to-br from-red-400 to-orange-400 bg-clip-text text-transparent"
            >
              404
            </motion.h1>

            {/* Error message */}
            <motion.p
              initial={{ opacity: 0 }}
              animate={{ opacity: 1 }}
              transition={{ delay: 0.4 }}
              className="text-lg md:text-xl text-white/80 mb-8"
            >
              {t('errors.notFound')}
            </motion.p>

            {/* Home button */}
            <motion.div
              initial={{ opacity: 0 }}
              animate={{ opacity: 1 }}
              transition={{ delay: 0.5 }}
            >
              <Button
                onClick={handleGoHome}
                size="lg"
                className="bg-gradient-to-r from-violet-500 to-purple-600 hover:from-violet-600 hover:to-purple-700 text-white"
                aria-label={t('nav.home')}
              >
                <Home className="w-5 h-5" aria-hidden="true" />
                {t('nav.home')}
              </Button>
            </motion.div>
          </div>
        </motion.div>
      </div>
    </div>
  );
}
````

## File: packages/frontend/screens/ProjectsScreen.tsx
````typescript
/**
 * Projects Screen - User's Project Dashboard
 *
 * Displays all user projects in a grid with:
 * - Search and filter capabilities
 * - Create new project options
 * - Recent projects section
 * - Favorite projects section
 */

import React, { useEffect, useState, useMemo, useRef } from 'react';
import { useNavigate, useLocation } from 'react-router-dom';
import { motion, AnimatePresence } from 'framer-motion';
import {
  Plus,
  Search,
  Folder,
  Video,
  Film,
  AudioWaveform,
  Star,
  Clock,
  Grid3X3,
  List,
  SortAsc,
  SortDesc,
  Loader2,
  FolderOpen,
  Sparkles,
} from 'lucide-react';
import { useLanguage } from '@/i18n/useLanguage';
import { cn } from '@/lib/utils';
import { Header } from '@/components/layout/Header';
import { ProjectCard } from '@/components/projects/ProjectCard';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from '@/components/ui/select';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import {
  listUserProjects,
  deleteProject,
  toggleFavorite,
  createProject,
  type Project,
  type ProjectType,
} from '@/services/projectService';
import { useAuth } from '@/hooks/useAuth';

type SortField = 'updatedAt' | 'createdAt' | 'title';
type SortOrder = 'asc' | 'desc';
type FilterType = 'all' | ProjectType;
type ViewMode = 'grid' | 'list';

const CREATE_OPTIONS: Array<{
  type: ProjectType;
  icon: typeof Video;
  titleKey: string;
  gradient: string;
  iconColor: string;
}> = [
  {
    type: 'production',
    icon: Video,
    titleKey: 'projects.createVideo',
    gradient: 'from-primary/80 to-primary/40',
    iconColor: 'text-primary',
  },
  {
    type: 'story',
    icon: Film,
    titleKey: 'projects.createStory',
    gradient: 'from-accent/80 to-accent/40',
    iconColor: 'text-accent',
  },
  {
    type: 'visualizer',
    icon: AudioWaveform,
    titleKey: 'projects.createVisualizer',
    gradient: 'from-ring/80 to-ring/40',
    iconColor: 'text-ring',
  },
];

export default function ProjectsScreen() {
  const { t, isRTL } = useLanguage();
  const navigate = useNavigate();
  const location = useLocation();
  const mainContentRef = useRef<HTMLElement>(null);

  // State
  const [projects, setProjects] = useState<Project[]>([]);
  const [isLoading, setIsLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);
  const [searchQuery, setSearchQuery] = useState('');
  const [filterType, setFilterType] = useState<FilterType>('all');
  const [sortField, setSortField] = useState<SortField>('updatedAt');
  const [sortOrder, setSortOrder] = useState<SortOrder>('desc');
  const [viewMode, setViewMode] = useState<ViewMode>('grid');
  const [isCreating, setIsCreating] = useState(false);

  // Check authentication
  const { user, isAuthenticated, isLoading: authLoading } = useAuth();

  // Redirect to sign in if not authenticated (only after auth check completes)
  useEffect(() => {
    if (!authLoading && !isAuthenticated) {
      navigate('/signin', { state: { from: location.pathname } });
    }
  }, [authLoading, isAuthenticated, navigate, location]);

  // Focus main content on navigation
  useEffect(() => {
    const timer = setTimeout(() => {
      mainContentRef.current?.focus();
    }, 100);
    return () => clearTimeout(timer);
  }, [location.pathname]);

  // Load projects (only when authenticated)
  useEffect(() => {
    async function loadProjects() {
      if (!isAuthenticated) return;

      setIsLoading(true);
      setError(null);

      try {
        const userProjects = await listUserProjects(100);
        setProjects(userProjects);
      } catch (err) {
        console.error('[ProjectsScreen] Failed to load projects:', err);
        setError(t('projects.loadError') || 'Failed to load projects');
      } finally {
        setIsLoading(false);
      }
    }

    loadProjects();
  }, [isAuthenticated, t]);

  // Filtered and sorted projects
  const filteredProjects = useMemo(() => {
    let result = [...projects];

    // Apply search filter
    if (searchQuery) {
      const query = searchQuery.toLowerCase();
      result = result.filter(
        (p) =>
          p.title.toLowerCase().includes(query) ||
          p.topic?.toLowerCase().includes(query) ||
          p.description?.toLowerCase().includes(query)
      );
    }

    // Apply type filter
    if (filterType !== 'all') {
      result = result.filter((p) => p.type === filterType);
    }

    // Apply sorting
    result.sort((a, b) => {
      let comparison = 0;

      if (sortField === 'title') {
        comparison = a.title.localeCompare(b.title);
      } else if (sortField === 'createdAt') {
        comparison = a.createdAt.getTime() - b.createdAt.getTime();
      } else {
        comparison = a.updatedAt.getTime() - b.updatedAt.getTime();
      }

      return sortOrder === 'desc' ? -comparison : comparison;
    });

    return result;
  }, [projects, searchQuery, filterType, sortField, sortOrder]);

  // Recent projects (last 5 accessed)
  const recentProjects = useMemo(() => {
    return [...projects]
      .filter((p) => p.lastAccessedAt)
      .sort((a, b) => {
        const aTime = a.lastAccessedAt?.getTime() || 0;
        const bTime = b.lastAccessedAt?.getTime() || 0;
        return bTime - aTime;
      })
      .slice(0, 5);
  }, [projects]);

  // Favorite projects
  const favoriteProjects = useMemo(() => {
    return projects.filter((p) => p.isFavorite);
  }, [projects]);

  // Handlers
  const handleCreateProject = async (type: ProjectType) => {
    setIsCreating(true);

    try {
      const title =
        type === 'production'
          ? 'New Video'
          : type === 'story'
            ? 'New Story'
            : 'New Visualizer';

      const project = await createProject({
        title,
        type,
      });

      if (project) {
        // Navigate to the appropriate screen with the new project
        const routes: Record<ProjectType, string> = {
          production: '/studio?mode=video',
          story: '/studio?mode=story',
          visualizer: '/visualizer',
        };

        const route = routes[type];
        const separator = route.includes('?') ? '&' : '?';
        navigate(`${route}${separator}projectId=${project.id}`);
      }
    } catch (err) {
      console.error('[ProjectsScreen] Failed to create project:', err);
      setError(t('projects.createError') || 'Failed to create project');
    } finally {
      setIsCreating(false);
    }
  };

  const handleDeleteProject = async (projectId: string) => {
    const success = await deleteProject(projectId);
    if (success) {
      setProjects((prev) => prev.filter((p) => p.id !== projectId));
    }
  };

  const handleToggleFavorite = async (projectId: string) => {
    const success = await toggleFavorite(projectId);
    if (success) {
      setProjects((prev) =>
        prev.map((p) =>
          p.id === projectId ? { ...p, isFavorite: !p.isFavorite } : p
        )
      );
    }
  };

  const toggleSortOrder = () => {
    setSortOrder((prev) => (prev === 'asc' ? 'desc' : 'asc'));
  };

  // Render loading state
  if (isLoading) {
    return (
      <div className="min-h-screen bg-background text-foreground flex items-center justify-center">
        <div className="flex flex-col items-center gap-4">
          <Loader2 className="w-8 h-8 animate-spin text-primary" />
          <p className="text-muted-foreground font-editorial">{t('projects.loading') || 'Loading projects...'}</p>
        </div>
      </div>
    );
  }

  return (
    <div className="min-h-screen bg-background text-foreground overflow-hidden flex flex-col">
      {/* Background ambient glow */}
      <div className="fixed inset-0 pointer-events-none" aria-hidden="true">
        <div
          className="absolute top-[-10%] left-[10%] w-[50%] h-[50%] rounded-full blur-[160px] mix-blend-screen"
          style={{ backgroundColor: 'oklch(0.70 0.15 190 / 0.08)' }}
        />
        <div
          className="absolute bottom-[-10%] right-[10%] w-[40%] h-[40%] rounded-full blur-[140px] mix-blend-screen"
          style={{ backgroundColor: 'oklch(0.65 0.25 30 / 0.05)' }}
        />
      </div>

      {/* Content */}
      <div className="relative z-10 flex flex-col flex-1">
        {/* Header */}
        <div className="p-4 md:p-6">
          <Header />
        </div>

        {/* Main Content */}
        <main
          id="main-content"
          ref={mainContentRef}
          className="flex-1 px-4 md:px-6 pb-6 overflow-auto"
          tabIndex={-1}
          aria-label={t('projects.title') || 'My Projects'}
        >
          <div className="max-w-7xl mx-auto">
            {/* Page Title & Create Button */}
            <motion.div
              initial={{ opacity: 0, y: 12 }}
              animate={{ opacity: 1, y: 0 }}
              transition={{ duration: 0.5, ease: [0.22, 1, 0.36, 1] }}
              className={cn(
                'flex flex-col sm:flex-row sm:items-center justify-between gap-4 mb-8',
                isRTL && 'sm:flex-row-reverse'
              )}
            >
              <div className={cn(isRTL && 'text-right')}>
                <h1 className="text-2xl md:text-3xl font-display font-bold flex items-center gap-3 text-foreground">
                  <div className="w-10 h-10 rounded-xl bg-gradient-to-br from-primary/20 to-accent/10 border border-border flex items-center justify-center">
                    <Folder className="w-5 h-5 text-primary" />
                  </div>
                  {t('projects.title') || 'My Projects'}
                </h1>
                <p className="text-muted-foreground mt-1.5 font-editorial text-sm">
                  {projects.length} {projects.length === 1 ? 'project' : 'projects'}
                </p>
              </div>

              {/* Create New Dropdown */}
              <DropdownMenu>
                <DropdownMenuTrigger asChild>
                  <Button
                    disabled={isCreating}
                    className="bg-primary hover:bg-primary/90 text-primary-foreground shadow-lg shadow-primary/20"
                  >
                    {isCreating ? (
                      <Loader2 className="w-4 h-4 mr-2 animate-spin" />
                    ) : (
                      <Plus className="w-4 h-4 mr-2" />
                    )}
                    {t('projects.create') || 'Create New'}
                  </Button>
                </DropdownMenuTrigger>
                <DropdownMenuContent align="end" className="w-56">
                  {CREATE_OPTIONS.map((option) => {
                    const Icon = option.icon;
                    return (
                      <DropdownMenuItem
                        key={option.type}
                        onClick={() => handleCreateProject(option.type)}
                        className="cursor-pointer"
                      >
                        <div
                          className={cn(
                            'w-8 h-8 rounded-lg flex items-center justify-center mr-3',
                            'bg-gradient-to-br',
                            option.gradient
                          )}
                        >
                          <Icon className="w-4 h-4 text-white" />
                        </div>
                        {t(option.titleKey) || option.type}
                      </DropdownMenuItem>
                    );
                  })}
                </DropdownMenuContent>
              </DropdownMenu>
            </motion.div>

            {/* Error Display */}
            {error && (
              <div className="mb-6 p-4 rounded-xl bg-destructive/10 border border-destructive/20 text-destructive">
                {error}
              </div>
            )}

            {/* Show Recent & Favorites if there are projects */}
            {projects.length > 0 && (
              <>
                {/* Favorites Section */}
                {favoriteProjects.length > 0 && (
                  <motion.section
                    initial={{ opacity: 0, y: 12 }}
                    animate={{ opacity: 1, y: 0 }}
                    transition={{ duration: 0.5, delay: 0.1 }}
                    className="mb-10"
                  >
                    <h2
                      className={cn(
                        'text-sm font-editorial font-semibold mb-4 flex items-center gap-2 text-muted-foreground uppercase tracking-wider',
                        isRTL && 'flex-row-reverse'
                      )}
                    >
                      <Star className="w-4 h-4 text-accent fill-accent" />
                      {t('projects.favorites') || 'Favorites'}
                    </h2>
                    <div className="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4">
                      <AnimatePresence mode="popLayout">
                        {favoriteProjects.slice(0, 4).map((project) => (
                          <ProjectCard
                            key={project.id}
                            project={project}
                            onDelete={handleDeleteProject}
                            onToggleFavorite={handleToggleFavorite}
                          />
                        ))}
                      </AnimatePresence>
                    </div>
                  </motion.section>
                )}

                {/* Recent Section */}
                {recentProjects.length > 0 && (
                  <motion.section
                    initial={{ opacity: 0, y: 12 }}
                    animate={{ opacity: 1, y: 0 }}
                    transition={{ duration: 0.5, delay: 0.15 }}
                    className="mb-10"
                  >
                    <h2
                      className={cn(
                        'text-sm font-editorial font-semibold mb-4 flex items-center gap-2 text-muted-foreground uppercase tracking-wider',
                        isRTL && 'flex-row-reverse'
                      )}
                    >
                      <Clock className="w-4 h-4 text-primary" />
                      {t('projects.recent') || 'Recent'}
                    </h2>
                    <div className="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 xl:grid-cols-5 gap-4">
                      <AnimatePresence mode="popLayout">
                        {recentProjects.map((project) => (
                          <ProjectCard
                            key={project.id}
                            project={project}
                            onDelete={handleDeleteProject}
                            onToggleFavorite={handleToggleFavorite}
                          />
                        ))}
                      </AnimatePresence>
                    </div>
                  </motion.section>
                )}
              </>
            )}

            {/* All Projects Section */}
            <motion.section
              initial={{ opacity: 0, y: 12 }}
              animate={{ opacity: 1, y: 0 }}
              transition={{ duration: 0.5, delay: 0.2 }}
            >
              <div
                className={cn(
                  'flex flex-col sm:flex-row sm:items-center justify-between gap-4 mb-4',
                  isRTL && 'sm:flex-row-reverse'
                )}
              >
                <h2
                  className={cn(
                    'text-sm font-editorial font-semibold flex items-center gap-2 text-muted-foreground uppercase tracking-wider',
                    isRTL && 'flex-row-reverse'
                  )}
                >
                  <FolderOpen className="w-4 h-4" />
                  {t('projects.allProjects') || 'All Projects'}
                </h2>

                {/* Filters & Search */}
                <div
                  className={cn(
                    'flex flex-wrap items-center gap-2',
                    isRTL && 'flex-row-reverse'
                  )}
                >
                  {/* Search */}
                  <div className="relative">
                    <Search className="absolute left-3 top-1/2 -translate-y-1/2 w-4 h-4 text-muted-foreground" />
                    <Input
                      type="text"
                      placeholder={t('projects.search') || 'Search...'}
                      value={searchQuery}
                      onChange={(e) => setSearchQuery(e.target.value)}
                      className="pl-9 w-48 bg-secondary border-border"
                    />
                  </div>

                  {/* Type Filter */}
                  <Select
                    value={filterType}
                    onValueChange={(value) => setFilterType(value as FilterType)}
                  >
                    <SelectTrigger className="w-32 bg-secondary border-border">
                      <SelectValue placeholder="All types" />
                    </SelectTrigger>
                    <SelectContent>
                      <SelectItem value="all">All types</SelectItem>
                      <SelectItem value="production">Video</SelectItem>
                      <SelectItem value="story">Story</SelectItem>
                      <SelectItem value="visualizer">Visualizer</SelectItem>
                    </SelectContent>
                  </Select>

                  {/* Sort */}
                  <Select
                    value={sortField}
                    onValueChange={(value) => setSortField(value as SortField)}
                  >
                    <SelectTrigger className="w-32 bg-secondary border-border">
                      <SelectValue placeholder="Sort by" />
                    </SelectTrigger>
                    <SelectContent>
                      <SelectItem value="updatedAt">Last updated</SelectItem>
                      <SelectItem value="createdAt">Created</SelectItem>
                      <SelectItem value="title">Title</SelectItem>
                    </SelectContent>
                  </Select>

                  {/* Sort Order */}
                  <Button
                    variant="outline"
                    size="icon"
                    onClick={toggleSortOrder}
                    className="bg-secondary border-border"
                  >
                    {sortOrder === 'desc' ? (
                      <SortDesc className="w-4 h-4" />
                    ) : (
                      <SortAsc className="w-4 h-4" />
                    )}
                  </Button>

                  {/* View Mode */}
                  <div className="flex rounded-lg border border-border overflow-hidden">
                    <Button
                      variant="ghost"
                      size="icon"
                      onClick={() => setViewMode('grid')}
                      className={cn(
                        'rounded-none',
                        viewMode === 'grid' && 'bg-secondary'
                      )}
                    >
                      <Grid3X3 className="w-4 h-4" />
                    </Button>
                    <Button
                      variant="ghost"
                      size="icon"
                      onClick={() => setViewMode('list')}
                      className={cn(
                        'rounded-none',
                        viewMode === 'list' && 'bg-secondary'
                      )}
                    >
                      <List className="w-4 h-4" />
                    </Button>
                  </div>
                </div>
              </div>

              {/* Projects Grid/List */}
              {filteredProjects.length > 0 ? (
                <div
                  className={cn(
                    viewMode === 'grid'
                      ? 'grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-4'
                      : 'flex flex-col gap-2'
                  )}
                >
                  <AnimatePresence mode="popLayout">
                    {filteredProjects.map((project) => (
                      <ProjectCard
                        key={project.id}
                        project={project}
                        onDelete={handleDeleteProject}
                        onToggleFavorite={handleToggleFavorite}
                      />
                    ))}
                  </AnimatePresence>
                </div>
              ) : (
                <motion.div
                  initial={{ opacity: 0 }}
                  animate={{ opacity: 1 }}
                  className="py-20 text-center"
                >
                  {searchQuery || filterType !== 'all' ? (
                    <>
                      <div className="w-16 h-16 mx-auto mb-5 rounded-2xl bg-secondary border border-border flex items-center justify-center">
                        <Search className="w-7 h-7 text-muted-foreground" />
                      </div>
                      <p className="text-foreground/70 font-editorial">
                        {t('projects.noResults') || 'No projects found'}
                      </p>
                      <p className="text-muted-foreground text-sm mt-1.5">
                        {t('projects.tryDifferentSearch') ||
                          'Try a different search or filter'}
                      </p>
                    </>
                  ) : (
                    <>
                      <div className="w-20 h-20 mx-auto mb-6 rounded-2xl bg-gradient-to-br from-primary/10 to-accent/10 border border-border flex items-center justify-center">
                        <Sparkles className="w-9 h-9 text-primary/60" />
                      </div>
                      <p className="text-foreground/80 text-lg mb-2 font-display">
                        {t('projects.empty') || 'No projects yet'}
                      </p>
                      <p className="text-muted-foreground text-sm mb-8 max-w-sm mx-auto">
                        {t('projects.emptyHint') ||
                          'Create your first project to get started'}
                      </p>
                      <div className="flex justify-center gap-3">
                        {CREATE_OPTIONS.map((option) => {
                          const Icon = option.icon;
                          return (
                            <Button
                              key={option.type}
                              variant="outline"
                              onClick={() => handleCreateProject(option.type)}
                              className="bg-secondary border-border hover:bg-muted hover:border-primary/30 transition-all"
                            >
                              <Icon className={cn("w-4 h-4 mr-2", option.iconColor)} />
                              {t(option.titleKey) || option.type}
                            </Button>
                          );
                        })}
                      </div>
                    </>
                  )}
                </motion.div>
              )}
            </motion.section>
          </div>
        </main>
      </div>
    </div>
  );
}
````

## File: packages/frontend/screens/SettingsScreen.tsx
````typescript
/**
 * Settings Screen - API key management and app configuration
 * Requirements: Settings page for managing API keys and preferences
 */

import React, { useState, useEffect } from 'react';
import { useNavigate } from 'react-router-dom';
import { motion } from 'framer-motion';
import {
  Key,
  CheckCircle2,
  XCircle,
  ExternalLink,
  Copy,
  Eye,
  EyeOff,
  RefreshCw,
  Info,
  Sparkles,
  Video,
  Music,
  Volume2,
} from 'lucide-react';
import { Button } from '@/components/ui/button';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/i18n/useLanguage';
import { ScreenLayout } from '@/components/layout/ScreenLayout';

// Import API status checkers
import { isDeApiConfigured, getImg2VideoWaitTime, getImg2VideoQueueLength } from '@/services/deapiService';

// ============================================================
// Types
// ============================================================

interface ApiKeyConfig {
  id: string;
  name: string;
  envVar: string;
  description: string;
  required: boolean;
  docsUrl: string;
  icon: React.ComponentType<{ className?: string }>;
  checkConfigured: () => boolean;
  features: string[];
}

// ============================================================
// API Key Configurations
// ============================================================

const API_KEYS: ApiKeyConfig[] = [
  {
    id: 'gemini',
    name: 'Google Gemini',
    envVar: 'VITE_GEMINI_API_KEY',
    description: 'Powers AI content generation, narration, and visual prompts',
    required: true,
    docsUrl: 'https://aistudio.google.com/apikey',
    icon: Sparkles,
    checkConfigured: () => {
      // @ts-ignore
      const key = import.meta.env?.VITE_GEMINI_API_KEY;
      return Boolean(key && key.trim().length > 0);
    },
    features: ['Content Planning', 'Narration (TTS)', 'Image Generation (Imagen)', 'Story Analysis'],
  },
  {
    id: 'deapi',
    name: 'DeAPI',
    envVar: 'VITE_DEAPI_API_KEY',
    description: 'Converts still images to animated video clips',
    required: false,
    docsUrl: 'https://deapi.ai',
    icon: Video,
    checkConfigured: isDeApiConfigured,
    features: ['Image-to-Video Animation', 'Motion Effects', 'Video Loops'],
  },
  {
    id: 'suno',
    name: 'Suno',
    envVar: 'VITE_SUNO_API_KEY',
    description: 'AI-powered music and song generation',
    required: false,
    docsUrl: 'https://suno.ai',
    icon: Music,
    checkConfigured: () => {
      // @ts-ignore
      const key = import.meta.env?.VITE_SUNO_API_KEY;
      return Boolean(key && key.trim().length > 0);
    },
    features: ['Full Song Generation', 'Instrumental Tracks', 'Custom Lyrics'],
  },
  {
    id: 'freesound',
    name: 'Freesound',
    envVar: 'VITE_FREESOUND_API_KEY',
    description: 'Access to sound effects library for ambient audio',
    required: false,
    docsUrl: 'https://freesound.org/apiv2/apply/',
    icon: Volume2,
    checkConfigured: () => {
      // @ts-ignore
      const key = import.meta.env?.VITE_FREESOUND_API_KEY;
      return Boolean(key && key.trim().length > 0);
    },
    features: ['Ambient SFX', 'Sound Effects', 'Audio Transitions'],
  },
];

// ============================================================
// Components
// ============================================================

interface ApiKeyCardProps {
  config: ApiKeyConfig;
  isRTL: boolean;
}

function ApiKeyCard({ config, isRTL }: ApiKeyCardProps) {
  const [isConfigured, setIsConfigured] = useState(false);
  const [showEnvVar, setShowEnvVar] = useState(false);

  useEffect(() => {
    setIsConfigured(config.checkConfigured());
  }, [config]);

  const Icon = config.icon;

  const copyEnvVar = () => {
    navigator.clipboard.writeText(`${config.envVar}=your_api_key_here`);
  };

  return (
    <motion.div
      initial={{ opacity: 0, y: 10 }}
      animate={{ opacity: 1, y: 0 }}
      className={cn(
        'p-5 rounded-xl border transition-all',
        isConfigured
          ? 'bg-emerald-500/5 border-emerald-500/20'
          : config.required
          ? 'bg-red-500/5 border-red-500/20'
          : 'bg-white/5 border-white/10'
      )}
    >
      {/* Header */}
      <div className={cn('flex items-start justify-between mb-3', isRTL && 'flex-row-reverse')}>
        <div className={cn('flex items-center gap-3', isRTL && 'flex-row-reverse')}>
          <div
            className={cn(
              'w-10 h-10 rounded-lg flex items-center justify-center',
              isConfigured ? 'bg-emerald-500/20' : 'bg-white/10'
            )}
          >
            <Icon className={cn('w-5 h-5', isConfigured ? 'text-emerald-400' : 'text-white/60')} />
          </div>
          <div>
            <h3 className="font-medium text-white flex items-center gap-2">
              {config.name}
              {config.required && (
                <span className="text-xs px-1.5 py-0.5 rounded bg-violet-500/20 text-violet-300">
                  Required
                </span>
              )}
            </h3>
            <p className="text-sm text-white/50">{config.description}</p>
          </div>
        </div>

        {/* Status Badge */}
        <div
          className={cn(
            'flex items-center gap-1.5 px-2 py-1 rounded-full text-xs font-medium',
            isConfigured ? 'bg-emerald-500/20 text-emerald-300' : 'bg-white/10 text-white/50'
          )}
        >
          {isConfigured ? (
            <>
              <CheckCircle2 className="w-3.5 h-3.5" />
              Configured
            </>
          ) : (
            <>
              <XCircle className="w-3.5 h-3.5" />
              Not Set
            </>
          )}
        </div>
      </div>

      {/* Features */}
      <div className={cn('flex flex-wrap gap-1.5 mb-4', isRTL && 'justify-end')}>
        {config.features.map((feature) => (
          <span
            key={feature}
            className="px-2 py-0.5 text-xs rounded-full bg-white/5 text-white/60"
          >
            {feature}
          </span>
        ))}
      </div>

      {/* Environment Variable */}
      <div className="p-3 rounded-lg bg-black/30 border border-white/5">
        <div className={cn('flex items-center justify-between mb-2', isRTL && 'flex-row-reverse')}>
          <span className="text-xs text-white/40">Environment Variable</span>
          <div className={cn('flex items-center gap-1', isRTL && 'flex-row-reverse')}>
            <button
              onClick={() => setShowEnvVar(!showEnvVar)}
              className="p-1 hover:bg-white/10 rounded transition-colors"
              title={showEnvVar ? 'Hide' : 'Show'}
            >
              {showEnvVar ? (
                <EyeOff className="w-3.5 h-3.5 text-white/40" />
              ) : (
                <Eye className="w-3.5 h-3.5 text-white/40" />
              )}
            </button>
            <button
              onClick={copyEnvVar}
              className="p-1 hover:bg-white/10 rounded transition-colors"
              title="Copy"
            >
              <Copy className="w-3.5 h-3.5 text-white/40" />
            </button>
          </div>
        </div>
        <code className="text-sm font-mono text-violet-300">
          {showEnvVar ? `${config.envVar}=your_api_key_here` : `${config.envVar}=••••••••`}
        </code>
      </div>

      {/* Action Button */}
      <div className={cn('mt-4 flex items-center gap-2', isRTL && 'flex-row-reverse justify-end')}>
        <a
          href={config.docsUrl}
          target="_blank"
          rel="noopener noreferrer"
          className={cn(
            'inline-flex items-center gap-1.5 px-3 py-1.5 rounded-lg text-sm font-medium transition-colors',
            'bg-white/5 hover:bg-white/10 text-white/70 hover:text-white'
          )}
        >
          Get API Key
          <ExternalLink className="w-3.5 h-3.5" />
        </a>
      </div>
    </motion.div>
  );
}

// ============================================================
// Main Component
// ============================================================

export default function SettingsScreen() {
  const { t, isRTL } = useLanguage();
  const navigate = useNavigate();
  const [refreshKey, setRefreshKey] = useState(0);

  // DeAPI rate limit info
  const [deapiWaitTime, setDeapiWaitTime] = useState(0);
  const [deapiQueueLength, setDeapiQueueLength] = useState(0);

  useEffect(() => {
    const interval = setInterval(() => {
      setDeapiWaitTime(getImg2VideoWaitTime());
      setDeapiQueueLength(getImg2VideoQueueLength());
    }, 1000);
    return () => clearInterval(interval);
  }, []);

  const handleRefresh = () => {
    setRefreshKey((k) => k + 1);
  };

  const configuredCount = API_KEYS.filter((k) => k.checkConfigured()).length;
  const requiredConfigured = API_KEYS.filter((k) => k.required && k.checkConfigured()).length;
  const requiredTotal = API_KEYS.filter((k) => k.required).length;

  return (
    <ScreenLayout
      title={t('nav.settings')}
      showBackButton
      onBack={() => navigate('/')}
      headerActions={
        <Button
          variant="ghost"
          size="sm"
          onClick={handleRefresh}
          className="text-white/50 hover:text-white"
        >
          <RefreshCw className="w-4 h-4 me-2" />
          Refresh
        </Button>
      }
      maxWidth="2xl"
      contentClassName="py-8"
    >
      {/* Status Overview */}
      <motion.div
        initial={{ opacity: 0, y: 20 }}
        animate={{ opacity: 1, y: 0 }}
        className="mb-8 p-6 rounded-xl bg-white/5 border border-white/10"
      >
        <div className={cn('flex items-center gap-4 mb-4', isRTL && 'flex-row-reverse')}>
          <div className="w-12 h-12 rounded-xl bg-gradient-to-br from-violet-500/20 to-purple-500/20 flex items-center justify-center">
            <Key className="w-6 h-6 text-violet-400" />
          </div>
          <div>
            <h2 className="text-xl font-semibold text-white">API Configuration</h2>
            <p className="text-sm text-white/50">
              {configuredCount} of {API_KEYS.length} services configured
            </p>
          </div>
        </div>

        {/* Status Bars */}
        <div className="space-y-3">
          <div>
            <div className={cn('flex justify-between text-sm mb-1', isRTL && 'flex-row-reverse')}>
              <span className="text-white/60">Required APIs</span>
              <span className={requiredConfigured === requiredTotal ? 'text-emerald-400' : 'text-amber-400'}>
                {requiredConfigured}/{requiredTotal}
              </span>
            </div>
            <div className="h-2 bg-white/10 rounded-full overflow-hidden">
              <div
                className={cn(
                  'h-full transition-all duration-500',
                  requiredConfigured === requiredTotal ? 'bg-emerald-500' : 'bg-amber-500'
                )}
                style={{ width: `${(requiredConfigured / requiredTotal) * 100}%` }}
              />
            </div>
          </div>

          <div>
            <div className={cn('flex justify-between text-sm mb-1', isRTL && 'flex-row-reverse')}>
              <span className="text-white/60">Optional APIs</span>
              <span className="text-white/40">
                {configuredCount - requiredConfigured}/{API_KEYS.length - requiredTotal}
              </span>
            </div>
            <div className="h-2 bg-white/10 rounded-full overflow-hidden">
              <div
                className="h-full bg-violet-500 transition-all duration-500"
                style={{
                  width: `${((configuredCount - requiredConfigured) / (API_KEYS.length - requiredTotal)) * 100}%`,
                }}
              />
            </div>
          </div>
        </div>

        {/* DeAPI Rate Limit Status */}
        {isDeApiConfigured() && (deapiWaitTime > 0 || deapiQueueLength > 0) && (
          <div className="mt-4 p-3 rounded-lg bg-amber-500/10 border border-amber-500/20">
            <div className={cn('flex items-center gap-2 text-sm text-amber-300', isRTL && 'flex-row-reverse')}>
              <Info className="w-4 h-4" />
              <span>
                DeAPI Rate Limit: {deapiQueueLength > 0 ? `${deapiQueueLength} queued, ` : ''}
                {deapiWaitTime > 0 ? `~${deapiWaitTime}s until next request` : 'Ready'}
              </span>
            </div>
          </div>
        )}
      </motion.div>

      {/* Instructions */}
      <div className="mb-6 p-4 rounded-lg bg-blue-500/10 border border-blue-500/20">
        <div className={cn('flex items-start gap-3', isRTL && 'flex-row-reverse')}>
          <Info className="w-5 h-5 text-blue-400 shrink-0 mt-0.5" />
          <div className="text-sm text-blue-200/80">
            <p className="font-medium mb-1">How to configure API keys:</p>
            <ol className={cn('list-decimal list-inside space-y-1 text-blue-200/60', isRTL && 'text-right')}>
              <li>Create a <code className="px-1 py-0.5 rounded bg-blue-500/20">.env.local</code> file in the project root</li>
              <li>Add your API keys in the format shown below each service</li>
              <li>Restart the development server (<code className="px-1 py-0.5 rounded bg-blue-500/20">npm run dev:all</code>)</li>
            </ol>
          </div>
        </div>
      </div>

      {/* API Key Cards */}
      <div className="grid gap-4" key={refreshKey}>
        {API_KEYS.map((config) => (
          <ApiKeyCard key={config.id} config={config} isRTL={isRTL} />
        ))}
      </div>

      {/* Footer */}
      <div className="mt-8 text-center text-sm text-white/30">
        API keys are stored in environment variables for security.
        <br />
        They are never exposed to the browser or stored in localStorage.
      </div>
    </ScreenLayout>
  );
}
````

## File: packages/frontend/screens/SignInScreen.tsx
````typescript
/**
 * SignInScreen - Split-screen authentication page for AIsoul Studio
 *
 * Design: "Digital Soul Awakening"
 * Left: Animated neural mesh background with floating particles
 * Right: Refined dark minimalism with elegant typography
 */

import { useState, useEffect, useMemo } from 'react';
import { useNavigate } from 'react-router-dom';
import { motion, AnimatePresence } from 'framer-motion';
import { Loader2, AlertCircle, Sparkles } from 'lucide-react';
import { useAuth } from '@/hooks/useAuth';
import { cn } from '@/lib/utils';

// Generate deterministic positions for neural nodes
function generateNodes(count: number, seed: number) {
  const nodes: Array<{ x: number; y: number; size: number; delay: number }> = [];
  for (let i = 0; i < count; i++) {
    const pseudoRandom = Math.sin(seed + i * 12.9898) * 43758.5453;
    const x = (pseudoRandom % 100 + 100) % 100;
    const y = ((pseudoRandom * 1.3) % 100 + 100) % 100;
    const size = 2 + (pseudoRandom % 4);
    const delay = (i * 0.2) % 3;
    nodes.push({ x, y, size, delay });
  }
  return nodes;
}

// Generate connections between nearby nodes
function generateConnections(nodes: Array<{ x: number; y: number }>) {
  const connections: Array<{ x1: number; y1: number; x2: number; y2: number; delay: number }> = [];
  for (let i = 0; i < nodes.length; i++) {
    for (let j = i + 1; j < nodes.length; j++) {
      const dx = nodes[i]!.x - nodes[j]!.x;
      const dy = nodes[i]!.y - nodes[j]!.y;
      const distance = Math.sqrt(dx * dx + dy * dy);
      if (distance < 25 && connections.length < 20) {
        connections.push({
          x1: nodes[i]!.x,
          y1: nodes[i]!.y,
          x2: nodes[j]!.x,
          y2: nodes[j]!.y,
          delay: i * 0.1,
        });
      }
    }
  }
  return connections;
}

// Animated Neural Background Component
function NeuralBackground() {
  const nodes = useMemo(() => generateNodes(24, 42), []);
  const connections = useMemo(() => generateConnections(nodes), [nodes]);

  return (
    <div className="absolute inset-0 overflow-hidden">
      {/* Deep space gradient base */}
      <div className="absolute inset-0 bg-gradient-to-br from-[oklch(0.08_0.04_280)] via-[oklch(0.05_0.02_240)] to-[oklch(0.03_0.01_200)]" />

      {/* Animated mesh gradient blobs */}
      <motion.div
        className="absolute w-[600px] h-[600px] rounded-full opacity-30"
        style={{
          background: 'radial-gradient(circle, oklch(0.45 0.15 280 / 0.4) 0%, transparent 70%)',
          left: '10%',
          top: '20%',
          filter: 'blur(60px)',
        }}
        animate={{
          x: [0, 50, 0],
          y: [0, -30, 0],
          scale: [1, 1.2, 1],
        }}
        transition={{
          duration: 15,
          repeat: Infinity,
          ease: 'easeInOut',
        }}
      />
      <motion.div
        className="absolute w-[500px] h-[500px] rounded-full opacity-25"
        style={{
          background: 'radial-gradient(circle, oklch(0.50 0.18 190 / 0.5) 0%, transparent 70%)',
          right: '5%',
          bottom: '10%',
          filter: 'blur(80px)',
        }}
        animate={{
          x: [0, -40, 0],
          y: [0, 40, 0],
          scale: [1, 1.15, 1],
        }}
        transition={{
          duration: 18,
          repeat: Infinity,
          ease: 'easeInOut',
          delay: 2,
        }}
      />
      <motion.div
        className="absolute w-[400px] h-[400px] rounded-full opacity-20"
        style={{
          background: 'radial-gradient(circle, oklch(0.55 0.20 30 / 0.3) 0%, transparent 70%)',
          left: '50%',
          top: '60%',
          filter: 'blur(70px)',
        }}
        animate={{
          x: [0, 30, -20, 0],
          y: [0, -50, 20, 0],
          scale: [1, 1.1, 0.95, 1],
        }}
        transition={{
          duration: 20,
          repeat: Infinity,
          ease: 'easeInOut',
          delay: 5,
        }}
      />

      {/* Neural network connections */}
      <svg className="absolute inset-0 w-full h-full" xmlns="http://www.w3.org/2000/svg">
        <defs>
          <linearGradient id="lineGradient" x1="0%" y1="0%" x2="100%" y2="100%">
            <stop offset="0%" stopColor="oklch(0.70 0.15 190 / 0.3)" />
            <stop offset="100%" stopColor="oklch(0.55 0.20 280 / 0.1)" />
          </linearGradient>
        </defs>
        {connections.map((conn, i) => (
          <motion.line
            key={i}
            x1={`${conn.x1}%`}
            y1={`${conn.y1}%`}
            x2={`${conn.x2}%`}
            y2={`${conn.y2}%`}
            stroke="url(#lineGradient)"
            strokeWidth="1"
            initial={{ pathLength: 0, opacity: 0 }}
            animate={{ pathLength: 1, opacity: [0, 0.6, 0.3] }}
            transition={{
              duration: 3,
              delay: conn.delay,
              repeat: Infinity,
              repeatType: 'reverse',
              ease: 'easeInOut',
            }}
          />
        ))}
      </svg>

      {/* Neural nodes */}
      {nodes.map((node, i) => (
        <motion.div
          key={i}
          className="absolute rounded-full"
          style={{
            width: node.size,
            height: node.size,
            left: `${node.x}%`,
            top: `${node.y}%`,
            background: i % 3 === 0
              ? 'oklch(0.70 0.15 190)'
              : i % 3 === 1
                ? 'oklch(0.60 0.18 280)'
                : 'oklch(0.65 0.12 30)',
            boxShadow: `0 0 ${node.size * 3}px ${node.size}px oklch(0.70 0.15 190 / 0.3)`,
          }}
          initial={{ opacity: 0, scale: 0 }}
          animate={{
            opacity: [0.4, 0.9, 0.4],
            scale: [1, 1.3, 1],
          }}
          transition={{
            duration: 4 + (i % 3),
            delay: node.delay,
            repeat: Infinity,
            ease: 'easeInOut',
          }}
        />
      ))}

      {/* Floating large orbs */}
      {[...Array(5)].map((_, i) => (
        <motion.div
          key={`orb-${i}`}
          className="absolute rounded-full pointer-events-none"
          style={{
            width: 150 + i * 50,
            height: 150 + i * 50,
            left: `${15 + i * 18}%`,
            top: `${10 + i * 15}%`,
            background: `radial-gradient(circle at 30% 30%, oklch(0.70 0.15 ${190 + i * 25} / 0.08) 0%, transparent 60%)`,
          }}
          animate={{
            x: [0, 20 - i * 5, 0],
            y: [0, 15 + i * 3, 0],
            rotate: [0, 180, 360],
          }}
          transition={{
            duration: 20 + i * 5,
            repeat: Infinity,
            ease: 'linear',
          }}
        />
      ))}

      {/* Scan line effect */}
      <motion.div
        className="absolute inset-0 pointer-events-none"
        style={{
          background: 'linear-gradient(to bottom, transparent 0%, oklch(0.70 0.15 190 / 0.03) 50%, transparent 100%)',
          backgroundSize: '100% 4px',
        }}
        animate={{ y: ['-100%', '100%'] }}
        transition={{
          duration: 8,
          repeat: Infinity,
          ease: 'linear',
        }}
      />

      {/* Vignette overlay */}
      <div className="absolute inset-0 bg-[radial-gradient(ellipse_at_center,transparent_20%,oklch(0.03_0.01_240)_100%)]" />

      {/* Film grain */}
      <div className="absolute inset-0 cinema-grain opacity-20" />

      {/* Right edge fade for seamless transition */}
      <div className="absolute inset-y-0 right-0 w-32 bg-gradient-to-l from-[var(--cinema-void)] to-transparent" />
    </div>
  );
}

export default function SignInScreen() {
  const navigate = useNavigate();
  const { user, isLoading, error, signInWithGoogle, signInWithEmail, createAccount, clearError, isAuthenticated } = useAuth();
  const [isSigningIn, setIsSigningIn] = useState(false);
  const [mode, setMode] = useState<'signin' | 'signup'>('signin');
  const [email, setEmail] = useState('');
  const [password, setPassword] = useState('');

  // Redirect if already authenticated
  useEffect(() => {
    if (isAuthenticated && user) {
      navigate('/', { replace: true });
    }
  }, [isAuthenticated, user, navigate]);

  const handleGoogleSignIn = async () => {
    setIsSigningIn(true);
    clearError();
    try {
      await signInWithGoogle();
    } finally {
      setIsSigningIn(false);
    }
  };

  const handleContinueWithoutSignIn = () => {
    navigate('/');
  };

  const handleEmailSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setIsSigningIn(true);
    clearError();
    try {
      if (mode === 'signin') {
        await signInWithEmail(email, password);
      } else {
        await createAccount(email, password);
      }
    } finally {
      setIsSigningIn(false);
    }
  };

  const switchMode = () => {
    setMode(mode === 'signin' ? 'signup' : 'signin');
    clearError();
    setEmail('');
    setPassword('');
  };

  return (
    <div className="fixed inset-0 flex bg-[var(--cinema-void)]">
      {/* ===== LEFT PANEL: Animated Neural Background ===== */}
      <motion.div
        className="hidden lg:flex lg:w-1/2 xl:w-[55%] relative overflow-hidden"
        initial={{ opacity: 0, x: -50 }}
        animate={{ opacity: 1, x: 0 }}
        transition={{ duration: 0.8, ease: [0.22, 1, 0.36, 1] }}
      >
        <NeuralBackground />

        {/* Floating brand element */}
        <motion.div
          className="absolute bottom-12 left-12 z-10"
          initial={{ opacity: 0, y: 20 }}
          animate={{ opacity: 1, y: 0 }}
          transition={{ delay: 0.6, duration: 0.8 }}
        >
          <p className="text-[var(--cinema-silver)]/40 text-sm tracking-[0.3em] uppercase font-light">
            Where AI Meets Creativity
          </p>
        </motion.div>
      </motion.div>

      {/* ===== RIGHT PANEL: Authentication ===== */}
      <motion.div
        className="w-full lg:w-1/2 xl:w-[45%] flex flex-col items-center justify-center p-8 md:p-12 lg:p-16 relative"
        initial={{ opacity: 0 }}
        animate={{ opacity: 1 }}
        transition={{ duration: 0.6, delay: 0.2 }}
      >
        {/* Subtle background gradient */}
        <div className="absolute inset-0 bg-gradient-to-br from-[var(--cinema-celluloid)] via-[var(--cinema-void)] to-[var(--cinema-void)]" />

        {/* Decorative corner accents */}
        <div className="absolute top-0 right-0 w-64 h-64 pointer-events-none">
          <div className="absolute top-0 right-0 w-full h-full bg-[radial-gradient(circle_at_100%_0%,oklch(0.70_0.15_190_/_0.05)_0%,transparent_50%)]" />
        </div>
        <div className="absolute bottom-0 left-0 w-48 h-48 pointer-events-none">
          <div className="absolute bottom-0 left-0 w-full h-full bg-[radial-gradient(circle_at_0%_100%,oklch(0.55_0.20_280_/_0.03)_0%,transparent_50%)]" />
        </div>

        {/* Content container */}
        <div className="relative z-10 w-full max-w-md space-y-10">

          {/* Brand Section */}
          <motion.div
            className="text-center space-y-6"
            initial={{ opacity: 0, y: 20 }}
            animate={{ opacity: 1, y: 0 }}
            transition={{ delay: 0.4, duration: 0.6 }}
          >
            {/* Logo Icon */}
            <motion.div
              className="mx-auto w-20 h-20 rounded-2xl bg-gradient-to-br from-[oklch(0.70_0.15_190)] to-[oklch(0.55_0.20_280)] flex items-center justify-center shadow-2xl"
              initial={{ scale: 0.8, rotate: -10 }}
              animate={{ scale: 1, rotate: 0 }}
              transition={{ delay: 0.5, type: "spring", stiffness: 200 }}
              style={{
                boxShadow: '0 20px 60px -10px oklch(0.70 0.15 190 / 0.4)',
              }}
            >
              <Sparkles className="w-10 h-10 text-white" />
            </motion.div>

            {/* Brand Name */}
            <div>
              <h1 className="font-display text-4xl md:text-5xl tracking-tight text-[var(--cinema-silver)]">
                {mode === 'signin' ? 'Welcome Back' : 'Join'}{' '}
                <span className="bg-clip-text text-transparent bg-gradient-to-r from-[oklch(0.70_0.15_190)] to-[oklch(0.65_0.25_30)]">
                  {mode === 'signin' ? '' : 'AIsoul Studio'}
                </span>
              </h1>
              <p className="mt-3 text-[var(--cinema-silver)]/50 text-base font-light tracking-wide">
                {mode === 'signin'
                  ? 'Sign in to sync your projects across devices'
                  : 'Create an account to save your work to the cloud'}
              </p>
            </div>
          </motion.div>

          {/* Sign-in Section */}
          <motion.div
            className="space-y-6"
            initial={{ opacity: 0, y: 20 }}
            animate={{ opacity: 1, y: 0 }}
            transition={{ delay: 0.6, duration: 0.6 }}
          >
            {/* Error Message */}
            <AnimatePresence>
              {error && (
                <motion.div
                  initial={{ opacity: 0, y: -10, height: 0 }}
                  animate={{ opacity: 1, y: 0, height: 'auto' }}
                  exit={{ opacity: 0, y: -10, height: 0 }}
                  className="flex items-center gap-3 p-4 rounded-xl bg-[oklch(0.35_0.15_25_/_0.15)] border border-[oklch(0.35_0.15_25_/_0.3)] text-[oklch(0.75_0.12_25)]"
                >
                  <AlertCircle className="w-5 h-5 shrink-0" />
                  <span className="text-sm">{error}</span>
                </motion.div>
              )}
            </AnimatePresence>

            {/* Google Sign-in Button */}
            <motion.button
              onClick={handleGoogleSignIn}
              disabled={isSigningIn || isLoading}
              className={cn(
                "group w-full flex items-center justify-center gap-3 px-6 py-4 rounded-xl",
                "bg-white hover:bg-gray-50 text-gray-800 font-medium",
                "transition-all duration-300 ease-out",
                "focus:outline-none focus:ring-2 focus:ring-[oklch(0.70_0.15_190)] focus:ring-offset-2 focus:ring-offset-[var(--cinema-void)]",
                "disabled:opacity-50 disabled:cursor-not-allowed",
                "shadow-lg hover:shadow-xl hover:shadow-white/10"
              )}
              whileHover={{ scale: isSigningIn ? 1 : 1.02, y: isSigningIn ? 0 : -2 }}
              whileTap={{ scale: 0.98 }}
            >
              {isSigningIn ? (
                <Loader2 className="w-5 h-5 animate-spin text-gray-600" />
              ) : (
                <svg className="w-5 h-5" viewBox="0 0 24 24" aria-hidden="true">
                  <path
                    fill="#4285F4"
                    d="M22.56 12.25c0-.78-.07-1.53-.2-2.25H12v4.26h5.92c-.26 1.37-1.04 2.53-2.21 3.31v2.77h3.57c2.08-1.92 3.28-4.74 3.28-8.09z"
                  />
                  <path
                    fill="#34A853"
                    d="M12 23c2.97 0 5.46-.98 7.28-2.66l-3.57-2.77c-.98.66-2.23 1.06-3.71 1.06-2.86 0-5.29-1.93-6.16-4.53H2.18v2.84C3.99 20.53 7.7 23 12 23z"
                  />
                  <path
                    fill="#FBBC05"
                    d="M5.84 14.09c-.22-.66-.35-1.36-.35-2.09s.13-1.43.35-2.09V7.07H2.18C1.43 8.55 1 10.22 1 12s.43 3.45 1.18 4.93l2.85-2.22.81-.62z"
                  />
                  <path
                    fill="#EA4335"
                    d="M12 5.38c1.62 0 3.06.56 4.21 1.64l3.15-3.15C17.45 2.09 14.97 1 12 1 7.7 1 3.99 3.47 2.18 7.07l3.66 2.84c.87-2.6 3.3-4.53 6.16-4.53z"
                  />
                </svg>
              )}
              <span className="text-base">
                {isSigningIn ? 'Signing in...' : 'Sign in with Google'}
              </span>
            </motion.button>

            {/* Divider */}
            <div className="relative">
              <div className="absolute inset-0 flex items-center">
                <div className="w-full border-t border-[var(--cinema-silver)]/10" />
              </div>
              <div className="relative flex justify-center">
                <span className="px-4 text-xs text-[var(--cinema-silver)]/30 bg-[var(--cinema-void)] uppercase tracking-widest">
                  or continue with email
                </span>
              </div>
            </div>

            {/* Email/Password Form */}
            <form onSubmit={handleEmailSubmit} className="space-y-4">
              <div>
                <input
                  type="email"
                  value={email}
                  onChange={(e) => setEmail(e.target.value)}
                  placeholder="Email address"
                  required
                  disabled={isSigningIn}
                  className={cn(
                    "w-full px-4 py-3.5 rounded-xl",
                    "bg-[var(--cinema-celluloid)] border border-[var(--cinema-silver)]/10",
                    "text-[var(--cinema-silver)] placeholder:text-[var(--cinema-silver)]/30",
                    "focus:outline-none focus:border-[oklch(0.70_0.15_190)] focus:ring-1 focus:ring-[oklch(0.70_0.15_190_/_0.3)]",
                    "transition-all duration-200",
                    "disabled:opacity-50"
                  )}
                />
              </div>
              <div>
                <input
                  type="password"
                  value={password}
                  onChange={(e) => setPassword(e.target.value)}
                  placeholder="Password"
                  required
                  minLength={6}
                  disabled={isSigningIn}
                  className={cn(
                    "w-full px-4 py-3.5 rounded-xl",
                    "bg-[var(--cinema-celluloid)] border border-[var(--cinema-silver)]/10",
                    "text-[var(--cinema-silver)] placeholder:text-[var(--cinema-silver)]/30",
                    "focus:outline-none focus:border-[oklch(0.70_0.15_190)] focus:ring-1 focus:ring-[oklch(0.70_0.15_190_/_0.3)]",
                    "transition-all duration-200",
                    "disabled:opacity-50"
                  )}
                />
              </div>

              <motion.button
                type="submit"
                disabled={isSigningIn || isLoading}
                className={cn(
                  "w-full flex items-center justify-center gap-3 px-6 py-4 rounded-xl",
                  "bg-gradient-to-r from-[oklch(0.70_0.15_190)] to-[oklch(0.55_0.20_280)]",
                  "hover:from-[oklch(0.75_0.15_190)] hover:to-[oklch(0.60_0.20_280)]",
                  "text-white font-medium",
                  "transition-all duration-300 ease-out",
                  "focus:outline-none focus:ring-2 focus:ring-[oklch(0.70_0.15_190)] focus:ring-offset-2 focus:ring-offset-[var(--cinema-void)]",
                  "disabled:opacity-50 disabled:cursor-not-allowed",
                  "shadow-lg hover:shadow-xl hover:shadow-[oklch(0.70_0.15_190_/_0.3)]"
                )}
                whileHover={{ scale: isSigningIn ? 1 : 1.02, y: isSigningIn ? 0 : -2 }}
                whileTap={{ scale: 0.98 }}
              >
                {isSigningIn ? (
                  <Loader2 className="w-5 h-5 animate-spin" />
                ) : null}
                <span className="text-base">
                  {isSigningIn
                    ? (mode === 'signin' ? 'Signing in...' : 'Creating account...')
                    : (mode === 'signin' ? 'Sign in' : 'Create account')}
                </span>
              </motion.button>
            </form>

            {/* Mode switcher */}
            <div className="text-center text-sm text-[var(--cinema-silver)]/50">
              {mode === 'signin' ? (
                <>
                  Don't have an account?{' '}
                  <button
                    onClick={switchMode}
                    className="text-[oklch(0.70_0.15_190)] hover:text-[oklch(0.75_0.15_190)] transition-colors font-medium"
                  >
                    Sign up
                  </button>
                </>
              ) : (
                <>
                  Already have an account?{' '}
                  <button
                    onClick={switchMode}
                    className="text-[oklch(0.70_0.15_190)] hover:text-[oklch(0.75_0.15_190)] transition-colors font-medium"
                  >
                    Sign in
                  </button>
                </>
              )}
            </div>

            {/* Second Divider */}
            <div className="relative">
              <div className="absolute inset-0 flex items-center">
                <div className="w-full border-t border-[var(--cinema-silver)]/10" />
              </div>
              <div className="relative flex justify-center">
                <span className="px-4 text-xs text-[var(--cinema-silver)]/30 bg-[var(--cinema-void)] uppercase tracking-widest">
                  or
                </span>
              </div>
            </div>

            {/* Continue without sign-in */}
            <motion.button
              onClick={handleContinueWithoutSignIn}
              className={cn(
                "w-full px-6 py-3.5 rounded-xl",
                "border border-[var(--cinema-silver)]/10 hover:border-[var(--cinema-silver)]/20",
                "text-[var(--cinema-silver)]/60 hover:text-[var(--cinema-silver)]/80",
                "text-sm font-light tracking-wide",
                "transition-all duration-300",
                "focus:outline-none focus:ring-2 focus:ring-[oklch(0.70_0.15_190_/_0.3)] focus:ring-offset-2 focus:ring-offset-[var(--cinema-void)]"
              )}
              whileHover={{ scale: 1.01 }}
              whileTap={{ scale: 0.99 }}
            >
              Continue without signing in
            </motion.button>
          </motion.div>

          {/* Footer Section */}
          <motion.div
            className="pt-8 text-center space-y-4"
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            transition={{ delay: 0.8, duration: 0.6 }}
          >
            <p className="text-xs text-[var(--cinema-silver)]/30 leading-relaxed">
              By signing in, you agree to our{' '}
              <a href="#" className="text-[var(--cinema-silver)]/50 hover:text-[var(--cinema-silver)]/70 transition-colors underline underline-offset-2">
                Terms of Service
              </a>{' '}
              and{' '}
              <a href="#" className="text-[var(--cinema-silver)]/50 hover:text-[var(--cinema-silver)]/70 transition-colors underline underline-offset-2">
                Privacy Policy
              </a>
            </p>

            <p className="text-xs text-[var(--cinema-silver)]/20">
              Powered by Gemini AI
            </p>
          </motion.div>
        </div>

        {/* Mobile background hint */}
        <motion.div
          className="lg:hidden absolute bottom-8 left-0 right-0 flex justify-center"
          initial={{ opacity: 0 }}
          animate={{ opacity: 1 }}
          transition={{ delay: 1, duration: 0.6 }}
        >
          <div className="flex items-center gap-2 text-[var(--cinema-silver)]/30 text-xs">
            <div className="w-8 h-0.5 bg-gradient-to-r from-transparent via-[var(--cinema-silver)]/20 to-transparent" />
            <span>AI-Powered Video Creation</span>
            <div className="w-8 h-0.5 bg-gradient-to-r from-transparent via-[var(--cinema-silver)]/20 to-transparent" />
          </div>
        </motion.div>
      </motion.div>
    </div>
  );
}
````

## File: packages/frontend/screens/StudioScreen.tsx
````typescript
/**
 * Studio Screen - Unified creation workspace
 *
 * Refactored to use extracted components for better maintainability.
 * Requirements: 6.1-6.6, 2.5, 1.5, 9.1, 9.4
 */

import React, { useState, useCallback, useRef, useEffect, useMemo } from 'react';
import { useSearchParams, useNavigate } from 'react-router-dom';
import { motion } from 'framer-motion';
import {
  Video,
  Music as MusicIcon,
  Image as ImageIcon,
  Download,
  RotateCcw,
  Wand2,
  BarChart3,
  Edit3,
  Layers,
  Upload,
  Settings,
  Loader2,
} from 'lucide-react';
import { Button } from '@/components/ui/button';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/i18n/useLanguage';
import { useVideoProductionRefactored } from '@/hooks/useVideoProductionRefactored';
import { useModalState } from '@/hooks/useModalState';
import { AppState } from '@/types';
import { getEffectiveLegacyTone } from '@/services/tripletUtils';

// Layout & UI Components
import { ScreenLayout } from '@/components/layout/ScreenLayout';
import { SlidePanel } from '@/components/ui/SlidePanel';

// Chat Components
import { MessageBubble, ChatInput, QuickActions, type ChatMessage } from '@/components/chat';

// Feature Components
import { QuickExport } from '@/components/QuickExport';
import { VideoPreviewCard } from '@/components/VideoPreviewCard';
import { studioAgent, type AgentResponse, type QuickAction } from '@/services/ai/studioAgent';
import QualityDashboard from '@/components/QualityDashboard';
import SceneEditor from '@/components/SceneEditor';
import MusicGeneratorModal from '@/components/MusicGeneratorModal';
import { SettingsModal } from '@/components/SettingsModal';
import { GraphiteTimeline } from '@/components/TimelineEditor';
import { useAppStore } from '@/stores';
import { useStoryGeneration } from '@/hooks/useStoryGeneration';
import { useFormatPipeline } from '@/hooks/useFormatPipeline';
import { useProjectSession } from '@/hooks/useProjectSession';
import { getCurrentUser } from '@/services/firebase/authService';
import { StoryWorkspace } from '@/components/story';
import { StoryWorkspaceErrorBoundary } from '@/components/story/StoryWorkspaceErrorBoundary';
import { VideoEditor } from '@/components/VideoEditor';
import { useVideoEditorStore } from '@/components/VideoEditor/hooks/useVideoEditorStore';

// ============================================================
// Types & Helpers
// ============================================================

export interface StudioParams {
  mode?: 'video' | 'music' | 'story';
  style?: string;
  duration?: number;
  topic?: string;
  projectId?: string;
}

export function parseStudioParams(searchParams: URLSearchParams): StudioParams {
  const mode = searchParams.get('mode');
  const style = searchParams.get('style');
  const duration = searchParams.get('duration');
  const topic = searchParams.get('topic');
  const projectId = searchParams.get('projectId');

  return {
    mode: mode === 'video' || mode === 'music' || mode === 'story' ? mode : undefined,
    style: style || undefined,
    duration: duration ? parseInt(duration, 10) : undefined,
    topic: topic || undefined,
    projectId: projectId || undefined,
  };
}

// ============================================================
// Main Component
// ============================================================

export default function StudioScreen() {
  const { t, isRTL } = useLanguage();
  const navigate = useNavigate();
  const [searchParams] = useSearchParams();
  const params = parseStudioParams(searchParams);

  // Project session management
  const {
    project,
    isLoading: isProjectLoading,
    error: projectError,
    restoredState,
    syncProjectMetadata,
  } = useProjectSession(params.projectId);

  // View mode toggle (Requirement 6.6)
  const [viewMode, setViewMode] = useState<'simple' | 'advanced'>('simple');
  const [studioMode, setStudioMode] = useState<'chat' | 'story' | 'editor'>(
    params.mode === 'story' ? 'story' : 'chat'
  );

  // Story Generation Hook - pass projectId so it resets state for new projects
  const storyHook = useStoryGeneration(params.projectId);

  // Format Pipeline Hook - for non-movie-animation format pipelines
  const formatPipelineHook = useFormatPipeline();

  // Modal state (unified)
  const {
    showExport, setShowExport,
    showQuality, setShowQuality,
    showSceneEditor, setShowSceneEditor,
    showMusic, setShowMusic,
    showTimeline, setShowTimeline,
  } = useModalState();
  const [showSettings, setShowSettings] = useState(false);

  const [musicModalMode, setMusicModalMode] = useState<'generate' | 'remix'>('generate');

  // Video production hook
  const {
    appState,
    contentPlan,
    narrationSegments,
    sfxPlan,
    error,
    setTopic,
    targetAudience,
    setTargetAudience,
    setTargetDuration,
    setVideoPurpose,
    setVisualStyle,
    startProduction,
    reset,
    visuals,
    getVisualsMap,
    getAudioUrlMap,
    updateScenes,
    generateMusic,
    generateLyrics,
    musicState,
    selectTrack,
    addMusicToTimeline,
    refreshCredits,
    regenerateSceneNarration,
    playNarration,
    qualityReport,
    playingSceneId,
    browseSfx,
    setPreferredCameraAngle,
    setPreferredLightingMood,
    uploadAudio,
    uploadAndCover,
    addVocals,
    addInstrumental,
    veoVideoCount,
    setVeoVideoCount,
    topic,
    // Test/Debug setters
    setVisuals,
    setContentPlan,
    setNarrationSegments,
    setAppState,
  } = useVideoProductionRefactored();

  // App Store - Chat & UI State (persistent)
  const storeMessages = useAppStore((s) => s.messages);
  const addMessage = useAppStore((s) => s.addMessage);
  const clearMessages = useAppStore((s) => s.clearMessages);
  const updateLastMessage = useAppStore((s) => s.updateLastMessage);
  const setTyping = useAppStore((s) => s.setTyping);
  const trackVideoCreation = useAppStore((s) => s.trackVideoCreation);
  const trackMusicGeneration = useAppStore((s) => s.trackMusicGeneration);
  const recordFeedback = useAppStore((s) => s.recordFeedback);

  // Local UI state
  const [input, setInput] = useState('');
  const [isProcessing, setIsProcessing] = useState(false);
  const [storyInitialTopic, setStoryInitialTopic] = useState(params.mode === 'story' ? (params.topic || '') : '');

  // Video preview state
  const [isPlaying, setIsPlaying] = useState(false);
  const [currentSceneIndex, setCurrentSceneIndex] = useState(0);
  const previewIntervalRef = useRef<NodeJS.Timeout | null>(null);

  // Timeline playback state
  const [playbackTime, setPlaybackTime] = useState(0);
  const [selectedSceneId, setSelectedSceneId] = useState<string | null>(null);
  const timelineAudioRef = useRef<HTMLAudioElement>(null);
  const [mergedAudioUrl, setMergedAudioUrl] = useState<string | null>(null);

  const messagesEndRef = useRef<HTMLDivElement>(null);
  const paramsAppliedRef = useRef(false);
  const lastProcessedResultRef = useRef<string | null>(null);

  // ============================================================
  // Effects
  // ============================================================

  // Sync studioMode from URL params when navigating between projects
  useEffect(() => {
    if (params.mode === 'story' && studioMode !== 'story') {
      setStudioMode('story');
    }
  }, [params.mode]);

  // Apply URL parameters OR restore project state on mount (Requirement 2.5)
  useEffect(() => {
    if (paramsAppliedRef.current) return;

    // Wait for project loading to complete if we have a projectId
    if (params.projectId && isProjectLoading) return;

    paramsAppliedRef.current = true;

    // If we have restored state from a project session, apply it
    if (restoredState?.contentPlan) {
      console.log('[StudioScreen] Restoring from project session');
      setContentPlan(restoredState.contentPlan);
      if (restoredState.visuals?.length) {
        setVisuals(restoredState.visuals);
      }
      if (restoredState.narrationSegments?.length) {
        setNarrationSegments(restoredState.narrationSegments);
      }
      // Set topic from restored content plan
      if (restoredState.contentPlan.title) {
        setTopic(restoredState.contentPlan.title);
      }
      setAppState(AppState.READY);
      return;
    }

    // Apply project metadata as initial config (if project exists but no restored state)
    if (project) {
      if (project.topic) setTopic(project.topic);
      if (project.style) setVisualStyle(project.style);
    }

    // Fall back to URL params for mode/style/topic
    if (params.mode === 'video') {
      if (params.style) setVisualStyle(params.style);
      if (params.duration) setTargetDuration(params.duration);
      setVideoPurpose('documentary');

      // Use project topic or URL param topic
      const effectiveTopic = project?.topic || params.topic;
      if (effectiveTopic && !restoredState?.contentPlan) {
        setTopic(effectiveTopic);
        addMessage('assistant', t('studio.generating'));
        setTimeout(() => {
          startProduction({
            skipNarration: false,
            targetDuration: params.duration || 60,
            visualStyle: params.style || project?.style || 'Cinematic',
            contentPlannerConfig: {
              videoPurpose: 'documentary',
              visualStyle: params.style || project?.style || 'Cinematic',
            }
          }, effectiveTopic);
        }, 500);
      }
    } else if (params.mode === 'story') {
      // Apply topic from URL params or project for story mode
      const storyTopic = project?.topic || params.topic;
      if (storyTopic) {
        setStoryInitialTopic(storyTopic);
      }
    } else if (params.mode === 'music') {
      setShowMusic(true);
    }
  }, [params, isProjectLoading, project, restoredState, setVisualStyle, setTargetDuration, setVideoPurpose, setTopic, startProduction, addMessage, t, setShowMusic, setContentPlan, setVisuals, setNarrationSegments, setAppState]);

  // Sync project metadata when production state changes
  useEffect(() => {
    if (!params.projectId || !project) return;

    const updates: Record<string, unknown> = {};

    if (contentPlan) {
      updates.sceneCount = contentPlan.scenes.length;
      updates.status = 'in_progress';
    }

    if (visuals.length > 0) {
      updates.hasVisuals = true;
      // Use first scene visual as thumbnail
      const firstVisual = visuals.find(v => v.imageUrl);
      if (firstVisual?.imageUrl) {
        updates.thumbnailUrl = firstVisual.imageUrl;
      }
    }

    if (narrationSegments.length > 0) {
      updates.hasNarration = true;
      updates.duration = narrationSegments.reduce((sum, n) => sum + n.audioDuration, 0);
    }

    if (sfxPlan?.generatedMusic?.audioUrl) {
      updates.hasMusic = true;
    }

    if (appState === AppState.READY && contentPlan) {
      updates.status = 'completed';
    }

    // Only sync if we have updates
    if (Object.keys(updates).length > 0) {
      syncProjectMetadata(updates);
    }
  }, [params.projectId, project, contentPlan, visuals, narrationSegments, sfxPlan, appState, syncProjectMetadata]);

  // Auto-scroll to bottom
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [storeMessages]);

  // Merge audio for timeline
  useEffect(() => {
    const mergeAudio = async () => {
      if (!contentPlan || narrationSegments.length === 0) return;
      try {
        const orderedBlobs: Blob[] = [];
        for (const scene of contentPlan.scenes) {
          const narration = narrationSegments.find(n => n.sceneId === scene.id);
          if (narration?.audioBlob) orderedBlobs.push(narration.audioBlob);
        }
        if (orderedBlobs.length === 0) return;

        const sampleRate = 24000;
        const bytesPerSample = 2;
        const WAV_HEADER_SIZE = 44;
        let totalPcmSize = 0;
        const pcmDataArrays: Uint8Array[] = [];

        for (const blob of orderedBlobs) {
          const arrayBuffer = await blob.arrayBuffer();
          const fullData = new Uint8Array(arrayBuffer);
          const pcmData = fullData.slice(WAV_HEADER_SIZE);
          pcmDataArrays.push(pcmData);
          totalPcmSize += pcmData.length;
        }

        const mergedPcm = new Uint8Array(totalPcmSize);
        let offset = 0;
        for (const pcmData of pcmDataArrays) {
          mergedPcm.set(pcmData, offset);
          offset += pcmData.length;
        }

        const wavBuffer = new ArrayBuffer(WAV_HEADER_SIZE + totalPcmSize);
        const view = new DataView(wavBuffer);
        const writeString = (off: number, str: string) => {
          for (let i = 0; i < str.length; i++) view.setUint8(off + i, str.charCodeAt(i));
        };

        writeString(0, 'RIFF');
        view.setUint32(4, 36 + totalPcmSize, true);
        writeString(8, 'WAVE');
        writeString(12, 'fmt ');
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true);
        view.setUint16(22, 1, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * bytesPerSample, true);
        view.setUint16(32, bytesPerSample, true);
        view.setUint16(34, 16, true);
        writeString(36, 'data');
        view.setUint32(40, totalPcmSize, true);
        new Uint8Array(wavBuffer, WAV_HEADER_SIZE).set(mergedPcm);

        const mergedBlob = new Blob([wavBuffer], { type: 'audio/wav' });
        setMergedAudioUrl(URL.createObjectURL(mergedBlob));
      } catch (err) {
        console.error('Failed to merge audio:', err);
      }
    };
    mergeAudio();
  }, [contentPlan, narrationSegments]);

  // Handle preview playback
  useEffect(() => {
    if (isPlaying && contentPlan) {
      previewIntervalRef.current = setInterval(() => {
        setCurrentSceneIndex(prev => (prev + 1) % contentPlan.scenes.length);
      }, 3000);
    } else {
      if (previewIntervalRef.current) clearInterval(previewIntervalRef.current);
    }
    return () => {
      if (previewIntervalRef.current) clearInterval(previewIntervalRef.current);
    };
  }, [isPlaying, contentPlan]);

  // ============================================================
  // Computed Values
  // ============================================================

  const messages: ChatMessage[] = useMemo(() => {
    if (storeMessages.length === 0) {
      return [{
        id: 'welcome',
        role: 'assistant' as const,
        content: t('studio.placeholder'),
        timestamp: Date.now(),
      }];
    }
    return storeMessages as ChatMessage[];
  }, [storeMessages, t]);

  const isVideoReady = useMemo(() => {
    return contentPlan && narrationSegments.length > 0 && appState === AppState.READY;
  }, [contentPlan, narrationSegments, appState]);

  const visualsMap = getVisualsMap();

  const totalDuration = useMemo(() => {
    return narrationSegments.reduce((sum, n) => sum + n.audioDuration, 0);
  }, [narrationSegments]);

  // ============================================================
  // Handlers
  // ============================================================

  const handleReset = useCallback(() => {
    reset();
    clearMessages();
    setCurrentSceneIndex(0);
    setIsPlaying(false);
    setShowTimeline(false);
    setPlaybackTime(0);
    setSelectedSceneId(null);
    studioAgent.resetConversation();
  }, [reset, clearMessages, setShowTimeline]);

  const handleTimelinePlayPause = useCallback(() => {
    if (timelineAudioRef.current) {
      if (isPlaying) timelineAudioRef.current.pause();
      else timelineAudioRef.current.play();
    }
    setIsPlaying(prev => !prev);
  }, [isPlaying]);

  const handleTimelineSeek = useCallback((time: number) => {
    setPlaybackTime(time);
    if (timelineAudioRef.current) timelineAudioRef.current.currentTime = time;
    if (contentPlan) {
      let elapsed = 0;
      for (let i = 0; i < contentPlan.scenes.length; i++) {
        const scene = contentPlan.scenes[i];
        if (!scene) continue;
        const sceneDuration = narrationSegments.find(n => n.sceneId === scene.id)?.audioDuration || scene.duration;
        if (time < elapsed + sceneDuration) {
          setCurrentSceneIndex(i);
          break;
        }
        elapsed += sceneDuration;
      }
    }
  }, [contentPlan, narrationSegments]);

  const handleSceneSelect = useCallback((sceneId: string) => {
    setSelectedSceneId(sceneId);
    if (contentPlan) {
      let elapsed = 0;
      for (const scene of contentPlan.scenes) {
        if (scene.id === sceneId) {
          setPlaybackTime(elapsed);
          if (timelineAudioRef.current) timelineAudioRef.current.currentTime = elapsed;
          break;
        }
        const sceneDuration = narrationSegments.find(n => n.sceneId === scene.id)?.audioDuration || scene.duration;
        elapsed += sceneDuration;
      }
    }
  }, [contentPlan, narrationSegments]);

  const handleSubmit = useCallback(async () => {
    if (!input.trim() || isProcessing) return;

    const userInput = input.trim();
    addMessage('user', userInput);
    setInput('');
    setIsProcessing(true);
    setTyping(true);
    addMessage('assistant', t('common.loading'));

    try {
      const agentResponse: AgentResponse = await studioAgent.processMessage(userInput);
      const messageUpdate: { content: string; quickActions?: QuickAction[] } = {
        content: agentResponse.message,
        quickActions: agentResponse.quickActions || []
      };

      switch (agentResponse.action.type) {
        case 'generate_music': {
          const params = agentResponse.action.params as { prompt?: string; style?: string; title?: string; instrumental?: boolean; customMode?: boolean };
          updateLastMessage(messageUpdate);
          generateMusic({
            prompt: params.prompt ?? "",
            style: params.style,
            title: params.title,
            instrumental: params.instrumental,
            customMode: params.customMode,
            model: 'V5'
          });
          trackMusicGeneration();
          setShowMusic(true);
          break;
        }
        case 'create_video': {
          const params = agentResponse.action.params as { topic: string; duration?: number; style?: string };
          updateLastMessage(messageUpdate);
          setTopic(params.topic);
          setTargetDuration(params.duration || 60);
          setVisualStyle(params.style || 'Cinematic');
          setVideoPurpose('documentary');

          // Track video creation with actual params
          trackVideoCreation({
            style: params.style || 'Cinematic',
            duration: params.duration || 60
          });

          startProduction({
            skipNarration: false,
            targetDuration: params.duration || 60,
            visualStyle: params.style || 'Cinematic',
            contentPlannerConfig: {
              videoPurpose: 'documentary',
              visualStyle: params.style || 'Cinematic',
            }
          }, params.topic);
          break;
        }
        case 'export_video': {
          setShowExport(true);
          updateLastMessage(messageUpdate);
          break;
        }
        case 'reset': {
          handleReset();
          break;
        }
        case 'browse_sfx': {
          const params = agentResponse.action.params as { category: string };
          updateLastMessage(messageUpdate);
          try {
            const sound = await browseSfx(params.category);
            if (sound) {
              addMessage('assistant', `Found SFX: "${sound.name}" (${sound.duration.toFixed(1)}s)`);
            }
          } catch (err: unknown) {
            const errMsg = err instanceof Error ? err.message : String(err);
            addMessage('assistant', `SFX search failed: ${errMsg}`);
          }
          break;
        }
        case 'set_camera_style': {
          const params = agentResponse.action.params as { angle?: string; lighting?: string };
          if (params.angle) setPreferredCameraAngle(params.angle);
          if (params.lighting) setPreferredLightingMood(params.lighting);
          updateLastMessage(messageUpdate);
          break;
        }
        case 'show_quality_report': {
          if (qualityReport) {
            setShowQuality(true);
            updateLastMessage(messageUpdate);
          } else {
            updateLastMessage({ content: t('common.error') });
          }
          break;
        }
        default: {
          updateLastMessage(messageUpdate);
        }
      }
    } catch (err) {
      console.error('Agent error:', err);
      updateLastMessage({ content: t('errors.generic') });
    }

    setTyping(false);
    setIsProcessing(false);
  }, [input, isProcessing, addMessage, updateLastMessage, setTyping, setTopic, setTargetDuration, setVisualStyle, setVideoPurpose, startProduction, handleReset, browseSfx, setPreferredCameraAngle, setPreferredLightingMood, qualityReport, generateMusic, t, setShowMusic, setShowExport, setShowQuality]);

  const handleQuickAction = useCallback(async (action: { type: string; params?: Record<string, unknown> }) => {
    if (isProcessing) return;

    setIsProcessing(true);
    setTyping(true);
    updateLastMessage({ quickActions: [] });

    try {
      switch (action.type) {
        case 'create_video': {
          const params = action.params as { topic: string; duration?: number; style?: string } | undefined;
          if (!params?.topic) break;
          addMessage('assistant', `🎬 Creating ${params.duration || 60}s ${params.style || 'Cinematic'} video...`);
          setTopic(params.topic);
          setTargetDuration(params.duration || 60);
          setVisualStyle(params.style || 'Cinematic');
          setVideoPurpose('documentary');

          // Track video creation with actual params
          trackVideoCreation({
            style: params.style || 'Cinematic',
            duration: params.duration || 60
          });

          startProduction({
            skipNarration: false,
            targetDuration: params.duration || 60,
            visualStyle: params.style || 'Cinematic',
            contentPlannerConfig: {
              videoPurpose: 'documentary',
              visualStyle: params.style || 'Cinematic',
            }
          }, params.topic);
          break;
        }
        case 'generate_music': {
          const params = action.params as { prompt?: string; style?: string; instrumental?: boolean } | undefined;
          addMessage('assistant', `🎵 Creating ${params?.style || 'music'}...`);
          generateMusic({
            prompt: params?.prompt ?? "",
            style: params?.style,
            instrumental: params?.instrumental ?? true,
            model: 'V5'
          });
          trackMusicGeneration();
          setShowMusic(true);
          break;
        }
        case 'ask_clarification': {
          // Handle clarification requests - just send the message
          const params = action.params as { message?: string } | undefined;
          if (params?.message) {
            setInput(params.message);
          }
          break;
        }
        default:
          console.warn('Unknown quick action type:', action.type);
      }
    } catch (err) {
      console.error('Quick action error:', err);
      addMessage('assistant', t('errors.generic'));
    }

    setTyping(false);
    setIsProcessing(false);
  }, [isProcessing, addMessage, updateLastMessage, setTyping, setTopic, setTargetDuration, setVisualStyle, setVideoPurpose, startProduction, generateMusic, t, setShowMusic, setInput]);

  // Feedback handler
  const handleFeedback = useCallback((
    messageId: string,
    feedback: { helpful: boolean; rating: number; comment?: string }
  ) => {
    const messageIndex = messages.findIndex(m => m.id === messageId);
    if (messageIndex === -1) return;

    const agentMessage = messages[messageIndex];
    const userMessage = messages[messageIndex - 1];

    if (!agentMessage) return;

    recordFeedback({
      messageId,
      userMessage: userMessage?.content || '',
      agentResponse: agentMessage.content,
      helpful: feedback.helpful,
      rating: feedback.rating,
      comment: feedback.comment,
      timestamp: Date.now(),
    });

    console.log('[Feedback] Recorded:', feedback.helpful ? '👍 Helpful' : '👎 Not helpful');
  }, [messages, recordFeedback]);

  const handleExport = useCallback(async (
    config: { presetId: string; width: number; height: number; orientation: 'landscape' | 'portrait'; quality: string },
    onProgress?: (percent: number) => void
  ) => {
    if (!contentPlan || narrationSegments.length === 0 || !mergedAudioUrl) {
      throw new Error('Video not ready for export');
    }

    let currentTime = 0;
    const parsedSubtitles = contentPlan.scenes.map((scene, idx) => {
      const narration = narrationSegments.find(n => n.sceneId === scene.id);
      const duration = narration?.audioDuration || scene.duration;
      const subtitle = {
        id: idx + 1,
        startTime: currentTime,
        endTime: currentTime + duration,
        text: narration?.transcript || scene.narrationScript,
      };
      currentTime += duration;
      return subtitle;
    });

    const prompts = contentPlan.scenes.map((scene, idx) => ({
      id: scene.id,
      text: scene.visualDescription,
      mood: getEffectiveLegacyTone(scene),
      timestampSeconds: parsedSubtitles[idx]?.startTime || 0,
    }));

    // Use full visuals array to preserve type, cachedBlobUrl, etc.
    const generatedImages = visuals.filter(v => v.imageUrl).map(v => ({
      ...v,
      // Prefer cached blob URL over original URL (prevents expired URL issues)
      imageUrl: v.cachedBlobUrl || v.imageUrl,
    }));

    const songData = {
      fileName: contentPlan.title || 'ai-video',
      audioUrl: mergedAudioUrl,
      srtContent: '',
      parsedSubtitles,
      prompts,
      generatedImages,
    };

    const sceneTimings = contentPlan.scenes.map((scene, idx) => {
      const narration = narrationSegments.find(n => n.sceneId === scene.id);
      const subtitle = parsedSubtitles[idx];
      return {
        sceneId: scene.id,
        startTime: subtitle?.startTime ?? 0,
        duration: narration?.audioDuration || scene.duration,
      };
    });

    const { exportVideoWithFFmpeg } = await import('@/services/ffmpeg/exporters');

    const result = await exportVideoWithFFmpeg(
      songData,
      (p) => onProgress?.(p.progress),
      {
        orientation: config.orientation,
        useModernEffects: true,
        transitionType: 'dissolve',
        transitionDuration: 1.5,
        contentMode: 'story',
        sfxPlan,
        sceneTimings,
      },
      // Pass export options for history tracking
      {
        projectId: params.projectId,
        cloudSessionId: project?.cloudSessionId,
        userId: getCurrentUser()?.uid,
      }
    );

    // Use blob from result (may include cloudUrl for cloud exports)
    const blob = result.blob ?? result;

    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `${contentPlan.title || 'video'}-${config.presetId}.mp4`;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
  }, [contentPlan, narrationSegments, mergedAudioUrl, visuals, sfxPlan, params.projectId, project]);

  // ============================================================
  // TEST: Load saved media from local folder
  // ============================================================
  const loadTestMedia = useCallback(async () => {
    const basePath = '/production_prod_1769364025193_ch60ee8c1';

    // Create mock content plan
    const mockContentPlan = {
      title: 'زئير الفينيق: الثواني الأخيرة',
      totalDuration: 78,
      targetAudience: 'General audience',
      overallTone: 'Cinematic',
      scenes: [
        { id: 'scene-1', name: 'الصمت قبل العاصفة', duration: 15, visualDescription: 'Scene 1', narrationScript: '', emotionalTone: 'dramatic' as const, instructionTriplet: { primaryEmotion: 'visceral-dread', cinematicDirection: 'slow-push-in', environmentalAtmosphere: 'desert-silence' } },
        { id: 'scene-2', name: 'الاستيقاظ الداخلي', duration: 14, visualDescription: 'Scene 2', narrationScript: '', emotionalTone: 'dramatic' as const, instructionTriplet: { primaryEmotion: 'bittersweet-longing', cinematicDirection: 'close-up', environmentalAtmosphere: 'cathedral-reverb' } },
        { id: 'scene-3', name: 'زئير الفينيق', duration: 16, visualDescription: 'Scene 3', narrationScript: '', emotionalTone: 'dramatic' as const, instructionTriplet: { primaryEmotion: 'euphoric-wonder', cinematicDirection: 'tracking-shot', environmentalAtmosphere: 'ethereal-echo' } },
        { id: 'scene-4', name: 'الرمية المقدسة', duration: 17, visualDescription: 'Scene 4', narrationScript: '', emotionalTone: 'dramatic' as const, instructionTriplet: { primaryEmotion: 'seething-rage', cinematicDirection: 'handheld-float', environmentalAtmosphere: 'tension-drone' } },
        { id: 'scene-5', name: 'الاحتراق والنصر', duration: 16, visualDescription: 'Scene 5', narrationScript: '', emotionalTone: 'dramatic' as const, instructionTriplet: { primaryEmotion: 'stoic-resignation', cinematicDirection: 'pull-back', environmentalAtmosphere: 'golden-hour-decay' } },
      ],
    };

    // Create visuals from local video files
    const mockVisuals = [
      { promptId: 'scene-1', imageUrl: `${basePath}/video_clips/scene_0_veo.mp4`, type: 'video' as const, generatedWithVeo: true },
      { promptId: 'scene-2', imageUrl: `${basePath}/video_clips/scene_1_veo.mp4`, type: 'video' as const, generatedWithVeo: true },
      { promptId: 'scene-3', imageUrl: `${basePath}/video_clips/scene_2_veo.mp4`, type: 'video' as const, generatedWithVeo: true },
      { promptId: 'scene-4', imageUrl: `${basePath}/video_clips/scene_3_veo.mp4`, type: 'video' as const, generatedWithVeo: true },
      { promptId: 'scene-5', imageUrl: `${basePath}/video_clips/scene_4_veo.mp4`, type: 'video' as const, generatedWithVeo: true },
    ];

    // Create narration segments from local audio files
    const mockNarrationSegments = await Promise.all([
      { sceneId: 'scene-1', audioUrl: `${basePath}/audio/narration_scene-1.wav`, audioDuration: 13.9, transcript: 'Scene 1 narration' },
      { sceneId: 'scene-2', audioUrl: `${basePath}/audio/narration_scene-2.wav`, audioDuration: 12.3, transcript: 'Scene 2 narration' },
      { sceneId: 'scene-3', audioUrl: `${basePath}/audio/narration_scene-3.wav`, audioDuration: 14.3, transcript: 'Scene 3 narration' },
      { sceneId: 'scene-4', audioUrl: `${basePath}/audio/narration_scene-4.wav`, audioDuration: 15.1, transcript: 'Scene 4 narration' },
      { sceneId: 'scene-5', audioUrl: `${basePath}/audio/narration_scene-5.wav`, audioDuration: 14.8, transcript: 'Scene 5 narration' },
    ].map(async (seg) => {
      // Fetch audio and create blob
      const response = await fetch(seg.audioUrl);
      const blob = await response.blob();
      return {
        sceneId: seg.sceneId,
        audioBlob: blob,
        audioDuration: seg.audioDuration,
        transcript: seg.transcript,
      };
    }));

    // Set the data
    setContentPlan(mockContentPlan as any);
    setVisuals(mockVisuals);
    setNarrationSegments(mockNarrationSegments);
    setAppState(AppState.READY);

    console.log('[TEST] Loaded test media from local folder');
    alert('Test media loaded! You can now export.');
  }, [setContentPlan, setVisuals, setNarrationSegments, setAppState]);

  // Handle format pipeline execution — delegates movie-animation to storyHook
  const handleFormatExecute = useCallback(() => {
    if (formatPipelineHook.selectedFormat === 'movie-animation') {
      // Delegate to existing story generation pipeline
      const idea = formatPipelineHook.idea || storyInitialTopic || topic || '';
      const genre = formatPipelineHook.selectedGenre || 'Drama';
      setStoryInitialTopic(idea);
      storyHook.updateGenre(genre);
      storyHook.generateBreakdown(idea, genre);
    } else {
      // Run format-specific pipeline
      const user = getCurrentUser();
      const userId = user?.uid ?? 'anonymous';
      const projectId = params.projectId ?? `fp_${Date.now()}`;
      formatPipelineHook.execute(userId, projectId);
    }
  }, [formatPipelineHook, storyHook, storyInitialTopic, topic, params.projectId]);

  const handleOpenInEditor = useCallback(() => {
    const editorStore = useVideoEditorStore.getState();
    editorStore.reset();

    // Create Tracks
    editorStore.addTrack('video', 'Visuals');
    editorStore.addTrack('audio', 'Voiceover');

    let visuals: any[] = [];
    let narrations: any[] = [];
    let breakdown: any[] = [];

    if (formatPipelineHook.result?.success) {
      const pr = formatPipelineHook.result.partialResults;
      visuals = pr?.visuals || [];
      narrations = pr?.narrationSegments || [];
      breakdown = pr?.screenplay || [];
    } else {
      visuals = storyHook.state.scenesWithVisuals || [];
      narrations = storyHook.state.narrationSegments || [];
      breakdown = storyHook.state.breakdown || [];
    }

    setTimeout(() => {
      const state = useVideoEditorStore.getState();
      const videoTrack = state.tracks.find(t => t.type === 'video');
      const audioTrack = state.tracks.find(t => t.type === 'audio');

      let currentTime = 0;
      breakdown.forEach((scene: any, idx: number) => {
        const visual = visuals.find((v: any) => v.sceneId === scene.id || v.sceneId === scene.sceneId);
        const narration = narrations.find((n: any) => n.sceneId === scene.id || n.sceneId === scene.sceneId);
        const duration = narration?.audioDuration || scene.duration || 5;

        if (visual && videoTrack) {
          state.addClip({
            trackId: videoTrack.id,
            type: 'video',
            startTime: currentTime,
            duration: duration,
            name: scene.heading || `Scene ${idx + 1}`,
            sourceUrl: visual.imageUrl,
            thumbnailUrl: visual.imageUrl,
            inPoint: 0,
            outPoint: duration
          });
        }

        if (narration && audioTrack) {
          const url = narration.audioBlob ? URL.createObjectURL(narration.audioBlob) : narration.audioUrl;
          if (url) {
            state.addClip({
              trackId: audioTrack.id,
              type: 'audio',
              startTime: currentTime,
              duration: duration,
              name: `Voiceover ${idx + 1}`,
              sourceUrl: url,
              inPoint: 0,
              outPoint: duration
            });
          }
        }
        currentTime += duration;
      });

      if (mergedAudioUrl) {
        editorStore.addTrack('audio', 'Music');
        setTimeout(() => {
          const state2 = useVideoEditorStore.getState();
          const musicTrack = [...state2.tracks].reverse().find(t => t.type === 'audio' && t.name === 'Music');
          if (musicTrack) {
            state2.addClip({
              trackId: musicTrack.id,
              type: 'audio',
              startTime: 0,
              duration: currentTime,
              name: 'Background Music',
              sourceUrl: mergedAudioUrl,
              inPoint: 0,
              outPoint: currentTime
            });
          }
        }, 0);
      }

      setStudioMode('editor');
    }, 50);
  }, [formatPipelineHook.result, storyHook.state, mergedAudioUrl]);

  // Auto-transition to Editor upon format pipeline completion
  useEffect(() => {
    if (!formatPipelineHook.isRunning && formatPipelineHook.result?.success) {
      const resultHash = (params.projectId || 'fp') + '_' + (formatPipelineHook.result.partialResults?.totalDuration || Date.now());
      if (lastProcessedResultRef.current !== resultHash) {
        lastProcessedResultRef.current = resultHash;
        handleOpenInEditor();
      }
    }
  }, [formatPipelineHook.isRunning, formatPipelineHook.result, handleOpenInEditor, params.projectId]);

  // Quick actions for welcome state
  const quickActionItems = useMemo(() => [
    { icon: MusicIcon, label: t('home.createMusic'), prompt: 'Generate an upbeat synthwave track about city lights at night' },
    { icon: Video, label: t('home.createVideo'), prompt: 'Create a cinematic travel video about exploring ancient Rome' },
    { icon: ImageIcon, label: t('home.visualizer'), prompt: 'Generate a documentary about the journey of a coffee bean' },
  ], [t]);

  // ============================================================
  // Render
  // ============================================================

  const headerActions = (
    <div className="flex items-center gap-2" role="toolbar" aria-label="Studio actions">
      {/* Mode Toggle Selection */}
      <div className="flex items-center bg-secondary border border-border rounded-lg p-0.5 me-4">
        <Button
          variant="ghost"
          size="sm"
          onClick={() => setStudioMode('chat')}
          className={cn(
            "h-7 px-3 text-[10px] uppercase font-bold transition-all",
            studioMode === 'chat' ? "bg-primary text-primary-foreground shadow-lg" : "text-muted-foreground hover:text-foreground"
          )}
        >
          Chat Mode
        </Button>
        <Button
          variant="ghost"
          size="sm"
          onClick={() => setStudioMode('story')}
          className={cn(
            "h-7 px-3 text-[10px] uppercase font-bold transition-all",
            studioMode === 'story' ? "bg-primary text-primary-foreground shadow-lg" : "text-muted-foreground hover:text-foreground"
          )}
        >
          Story Mode
        </Button>
        <Button
          variant="ghost"
          size="sm"
          onClick={() => setStudioMode('editor')}
          className={cn(
            "h-7 px-3 text-[10px] uppercase font-bold transition-all",
            studioMode === 'editor' ? "bg-primary text-primary-foreground shadow-lg" : "text-muted-foreground hover:text-foreground"
          )}
        >
          Editor
        </Button>
      </div>

      {/* Simple/Advanced toggle */}
      <div className="flex items-center gap-1 bg-secondary rounded-lg p-1" role="group" aria-label="View mode">
        <button
          onClick={() => setViewMode('simple')}
          className={cn(
            'px-3 py-1.5 text-xs font-medium rounded-md transition-all',
            viewMode === 'simple'
              ? 'bg-primary text-primary-foreground'
              : 'text-muted-foreground hover:text-foreground'
          )}
          aria-pressed={viewMode === 'simple'}
        >
          {t('studio.simpleMode')}
        </button>
        <button
          onClick={() => setViewMode('advanced')}
          className={cn(
            'px-3 py-1.5 text-xs font-medium rounded-md transition-all',
            viewMode === 'advanced'
              ? 'bg-primary text-primary-foreground'
              : 'text-muted-foreground hover:text-foreground'
          )}
          aria-pressed={viewMode === 'advanced'}
        >
          {t('studio.advancedMode')}
        </button>
      </div>

      <Button
        variant="ghost"
        size="sm"
        onClick={() => setShowSettings(true)}
        className="text-muted-foreground hover:text-foreground hover:bg-secondary"
      >
        <Settings className="w-4 h-4 me-2" />
        {t('studio.settings')}
      </Button>

      {/* Action buttons */}
      {(messages.length > 1 || contentPlan) && (
        <Button
          variant="ghost"
          size="sm"
          onClick={handleReset}
          className="text-muted-foreground hover:text-foreground hover:bg-secondary"
        >
          <RotateCcw className="w-4 h-4 me-2" aria-hidden="true" />
          {t('studio.newProject')}
        </Button>
      )}
      {isVideoReady && (
        <>
          <Button
            variant={showSceneEditor ? 'secondary' : 'ghost'}
            size="sm"
            onClick={() => setShowSceneEditor(!showSceneEditor)}
            className={cn(
              'gap-2',
              showSceneEditor
                ? 'bg-primary/20 text-primary border border-primary/30'
                : 'text-muted-foreground hover:text-foreground hover:bg-secondary'
            )}
            aria-pressed={showSceneEditor}
          >
            <Edit3 className="w-4 h-4" aria-hidden="true" />
            {t('studio.edit')}
          </Button>
          {qualityReport && (
            <Button
              variant="ghost"
              size="sm"
              onClick={() => setShowQuality(true)}
              className="text-muted-foreground hover:text-foreground hover:bg-secondary gap-2"
            >
              <BarChart3 className="w-4 h-4" />
              {t('studio.quality')}
            </Button>
          )}
          <Button
            variant={showTimeline ? 'secondary' : 'ghost'}
            size="sm"
            onClick={() => setShowTimeline(!showTimeline)}
            className={cn(
              'gap-2',
              showTimeline
                ? 'bg-primary/20 text-primary border border-primary/30'
                : 'text-muted-foreground hover:text-foreground hover:bg-secondary'
            )}
          >
            <Layers className="w-4 h-4" />
            {t('studio.timeline')}
          </Button>
          <Button
            onClick={() => setShowExport(true)}
            size="sm"
            className="bg-primary hover:bg-primary/90 text-primary-foreground gap-2 shadow-lg shadow-primary/20"
          >
            <Download className="w-4 h-4" />
            {t('studio.export')}
          </Button>
        </>
      )}
    </div>
  );

  // Loading state for project
  if (isProjectLoading) {
    return (
      <ScreenLayout
        title={t('studio.title')}
        showBackButton
        onBack={() => navigate('/')}
      >
        <div className="flex items-center justify-center h-full min-h-[50vh]">
          <div className="flex flex-col items-center gap-4">
            <Loader2 className="w-8 h-8 animate-spin text-primary" />
            <p className="text-white/60">{t('studio.loadingProject') || 'Loading project...'}</p>
          </div>
        </div>
      </ScreenLayout>
    );
  }

  // Error state for project loading
  if (projectError) {
    return (
      <ScreenLayout
        title={t('studio.title')}
        showBackButton
        onBack={() => navigate('/')}
      >
        <div className="flex items-center justify-center h-full min-h-[50vh]">
          <div className="p-6 rounded-xl bg-red-500/10 border border-red-500/20 text-center max-w-md">
            <p className="text-red-400 mb-4">{projectError}</p>
            <Button onClick={() => navigate('/projects')}>
              {t('common.backToProjects') || 'Back to Projects'}
            </Button>
          </div>
        </div>
      </ScreenLayout>
    );
  }

  return (
    <ScreenLayout
      title={t('studio.title')}
      showBackButton
      onBack={() => navigate('/')}
      headerActions={headerActions}
      contentClassName={cn("py-8", studioMode === 'story' && "p-0 h-full")}
      maxWidth={studioMode === 'story' ? 'full' : '3xl'}
      footer={
        studioMode === 'chat' ? (
          <ChatInput
            value={input}
            onChange={setInput}
            onSubmit={handleSubmit}
            placeholder={t('studio.placeholder')}
            disabled={appState !== AppState.IDLE}
            isLoading={isProcessing || appState !== AppState.IDLE}
            isRTL={isRTL}
            hintText={`${t('studio.send')}(Enter)`}
            inputId="studio-input"
          />
        ) : null
      }
    >
      {studioMode === 'editor' ? (
        <VideoEditor className="h-full" />
      ) : studioMode === 'story' ? (
        <StoryWorkspaceErrorBoundary
          storyState={storyHook.state}
          onRestore={() => {
            // Restore from version history or last saved state
            console.log('[StoryWorkspace] Restoring from last saved state');
            // The version history system already handles auto-save
          }}
        >
          <StoryWorkspace
            storyState={storyHook.state}
            initialTopic={storyInitialTopic || topic || ''}
            formatPipelineHook={formatPipelineHook}
            onFormatExecute={handleFormatExecute}
            onOpenInEditor={handleOpenInEditor}
            onGenerateIdea={(storyTopic, genre) => {
              setStoryInitialTopic(storyTopic);
              storyHook.updateGenre(genre);
              storyHook.generateBreakdown(storyTopic, genre);
            }}
            onExportScript={storyHook.exportScreenplay}
            onRegenerateScene={storyHook.regenerateScene}
            onVerifyConsistency={storyHook.verifyConsistency}
            onGenerateScreenplay={storyHook.generateScreenplay}
            onGenerateCharacters={storyHook.generateCharacters}
            onGenerateCharacterImage={storyHook.generateCharacterImage}
            onUndo={storyHook.undo}
            onRedo={storyHook.redo}
            canUndo={storyHook.canUndo}
            canRedo={storyHook.canRedo}
            onNextStep={() => {
              const step = storyHook.state.currentStep;
              const isLocked = storyHook.state.isLocked;

              if (step === 'idea') {
                // Idea → Breakdown: Generate story outline
                storyHook.generateBreakdown(storyInitialTopic || topic || "A generic story", "Drama");
              } else if (step === 'breakdown') {
                // Breakdown → Script: Generate full screenplay
                storyHook.generateScreenplay();
              } else if (step === 'script') {
                // Script → Characters: Generate character profiles
                // Note: Lock is handled separately via onLockStory
                storyHook.generateCharacters();
              } else if (step === 'characters') {
                // Characters → Shots: Generate shot breakdown
                // Story must be locked at this point
                if (isLocked) {
                  storyHook.generateShots();
                } else {
                  // Shouldn't happen, but fallback to showing lock dialog
                  console.warn('Story should be locked before generating shots');
                  storyHook.setStep('script');
                }
              } else if (step === 'shots') {
                // Shots → Style: Move to visual style selection
                storyHook.setStep('style');
              } else if (step === 'style') {
                // Style → Storyboard: Generate storyboard visuals
                storyHook.generateVisuals();
              }
            }}
            onGenerateShots={storyHook.generateShots}
            onGenerateVisuals={storyHook.generateVisuals}
            stageProgress={storyHook.getStageProgress()}
            isProcessing={storyHook.isProcessing}
            progress={storyHook.progress}
            // Storyboarder.ai-style workflow props
            onLockStory={storyHook.lockStory}
            onUpdateVisualStyle={storyHook.updateVisualStyle}
            onUpdateAspectRatio={storyHook.updateAspectRatio}
            onUpdateImageProvider={storyHook.updateImageProvider}
            // Error handling
            error={storyHook.error}
            onClearError={storyHook.clearError}
            onRetry={storyHook.retryLastOperation}
            onUpdateShot={storyHook.updateShot}
            // Narration, Animation, and Export
            onGenerateNarration={storyHook.generateNarration}
            onAnimateShots={storyHook.animateShots}
            onExportFinalVideo={storyHook.exportFinalVideo}
            onDownloadVideo={storyHook.downloadVideo}
            allScenesHaveNarration={storyHook.allScenesHaveNarration}
            allShotsHaveAnimation={storyHook.allShotsHaveAnimation}
            // Template and project management
            projectId={storyHook.sessionId ?? undefined}
            onApplyTemplate={storyHook.applyTemplate}
            onImportProject={storyHook.importProject}
          />
        </StoryWorkspaceErrorBoundary>
      ) : (
        <>
          {/* Welcome State */}
          {messages.length === 1 && !contentPlan && (
            <div className="text-center mb-12 pt-12">
              <div className="w-16 h-16 mx-auto mb-6 rounded-2xl bg-linear-to-br from-primary/20 to-accent/20 border border-border flex items-center justify-center" aria-hidden="true">
                <Wand2 className="w-8 h-8 text-primary" />
              </div>
              <h1 className="text-3xl font-light text-white mb-3">{t('studio.placeholder')}</h1>
            </div>
          )}

          {/* Messages */}
          <div className="space-y-6" role="log" aria-live="polite" aria-label="Chat messages">
            {messages.map((message) => (
              <MessageBubble
                key={message.id}
                message={message}
                isRTL={isRTL}
                onQuickAction={handleQuickAction}
                onFeedback={handleFeedback}
              />
            ))}
            {error && (
              <div className="p-4 rounded-xl bg-red-500/10 border border-red-500/20 text-red-200 text-sm max-w-2xl mx-auto" role="alert">
                {error}
              </div>
            )}
            <div ref={messagesEndRef} />
          </div>

          {/* Video Preview Card */}
          {contentPlan && (
            <VideoPreviewCard
              scenes={contentPlan.scenes}
              visualsMap={visualsMap}
              currentSceneIndex={currentSceneIndex}
              onSceneSelect={setCurrentSceneIndex}
              isPlaying={isPlaying}
              onPlayPause={() => setIsPlaying(!isPlaying)}
              isReady={isVideoReady ?? false}
              totalDuration={totalDuration}
              scenesLabel={t('studio.scenes')}
              doneLabel={t('common.done')}
              isRTL={isRTL}
              className="mt-8 mb-4"
            />
          )}

          {/* Timeline Editor (Requirement 6.3) */}
          {showTimeline && contentPlan && (
            <motion.div
              initial={{ opacity: 0, y: 20 }}
              animate={{ opacity: 1, y: 0 }}
              exit={{ opacity: 0, y: 20 }}
              className="mt-4"
            >
              <GraphiteTimeline
                scenes={contentPlan.scenes}
                visuals={visualsMap}
                narrationSegments={narrationSegments}
                currentTime={playbackTime}
                duration={totalDuration}
                isPlaying={isPlaying}
                onPlayPause={handleTimelinePlayPause}
                onSeek={handleTimelineSeek}
                onSceneSelect={handleSceneSelect}
                selectedSceneId={selectedSceneId}
                projectName={contentPlan.title}
                sfxPlan={sfxPlan}
                className="rounded-xl overflow-hidden border border-white/5"
              />
              <audio
                ref={timelineAudioRef}
                src={mergedAudioUrl || undefined}
                onTimeUpdate={(e) => setPlaybackTime(e.currentTarget.currentTime)}
                onEnded={() => setIsPlaying(false)}
              />
            </motion.div>
          )}

          {/* Quick Actions */}
          {messages.length === 1 && !contentPlan && (
            <>
              <QuickActions
                actions={quickActionItems}
                onSelect={(action) => setInput(action.prompt || '')}
                isRTL={isRTL}
              />
              <div className="flex justify-center">
                <button
                  onClick={() => {
                    setMusicModalMode('remix');
                    setShowMusic(true);
                  }}
                  className="flex items-center gap-2 px-4 py-2.5 rounded-full bg-white/5 hover:bg-white/10 border border-white/10 text-sm text-white/70 hover:text-white transition-all"
                >
                  <Upload className="w-4 h-4 text-primary" />
                  {t('common.upload')}
                </button>
              </div>
            </>
          )}

          {/* Modals & Panels */}
          <MusicGeneratorModal
            open={showMusic}
            onClose={() => {
              setShowMusic(false);
              setMusicModalMode('generate');
            }}
            musicState={musicState}
            onGenerateMusic={generateMusic}
            onGenerateLyrics={generateLyrics}
            onSelectTrack={selectTrack}
            onAddToTimeline={() => {
              addMusicToTimeline();
              setShowTimeline(true);
            }}
            onRefreshCredits={refreshCredits}
            onUploadAudio={uploadAudio}
            onUploadAndCover={uploadAndCover}
            onAddVocals={addVocals}
            onAddInstrumental={addInstrumental}
            initialMode={musicModalMode}
          />

          {qualityReport && (
            <QualityDashboard
              report={qualityReport}
              isOpen={showQuality}
              onClose={() => setShowQuality(false)}
            />
          )}

          <SlidePanel
            isOpen={showSceneEditor && !!contentPlan}
            onClose={() => setShowSceneEditor(false)}
            title={t('studio.edit')}
            isRTL={isRTL}
          >
            {contentPlan && (
              <SceneEditor
                scenes={contentPlan.scenes}
                onChange={updateScenes}
                onPlayNarration={playNarration}
                onRegenerateNarration={regenerateSceneNarration}
                playingSceneId={playingSceneId}
                visuals={visualsMap}
                narrationUrls={getAudioUrlMap()}
              />
            )}
          </SlidePanel>

          <QuickExport
            isOpen={showExport}
            onClose={() => setShowExport(false)}
            onExport={handleExport}
            videoTitle={contentPlan?.title}
            duration={totalDuration}
          />

          <SettingsModal
            isOpen={showSettings}
            onClose={() => setShowSettings(false)}
            contentType={params.mode === 'music' ? 'music' : 'story'}
            onContentTypeChange={() => { }}
            videoPurpose={params.mode === 'video' ? 'documentary' : 'music_video'}
            onVideoPurposeChange={(purpose) => {
              if (purpose !== 'documentary') setVideoPurpose(purpose);
            }}
            targetAudience={targetAudience}
            onTargetAudienceChange={setTargetAudience}
            generationMode={params.mode === 'music' ? 'image' : 'video'}
            onGenerationModeChange={() => { }}
            videoProvider="veo"
            onVideoProviderChange={() => { }}
            veoVideoCount={veoVideoCount}
            onVeoVideoCountChange={setVeoVideoCount}
            aspectRatio="16:9"
            onAspectRatioChange={() => { }}
            selectedStyle={params.style || 'Cinematic'}
            onStyleChange={(style: string) => setVisualStyle(style)}
            globalSubject=""
            onGlobalSubjectChange={() => { }}
          />
        </>
      )}
    </ScreenLayout>
  );
}
````

## File: packages/frontend/screens/VisualizerScreen.tsx
````typescript
/**
 * Visualizer Screen - Audio-first lyric video creation
 *
 * Refactored to use extracted components for better maintainability.
 * Requirements: 1.1, 9.1, 9.4
 */

import React, { useState, useCallback, useEffect, useMemo } from 'react';
import { useNavigate, useSearchParams } from 'react-router-dom';
import { motion, AnimatePresence } from 'framer-motion';
import {
  Download,
  RotateCcw,
  Wand2,
  Video,
  Loader2,
  X,
} from 'lucide-react';
import { Button } from '@/components/ui/button';
import { cn } from '@/lib/utils';
import { useLanguage } from '@/i18n/useLanguage';
import { useLyricLens } from '@/hooks/useLyricLens';
import { useModalState } from '@/hooks/useModalState';
import { useProjectSession } from '@/hooks/useProjectSession';
import { useAuth } from '@/hooks/useAuth';
import { AppState, GeneratedImage } from '@/types';

// Layout Components
import { ScreenLayout } from '@/components/layout/ScreenLayout';

// Visualizer Components
import { AudioUploadForm, VisualPreview, SceneThumbnails } from '@/components/visualizer';

// Feature Components
import { TimelinePlayer } from '@/components/TimelinePlayer';
import { QuickExport } from '@/components/QuickExport';
import { ErrorState } from '@/components/ui/ErrorState';

// Services
import { animateImageWithDeApi, animateImageBatch } from '@/services/deapiService';

export default function VisualizerScreen() {
  const { t, isRTL } = useLanguage();
  const navigate = useNavigate();
  const [searchParams] = useSearchParams();

  // Get projectId from URL (e.g., /visualizer?projectId=abc123)
  const projectId = searchParams.get('projectId') || undefined;

  // Project and auth hooks
  const { project, sessionId } = useProjectSession(projectId);
  const { user } = useAuth();

  // Modal state
  const { showExport, setShowExport } = useModalState();

  // LyricLens hook for audio processing
  const {
    appState,
    songData,
    setSongData,
    errorMsg,
    isBulkGenerating,
    processFile,
    handleGenerateAll,
    resetApp,
    imageProvider,
    setImageProvider,
    directorMode,
    setDirectorMode,
    globalSubject,
    setGlobalSubject,
  } = useLyricLens();

  // Local state
  const [audioFile, setAudioFile] = useState<File | null>(null);
  const [selectedStyle, setSelectedStyle] = useState('Cinematic');
  const [isPlaying, setIsPlaying] = useState(false);
  const [currentTime, setCurrentTime] = useState(0);
  const [duration, setDuration] = useState(0);
  const [currentSceneIndex, setCurrentSceneIndex] = useState(0);

  // Animation state
  const [animatingPromptId, setAnimatingPromptId] = useState<string | null>(null);
  const [animationError, setAnimationError] = useState<string | null>(null);
  const [isBatchAnimating, setIsBatchAnimating] = useState(false);
  const [batchAnimationProgress, setBatchAnimationProgress] = useState({ current: 0, total: 0 });

  // ============================================================
  // Computed Values
  // ============================================================

  const hasVisuals = useMemo(() => {
    return songData?.generatedImages && songData.generatedImages.length > 0;
  }, [songData]);

  const currentVisual = useMemo(() => {
    if (!songData || !hasVisuals) return null;
    const currentPrompt = songData.prompts[currentSceneIndex];
    if (!currentPrompt) return null;
    return songData.generatedImages.find(img => img.promptId === currentPrompt.id) || null;
  }, [songData, hasVisuals, currentSceneIndex]);

  const isReadyForExport = useMemo(() => {
    return songData && hasVisuals && songData.generatedImages.length >= songData.prompts.length * 0.5;
  }, [songData, hasVisuals]);

  const animatableImagesCount = useMemo(() => {
    if (!songData) return 0;
    return songData.generatedImages.filter(img => img.type !== 'video').length;
  }, [songData]);

  const isProcessing = appState === AppState.PROCESSING_AUDIO ||
    appState === AppState.TRANSCRIBING ||
    appState === AppState.ANALYZING_LYRICS ||
    appState === AppState.GENERATING_PROMPTS;

  const isReady = appState === AppState.READY && songData;

  // ============================================================
  // Effects
  // ============================================================

  // Update current scene based on playback time using binary search for accurate sync
  // This uses absolute video currentTime to prevent cumulative drift over long durations
  useEffect(() => {
    if (!songData || !songData.prompts.length || duration <= 0) return;

    // Binary search to find the correct scene index for the current time
    // This is more efficient and accurate than linear search, especially for long videos
    const prompts = songData.prompts;
    let left = 0;
    let right = prompts.length - 1;
    let targetIndex = 0;

    while (left <= right) {
      const mid = Math.floor((left + right) / 2);
      const prompt = prompts[mid];
      if (!prompt) break;

      const startTime = prompt.timestampSeconds || 0;
      const nextPrompt = prompts[mid + 1];
      const endTime = nextPrompt?.timestampSeconds || duration;

      if (currentTime >= startTime && currentTime < endTime) {
        // Found the exact scene
        targetIndex = mid;
        break;
      } else if (currentTime < startTime) {
        // Current time is before this scene, search left
        right = mid - 1;
        targetIndex = Math.max(0, mid - 1);
      } else {
        // Current time is after this scene, search right
        left = mid + 1;
        targetIndex = mid;
      }
    }

    // Clamp to valid range
    targetIndex = Math.max(0, Math.min(targetIndex, prompts.length - 1));

    // Only update if the index actually changed (prevents unnecessary re-renders)
    if (targetIndex !== currentSceneIndex) {
      setCurrentSceneIndex(targetIndex);
    }
  }, [currentTime, songData, duration]); // Removed currentSceneIndex from deps to prevent circular updates

  // ============================================================
  // Handlers
  // ============================================================

  const handleStartProcessing = useCallback(async () => {
    if (!audioFile) return;
    await processFile(audioFile, selectedStyle);
  }, [audioFile, selectedStyle, processFile]);

  const handleGenerateVisuals = useCallback(async () => {
    await handleGenerateAll(selectedStyle, '16:9');
  }, [selectedStyle, handleGenerateAll]);

  const handleAnimateImage = useCallback(async (promptId: string) => {
    if (!songData || animatingPromptId) return;

    const image = songData.generatedImages.find(img => img.promptId === promptId);
    if (!image || !image.imageUrl || image.type === 'video') return;

    const prompt = songData.prompts.find(p => p.id === promptId);
    if (!prompt) return;

    setAnimatingPromptId(promptId);
    setAnimationError(null);

    try {
      const videoBase64 = await animateImageWithDeApi(
        image.imageUrl,
        prompt.text,
        '16:9'
      );

      const updatedImage: GeneratedImage = {
        ...image,
        promptId: image.promptId, // Explicitly pass to avoid optional spread issues
        imageUrl: videoBase64,
        type: 'video',
        baseImageUrl: image.imageUrl,
      };

      setSongData(prev => {
        if (!prev) return null;
        return {
          ...prev,
          generatedImages: prev.generatedImages.map(img =>
            img.promptId === promptId ? updatedImage : img
          ),
        };
      });
    } catch (error: any) {
      console.error('Animation failed:', error);
      setAnimationError(error.message || 'Animation failed');
    } finally {
      setAnimatingPromptId(null);
    }
  }, [songData, animatingPromptId, setSongData]);

  const handleAnimateAll = useCallback(async () => {
    if (!songData || isBatchAnimating) return;

    const imagesToAnimate = songData.generatedImages.filter(img => img.type !== 'video');

    if (imagesToAnimate.length === 0) {
      setAnimationError('All images are already animated');
      return;
    }

    setIsBatchAnimating(true);
    setBatchAnimationProgress({ current: 0, total: imagesToAnimate.length });
    setAnimationError(null);

    // Prepare batch items with prompts
    const batchItems = imagesToAnimate
      .map(image => {
        const prompt = songData.prompts.find(p => p.id === image.promptId);
        if (!prompt) return null;
        return {
          id: image.promptId,
          imageUrl: image.imageUrl,
          prompt: prompt.text,
          aspectRatio: '16:9' as const,
        };
      })
      .filter((item): item is NonNullable<typeof item> => item !== null);

    try {
      // Use parallel batch animation (concurrency: 2 for video generation)
      const results = await animateImageBatch(
        batchItems,
        2, // Lower concurrency for video generation (more resource intensive)
        (progress) => {
          setBatchAnimationProgress({ current: progress.completed, total: progress.total });
          // Show which one is currently being processed
          const currentItem = batchItems[progress.completed - 1];
          if (currentItem) {
            setAnimatingPromptId(currentItem.id);
          }
        }
      );

      // Update all successful results
      setSongData(prev => {
        if (!prev) return null;
        const updatedImages = prev.generatedImages.map(img => {
          const result = results.find(r => r.id === img.promptId);
          if (result?.success && result.imageUrl) {
            return {
              ...img,
              imageUrl: result.imageUrl,
              type: 'video' as const,
              baseImageUrl: img.imageUrl,
            };
          }
          return img;
        });
        return { ...prev, generatedImages: updatedImages };
      });

      // Report any failures
      const failures = results.filter(r => !r.success);
      if (failures.length > 0) {
        console.error(`${failures.length} animations failed:`, failures.map(f => f.error));
      }
    } catch (error: any) {
      console.error('Batch animation failed:', error);
      setAnimationError(error.message || 'Batch animation failed');
    }

    setIsBatchAnimating(false);
    setAnimatingPromptId(null);
    setBatchAnimationProgress({ current: 0, total: 0 });
  }, [songData, isBatchAnimating, setSongData]);

  const handleReset = useCallback(() => {
    resetApp();
    setAudioFile(null);
    setIsPlaying(false);
    setCurrentTime(0);
    setDuration(0);
    setAnimationError(null);
  }, [resetApp]);

  const handlePlayPause = useCallback(() => {
    setIsPlaying(prev => !prev);
  }, []);

  const handleSeek = useCallback((time: number) => {
    setCurrentTime(time);
  }, []);

  const handleSceneSelect = useCallback((index: number, timestampSeconds?: number) => {
    setCurrentSceneIndex(index);
    if (timestampSeconds !== undefined) {
      handleSeek(timestampSeconds);
    }
  }, [handleSeek]);

  const handleExport = useCallback(async (
    config: { presetId: string; width: number; height: number; orientation: 'landscape' | 'portrait'; quality: string },
    onProgress?: (percent: number) => void
  ) => {
    if (!songData || !hasVisuals) {
      throw new Error('Video not ready for export');
    }

    const { exportVideoWithFFmpeg } = await import('@/services/ffmpeg/exporters');

    const result = await exportVideoWithFFmpeg(
      songData,
      (p) => onProgress?.(p.progress),
      {
        orientation: config.orientation,
        useModernEffects: true,
        transitionType: 'dissolve',
        transitionDuration: 1.5,
        contentMode: 'music',
      },
      {
        cloudSessionId: sessionId || undefined,
        userId: user?.uid,
        projectId: projectId,
      }
    );

    const url = URL.createObjectURL(result.blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `${songData.fileName?.replace(/\.[^/.]+$/, '') || 'lyric-video'}-${config.presetId}.mp4`;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
  }, [songData, hasVisuals, sessionId, user?.uid, projectId]);

  // ============================================================
  // Render Helpers
  // ============================================================

  const headerActions = isReady ? (
    <div className={cn('flex items-center gap-2', isRTL && 'flex-row-reverse')}>
      <Button
        variant="ghost"
        size="sm"
        onClick={handleReset}
        className="text-white/60 hover:text-white"
      >
        <RotateCcw className="w-4 h-4 me-2" aria-hidden="true" />
        {t('common.reset')}
      </Button>

      {!isBulkGenerating && songData && songData.generatedImages.length < songData.prompts.length && (
        <Button
          onClick={handleGenerateVisuals}
          size="sm"
          className="bg-cyan-600 hover:bg-cyan-500"
        >
          <Wand2 className="w-4 h-4 me-2" aria-hidden="true" />
          Generate Visuals
        </Button>
      )}

      {isBulkGenerating && songData && (
        <div className={cn('flex items-center gap-2 text-cyan-400 text-sm', isRTL && 'flex-row-reverse')}>
          <Loader2 className="w-4 h-4 animate-spin" aria-hidden="true" />
          Generating ({songData.generatedImages.length}/{songData.prompts.length})
        </div>
      )}

      {hasVisuals && animatableImagesCount > 0 && !isBatchAnimating && (
        <Button
          onClick={handleAnimateAll}
          size="sm"
          className="bg-purple-600 hover:bg-purple-500"
        >
          <Video className="w-4 h-4 me-2" aria-hidden="true" />
          Animate All ({animatableImagesCount})
        </Button>
      )}

      {isBatchAnimating && (
        <div className={cn('flex items-center gap-2 text-purple-400 text-sm', isRTL && 'flex-row-reverse')}>
          <Loader2 className="w-4 h-4 animate-spin" aria-hidden="true" />
          Animating ({batchAnimationProgress.current}/{batchAnimationProgress.total})
        </div>
      )}

      {isReadyForExport && (
        <Button
          onClick={() => setShowExport(true)}
          size="sm"
          className="bg-gradient-to-r from-violet-600 to-purple-600 hover:from-violet-500 hover:to-purple-500"
        >
          <Download className="w-4 h-4 me-2" aria-hidden="true" />
          {t('studio.export')}
        </Button>
      )}
    </div>
  ) : undefined;

  // ============================================================
  // Render
  // ============================================================

  return (
    <ScreenLayout
      title={t('visualizer.title')}
      showBackButton
      onBack={() => navigate('/')}
      headerActions={headerActions}
      maxWidth="full"
      centerContent={!isReady}
      footer={
        <footer className="p-4 md:p-6 text-center text-sm text-white/40">
          Powered by Gemini AI
        </footer>
      }
    >
      <AnimatePresence mode="wait">
        {!isReady ? (
          /* Upload State */
          <motion.div
            key="upload"
            initial={{ opacity: 0, y: 20 }}
            animate={{ opacity: 1, y: 0 }}
            exit={{ opacity: 0, y: -20 }}
            className="flex justify-center"
          >
            <AudioUploadForm
              audioFile={audioFile}
              onAudioFileChange={setAudioFile}
              selectedStyle={selectedStyle}
              onStyleChange={setSelectedStyle}
              imageProvider={imageProvider}
              onProviderChange={setImageProvider}
              directorMode={directorMode}
              onDirectorModeChange={setDirectorMode}
              globalSubject={globalSubject}
              onGlobalSubjectChange={setGlobalSubject}
              appState={appState}
              errorMsg={errorMsg || undefined}
              onStartProcessing={handleStartProcessing}
            />
          </motion.div>
        ) : (
          /* Ready State - Player with Visual Preview */
          <motion.div
            key="player"
            initial={{ opacity: 0, y: 20 }}
            animate={{ opacity: 1, y: 0 }}
            exit={{ opacity: 0, y: -20 }}
            className="max-w-5xl mx-auto w-full px-4"
          >
            {/* Header Info */}
            <div className={cn('mb-6', isRTL && 'text-right')}>
              <h2 className="text-xl font-semibold">{songData?.fileName}</h2>
              <p className="text-sm text-white/60">
                {songData?.parsedSubtitles.length} subtitles • {songData?.prompts.length} scenes
                {hasVisuals && ` • ${songData?.generatedImages.length} visuals`}
              </p>
            </div>

            {/* Visual Preview Section */}
            {hasVisuals && songData && (
              <div className="mb-6">
                <VisualPreview
                  currentVisual={currentVisual}
                  currentSceneIndex={currentSceneIndex}
                  totalScenes={songData.prompts.length}
                  isPlaying={isPlaying}
                  onPlayPause={handlePlayPause}
                  currentTime={currentTime}
                  subtitles={songData.parsedSubtitles}
                  animatingPromptId={animatingPromptId}
                  onAnimateImage={handleAnimateImage}
                  isRTL={isRTL}
                  className="mb-4"
                />

                <SceneThumbnails
                  prompts={songData.prompts}
                  generatedImages={songData.generatedImages}
                  currentSceneIndex={currentSceneIndex}
                  onSceneSelect={handleSceneSelect}
                  animatingPromptId={animatingPromptId}
                />

                {/* Animation error message */}
                {animationError && (
                  <div className="mt-2 p-3 rounded-lg bg-red-500/10 border border-red-500/20 text-red-200 text-sm flex items-center justify-between">
                    <span>{animationError}</span>
                    <button
                      onClick={() => setAnimationError(null)}
                      className="p-1 hover:bg-white/10 rounded"
                      aria-label="Dismiss error"
                    >
                      <X className="w-4 h-4" aria-hidden="true" />
                    </button>
                  </div>
                )}
              </div>
            )}

            {/* Timeline Player */}
            {songData && (
              <TimelinePlayer
                audioUrl={songData.audioUrl}
                subtitles={songData.parsedSubtitles}
                currentTime={currentTime}
                duration={duration}
                isPlaying={isPlaying}
                onPlayPause={handlePlayPause}
                onSeek={handleSeek}
                onTimeUpdate={setCurrentTime}
                onDurationChange={setDuration}
                onEnded={() => setIsPlaying(false)}
                contentMode="music"
              />
            )}
          </motion.div>
        )}
      </AnimatePresence>

      {/* Export Modal */}
      <QuickExport
        isOpen={showExport}
        onClose={() => setShowExport(false)}
        onExport={handleExport}
        videoTitle={songData?.fileName?.replace(/\.[^/.]+$/, '') || 'lyric-video'}
        duration={duration}
      />
    </ScreenLayout>
  );
}
````

## File: packages/server/env.ts
````typescript
/**
 * Environment loader - must be imported FIRST before any modules that use env vars
 */
import { config } from 'dotenv';
import { fileURLToPath } from 'url';
import path from 'path';

// Resolve paths relative to this file so they work regardless of process.cwd()
// packages/server/env.ts → ../../ → workspace root
const __dirname = path.dirname(fileURLToPath(import.meta.url));
const root = path.resolve(__dirname, '../../');

// Load environment variables from .env files
// .env.local takes precedence over .env
config({ path: path.join(root, '.env.local'), debug: process.env.DEBUG === 'true' });
config({ path: path.join(root, '.env'), debug: process.env.DEBUG === 'true' });

// Re-export for convenience
export const env = process.env;
````

## File: packages/server/index.ts
````typescript
// MUST be first import to load environment variables before other modules
import './env.js';

import express from 'express';
import cors from 'cors';
import os from 'os';
import { createLogger } from '@studio/shared/src/services/logger.js';
import { ensureTempDir, ensureJobsDir, TEMP_DIR } from './utils/index.js';

// Import modular routes
import exportRoutes from './routes/export.js';
import importRoutes from './routes/import.js';
import healthRoutes from './routes/health.js';
import geminiRoutes from './routes/gemini.js';
import deapiRoutes from './routes/deapi.js';
import sunoRoutes from './routes/suno.js';
import cloudRoutes from './routes/cloud.js';
import videoRoutes from './routes/video.js';
import directorRoutes from './routes/director.js';

// Import job queue and worker pool
import { jobQueue } from './services/jobQueue/index.js';
import { workerPool } from './workers/workerPool.js';
import { detectEncoders } from './services/encoding/encoderStrategy.js';

const serverLog = createLogger('Server');
const PORT = process.env.PORT || 3001;

// --- App Initialization ---
const app = express();

// Ensure directories exist
ensureTempDir();
ensureJobsDir();

// --- Middleware ---
app.use(cors());
app.use(express.json({ limit: '50mb' }));

// --- Modular Routes ---
app.use('/api/export', exportRoutes);
app.use('/api/import', importRoutes);
app.use('/api/health', healthRoutes);
app.use('/api/gemini', geminiRoutes);
app.use('/api/deapi', deapiRoutes);
app.use('/api/suno', sunoRoutes);
app.use('/api/cloud', cloudRoutes);
app.use('/api/video', videoRoutes);
app.use('/api/director', directorRoutes);

// Get network IP for display
function getNetworkIP(): string {
  const nets = os.networkInterfaces();
  for (const name of Object.keys(nets)) {
    const interfaces = nets[name];
    if (!interfaces) continue;
    for (const net of interfaces) {
      // Skip internal and non-IPv4 addresses
      if (net.family === 'IPv4' && !net.internal) {
        return net.address;
      }
    }
  }
  return 'localhost';
}

// Initialize rendering infrastructure
async function initializeRenderingInfrastructure(): Promise<void> {
  try {
    // Detect available encoders
    await detectEncoders();

    // Initialize worker pool
    workerPool.setMessageHandler((msg) => {
      // Forward worker messages to job queue
      switch (msg.type) {
        case 'PROGRESS':
          if (msg.data) {
            jobQueue.updateJobProgress(
              msg.jobId,
              msg.data.progress || 0,
              msg.data.currentFrame
            );
          }
          break;

        case 'COMPLETE':
          jobQueue.updateJobStatus(msg.jobId, 'complete', {
            outputPath: msg.data?.outputPath,
            outputSize: msg.data?.outputSize,
            progress: 100,
          });
          break;

        case 'HEARTBEAT':
          jobQueue.recordHeartbeat(msg.jobId);
          break;

        case 'ERROR':
          jobQueue.handleJobError(msg.jobId, new Error(msg.data?.error || 'Unknown error'));
          break;
      }
    });
    await workerPool.initialize();

    // Initialize job queue with worker pool as processor
    jobQueue.setJobProcessor(async (job) => {
      await jobQueue.updateJobStatus(job.jobId, 'encoding');
      await workerPool.submitJob(job);
    });
    await jobQueue.initialize();

    serverLog.info('Rendering infrastructure initialized');
  } catch (error) {
    serverLog.error('Failed to initialize rendering infrastructure:', error);
    // Continue without rendering - server can still handle other routes
  }
}

// Listen on all interfaces (0.0.0.0) for network access
app.listen(Number(PORT), '0.0.0.0', async () => {
  const networkIP = getNetworkIP();
  serverLog.info(`API server running on:`);
  serverLog.info(`  ➜  Local:   http://localhost:${PORT}`);
  serverLog.info(`  ➜  Network: http://${networkIP}:${PORT}`);
  serverLog.info(`Temp directory: ${TEMP_DIR}`);

  // Initialize rendering infrastructure after server starts
  await initializeRenderingInfrastructure();
});

// Graceful shutdown
process.on('SIGTERM', async () => {
  serverLog.info('SIGTERM received, shutting down gracefully...');
  jobQueue.shutdown();
  await workerPool.shutdown();
  process.exit(0);
});

process.on('SIGINT', async () => {
  serverLog.info('SIGINT received, shutting down gracefully...');
  jobQueue.shutdown();
  await workerPool.shutdown();
  process.exit(0);
});
````

## File: packages/server/routes/cloud.ts
````typescript
import { Router, Response, Request } from 'express';
import { createLogger } from '@studio/shared/src/services/logger.js';
import multer from 'multer';
import path from 'path';
import type { Storage } from '@google-cloud/storage';
import { MAX_FILE_SIZE } from '../utils/index.js';

const cloudLog = createLogger('Cloud');
const router = Router();
const GCS_BUCKET_NAME = process.env.GOOGLE_CLOUD_STORAGE_BUCKET || 'aisoul-studio-storage';

let gcsStorage: Storage | null = null;
let GcsStorageClass: any = null;

async function getGcsStorageClient(): Promise<Storage> {
    if (gcsStorage) return gcsStorage;
    try {
        if (!GcsStorageClass) {
            const gcs = await import('@google-cloud/storage');
            GcsStorageClass = gcs.Storage;
        }
        const projectId = process.env.GOOGLE_CLOUD_PROJECT || process.env.VITE_GOOGLE_CLOUD_PROJECT;
        gcsStorage = projectId ? new GcsStorageClass({ projectId }) : new GcsStorageClass();
        return gcsStorage!;
    } catch (error) {
        cloudLog.error('Failed to initialize GCS:', error);
        throw error;
    }
}

const memoryUpload = multer({
    storage: multer.memoryStorage(),
    limits: { fileSize: MAX_FILE_SIZE }
});

/**
 * Build storage path based on whether userId is provided.
 * User-aware path: users/{userId}/projects/{sessionId}/
 * Legacy path: production_{sessionId}/
 */
function buildStoragePath(sessionId: string, userId?: string, assetType?: string, filename?: string): string {
    const basePath = userId
        ? `users/${userId}/projects/${sessionId}`
        : `production_${sessionId}`;

    if (assetType && filename) {
        return `${basePath}/${assetType}/${filename}`;
    }
    if (assetType) {
        return `${basePath}/${assetType}`;
    }
    return basePath;
}

router.post('/init', async (req: Request, res: Response): Promise<void> => {
    const { sessionId, userId } = req.body;
    if (!sessionId) {
        res.status(400).json({ error: 'sessionId is required' });
        return;
    }

    try {
        const storage = await getGcsStorageClient();
        const bucket = storage.bucket(GCS_BUCKET_NAME);
        const [exists] = await bucket.exists();
        if (!exists) {
            res.status(404).json({ error: 'Bucket not found' });
            return;
        }

        const folderPath = buildStoragePath(sessionId, userId);
        const markerContent = userId
            ? `Session Started: ${new Date().toISOString()}\nSessionId: ${sessionId}\nUserId: ${userId}`
            : `Session Started: ${new Date().toISOString()}\nSessionId: ${sessionId}`;

        await bucket.file(`${folderPath}/_session_started.txt`).save(markerContent);

        res.json({
            success: true,
            message: `Session ${sessionId} initialized`,
            folderPath,
            userAware: !!userId
        });
    } catch (error: any) {
        cloudLog.error('Cloud init error:', error);
        res.status(500).json({ error: error.message || 'Cloud init failed' });
    }
});

router.post('/upload-asset', memoryUpload.single('file'), async (req: Request, res: Response): Promise<void> => {
    const { sessionId, assetType, filename, makePublic, userId } = req.body;
    if (!req.file || !sessionId) {
        res.status(400).json({ error: 'Missing file or sessionId' });
        return;
    }

    const validAssetTypes = ['visuals', 'audio', 'music', 'video_clips', 'sfx', 'subtitles', 'exports', 'ai_logs'];
    const safeAssetType = validAssetTypes.includes(assetType) ? assetType : 'misc';
    const safeFilename = path.basename(filename || `asset_${Date.now()}`);
    const destination = buildStoragePath(sessionId, userId, safeAssetType, safeFilename);
    const shouldMakePublic = makePublic === 'true' || makePublic === true;

    try {
        const storage = await getGcsStorageClient();
        const bucket = storage.bucket(GCS_BUCKET_NAME);
        const blob = bucket.file(destination);

        const blobStream = blob.createWriteStream({
            resumable: false,
            metadata: { contentType: req.file.mimetype || 'application/octet-stream' }
        });

        blobStream.on('error', (err: any) => res.status(500).json({ error: err.message }));
        blobStream.on('finish', async () => {
            let publicUrl: string | undefined;
            if (shouldMakePublic) {
                // Use proxy URL to avoid CORS issues
                // The proxy endpoint serves files through our server
                publicUrl = `/api/cloud/file?path=${encodeURIComponent(destination)}`;
            }
            res.json({ success: true, publicUrl, gsUri: `gs://${GCS_BUCKET_NAME}/${destination}` });
        });
        blobStream.end(req.file.buffer);
    } catch (error: any) {
        res.status(500).json({ error: error.message });
    }
});

router.get('/status', async (req: Request, res: Response): Promise<void> => {
    try {
        const storage = await getGcsStorageClient();
        const bucket = storage.bucket(GCS_BUCKET_NAME);
        const [exists] = await bucket.exists();
        res.json({ available: exists, bucketName: GCS_BUCKET_NAME });
    } catch (error: any) {
        res.json({ available: false, message: error.message });
    }
});

/**
 * Proxy endpoint to serve GCS files - avoids CORS issues
 * Query params:
 *   - path: The GCS file path (e.g., production_story_1770746549079/audio/narration_scene_0.wav)
 * 
 * Supports both user-aware and legacy paths:
 * - User-aware: users/{userId}/projects/{sessionId}/{assetType}/{filename}
 * - Legacy: production_{sessionId}/{assetType}/{filename}
 */
router.get('/file', async (req: Request, res: Response): Promise<void> => {
    const filePath = req.query.path as string;
    
    if (!filePath) {
        res.status(400).json({ error: 'File path is required (use ?path=...)' });
        return;
    }

    try {
        const storage = await getGcsStorageClient();
        const bucket = storage.bucket(GCS_BUCKET_NAME);
        const file = bucket.file(filePath);

        // Check if file exists
        const [exists] = await file.exists();
        if (!exists) {
            res.status(404).json({ error: 'File not found' });
            return;
        }

        // Get file metadata for content-type
        const [metadata] = await file.getMetadata();
        const contentType = metadata.contentType || 'application/octet-stream';

        // Set response headers
        res.setHeader('Content-Type', contentType);
        res.setHeader('Cache-Control', 'public, max-age=31536000'); // Cache for 1 year
        
        // Stream the file to response
        file.createReadStream().pipe(res);
    } catch (error: any) {
        cloudLog.error('File proxy error:', error);
        res.status(500).json({ error: error.message || 'Failed to retrieve file' });
    }
});

export default router;
````

## File: packages/server/routes/deapi.ts
````typescript
import { Router, Response, Request } from 'express';
import { ApiProxyRequest } from '../types.js';
import { DEAPI_API_KEY, MAX_SINGLE_FILE } from '../utils/index.js';
import { createLogger } from '@studio/shared/src/services/logger.js';
import fs from 'fs';
import multer from 'multer';
import type { Txt2ImgParams, DeApiImageModel } from '@studio/shared/src/services/deapiService.js';

const deapiLog = createLogger('DeAPI');
const router = Router();
const upload = multer({
    dest: 'temp/',
    limits: { fileSize: MAX_SINGLE_FILE }
});

router.post('/image', async (req: ApiProxyRequest, res: Response): Promise<void> => {
    if (!DEAPI_API_KEY) {
        res.status(500).json({ success: false, error: 'DEAPI key not configured' });
        return;
    }

    try {
        // Safely destructure with fallback to prevent TypeError when req.body is undefined
        const { prompt, options = {} } = req.body ?? {};

        if (!prompt) {
            res.status(400).json({ success: false, error: 'Prompt is required' });
            return;
        }

        const { generateImageWithDeApi } = await import('@studio/shared/src/services/deapiService.js');

        const params: Txt2ImgParams = {
            prompt: prompt,
            model: (options.model as DeApiImageModel) || "Flux1schnell",
            ...options
        };

        const result = await generateImageWithDeApi(params);
        res.json({ success: true, data: result });
    } catch (error) {
        deapiLog.error('Image proxy error:', error);
        res.status(500).json({
            success: false,
            error: error instanceof Error ? error.message : 'Unknown error'
        });
    }
});

router.post('/animate', async (req: ApiProxyRequest, res: Response) => {
    if (!DEAPI_API_KEY) {
        return res.status(500).json({ success: false, error: 'DEAPI key not configured' });
    }

    try {
        // Safely destructure with fallback to prevent TypeError when req.body is undefined
        const { imageUrl, options = {} } = req.body ?? {};

        if (!imageUrl) {
            return res.status(400).json({ success: false, error: 'Image URL is required' });
        }

        if (!options.prompt) {
            return res.status(400).json({ success: false, error: 'Animation prompt is required' });
        }

        const { animateImageWithDeApi } = await import('@studio/shared/src/services/deapiService.js');

        const base64Image = imageUrl;
        const aspectRatio = (options.aspectRatio as "16:9" | "9:16" | "1:1") || "16:9";

        const result = await animateImageWithDeApi(base64Image, options.prompt as string, aspectRatio);
        return res.json({ success: true, data: result });
    } catch (error) {
        deapiLog.error('Animate proxy error:', error);
        return res.status(500).json({
            success: false,
            error: error instanceof Error ? error.message : 'Unknown error'
        });
    }
});

// Multipart proxy for img2video
router.post('/img2video', upload.single('first_frame_image'), async (req: Request, res: Response): Promise<void> => {
    if (!DEAPI_API_KEY) {
        res.status(500).json({ error: 'DeAPI API key not configured on server' });
        return;
    }

    if (!req.file) {
        res.status(400).json({ error: 'No image file provided' });
        return;
    }

    try {
        const fileBuffer = fs.readFileSync(req.file.path);
        const blob = new Blob([fileBuffer], { type: req.file.mimetype || 'image/png' });

        const formData = new FormData();
        formData.append('first_frame_image', blob, req.file.originalname || 'frame.png');

        const fields = ['prompt', 'frames', 'width', 'height', 'fps', 'model', 'guidance', 'steps', 'seed', 'negative_prompt'];
        fields.forEach(field => {
            if (req.body[field] !== undefined) {
                formData.append(field, req.body[field]);
            }
        });

        if (!req.body.guidance) {
            formData.append('guidance', '0');
        }

        const response = await fetch('https://api.deapi.ai/api/v1/client/img2video', {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${DEAPI_API_KEY}`,
                'Accept': 'application/json',
            },
            body: formData,
        });

        fs.unlinkSync(req.file.path);

        if (response.status === 429) {
            const retryAfter = response.headers.get('retry-after') || '30';
            res.status(429).json({ error: 'Rate limit exceeded', retryAfter: parseInt(retryAfter, 10) });
            return;
        }

        const data = await response.json();
        res.status(response.status).json(data);
    } catch (error: unknown) {
        if (req.file && fs.existsSync(req.file.path)) fs.unlinkSync(req.file.path);
        const err = error as Error;
        res.status(500).json({ error: err.message || 'DeAPI img2video failed' });
    }
});

// ============================================================
// txt2video - Direct text-to-video generation
// ============================================================
router.post('/txt2video', async (req: Request, res: Response): Promise<void> => {
    if (!DEAPI_API_KEY) {
        res.status(500).json({ error: 'DeAPI API key not configured on server' });
        return;
    }

    try {
        // Safely destructure with fallback to empty object to prevent TypeError
        // when req.body is undefined (missing Content-Type: application/json header)
        const { prompt, width, height, frames, model, guidance, steps, seed } = req.body ?? {};

        if (!prompt) {
            res.status(400).json({ error: 'Prompt is required' });
            return;
        }

        const response = await fetch('https://api.deapi.ai/api/v1/client/txt2video', {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${DEAPI_API_KEY}`,
                'Accept': 'application/json',
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                prompt,
                model: model || 'Ltxv_13B_0_9_8_Distilled_FP8',
                width: width || 768,
                height: height || 432,
                guidance: guidance ?? 0,
                steps: steps || 1,
                frames: frames || 120,
                fps: 30,
                seed: seed || -1,
            }),
        });

        const data = await response.json();
        res.status(response.status).json(data);
    } catch (error: unknown) {
        const err = error as Error;
        deapiLog.error('txt2video error:', err);
        res.status(500).json({ error: err.message || 'DeAPI txt2video failed' });
    }
});

// ============================================================
// Webhook handler for async job completion
// DeAPI sends: { event: 'job.completed'|'job.failed', request_id, result_url?, error? }
// ============================================================
const pendingJobs = new Map<string, {
    resolve: (url: string) => void;
    reject: (error: Error) => void;
    timeout: NodeJS.Timeout;
}>();

router.post('/webhook', async (req: Request, res: Response): Promise<void> => {
    try {
        // Safely destructure with fallback to prevent TypeError when req.body is undefined
        const { event, request_id, result_url, error } = req.body ?? {};

        // Validate webhook signature (recommended for production)
        // const signature = req.headers['x-deapi-signature'];
        // TODO: Implement HMAC-SHA256 validation

        deapiLog.info(`Webhook received: ${event} for ${request_id}`);

        const pending = pendingJobs.get(request_id);
        if (pending) {
            clearTimeout(pending.timeout);
            pendingJobs.delete(request_id);

            if (event === 'job.completed' && result_url) {
                pending.resolve(result_url);
            } else if (event === 'job.failed') {
                pending.reject(new Error(error || 'Job failed'));
            }
        }

        res.status(200).json({ received: true });
    } catch (error: unknown) {
        const err = error as Error;
        deapiLog.error('Webhook error:', err);
        res.status(500).json({ error: err.message });
    }
});

/**
 * Wait for a job to complete via webhook (with polling fallback)
 */
export const waitForJob = (requestId: string, timeoutMs: number = 300000): Promise<string> => {
    return new Promise((resolve, reject) => {
        const timeout = setTimeout(() => {
            pendingJobs.delete(requestId);
            reject(new Error(`Job ${requestId} timed out after ${timeoutMs}ms`));
        }, timeoutMs);

        pendingJobs.set(requestId, { resolve, reject, timeout });
    });
};

// ============================================================
// Batch generation endpoint with progress streaming
// ============================================================
router.post('/batch', async (req: Request, res: Response): Promise<void> => {
    if (!DEAPI_API_KEY) {
        res.status(500).json({ error: 'DeAPI API key not configured on server' });
        return;
    }

    try {
        const { items, concurrency = 5 } = req.body;

        if (!Array.isArray(items) || items.length === 0) {
            res.status(400).json({ error: 'Items array is required' });
            return;
        }

        const { generateImageBatch } = await import('@studio/shared/src/services/deapiService.js');

        const results = await generateImageBatch(
            items,
            Math.min(concurrency, 10),
            (progress) => {
                deapiLog.info(`Batch progress: ${progress.completed}/${progress.total}`);
            }
        );

        res.json({ success: true, results });
    } catch (error: unknown) {
        const err = error as Error;
        deapiLog.error('Batch generation error:', err);
        res.status(500).json({ error: err.message || 'Batch generation failed' });
    }
});

// General proxy
router.use('/proxy', async (req: Request, res: Response): Promise<void> => {
    if (!DEAPI_API_KEY) {
        res.status(500).json({ error: 'DeAPI API key not configured on server' });
        return;
    }

    const endpoint = req.path.startsWith('/') ? req.path.slice(1) : req.path;
    const deapiUrl = `https://api.deapi.ai/api/v1/client/${endpoint}`;

    try {
        const fetchOptions: RequestInit = {
            method: req.method,
            headers: {
                'Authorization': `Bearer ${DEAPI_API_KEY}`,
                'Accept': 'application/json',
            },
        };

        const contentType = req.headers['content-type'] || '';

        if (req.method !== 'GET' && req.method !== 'HEAD') {
            if (contentType.includes('application/json')) {
                (fetchOptions.headers as Record<string, string>)['Content-Type'] = 'application/json';
                fetchOptions.body = JSON.stringify(req.body);
            }
        }

        const response = await fetch(deapiUrl, fetchOptions);
        const data = await response.json();
        res.status(response.status).json(data);
    } catch (error: unknown) {
        const err = error as Error;
        res.status(500).json({ error: err.message || 'DeAPI proxy failed' });
    }
});

export default router;
````

## File: packages/server/routes/director.ts
````typescript
import { Router, Response, Request } from 'express';
import { createLogger } from '@studio/shared/src/services/logger.js';

const directorLog = createLogger('Director');
const router = Router();

router.post('/generate', async (req: Request, res: Response): Promise<void> => {
    try {
        const { srtContent, style, contentType, videoPurpose, globalSubject, config } = req.body;
        directorLog.info(`Generating prompts for ${contentType} (${style})`);

        const { generatePromptsWithLangChain } = await import('@studio/shared/src/services/directorService.js');
        const prompts = await generatePromptsWithLangChain(srtContent, style, contentType, videoPurpose, globalSubject, config);

        res.json({ success: true, prompts });
    } catch (error: unknown) {
        const err = error as Error;
        directorLog.error('Director Error:', err);
        res.status(500).json({ success: false, error: err.message });
    }
});

export default router;
````

## File: packages/server/routes/export.ts
````typescript
import { Router, Request, Response } from 'express';
import { spawn } from 'child_process';
import fs from 'fs';
import path from 'path';
import multer from 'multer';
import { sanitizeId, getSessionDir, cleanupSession, MAX_FILE_SIZE, MAX_SINGLE_FILE, MAX_FILES, generateJobId } from '../utils/index.js';
import { createLogger } from '@studio/shared/src/services/logger.js';
import { jobQueue } from '../services/jobQueue/index.js';
import { getSelectedEncoder, getEncoderInfo, getEncoderArgs, ENCODING_SPEC } from '../services/encoding/encoderStrategy.js';
import { validateSessionFrames, validateFrameSizes } from '../services/validation/frameValidator.js';
import { verifyOutputQuality, quickValidate } from '../services/validation/qualityVerifier.js';
import { RenderJob, JobProgress, FrameChecksum } from '../types/renderJob.js';

const exportLog = createLogger('Export');
const ffmpegLog = createLogger('FFmpeg');

const router = Router();

// Encoder selection is handled by encoderStrategy.ts (initialized in server/index.ts)

// Types
interface ExportRequest extends Request {
  sessionId?: string;
  jobId?: string;
  files?: Express.Multer.File[];
}

// Track active jobs by session
const sessionJobs = new Map<string, string>();

// Multer Configuration
const storage = multer.diskStorage({
  destination: (req, file, cb) => {
    const request = req as ExportRequest;
    let sessionId = (req.query.sessionId as string) || (req.headers['x-session-id'] as string);

    if (!sessionId && request.sessionId) {
      sessionId = request.sessionId;
    }

    if (!sessionId) {
      sessionId = Date.now().toString();
      request.sessionId = sessionId;
    }

    sessionId = sanitizeId(sessionId);
    request.sessionId = sessionId;

    const sessionDir = getSessionDir(sessionId);
    if (!fs.existsSync(sessionDir)) {
      fs.mkdirSync(sessionDir, { recursive: true });
    }

    cb(null, sessionDir);
  },
  filename: (_req, file, cb) => {
    if (file.fieldname === 'audio') {
      cb(null, 'audio.mp3');
    } else {
      cb(null, path.basename(file.originalname));
    }
  }
});

// Separate upload configurations for different endpoints
const uploadAudio = multer({
  storage,
  limits: { fileSize: MAX_SINGLE_FILE }
});

const uploadFrames = multer({
  storage,
  limits: { fileSize: MAX_FILE_SIZE, files: MAX_FILES }
});

/**
 * Initialize Export Session
 * Receives the audio file and creates the session directory.
 * Now also creates a job in the queue.
 */
router.post('/init', uploadAudio.single('audio'), async (req: Request, res: Response) => {
  try {
    const request = req as ExportRequest;
    if (!request.sessionId) {
      throw new Error('Failed to generate session ID');
    }

    const { totalFrames, fps = 24 } = req.body;

    // Create a job for this session
    const job = await jobQueue.createJob(request.sessionId, {
      fps: parseInt(fps, 10),
      encoder: getSelectedEncoder(),
    });

    // Track session → job mapping
    sessionJobs.set(request.sessionId, job.jobId);

    // Set total frames if provided
    if (totalFrames) {
      jobQueue.setTotalFrames(job.jobId, parseInt(totalFrames, 10));
    }

    exportLog.info(`Session initialized: ${request.sessionId}, job: ${job.jobId}`);
    res.json({
      success: true,
      sessionId: request.sessionId,
      jobId: job.jobId,
    });
  } catch (error: unknown) {
    const message = error instanceof Error ? error.message : 'Unknown error';
    exportLog.error('Session init error:', error);
    res.status(500).json({ success: false, error: message });
  }
});

/**
 * Upload Chunk of Frames
 * Receives a batch of images. Multer handles saving them to the session directory.
 * Now also tracks frame counts and optional checksums.
 */
router.post('/chunk', uploadFrames.array('frames'), (req: Request, res: Response) => {
  const request = req as ExportRequest;
  if (!request.sessionId) {
    res.status(400).json({ success: false, error: 'Session ID required' });
    return;
  }

  const count = request.files?.length || 0;
  const { checksums } = req.body;

  // Get job for this session
  const jobId = sessionJobs.get(request.sessionId);
  if (jobId) {
    // Parse checksums if provided
    let parsedChecksums: FrameChecksum[] | undefined;
    if (checksums) {
      try {
        parsedChecksums = JSON.parse(checksums);
      } catch {
        // Ignore checksum parsing errors
      }
    }

    jobQueue.registerFrames(jobId, count, parsedChecksums);
  }

  res.json({ success: true, count });
});

/**
 * Finalize Export (Async Version)
 * Queues the job for encoding and returns immediately.
 * Client should subscribe to SSE for progress.
 */
router.post('/finalize', async (req: Request, res: Response) => {
  const { sessionId: rawSessionId, fps = 24, totalFrames, sync = false } = req.body;

  if (!rawSessionId) {
    res.status(400).json({ error: 'Missing sessionId' });
    return;
  }

  const sessionId = sanitizeId(rawSessionId);
  const sessionDir = getSessionDir(sessionId);

  if (!fs.existsSync(sessionDir)) {
    res.status(404).json({ error: 'Session not found' });
    return;
  }

  // Get or create job
  let jobId = sessionJobs.get(sessionId);
  let job: RenderJob | undefined;

  if (jobId) {
    job = jobQueue.getJob(jobId);
  }

  if (!job) {
    // Create job if it doesn't exist (legacy client support)
    job = await jobQueue.createJob(sessionId, {
      fps: parseInt(fps, 10),
      encoder: getSelectedEncoder(),
    });
    jobId = job.jobId;
    sessionJobs.set(sessionId, jobId);
  }

  // Set total frames
  if (totalFrames) {
    jobQueue.setTotalFrames(jobId!, parseInt(totalFrames, 10));
  }

  // Validate frames before encoding
  const frameCount = job.frameManifest.totalFrames || parseInt(totalFrames, 10) || 0;
  if (frameCount > 0) {
    const sequenceResult = await validateSessionFrames(sessionDir, frameCount);
    if (!sequenceResult.valid && sequenceResult.missingFrames.length > 0) {
      res.status(400).json({
        error: 'Frame validation failed',
        missingFrames: sequenceResult.missingFrames.slice(0, 10),
        totalMissing: sequenceResult.missingFrames.length,
      });
      return;
    }

    const sizeResult = await validateFrameSizes(sessionDir);
    if (!sizeResult.valid) {
      res.status(400).json({
        error: 'Some frames appear corrupted',
        undersizedFrames: sizeResult.undersizedFrames.slice(0, 10),
      });
      return;
    }
  }

  exportLog.info(`Queuing job ${jobId} for encoding (${frameCount} frames, ${fps} FPS)`);

  // For backward compatibility: if sync=true, use the old synchronous approach
  if (sync === true || sync === 'true') {
    // Legacy synchronous encoding
    return handleSyncFinalize(req, res, sessionId, sessionDir, fps);
  }

  // Queue job for async processing
  try {
    await jobQueue.queueJob(jobId!);

    res.json({
      success: true,
      jobId,
      status: 'queued',
      message: 'Job queued for encoding. Subscribe to /api/export/events/:jobId for progress.',
    });
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error';
    exportLog.error('Failed to queue job:', error);
    res.status(500).json({ success: false, error: message });
  }
});

/**
 * Legacy synchronous finalize (for backward compatibility)
 */
async function handleSyncFinalize(
  req: Request,
  res: Response,
  sessionId: string,
  sessionDir: string,
  fps: number
): Promise<void> {
  const audioPath = path.join(sessionDir, 'audio.mp3');
  const outputPath = path.join(sessionDir, 'output.mp4');

  exportLog.info(`Finalizing session ${sessionId} at ${fps} FPS (sync mode)`);
  const startTime = Date.now();

  try {
    if (!fs.existsSync(audioPath)) {
      throw new Error('Audio file missing in session');
    }

    const encoder = getSelectedEncoder();
    const encoderArgs = getEncoderArgs(encoder);

    const ffmpegArgs = [
      '-framerate', String(fps),
      '-i', path.join(sessionDir, 'frame%06d.jpg'),
      '-i', audioPath,
      ...encoderArgs,
      '-c:a', 'aac',
      '-b:a', '256k',
      '-shortest',
      '-movflags', '+faststart',
      '-y',
      outputPath
    ];

    exportLog.info(`Using encoder: ${encoder} (${encoder !== 'libx264' ? 'GPU' : 'CPU'})`);

    await new Promise<void>((resolve, reject) => {
      const ffmpeg = spawn('ffmpeg', ffmpegArgs);
      let stderrOutput = '';

      ffmpeg.stderr.on('data', (data) => {
        const msg = data.toString();
        stderrOutput += msg;
        if (msg.includes('frame=') || msg.includes('error') || msg.includes('Error')) {
          ffmpegLog.debug(msg.trim());
        }
      });

      ffmpeg.on('close', (code) => {
        if (code === 0) resolve();
        else reject(new Error(`FFmpeg exited with code ${code}`));
      });

      ffmpeg.on('error', (err) => reject(err));
    });

    exportLog.info(`FFmpeg completed in ${((Date.now() - startTime) / 1000).toFixed(1)}s`);

    const stat = fs.statSync(outputPath);
    res.setHeader('Content-Type', 'video/mp4');
    res.setHeader('Content-Length', stat.size);

    const readStream = fs.createReadStream(outputPath);
    readStream.pipe(res);

    readStream.on('close', () => {
      cleanupSession(sessionId);
      sessionJobs.delete(sessionId);
    });

    readStream.on('error', (err) => {
      exportLog.error('Stream error:', err);
      cleanupSession(sessionId);
      sessionJobs.delete(sessionId);
      if (!res.headersSent) res.status(500).end();
    });

  } catch (error: unknown) {
    const message = error instanceof Error ? error.message : 'Unknown error';
    exportLog.error('Export error:', error);
    cleanupSession(sessionId);
    sessionJobs.delete(sessionId);
    if (!res.headersSent) {
      res.status(500).json({ success: false, error: message });
    }
  }
}

/**
 * Get Job Status
 * Returns current status of a job.
 */
router.get('/status/:jobId', (req: Request, res: Response) => {
  const jobId = req.params.jobId as string;
  const job = jobQueue.getJob(jobId);

  if (!job) {
    res.status(404).json({ error: 'Job not found' });
    return;
  }

  const progress: JobProgress = {
    jobId: job.jobId,
    status: job.status,
    progress: job.progress,
    message: getStatusMessage(job),
    currentFrame: job.currentFrame,
    totalFrames: job.frameManifest.totalFrames,
    error: job.lastError,
  };

  res.json(progress);
});

/**
 * SSE Events Endpoint
 * Streams progress updates for a job.
 */
router.get('/events/:jobId', (req: Request, res: Response) => {
  const jobId = req.params.jobId as string;
  const job = jobQueue.getJob(jobId);

  if (!job) {
    res.status(404).json({ error: 'Job not found' });
    return;
  }

  // Set up SSE
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');
  res.setHeader('Access-Control-Allow-Origin', '*');

  // Send initial state
  const sendProgress = (progress: JobProgress) => {
    res.write(`data: ${JSON.stringify(progress)}\n\n`);
  };

  // Subscribe to job updates
  const unsubscribe = jobQueue.subscribe(jobId, (progress) => {
    sendProgress(progress);

    // Close connection on terminal states
    if (progress.status === 'complete' || progress.status === 'failed') {
      setTimeout(() => {
        res.end();
      }, 100);
    }
  });

  // Handle client disconnect
  req.on('close', () => {
    unsubscribe();
    exportLog.debug(`SSE client disconnected for job ${jobId}`);
  });

  // Keep-alive ping every 30 seconds
  const keepAlive = setInterval(() => {
    res.write(': ping\n\n');
  }, 30000);

  req.on('close', () => {
    clearInterval(keepAlive);
  });
});

/**
 * Download completed video
 */
router.get('/download/:jobId', (req: Request, res: Response) => {
  const jobId = req.params.jobId as string;
  const job = jobQueue.getJob(jobId);

  if (!job) {
    res.status(404).json({ error: 'Job not found' });
    return;
  }

  if (job.status !== 'complete') {
    res.status(400).json({ error: 'Job not complete', status: job.status });
    return;
  }

  const outputPath = job.outputPath || path.join(getSessionDir(job.sessionId), 'output.mp4');

  if (!fs.existsSync(outputPath)) {
    res.status(404).json({ error: 'Output file not found' });
    return;
  }

  const stat = fs.statSync(outputPath);
  res.setHeader('Content-Type', 'video/mp4');
  res.setHeader('Content-Length', stat.size);
  res.setHeader('Content-Disposition', `attachment; filename="export-${job.sessionId}.mp4"`);

  const readStream = fs.createReadStream(outputPath);
  readStream.pipe(res);

  readStream.on('close', () => {
    // Cleanup after download
    cleanupSession(job.sessionId);
    sessionJobs.delete(job.sessionId);
  });

  readStream.on('error', (err) => {
    exportLog.error('Download stream error:', err);
    if (!res.headersSent) {
      res.status(500).json({ error: 'Failed to stream file' });
    }
  });
});

/**
 * Cancel a job
 */
router.post('/cancel/:jobId', async (req: Request, res: Response) => {
  const jobId = req.params.jobId as string;
  const job = jobQueue.getJob(jobId);

  if (!job) {
    res.status(404).json({ error: 'Job not found' });
    return;
  }

  if (job.status === 'complete' || job.status === 'failed') {
    res.status(400).json({ error: 'Cannot cancel completed job' });
    return;
  }

  // Update job status
  await jobQueue.updateJobStatus(jobId, 'failed', {
    lastError: 'Cancelled by user',
  });

  // Cleanup session
  cleanupSession(job.sessionId);
  sessionJobs.delete(job.sessionId);

  res.json({ success: true, message: 'Job cancelled' });
});

/**
 * Get queue statistics
 */
router.get('/stats', (_req: Request, res: Response) => {
  const stats = jobQueue.getStats();
  const encoderInfo = getEncoderInfo();

  res.json({
    queue: stats,
    encoder: encoderInfo,
  });
});

/**
 * Get status message for a job
 */
function getStatusMessage(job: RenderJob): string {
  switch (job.status) {
    case 'pending':
      return 'Initializing...';
    case 'uploading':
      return `Receiving frames (${job.frameManifest.receivedFrames}/${job.frameManifest.totalFrames || '?'})`;
    case 'queued':
      return 'Queued for encoding...';
    case 'encoding':
      return `Encoding frame ${job.currentFrame}/${job.frameManifest.totalFrames}`;
    case 'complete':
      return 'Export complete!';
    case 'failed':
      return job.lastError || 'Export failed';
    default:
      return 'Unknown status';
  }
}

export default router;
````

## File: packages/server/routes/gemini.ts
````typescript
import { Router, Response } from 'express';
import { ApiProxyRequest } from '../types.js';
import { GEMINI_API_KEY } from '../utils/index.js';
import { createLogger } from '@studio/shared/src/services/logger.js';

const geminiLog = createLogger('Gemini');
const router = Router();

// Gemini API Proxy - Generate Content (Text/Data)
router.post('/proxy/generateContent', async (req: ApiProxyRequest, res: Response) => {
    try {
        const { model, contents, config } = req.body;
        geminiLog.info(`Generating content with model: ${model}`);
        geminiLog.debug('Config:', config);

        if (!GEMINI_API_KEY) {
            throw new Error('VITE_GEMINI_API_KEY not configured on server');
        }

        // Dynamically import GoogleGenAI SDK
        const { GoogleGenAI } = await import('@google/genai');
        const client = new GoogleGenAI({ apiKey: GEMINI_API_KEY });

        // Build request params - config should be nested under 'config' key
        const requestParams: Record<string, unknown> = {
            model,
            contents
        };

        // Add config if provided
        if (config) {
            requestParams.config = config;
        }

        geminiLog.debug('Calling SDK with:', requestParams);

        // Call the actual SDK method
        const result = await (client as any).models.generateContent(requestParams);

        geminiLog.info(`Success - response type: ${typeof result}`);

        // The SDK's response.text is a getter that doesn't survive JSON serialization.
        const resultAsAny = result as any;
        const responseData = {
            ...resultAsAny,
            text: typeof resultAsAny.text === 'function' ? resultAsAny.text() : resultAsAny.text,
            candidates: resultAsAny.candidates,
        };

        geminiLog.debug(`Response text length: ${responseData.text?.length || 0} chars`);
        res.json(responseData);
    } catch (error: unknown) {
        const err = error as Error;
        geminiLog.error('generateContent Error:', err);

        let errorMessage = err.message;
        try {
            const parsed = JSON.parse(err.message);
            errorMessage = parsed.error?.message || err.message;
        } catch (_e) {
            // Not JSON
        }

        res.status(500).json({
            success: false,
            error: errorMessage || 'Gemini proxy failed',
            details: err.stack
        });
    }
});

// Gemini API Proxy - Generate Images
router.post('/proxy/generateImages', async (req: ApiProxyRequest, res: Response) => {
    try {
        const { model, prompt, config } = req.body;
        geminiLog.info(`Generating images with model: ${model}`);

        if (!GEMINI_API_KEY) {
            throw new Error('VITE_GEMINI_API_KEY not configured on server');
        }

        const { GoogleGenAI } = await import('@google/genai');
        const client = new GoogleGenAI({ apiKey: GEMINI_API_KEY });

        // Strip unsupported parameters: seed is not supported by imagen-4.0 via @google/genai
        const sanitizedConfig = config ? { ...config } : {};
        delete sanitizedConfig.seed;

        const result = await (client as any).models.generateImages({
            model,
            prompt,
            config: sanitizedConfig
        });

        geminiLog.info('Image generation success');
        res.json(result);
    } catch (error: unknown) {
        const err = error as any;
        // Extract the most useful error message from @google/genai SDK errors
        let errorMessage = err.message || '';
        const statusCode = err.status || err.statusCode || 500;

        // SDK errors may embed JSON in the message or use nested properties
        try {
            const parsed = JSON.parse(errorMessage);
            errorMessage = parsed.error?.message || parsed.message || errorMessage;
        } catch (_e) {
            // Not JSON - check for nested error properties
            if (err.error?.message) {
                errorMessage = err.error.message;
            } else if (err.errorDetails) {
                errorMessage = JSON.stringify(err.errorDetails);
            }
        }

        geminiLog.error(`generateImages Error (${statusCode}):`, errorMessage);
        geminiLog.error('Full error object:', JSON.stringify(err, Object.getOwnPropertyNames(err), 2));

        res.status(500).json({
            success: false,
            error: errorMessage || 'Gemini image generation failed',
            status: statusCode,
            details: err.stack
        });
    }
});

// Backward compatibility (old format)
router.post('/generate', async (req: ApiProxyRequest, res: Response): Promise<void> => {
    try {
        const { prompt, options = {} } = req.body;
        const model = 'gemini-3-pro-preview';
        const contents = { parts: [{ text: prompt }] };
        const config = options;

        geminiLog.info('Redirecting legacy call to generateContent');

        const { ai } = await import('@studio/shared/src/services/shared/apiClient.js');
        const result = await (ai as any).models.generateContent({ model, contents, config });

        const resultAsAny = result as any;
        res.json({
            success: true,
            data: {
                text: typeof resultAsAny.text === 'function' ? resultAsAny.text() : (resultAsAny.text || ''),
                raw: result
            }
        });
    } catch (error: unknown) {
        const err = error as Error;
        res.status(500).json({ success: false, error: err.message });
    }
});

router.post('/image', async (req: ApiProxyRequest, res: Response): Promise<void> => {
    if (!GEMINI_API_KEY) {
        res.status(500).json({ success: false, error: 'Gemini API key not configured' });
        return;
    }

    try {
        const { prompt, options = {} } = req.body;

        if (!prompt) {
            res.status(400).json({ success: false, error: 'Prompt is required' });
            return;
        }

        const { generateImageFromPrompt } = await import('@studio/shared/src/services/imageService.js');

        const result = await generateImageFromPrompt(
            prompt,
            (options.style as string) || "Cinematic",
            (options.globalSubject as string) || "",
            (options.aspectRatio as string) || "16:9",
            (options.skipRefine as boolean) || false,
            options.seed as number
        );
        res.json({ success: true, data: result });
    } catch (error) {
        geminiLog.error('Image proxy error:', error);
        res.status(500).json({
            success: false,
            error: error instanceof Error ? error.message : 'Unknown error'
        });
    }
});

export default router;
````

## File: packages/server/routes/health.ts
````typescript
import { Router, Request, Response } from 'express';
import { GEMINI_API_KEY, DEAPI_API_KEY } from '../utils/index.js';

const router = Router();

/**
 * Health check endpoint
 */
router.get('/', (_req: Request, res: Response) => {
  res.json({
    status: 'ok',
    timestamp: new Date().toISOString(),
    apis: {
      gemini: !!GEMINI_API_KEY,
      deapi: !!DEAPI_API_KEY,
      suno: !!process.env.VITE_SUNO_API_KEY,
    }
  });
});

export default router;
````

## File: packages/server/routes/import.ts
````typescript
import { Router, Request, Response } from 'express';
import { spawn } from 'child_process';
import fs from 'fs';
import path from 'path';
import { getSessionDir, cleanupSession } from '../utils/index.js';
import { createLogger } from '@studio/shared/src/services/logger.js';

const importLog = createLogger('Import');

const router = Router();

/**
 * Import from YouTube
 * Downloads audio from a YouTube URL using yt-dlp and streams it back.
 */
router.post('/youtube', async (req: Request, res: Response) => {
  const { url } = req.body;

  if (!url) {
    res.status(400).json({ error: 'Missing YouTube URL' });
    return;
  }

  // Basic URL validation
  try {
    new URL(url);
  } catch {
    res.status(400).json({ error: 'Invalid URL' });
    return;
  }

  const sessionId = Date.now().toString();
  const sessionDir = getSessionDir(sessionId);

  if (!fs.existsSync(sessionDir)) {
    fs.mkdirSync(sessionDir, { recursive: true });
  }

  const outputTemplate = path.join(sessionDir, 'audio.%(ext)s');
  const finalAudioPath = path.join(sessionDir, 'audio.mp3');

  importLog.info(`YouTube: Downloading: ${url}`);

  const args = [
    '-x',
    '--audio-format', 'mp3',
    '--audio-quality', '0',
    '-o', outputTemplate,
    url
  ];

  try {
    await new Promise<void>((resolve, reject) => {
      const ytdlp = spawn('yt-dlp', args);

      ytdlp.stderr.on('data', (data) => importLog.debug(`yt-dlp: ${data}`));

      ytdlp.on('close', (code) => {
        if (code === 0) resolve();
        else reject(new Error(`yt-dlp exited with code ${code}`));
      });

      ytdlp.on('error', (err) => reject(err));
    });

    if (!fs.existsSync(finalAudioPath)) {
      throw new Error('Download failed, file not found');
    }

    importLog.info(`YouTube: Download complete for ${sessionId}`);

    const stat = fs.statSync(finalAudioPath);
    res.setHeader('Content-Type', 'audio/mpeg');
    res.setHeader('Content-Length', stat.size);
    res.setHeader('Content-Disposition', 'attachment; filename="youtube_audio.mp3"');

    const readStream = fs.createReadStream(finalAudioPath);
    readStream.pipe(res);

    readStream.on('close', () => {
      cleanupSession(sessionId);
    });

    readStream.on('error', (err) => {
      importLog.error('Stream error:', err);
      cleanupSession(sessionId);
      if (!res.headersSent) res.status(500).end();
    });

  } catch (error: unknown) {
    const message = error instanceof Error ? error.message : 'Failed to download from YouTube';
    importLog.error('YouTube import error:', error);
    cleanupSession(sessionId);
    res.status(500).json({ success: false, error: message });
  }
});

export default router;
````

## File: packages/server/routes/suno.ts
````typescript
import { Router, Response, Request } from 'express';
import { createLogger } from '@studio/shared/src/services/logger.js';
import fs from 'fs';
import multer from 'multer';
import { MAX_SINGLE_FILE } from '../utils/index.js';

const sunoLog = createLogger('Suno');
const router = Router();
const upload = multer({
    dest: 'temp/',
    limits: { fileSize: MAX_SINGLE_FILE }
});

router.post('/upload', upload.single('file'), async (req: Request, res: Response): Promise<void> => {
    if (!req.file) {
        res.status(400).json({ error: 'No file provided' });
        return;
    }

    const SUNO_API_KEY = process.env.VITE_SUNO_API_KEY || process.env.SUNO_API_KEY;
    if (!SUNO_API_KEY) {
        res.status(500).json({ error: 'Suno API key not configured on server' });
        return;
    }

    const filePath = req.file.path;
    try {
        sunoLog.info(`Uploading file: ${req.file.originalname}`);
        const fileBuffer = fs.readFileSync(filePath);
        const blob = new Blob([fileBuffer], { type: req.file.mimetype });

        const formData = new FormData();
        formData.append('file', blob, req.file.originalname);
        formData.append('uploadPath', 'custom_uploads');

        const response = await fetch('https://sunoapiorg.redpandaai.co/api/file-stream-upload', {
            method: 'POST',
            headers: { 'Authorization': `Bearer ${SUNO_API_KEY}` },
            body: formData,
        });

        const data = await response.json();
        fs.unlinkSync(filePath);
        res.json(data);
    } catch (error: any) {
        if (fs.existsSync(filePath)) fs.unlinkSync(filePath);
        res.status(500).json({ error: error.message || 'Upload failed' });
    }
});

router.use('/proxy', async (req: Request, res: Response): Promise<void> => {
    const endpoint = req.path.startsWith('/') ? req.path.slice(1) : req.path;
    const SUNO_API_KEY = process.env.VITE_SUNO_API_KEY || process.env.SUNO_API_KEY;

    if (!SUNO_API_KEY) {
        res.status(500).json({ error: 'Suno API key not configured on server' });
        return;
    }

    try {
        const sunoUrl = `https://api.sunoapi.org/api/v1/${endpoint}`;
        sunoLog.info(`Proxying ${req.method} request to Suno: ${endpoint}`);
        const fetchOptions: RequestInit = {
            method: req.method,
            headers: {
                'Authorization': `Bearer ${SUNO_API_KEY}`,
                'Content-Type': 'application/json',
                'Accept': 'application/json',
            },
        };

        if (req.method !== 'GET' && req.method !== 'HEAD') {
            fetchOptions.body = JSON.stringify(req.body);
        }

        const response = await fetch(sunoUrl, fetchOptions);
        const data = await response.json();
        res.status(response.status).json(data);
    } catch (error: any) {
        res.status(500).json({ error: error.message || 'Suno proxy failed' });
    }
});

export default router;
````

## File: packages/server/routes/video.ts
````typescript
import { Router, Response, Request } from 'express';
import { createLogger } from '@studio/shared/src/services/logger.js';

const videoLog = createLogger('Video');
const router = Router();

router.post('/generate-prompt', async (req: Request, res: Response): Promise<void> => {
    try {
        const { sceneDescription, style, mood, globalSubject, videoPurpose, duration } = req.body;
        if (!sceneDescription) {
            res.status(400).json({ error: 'sceneDescription is required' });
            return;
        }

        const { generateProfessionalVideoPrompt } = await import('@studio/shared/src/services/promptService.js');
        const prompt = await generateProfessionalVideoPrompt(sceneDescription, style, mood, globalSubject, videoPurpose, duration);
        res.json({ success: true, prompt });
    } catch (error: unknown) {
        const err = error as Error;
        videoLog.error('Prompt error:', err);
        res.status(500).json({ error: err.message });
    }
});

router.post('/generate', async (req: Request, res: Response): Promise<void> => {
    try {
        const { prompt, style, aspectRatio, duration, useFastModel, globalSubject } = req.body;
        if (!prompt) {
            res.status(400).json({ error: 'prompt is required' });
            return;
        }

        const { generateVideoFromPrompt } = await import('@studio/shared/src/services/videoService.js');
        const videoUrl = await generateVideoFromPrompt(prompt, style, globalSubject, aspectRatio, duration, useFastModel);
        res.json({ success: true, videoUrl });
    } catch (error: unknown) {
        const err = error as Error;
        videoLog.error('Video error:', err);
        res.status(500).json({ error: err.message });
    }
});

router.get('/download', async (req: Request, res: Response): Promise<void> => {
    try {
        const { url } = req.query;
        if (!url || typeof url !== 'string') {
            res.status(400).json({ error: 'url is required' });
            return;
        }

        const response = await fetch(url);
        if (!response.ok) throw new Error('Download failed');

        res.setHeader('Content-Type', response.headers.get('content-type') || 'video/mp4');
        const buffer = await response.arrayBuffer();
        res.send(Buffer.from(buffer));
    } catch (error: unknown) {
        const err = error as Error;
        videoLog.error('Download error:', err);
        res.status(500).json({ error: err.message });
    }
});

export default router;
````

## File: packages/server/services/encoding/encoderStrategy.ts
````typescript
/**
 * Encoder Strategy Service
 *
 * Handles encoder selection, testing, and fallback logic.
 * Standardizes color space settings for consistent output.
 */

import { execSync, spawn } from 'child_process';
import fs from 'fs';
import path from 'path';
import os from 'os';
import { createLogger } from '@studio/shared/src/services/logger.js';

const log = createLogger('EncoderStrategy');

export type EncoderType = 'h264_nvenc' | 'h264_qsv' | 'h264_amf' | 'libx264';

interface EncoderConfig {
  name: string;
  type: EncoderType;
  isHardware: boolean;
  priority: number;
  available: boolean;
  testedAt?: number;
  testError?: string;
}

/**
 * Standardized encoding specification for color fidelity
 */
export const ENCODING_SPEC = {
  colorSpace: 'bt709',
  colorPrimaries: 'bt709',
  colorTrc: 'bt709',
  pixelFormat: 'yuv420p',
  quality: {
    nvenc: { cq: 21, preset: 'p4' },
    qsv: { cq: 21, preset: 'medium' },
    amf: { cq: 21, preset: 'quality' },
    libx264: { crf: 21, preset: 'fast' },
  },
} as const;

/**
 * Encoder configurations in priority order
 */
const ENCODERS: EncoderConfig[] = [
  {
    name: 'NVIDIA NVENC',
    type: 'h264_nvenc',
    isHardware: true,
    priority: 1,
    available: false,
  },
  {
    name: 'Intel Quick Sync',
    type: 'h264_qsv',
    isHardware: true,
    priority: 2,
    available: false,
  },
  {
    name: 'AMD AMF',
    type: 'h264_amf',
    isHardware: true,
    priority: 3,
    available: false,
  },
  {
    name: 'Software (libx264)',
    type: 'libx264',
    isHardware: false,
    priority: 4,
    available: true, // Always assume available as fallback
  },
];

// Cache encoder availability
let encoderCache: Map<EncoderType, boolean> | null = null;
let selectedEncoder: EncoderType | null = null;

/**
 * Check if FFmpeg is installed
 */
export function isFFmpegInstalled(): boolean {
  try {
    execSync('ffmpeg -version', { encoding: 'utf-8', stdio: 'pipe' });
    return true;
  } catch {
    return false;
  }
}

/**
 * Get list of available encoders from FFmpeg
 */
function getFFmpegEncoders(): string[] {
  try {
    const output = execSync('ffmpeg -encoders 2>&1', {
      encoding: 'utf-8',
      stdio: 'pipe',
    });
    return output.split('\n');
  } catch {
    return [];
  }
}

/**
 * Test an encoder with a null encode
 */
async function testEncoder(encoder: EncoderType): Promise<boolean> {
  return new Promise((resolve) => {
    // Create a minimal test - encode 1 frame of black
    // NOTE: Some hardware encoders (e.g. NVENC) require a minimum resolution
    // larger than tiny thumbnails like 64x64. Use a safe HD size so the
    // health check reflects real-world usage instead of failing on size.
    const testArgs = [
      '-f', 'lavfi',
      '-i', 'color=black:s=1280x720:d=0.1',
      '-c:v', encoder,
      '-f', 'null',
      '-',
    ];

    const ffmpeg = spawn('ffmpeg', testArgs, {
      stdio: 'pipe',
      timeout: 10000, // 10 second timeout
    });

    let timedOut = false;
    const timeout = setTimeout(() => {
      timedOut = true;
      ffmpeg.kill('SIGTERM');
    }, 10000);

    ffmpeg.on('close', (code) => {
      clearTimeout(timeout);
      if (timedOut) {
        resolve(false);
      } else {
        resolve(code === 0);
      }
    });

    ffmpeg.on('error', () => {
      clearTimeout(timeout);
      resolve(false);
    });
  });
}

/**
 * Detect available encoders (called at startup)
 */
export async function detectEncoders(): Promise<void> {
  log.info('Detecting available encoders...');

  encoderCache = new Map();
  const encoderList = getFFmpegEncoders();

  for (const encoder of ENCODERS) {
    // First check if encoder is listed
    const isListed = encoderList.some((line) =>
      line.includes(encoder.type)
    );

    if (!isListed && encoder.isHardware) {
      encoder.available = false;
      encoder.testError = 'Not listed in FFmpeg encoders';
      encoderCache.set(encoder.type, false);
      continue;
    }

    // Test the encoder
    if (encoder.isHardware) {
      try {
        const works = await testEncoder(encoder.type);
        encoder.available = works;
        encoder.testedAt = Date.now();

        if (works) {
          log.info(`✓ ${encoder.name} (${encoder.type}) available`);
        } else {
          encoder.testError = 'Test encode failed';
          log.debug(`✗ ${encoder.name} (${encoder.type}) test failed`);
        }

        encoderCache.set(encoder.type, works);
      } catch (error) {
        encoder.available = false;
        encoder.testError = error instanceof Error ? error.message : 'Unknown error';
        encoderCache.set(encoder.type, false);
      }
    } else {
      // libx264 is always assumed available
      encoder.available = true;
      encoderCache.set(encoder.type, true);
    }
  }

  // Select best available encoder
  selectedEncoder = selectBestEncoder();
  log.info(`Selected encoder: ${selectedEncoder}`);
}

/**
 * Select the best available encoder
 */
function selectBestEncoder(): EncoderType {
  const available = ENCODERS
    .filter((e) => e.available)
    .sort((a, b) => a.priority - b.priority);

  return available[0]?.type || 'libx264';
}

/**
 * Get the currently selected encoder
 */
export function getSelectedEncoder(): EncoderType {
  if (!selectedEncoder) {
    // If not initialized, do a quick sync check
    const encoderList = getFFmpegEncoders();

    if (encoderList.some((line) => line.includes('h264_nvenc'))) {
      return 'h264_nvenc';
    }
    if (encoderList.some((line) => line.includes('h264_qsv'))) {
      return 'h264_qsv';
    }
    if (encoderList.some((line) => line.includes('h264_amf'))) {
      return 'h264_amf';
    }

    return 'libx264';
  }

  return selectedEncoder;
}

/**
 * Check if an encoder is available
 */
export function isEncoderAvailable(encoder: EncoderType): boolean {
  if (encoderCache) {
    return encoderCache.get(encoder) ?? false;
  }
  return encoder === 'libx264';
}

/**
 * Get fallback encoder chain
 */
export function getFallbackChain(primary: EncoderType): EncoderType[] {
  const chain: EncoderType[] = [primary];

  // Add fallbacks in priority order
  for (const encoder of ENCODERS) {
    if (encoder.type !== primary && encoder.available) {
      chain.push(encoder.type);
    }
  }

  return chain;
}

/**
 * Build encoder-specific FFmpeg arguments
 */
export function getEncoderArgs(encoder: EncoderType): string[] {
  const args: string[] = ['-c:v', encoder];

  const quality = ENCODING_SPEC.quality;

  switch (encoder) {
    case 'h264_nvenc':
      args.push(
        '-preset', quality.nvenc.preset,
        '-rc', 'vbr',
        '-cq', String(quality.nvenc.cq),
        '-b:v', '8M',
        '-maxrate', '12M',
        '-bufsize', '16M'
      );
      break;

    case 'h264_qsv':
      args.push(
        '-preset', quality.qsv.preset,
        '-global_quality', String(quality.qsv.cq),
        '-look_ahead', '1'
      );
      break;

    case 'h264_amf':
      args.push(
        '-quality', quality.amf.preset,
        '-rc', 'vbr_latency',
        '-qp_i', String(quality.amf.cq),
        '-qp_p', String(quality.amf.cq + 2)
      );
      break;

    case 'libx264':
    default:
      const cpuCount = Math.max(2, os.cpus().length - 2);
      args.push(
        '-preset', quality.libx264.preset,
        '-crf', String(quality.libx264.crf),
        '-tune', 'film',
        '-threads', String(cpuCount)
      );
      break;
  }

  // Add standardized color space settings
  args.push(
    '-colorspace', ENCODING_SPEC.colorSpace,
    '-color_primaries', ENCODING_SPEC.colorPrimaries,
    '-color_trc', ENCODING_SPEC.colorTrc,
    '-pix_fmt', ENCODING_SPEC.pixelFormat
  );

  return args;
}

/**
 * Get encoder information for status display
 */
export function getEncoderInfo(): {
  selected: EncoderType;
  isHardware: boolean;
  available: Array<{ type: EncoderType; name: string; isHardware: boolean }>;
} {
  const selected = getSelectedEncoder();
  const selectedConfig = ENCODERS.find((e) => e.type === selected);

  return {
    selected,
    isHardware: selectedConfig?.isHardware ?? false,
    available: ENCODERS
      .filter((e) => e.available)
      .map((e) => ({
        type: e.type,
        name: e.name,
        isHardware: e.isHardware,
      })),
  };
}
````

## File: packages/server/services/jobQueue/index.ts
````typescript
/**
 * Job Queue Manager
 *
 * Manages the lifecycle of video encoding jobs:
 * - Queue management (pending → encoding → complete)
 * - Job persistence to disk
 * - SSE progress subscriptions
 * - Timeout monitoring
 * - Worker coordination
 */

import { EventEmitter } from 'events';
import { createLogger } from '@studio/shared/src/services/logger.js';
import {
  RenderJob,
  JobStatus,
  JobProgress,
  JobProgressCallback,
  createRenderJob,
  EncodingConfig,
  FrameChecksum,
} from '../../types/renderJob.js';
import {
  saveJob,
  loadJob,
  deleteJob,
  loadIncompleteJobs,
  cleanupOldJobs,
} from './jobStore.js';
import { timeoutManager } from './timeoutManager.js';

const log = createLogger('JobQueue');

// Maximum concurrent encoding jobs
const MAX_CONCURRENT_JOBS = 2;

// Job retention after completion
const COMPLETED_JOB_RETENTION_MS = 30 * 60 * 1000; // 30 minutes

export class JobQueueManager extends EventEmitter {
  private jobs: Map<string, RenderJob> = new Map();
  private subscribers: Map<string, Set<JobProgressCallback>> = new Map();
  private processingQueue: string[] = [];
  private activeJobs: Set<string> = new Set();
  private initialized = false;
  private isProcessing = false;

  // Callback for when a job is ready to be processed
  private onJobReady: ((job: RenderJob) => Promise<void>) | null = null;

  constructor() {
    super();
  }

  /**
   * Initialize the job queue (call on server startup)
   */
  async initialize(): Promise<void> {
    if (this.initialized) return;

    log.info('Initializing job queue...');

    // Start timeout manager
    timeoutManager.start((jobId, reason) => {
      this.handleJobTimeout(jobId, reason);
    });

    // Recover incomplete jobs from disk
    try {
      const incompleteJobs = await loadIncompleteJobs();
      for (const job of incompleteJobs) {
        // Reset jobs that were in progress
        if (job.status === 'encoding') {
          job.status = 'queued';
          job.retryCount++;
          log.info(`Recovered job ${job.jobId} - reset to queued (retry ${job.retryCount})`);
        }
        this.jobs.set(job.jobId, job);

        // Add queued jobs to processing queue
        if (job.status === 'queued') {
          this.processingQueue.push(job.jobId);
        }
      }

      if (incompleteJobs.length > 0) {
        log.info(`Recovered ${incompleteJobs.length} incomplete jobs`);
      }
    } catch (error) {
      log.error('Failed to recover jobs:', error);
    }

    // Cleanup old jobs periodically
    setInterval(() => {
      cleanupOldJobs(24).catch((err) =>
        log.error('Failed to cleanup old jobs:', err)
      );
    }, 60 * 60 * 1000); // Every hour

    this.initialized = true;
    log.info('Job queue initialized');

    // Process any recovered jobs
    this.processNextJob();
  }

  /**
   * Set the callback for processing jobs
   */
  setJobProcessor(processor: (job: RenderJob) => Promise<void>): void {
    this.onJobReady = processor;
  }

  /**
   * Create a new job for a session
   */
  async createJob(
    sessionId: string,
    config: Partial<EncodingConfig> = {}
  ): Promise<RenderJob> {
    const job = createRenderJob(sessionId, config);
    this.jobs.set(job.jobId, job);
    await saveJob(job);

    log.info(`Created job ${job.jobId} for session ${sessionId}`);
    this.emitProgress(job);

    return job;
  }

  /**
   * Get a job by ID
   */
  getJob(jobId: string): RenderJob | undefined {
    return this.jobs.get(jobId);
  }

  /**
   * Get job by session ID
   */
  getJobBySession(sessionId: string): RenderJob | undefined {
    for (const job of this.jobs.values()) {
      if (job.sessionId === sessionId) {
        return job;
      }
    }
    return undefined;
  }

  /**
   * Update job status
   */
  async updateJobStatus(
    jobId: string,
    status: JobStatus,
    additionalData?: Partial<RenderJob>
  ): Promise<void> {
    const job = this.jobs.get(jobId);
    if (!job) {
      log.warn(`Attempted to update non-existent job ${jobId}`);
      return;
    }

    job.status = status;
    if (additionalData) {
      Object.assign(job, additionalData);
    }

    // Handle state transitions
    switch (status) {
      case 'encoding':
        job.startedAt = Date.now();
        timeoutManager.trackJob(jobId);
        break;

      case 'complete':
        job.completedAt = Date.now();
        timeoutManager.untrackJob(jobId);
        this.activeJobs.delete(jobId);
        // Schedule cleanup
        setTimeout(() => {
          this.cleanupCompletedJob(jobId);
        }, COMPLETED_JOB_RETENTION_MS);
        break;

      case 'failed':
        timeoutManager.untrackJob(jobId);
        this.activeJobs.delete(jobId);
        break;
    }

    await saveJob(job);
    this.emitProgress(job);

    // If job completed or failed, process next
    if (status === 'complete' || status === 'failed') {
      this.processNextJob();
    }
  }

  /**
   * Update job progress
   */
  async updateJobProgress(
    jobId: string,
    progress: number,
    currentFrame?: number,
    message?: string
  ): Promise<void> {
    const job = this.jobs.get(jobId);
    if (!job) return;

    job.progress = progress;
    if (currentFrame !== undefined) {
      job.currentFrame = currentFrame;
    }
    job.lastHeartbeat = Date.now();

    // Record heartbeat with timeout manager
    timeoutManager.recordHeartbeat(jobId);

    // Emit progress (but don't save every update to disk - too frequent)
    this.emitProgress(job, message);
  }

  /**
   * Record a heartbeat for a job (from worker)
   */
  recordHeartbeat(jobId: string): void {
    timeoutManager.recordHeartbeat(jobId);
  }

  /**
   * Register frames received in a chunk
   */
  registerFrames(
    jobId: string,
    frameCount: number,
    checksums?: FrameChecksum[]
  ): void {
    const job = this.jobs.get(jobId);
    if (!job) return;

    job.frameManifest.receivedFrames += frameCount;

    if (checksums) {
      for (const cs of checksums) {
        job.frameManifest.checksums[cs.frameIndex] = cs;
      }
    }

    job.status = 'uploading';
    // Persist status change so uploads survive server restarts
    saveJob(job).catch((err) =>
      log.error(`Failed to persist frame registration for job ${jobId}:`, err)
    );
  }

  /**
   * Set total expected frames
   */
  setTotalFrames(jobId: string, totalFrames: number): void {
    const job = this.jobs.get(jobId);
    if (!job) return;

    job.frameManifest.totalFrames = totalFrames;
  }

  /**
   * Queue a job for processing (called after all frames uploaded)
   */
  async queueJob(jobId: string): Promise<void> {
    const job = this.jobs.get(jobId);
    if (!job) {
      throw new Error(`Job ${jobId} not found`);
    }

    job.status = 'queued';
    await saveJob(job);

    this.processingQueue.push(jobId);
    log.info(
      `Job ${jobId} queued (${this.processingQueue.length} in queue, ${this.activeJobs.size} active)`
    );

    this.emitProgress(job);
    this.processNextJob();
  }

  /**
   * Process the next job in the queue
   */
  private async processNextJob(): Promise<void> {
    // Guard against concurrent invocations racing past the capacity check
    if (this.isProcessing) return;
    this.isProcessing = true;

    try {
      await this._processNextJobInner();
    } finally {
      this.isProcessing = false;
    }
  }

  private async _processNextJobInner(): Promise<void> {
    // Check if we can process more jobs
    if (this.activeJobs.size >= MAX_CONCURRENT_JOBS) {
      return;
    }

    // Get next job from queue
    const jobId = this.processingQueue.shift();
    if (!jobId) {
      return;
    }

    const job = this.jobs.get(jobId);
    if (!job) {
      log.warn(`Queued job ${jobId} not found in memory`);
      this.processNextJob();
      return;
    }

    // Check if job has exceeded max retries
    if (job.retryCount > job.maxRetries) {
      log.error(`Job ${jobId} exceeded max retries (${job.maxRetries})`);
      await this.updateJobStatus(jobId, 'failed', {
        lastError: 'Exceeded maximum retry attempts',
      });
      this.processNextJob();
      return;
    }

    this.activeJobs.add(jobId);

    // Start processing
    if (this.onJobReady) {
      log.info(`Processing job ${jobId}`);
      try {
        await this.onJobReady(job);
      } catch (error) {
        log.error(`Job ${jobId} processor error:`, error);
        await this.handleJobError(jobId, error);
      }
    } else {
      log.warn('No job processor registered');
      this.activeJobs.delete(jobId);
    }
  }

  /**
   * Handle job timeout
   */
  private async handleJobTimeout(
    jobId: string,
    reason: 'stall' | 'timeout'
  ): Promise<void> {
    const job = this.jobs.get(jobId);
    if (!job) return;

    const message =
      reason === 'stall'
        ? 'Job stalled - no progress for 60 seconds'
        : 'Job exceeded maximum time limit (30 minutes)';

    log.error(`Job ${jobId} timeout: ${message}`);

    this.activeJobs.delete(jobId);

    // Retry if under limit
    if (job.retryCount < job.maxRetries) {
      job.retryCount++;
      job.status = 'queued';
      job.lastError = message;
      await saveJob(job);

      this.processingQueue.push(jobId);
      log.info(`Job ${jobId} re-queued for retry ${job.retryCount}`);
      this.emitProgress(job);
      this.processNextJob();
    } else {
      await this.updateJobStatus(jobId, 'failed', { lastError: message });
    }
  }

  /**
   * Handle job error from worker
   */
  async handleJobError(jobId: string, error: unknown): Promise<void> {
    const job = this.jobs.get(jobId);
    if (!job) return;

    const errorMessage =
      error instanceof Error ? error.message : 'Unknown error';

    this.activeJobs.delete(jobId);
    timeoutManager.untrackJob(jobId);

    // Retry if under limit
    if (job.retryCount < job.maxRetries) {
      job.retryCount++;
      job.status = 'queued';
      job.lastError = errorMessage;
      await saveJob(job);

      this.processingQueue.push(jobId);
      log.info(`Job ${jobId} re-queued for retry ${job.retryCount}`);
      this.emitProgress(job);
      this.processNextJob();
    } else {
      await this.updateJobStatus(jobId, 'failed', { lastError: errorMessage });
    }
  }

  /**
   * Subscribe to job progress updates
   */
  subscribe(jobId: string, callback: JobProgressCallback): () => void {
    if (!this.subscribers.has(jobId)) {
      this.subscribers.set(jobId, new Set());
    }
    this.subscribers.get(jobId)!.add(callback);

    // Send current state immediately
    const job = this.jobs.get(jobId);
    if (job) {
      callback(this.createProgressEvent(job));
    }

    // Return unsubscribe function
    return () => {
      const subs = this.subscribers.get(jobId);
      if (subs) {
        subs.delete(callback);
        if (subs.size === 0) {
          this.subscribers.delete(jobId);
        }
      }
    };
  }

  /**
   * Emit progress to all subscribers
   */
  private emitProgress(job: RenderJob, message?: string): void {
    const progress = this.createProgressEvent(job, message);
    const subs = this.subscribers.get(job.jobId);

    if (subs) {
      for (const callback of subs) {
        try {
          callback(progress);
        } catch (error) {
          log.error('Subscriber callback error:', error);
        }
      }
    }

    // Also emit on EventEmitter for general listeners
    this.emit('progress', progress);
  }

  /**
   * Create a progress event from job state
   */
  private createProgressEvent(job: RenderJob, message?: string): JobProgress {
    const statusMessages: Record<JobStatus, string> = {
      pending: 'Initializing...',
      uploading: `Receiving frames (${job.frameManifest.receivedFrames}/${job.frameManifest.totalFrames || '?'})`,
      queued: `Queued for encoding (position ${this.processingQueue.indexOf(job.jobId) + 1})`,
      encoding: `Encoding frame ${job.currentFrame}/${job.frameManifest.totalFrames}`,
      complete: 'Export complete!',
      failed: job.lastError || 'Export failed',
    };

    return {
      jobId: job.jobId,
      status: job.status,
      progress: job.progress,
      message: message || statusMessages[job.status],
      currentFrame: job.currentFrame,
      totalFrames: job.frameManifest.totalFrames,
      error: job.status === 'failed' ? job.lastError : undefined,
    };
  }

  /**
   * Cleanup a completed job after retention period
   */
  private async cleanupCompletedJob(jobId: string): Promise<void> {
    const job = this.jobs.get(jobId);
    if (!job || job.status !== 'complete') return;

    // Only delete if no active subscribers
    if (!this.subscribers.has(jobId) || this.subscribers.get(jobId)!.size === 0) {
      this.jobs.delete(jobId);
      await deleteJob(jobId);
      log.debug(`Cleaned up completed job ${jobId}`);
    }
  }

  /**
   * Get queue statistics
   */
  getStats(): {
    total: number;
    pending: number;
    uploading: number;
    queued: number;
    encoding: number;
    complete: number;
    failed: number;
  } {
    const stats = {
      total: this.jobs.size,
      pending: 0,
      uploading: 0,
      queued: 0,
      encoding: 0,
      complete: 0,
      failed: 0,
    };

    for (const job of this.jobs.values()) {
      stats[job.status]++;
    }

    return stats;
  }

  /**
   * Shutdown the job queue
   */
  shutdown(): void {
    timeoutManager.stop();
    this.subscribers.clear();
    log.info('Job queue shutdown');
  }
}

// Export singleton instance
export const jobQueue = new JobQueueManager();
````

## File: packages/server/services/jobQueue/jobStore.ts
````typescript
/**
 * Job Store - File-based Job Persistence
 *
 * Stores render jobs to disk for durability across server restarts.
 * Jobs are stored as JSON files in temp/jobs/{jobId}.json
 */

import fs from 'fs';
import path from 'path';
import { createLogger } from '@studio/shared/src/services/logger.js';
import { TEMP_DIR } from '../../utils/index.js';
import {
  RenderJob,
  SerializedRenderJob,
  serializeJob,
  deserializeJob,
} from '../../types/renderJob.js';

const log = createLogger('JobStore');

// Jobs directory
const JOBS_DIR = path.join(TEMP_DIR, 'jobs');

/**
 * Ensure jobs directory exists
 */
export function ensureJobsDir(): void {
  if (!fs.existsSync(JOBS_DIR)) {
    fs.mkdirSync(JOBS_DIR, { recursive: true });
    log.info(`Created jobs directory: ${JOBS_DIR}`);
  }
}

/**
 * Get job file path
 */
function getJobPath(jobId: string): string {
  // Sanitize jobId to prevent path traversal
  const safeId = jobId.replace(/[^a-zA-Z0-9_-]/g, '');
  return path.join(JOBS_DIR, `${safeId}.json`);
}

/**
 * Save a job to disk
 */
export async function saveJob(job: RenderJob): Promise<void> {
  ensureJobsDir();
  const jobPath = getJobPath(job.jobId);

  try {
    const serialized = serializeJob(job);
    await fs.promises.writeFile(
      jobPath,
      JSON.stringify(serialized, null, 2),
      'utf-8'
    );
    log.debug(`Saved job ${job.jobId} to disk`);
  } catch (error) {
    log.error(`Failed to save job ${job.jobId}:`, error);
    throw error;
  }
}

/**
 * Load a job from disk
 */
export async function loadJob(jobId: string): Promise<RenderJob | null> {
  const jobPath = getJobPath(jobId);

  try {
    if (!fs.existsSync(jobPath)) {
      return null;
    }

    const content = await fs.promises.readFile(jobPath, 'utf-8');
    const data = JSON.parse(content) as SerializedRenderJob;
    return deserializeJob(data);
  } catch (error) {
    log.error(`Failed to load job ${jobId}:`, error);
    return null;
  }
}

/**
 * Delete a job from disk
 */
export async function deleteJob(jobId: string): Promise<void> {
  const jobPath = getJobPath(jobId);

  try {
    if (fs.existsSync(jobPath)) {
      await fs.promises.unlink(jobPath);
      log.debug(`Deleted job ${jobId} from disk`);
    }
  } catch (error) {
    log.error(`Failed to delete job ${jobId}:`, error);
  }
}

/**
 * List all persisted jobs
 */
export async function listJobs(): Promise<RenderJob[]> {
  ensureJobsDir();

  try {
    const files = await fs.promises.readdir(JOBS_DIR);
    const jobs: RenderJob[] = [];

    for (const file of files) {
      if (!file.endsWith('.json')) continue;

      const jobId = file.replace('.json', '');
      const job = await loadJob(jobId);
      if (job) {
        jobs.push(job);
      }
    }

    return jobs;
  } catch (error) {
    log.error('Failed to list jobs:', error);
    return [];
  }
}

/**
 * Load all incomplete jobs (for recovery after restart)
 */
export async function loadIncompleteJobs(): Promise<RenderJob[]> {
  const jobs = await listJobs();
  return jobs.filter(
    (job) =>
      job.status !== 'complete' &&
      job.status !== 'failed'
  );
}

/**
 * Clean up old completed/failed jobs (older than maxAgeHours)
 */
export async function cleanupOldJobs(maxAgeHours: number = 24): Promise<number> {
  const jobs = await listJobs();
  const cutoff = Date.now() - maxAgeHours * 60 * 60 * 1000;
  let cleaned = 0;

  for (const job of jobs) {
    // Only clean up completed or failed jobs older than cutoff
    if (
      (job.status === 'complete' || job.status === 'failed') &&
      job.createdAt < cutoff
    ) {
      await deleteJob(job.jobId);
      cleaned++;
    }
  }

  if (cleaned > 0) {
    log.info(`Cleaned up ${cleaned} old jobs`);
  }

  return cleaned;
}

/**
 * Update specific fields of a job
 */
export async function updateJob(
  jobId: string,
  updates: Partial<RenderJob>
): Promise<RenderJob | null> {
  const job = await loadJob(jobId);
  if (!job) {
    log.warn(`Attempted to update non-existent job ${jobId}`);
    return null;
  }

  const updated = { ...job, ...updates };
  await saveJob(updated);
  return updated;
}
````

## File: packages/server/services/jobQueue/timeoutManager.ts
````typescript
/**
 * Timeout Manager - Heartbeat Monitoring
 *
 * Monitors active jobs for stalls and triggers timeouts.
 * Workers must send heartbeats every 5 seconds.
 * Jobs are killed after 60 seconds of silence.
 */

import { createLogger } from '@studio/shared/src/services/logger.js';
import { RenderJob, JobStatus } from '../../types/renderJob.js';

const log = createLogger('TimeoutManager');

// Configuration
const HEARTBEAT_INTERVAL_MS = 5000; // Workers send every 5s
const STALL_TIMEOUT_MS = 60000; // Kill after 60s silence
const MAX_JOB_TIME_MS = 30 * 60 * 1000; // 30 minute max job time
const CHECK_INTERVAL_MS = 5000; // How often to check for stalls

export type TimeoutCallback = (jobId: string, reason: 'stall' | 'timeout') => void;

interface TrackedJob {
  jobId: string;
  lastHeartbeat: number;
  startedAt: number;
}

export class TimeoutManager {
  private trackedJobs: Map<string, TrackedJob> = new Map();
  private checkInterval: NodeJS.Timeout | null = null;
  private onTimeout: TimeoutCallback | null = null;

  /**
   * Start the timeout manager
   */
  start(callback: TimeoutCallback): void {
    this.onTimeout = callback;

    if (this.checkInterval) {
      clearInterval(this.checkInterval);
    }

    this.checkInterval = setInterval(() => {
      this.checkForTimeouts();
    }, CHECK_INTERVAL_MS);

    log.info('Timeout manager started');
  }

  /**
   * Stop the timeout manager
   */
  stop(): void {
    if (this.checkInterval) {
      clearInterval(this.checkInterval);
      this.checkInterval = null;
    }
    this.trackedJobs.clear();
    log.info('Timeout manager stopped');
  }

  /**
   * Start tracking a job
   */
  trackJob(jobId: string): void {
    const now = Date.now();
    this.trackedJobs.set(jobId, {
      jobId,
      lastHeartbeat: now,
      startedAt: now,
    });
    log.debug(`Started tracking job ${jobId}`);
  }

  /**
   * Record a heartbeat for a job
   */
  recordHeartbeat(jobId: string): void {
    const tracked = this.trackedJobs.get(jobId);
    if (tracked) {
      tracked.lastHeartbeat = Date.now();
    }
  }

  /**
   * Stop tracking a job (completed or failed)
   */
  untrackJob(jobId: string): void {
    this.trackedJobs.delete(jobId);
    log.debug(`Stopped tracking job ${jobId}`);
  }

  /**
   * Check all tracked jobs for timeouts
   */
  private checkForTimeouts(): void {
    const now = Date.now();

    for (const [jobId, tracked] of this.trackedJobs) {
      // Check for stall (no heartbeat)
      const timeSinceHeartbeat = now - tracked.lastHeartbeat;
      if (timeSinceHeartbeat > STALL_TIMEOUT_MS) {
        log.warn(
          `Job ${jobId} stalled - no heartbeat for ${Math.round(timeSinceHeartbeat / 1000)}s`
        );
        this.triggerTimeout(jobId, 'stall');
        continue;
      }

      // Check for max job time
      const totalTime = now - tracked.startedAt;
      if (totalTime > MAX_JOB_TIME_MS) {
        log.warn(
          `Job ${jobId} exceeded max time - running for ${Math.round(totalTime / 60000)}m`
        );
        this.triggerTimeout(jobId, 'timeout');
      }
    }
  }

  /**
   * Trigger a timeout callback
   */
  private triggerTimeout(jobId: string, reason: 'stall' | 'timeout'): void {
    this.untrackJob(jobId);

    if (this.onTimeout) {
      this.onTimeout(jobId, reason);
    }
  }

  /**
   * Get tracking info for a job
   */
  getJobInfo(jobId: string): TrackedJob | undefined {
    return this.trackedJobs.get(jobId);
  }

  /**
   * Get all tracked jobs
   */
  getTrackedJobs(): TrackedJob[] {
    return Array.from(this.trackedJobs.values());
  }

  /**
   * Get configuration constants
   */
  getConfig(): {
    heartbeatIntervalMs: number;
    stallTimeoutMs: number;
    maxJobTimeMs: number;
  } {
    return {
      heartbeatIntervalMs: HEARTBEAT_INTERVAL_MS,
      stallTimeoutMs: STALL_TIMEOUT_MS,
      maxJobTimeMs: MAX_JOB_TIME_MS,
    };
  }
}

// Export singleton instance
export const timeoutManager = new TimeoutManager();
````

## File: packages/server/services/validation/frameValidator.ts
````typescript
/**
 * Frame Validator Service
 *
 * Validates uploaded frames via checksums and sequence integrity.
 */

import crypto from 'crypto';
import fs from 'fs';
import path from 'path';
import { createLogger } from '@studio/shared/src/services/logger.js';
import { FrameChecksum } from '../../types/renderJob.js';

const log = createLogger('FrameValidator');

/**
 * Generate SHA256 checksum for a buffer
 */
export function generateChecksum(data: Buffer): string {
  return crypto.createHash('sha256').update(data).digest('hex');
}

/**
 * Verify a frame's checksum
 */
export function verifyFrameChecksum(
  frameData: Buffer,
  expectedChecksum: string
): boolean {
  const actualChecksum = generateChecksum(frameData);
  return actualChecksum === expectedChecksum;
}

/**
 * Validate a batch of frames
 */
export interface FrameValidationResult {
  valid: boolean;
  totalFrames: number;
  validFrames: number;
  invalidFrames: number[];
  mismatches: Array<{
    frameIndex: number;
    expected: string;
    actual: string;
  }>;
}

export function validateFrameBatch(
  frames: Array<{ data: Buffer; index: number }>,
  checksums: Record<number, FrameChecksum>
): FrameValidationResult {
  const result: FrameValidationResult = {
    valid: true,
    totalFrames: frames.length,
    validFrames: 0,
    invalidFrames: [],
    mismatches: [],
  };

  for (const frame of frames) {
    const expectedChecksum = checksums[frame.index];

    if (!expectedChecksum) {
      // No checksum provided - skip validation for this frame
      result.validFrames++;
      continue;
    }

    const actualChecksum = generateChecksum(frame.data);

    if (actualChecksum === expectedChecksum.checksum) {
      result.validFrames++;
    } else {
      result.valid = false;
      result.invalidFrames.push(frame.index);
      result.mismatches.push({
        frameIndex: frame.index,
        expected: expectedChecksum.checksum,
        actual: actualChecksum,
      });
    }
  }

  return result;
}

/**
 * Validate frame sequence integrity (no gaps)
 */
export interface SequenceValidationResult {
  valid: boolean;
  totalExpected: number;
  totalReceived: number;
  missingFrames: number[];
  duplicateFrames: number[];
}

export function validateFrameSequence(
  receivedFrameIndices: number[],
  expectedTotalFrames: number
): SequenceValidationResult {
  const result: SequenceValidationResult = {
    valid: true,
    totalExpected: expectedTotalFrames,
    totalReceived: receivedFrameIndices.length,
    missingFrames: [],
    duplicateFrames: [],
  };

  // Check for duplicates
  const seen = new Set<number>();
  for (const index of receivedFrameIndices) {
    if (seen.has(index)) {
      result.duplicateFrames.push(index);
    }
    seen.add(index);
  }

  // Check for missing frames
  for (let i = 0; i < expectedTotalFrames; i++) {
    if (!seen.has(i)) {
      result.missingFrames.push(i);
    }
  }

  result.valid =
    result.missingFrames.length === 0 &&
    result.duplicateFrames.length === 0;

  return result;
}

/**
 * Validate all frames in a session directory
 */
export async function validateSessionFrames(
  sessionDir: string,
  expectedTotalFrames: number
): Promise<SequenceValidationResult> {
  const framePattern = /^frame(\d{6})\.jpg$/;
  const receivedIndices: number[] = [];

  try {
    const files = await fs.promises.readdir(sessionDir);

    for (const file of files) {
      const match = file.match(framePattern);
      if (match && match[1]) {
        receivedIndices.push(parseInt(match[1], 10));
      }
    }

    const result = validateFrameSequence(receivedIndices, expectedTotalFrames);

    if (!result.valid) {
      log.warn(
        `Session ${path.basename(sessionDir)} frame validation failed: ` +
        `missing=${result.missingFrames.length}, duplicates=${result.duplicateFrames.length}`
      );
    }

    return result;
  } catch (error) {
    log.error(`Failed to validate session frames:`, error);
    return {
      valid: false,
      totalExpected: expectedTotalFrames,
      totalReceived: 0,
      missingFrames: Array.from({ length: expectedTotalFrames }, (_, i) => i),
      duplicateFrames: [],
    };
  }
}

/**
 * Verify minimum frame file sizes (detect corrupt uploads)
 */
export async function validateFrameSizes(
  sessionDir: string,
  minSizeBytes: number = 1000 // 1KB minimum for a valid JPEG
): Promise<{ valid: boolean; undersizedFrames: string[] }> {
  const result = {
    valid: true,
    undersizedFrames: [] as string[],
  };

  try {
    const files = await fs.promises.readdir(sessionDir);
    const framePattern = /^frame\d{6}\.jpg$/;

    for (const file of files) {
      if (!framePattern.test(file)) continue;

      const filePath = path.join(sessionDir, file);
      const stats = await fs.promises.stat(filePath);

      if (stats.size < minSizeBytes) {
        result.valid = false;
        result.undersizedFrames.push(file);
      }
    }

    if (!result.valid) {
      log.warn(
        `Session ${path.basename(sessionDir)} has ${result.undersizedFrames.length} undersized frames`
      );
    }

    return result;
  } catch (error) {
    log.error(`Failed to validate frame sizes:`, error);
    return { valid: false, undersizedFrames: [] };
  }
}
````

## File: packages/server/services/validation/qualityVerifier.ts
````typescript
/**
 * Quality Verifier Service
 *
 * Post-encode validation to ensure output meets quality standards.
 * Verifies duration, resolution, audio presence, and file integrity.
 */

import { execSync } from 'child_process';
import fs from 'fs';
import { createLogger } from '@studio/shared/src/services/logger.js';

const log = createLogger('QualityVerifier');

export interface VideoMetadata {
  duration: number; // seconds
  width: number;
  height: number;
  fps: number;
  hasAudio: boolean;
  audioCodec?: string;
  videoCodec: string;
  bitrate: number; // kbps
  fileSize: number; // bytes
}

export interface QualityReport {
  valid: boolean;
  metadata: VideoMetadata | null;
  errors: string[];
  warnings: string[];
}

/**
 * Get video metadata using ffprobe
 */
export function getVideoMetadata(filePath: string): VideoMetadata | null {
  try {
    // Get format info
    const formatInfo = execSync(
      `ffprobe -v quiet -print_format json -show_format -show_streams "${filePath}"`,
      { encoding: 'utf-8' }
    );

    const info = JSON.parse(formatInfo);

    // Find video stream
    const videoStream = info.streams?.find(
      (s: any) => s.codec_type === 'video'
    );
    const audioStream = info.streams?.find(
      (s: any) => s.codec_type === 'audio'
    );

    if (!videoStream) {
      log.error('No video stream found in output');
      return null;
    }

    // Parse frame rate (can be "30/1" or "29.97")
    let fps = 30;
    if (videoStream.r_frame_rate) {
      const parts = videoStream.r_frame_rate.split('/');
      if (parts.length === 2) {
        fps = parseInt(parts[0]) / parseInt(parts[1]);
      } else {
        fps = parseFloat(videoStream.r_frame_rate);
      }
    }

    const stats = fs.statSync(filePath);

    return {
      duration: parseFloat(info.format?.duration || '0'),
      width: videoStream.width || 0,
      height: videoStream.height || 0,
      fps: Math.round(fps * 100) / 100,
      hasAudio: !!audioStream,
      audioCodec: audioStream?.codec_name,
      videoCodec: videoStream.codec_name || 'unknown',
      bitrate: Math.round(parseInt(info.format?.bit_rate || '0') / 1000),
      fileSize: stats.size,
    };
  } catch (error) {
    log.error('Failed to get video metadata:', error);
    return null;
  }
}

/**
 * Verify output quality against expectations
 */
export function verifyOutputQuality(
  outputPath: string,
  expectations: {
    expectedDurationSeconds: number;
    expectedFps: number;
    minWidth?: number;
    minHeight?: number;
    requireAudio?: boolean;
  }
): QualityReport {
  const report: QualityReport = {
    valid: true,
    metadata: null,
    errors: [],
    warnings: [],
  };

  // Check file exists
  if (!fs.existsSync(outputPath)) {
    report.valid = false;
    report.errors.push('Output file does not exist');
    return report;
  }

  // Get metadata
  const metadata = getVideoMetadata(outputPath);
  if (!metadata) {
    report.valid = false;
    report.errors.push('Failed to read video metadata');
    return report;
  }

  report.metadata = metadata;

  // Validate duration (within 1 second tolerance)
  const durationDiff = Math.abs(
    metadata.duration - expectations.expectedDurationSeconds
  );
  if (durationDiff > 1) {
    report.valid = false;
    report.errors.push(
      `Duration mismatch: expected ${expectations.expectedDurationSeconds}s, got ${metadata.duration}s`
    );
  } else if (durationDiff > 0.5) {
    report.warnings.push(
      `Duration slightly off: expected ${expectations.expectedDurationSeconds}s, got ${metadata.duration}s`
    );
  }

  // Validate FPS (within 10% tolerance)
  const fpsDiff = Math.abs(metadata.fps - expectations.expectedFps);
  const fpsTolerance = expectations.expectedFps * 0.1;
  if (fpsDiff > fpsTolerance) {
    report.valid = false;
    report.errors.push(
      `FPS mismatch: expected ${expectations.expectedFps}, got ${metadata.fps}`
    );
  }

  // Validate resolution
  if (expectations.minWidth && metadata.width < expectations.minWidth) {
    report.valid = false;
    report.errors.push(
      `Width too small: expected at least ${expectations.minWidth}, got ${metadata.width}`
    );
  }
  if (expectations.minHeight && metadata.height < expectations.minHeight) {
    report.valid = false;
    report.errors.push(
      `Height too small: expected at least ${expectations.minHeight}, got ${metadata.height}`
    );
  }

  // Validate audio presence
  if (expectations.requireAudio && !metadata.hasAudio) {
    report.valid = false;
    report.errors.push('Audio track missing');
  }

  // Check for suspiciously small file
  const expectedBitrate = 8000; // 8 Mbps approximate
  const expectedSize =
    (expectedBitrate * 1000 * expectations.expectedDurationSeconds) / 8;
  if (metadata.fileSize < expectedSize * 0.1) {
    report.warnings.push(
      `File size unexpectedly small: ${(metadata.fileSize / 1024 / 1024).toFixed(2)}MB`
    );
  }

  // Log result
  if (report.valid) {
    log.info(
      `Output verified: ${metadata.width}x${metadata.height} @ ${metadata.fps}fps, ` +
      `${metadata.duration.toFixed(1)}s, ${(metadata.fileSize / 1024 / 1024).toFixed(2)}MB`
    );
  } else {
    log.error(`Output validation failed:`, report.errors);
  }

  return report;
}

/**
 * Verify video file integrity (can be fully decoded)
 */
export async function verifyFileIntegrity(
  filePath: string
): Promise<{ valid: boolean; error?: string }> {
  try {
    // Use ffmpeg to decode the file to null (verifies it can be fully read)
    execSync(
      `ffmpeg -v error -i "${filePath}" -f null -`,
      { encoding: 'utf-8', timeout: 60000 }
    );
    return { valid: true };
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error';
    log.error('File integrity check failed:', message);
    return { valid: false, error: message };
  }
}

/**
 * Quick validation (just metadata, no full decode)
 */
export function quickValidate(
  outputPath: string,
  expectedDurationSeconds: number
): boolean {
  const metadata = getVideoMetadata(outputPath);
  if (!metadata) return false;

  const durationDiff = Math.abs(metadata.duration - expectedDurationSeconds);
  return durationDiff <= 1 && metadata.width > 0 && metadata.height > 0;
}
````

## File: packages/server/types.ts
````typescript
import { Request } from 'express';

export interface ApiProxyRequest extends Request {
    body: {
        prompt?: string;
        imageUrl?: string;
        options?: Record<string, unknown>;
        model?: string;
        contents?: unknown;
        config?: Record<string, unknown>;
        sceneDescription?: string;
        style?: string;
        mood?: string;
        globalSubject?: string;
        videoPurpose?: string;
        duration?: number;
        aspectRatio?: string;
        useFastModel?: boolean;
        skipRefine?: boolean;
        seed?: number;
        srtContent?: string;
        contentType?: string;
    };
}

export interface ExportRequest extends Request {
    sessionId?: string;
    files?: Express.Multer.File[];
}
````

## File: packages/server/types/renderJob.ts
````typescript
/**
 * Render Job Type Definitions
 *
 * Interfaces for the job queue system that manages video encoding jobs.
 */

/**
 * Job status state machine:
 * pending → uploading → queued → encoding → complete
 *                                    ↓
 *                                 failed (with retry)
 */
export type JobStatus =
  | 'pending'
  | 'uploading'
  | 'queued'
  | 'encoding'
  | 'complete'
  | 'failed';

/**
 * Frame checksum for validation
 */
export interface FrameChecksum {
  frameIndex: number;
  checksum: string;
  size: number;
}

/**
 * Frame manifest tracks all frames for a job
 */
export interface FrameManifest {
  totalFrames: number;
  receivedFrames: number;
  checksums: Record<number, FrameChecksum>;
  validated: boolean;
  missingFrames: number[];
}

/**
 * Encoding configuration for a job
 */
export interface EncodingConfig {
  fps: number;
  encoder: 'h264_nvenc' | 'h264_qsv' | 'h264_amf' | 'libx264';
  width?: number;
  height?: number;
  quality?: number; // CRF/CQ value (default: 21)
}

/**
 * Job progress information sent via SSE
 */
export interface JobProgress {
  jobId: string;
  status: JobStatus;
  progress: number; // 0-100
  message: string;
  currentFrame?: number;
  totalFrames?: number;
  encodingSpeed?: string; // e.g., "2.5x"
  estimatedTimeRemaining?: number; // seconds
  error?: string;
}

/**
 * Render job definition
 */
export interface RenderJob {
  jobId: string;
  sessionId: string;
  status: JobStatus;

  // Configuration
  config: EncodingConfig;

  // Frame tracking
  frameManifest: {
    totalFrames: number;
    receivedFrames: number;
    checksums: Record<number, FrameChecksum>;
    validated: boolean;
    missingFrames: number[];
  };

  // Progress
  progress: number;
  currentFrame: number;

  // Timing
  createdAt: number;
  startedAt?: number;
  completedAt?: number;
  lastHeartbeat?: number;

  // Worker info
  workerId?: string;

  // Retry handling
  retryCount: number;
  maxRetries: number;
  lastError?: string;

  // Output
  outputPath?: string;
  outputSize?: number;
}

/**
 * Worker-to-main process message types
 */
export type WorkerMessageType =
  | 'STARTED'
  | 'PROGRESS'
  | 'HEARTBEAT'
  | 'COMPLETE'
  | 'ERROR'
  | 'MEMORY_WARNING';

export interface WorkerMessage {
  type: WorkerMessageType;
  jobId: string;
  workerId: string;
  timestamp: number;
  data?: {
    progress?: number;
    currentFrame?: number;
    totalFrames?: number;
    encodingSpeed?: string;
    outputPath?: string;
    outputSize?: number;
    error?: string;
    memoryUsage?: number;
  };
}

/**
 * Main-to-worker message types
 */
export type MainMessageType =
  | 'START_JOB'
  | 'CANCEL_JOB'
  | 'SHUTDOWN';

export interface MainMessage {
  type: MainMessageType;
  job?: RenderJob;
}

/**
 * Job queue subscription callback
 */
export type JobProgressCallback = (progress: JobProgress) => void;

/**
 * Serializable version of RenderJob for JSON storage
 */
export interface SerializedRenderJob extends Omit<RenderJob, 'frameManifest'> {
  frameManifest: {
    totalFrames: number;
    receivedFrames: number;
    checksums: Record<string, FrameChecksum>;
    validated: boolean;
    missingFrames: number[];
  };
}

/**
 * Create a new render job
 */
export function createRenderJob(
  sessionId: string,
  config: Partial<EncodingConfig> = {}
): RenderJob {
  const jobId = `job_${Date.now()}_${Math.random().toString(36).substring(2, 10)}`;

  return {
    jobId,
    sessionId,
    status: 'pending',
    config: {
      fps: config.fps ?? 24,
      encoder: config.encoder ?? 'libx264',
      width: config.width,
      height: config.height,
      quality: config.quality ?? 21,
    },
    frameManifest: {
      totalFrames: 0,
      receivedFrames: 0,
      checksums: {},
      validated: false,
      missingFrames: [],
    },
    progress: 0,
    currentFrame: 0,
    createdAt: Date.now(),
    retryCount: 0,
    maxRetries: 3,
  };
}

/**
 * Serialize a job for JSON storage
 */
export function serializeJob(job: RenderJob): SerializedRenderJob {
  return {
    ...job,
    frameManifest: {
      ...job.frameManifest,
      checksums: Object.fromEntries(
        Object.entries(job.frameManifest.checksums)
      ),
    },
  };
}

/**
 * Deserialize a job from JSON storage
 */
export function deserializeJob(data: SerializedRenderJob): RenderJob {
  return {
    ...data,
    frameManifest: {
      ...data.frameManifest,
      checksums: Object.fromEntries(
        Object.entries(data.frameManifest.checksums).map(([k, v]) => [parseInt(k), v])
      ),
    },
  };
}
````

## File: packages/server/utils/index.ts
````typescript
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import { createLogger } from '@studio/shared/src/services/logger.js';

const cleanupLog = createLogger('Cleanup');

const __dirname = path.dirname(fileURLToPath(import.meta.url));
// packages/server/utils/ → ../../../ → workspace root → temp/
export const TEMP_DIR = path.resolve(__dirname, '../../../temp');
export const JOBS_DIR = path.join(TEMP_DIR, 'jobs');

// File size limits for different upload types
export const MAX_FILE_SIZE = 200 * 1024 * 1024; // 200MB - for frame chunks and video exports
export const MAX_SINGLE_FILE = 100 * 1024 * 1024; // 100MB - for individual file uploads
export const MAX_FILES = 10000;

// API Keys (server-side only)
// Note: These are evaluated at import time, so dotenv must be loaded first
// via the preload module (server/preload.ts)
export const GEMINI_API_KEY = process.env.VITE_GEMINI_API_KEY;
export const DEAPI_API_KEY = process.env.VITE_DEAPI_API_KEY;

/**
 * Sanitize session ID to prevent path traversal
 */
export const sanitizeId = (id: string): string => {
  return id.replace(/[^a-zA-Z0-9_-]/g, '');
};

/**
 * Get the session directory path
 */
export const getSessionDir = (sessionId: string): string => {
  return path.join(TEMP_DIR, sanitizeId(sessionId));
};

/**
 * Cleanup a session directory
 */
export const cleanupSession = (sessionId: string): void => {
  const dir = getSessionDir(sessionId);
  if (fs.existsSync(dir)) {
    try {
      fs.rmSync(dir, { recursive: true, force: true });
      cleanupLog.info(`Successfully removed session ${sessionId}`);
    } catch (e) {
      cleanupLog.error(`Failed to remove session ${sessionId}:`, e);
    }
  }
};

/**
 * Ensure temp directory exists
 */
export const ensureTempDir = (): void => {
  if (!fs.existsSync(TEMP_DIR)) {
    fs.mkdirSync(TEMP_DIR, { recursive: true });
  }
};

/**
 * Ensure jobs directory exists
 */
export const ensureJobsDir = (): void => {
  if (!fs.existsSync(JOBS_DIR)) {
    fs.mkdirSync(JOBS_DIR, { recursive: true });
  }
};

/**
 * Count frames in a session directory
 */
export const countSessionFrames = (sessionId: string): number => {
  const dir = getSessionDir(sessionId);
  if (!fs.existsSync(dir)) return 0;

  const files = fs.readdirSync(dir);
  return files.filter(f => /^frame\d{6}\.jpg$/.test(f)).length;
};

/**
 * Get session output path
 */
export const getSessionOutputPath = (sessionId: string): string => {
  return path.join(getSessionDir(sessionId), 'output.mp4');
};

/**
 * Check if session has completed output
 */
export const hasSessionOutput = (sessionId: string): boolean => {
  return fs.existsSync(getSessionOutputPath(sessionId));
};

/**
 * Generate a unique job ID
 */
export const generateJobId = (): string => {
  const timestamp = Date.now();
  const random = Math.random().toString(36).substring(2, 10);
  return `job_${timestamp}_${random}`;
};
````

## File: packages/server/workers/ffmpegWorker.ts
````typescript
/**
 * FFmpeg Worker Process
 *
 * Isolated process for running FFmpeg encoding.
 * Runs with memory limits and sends progress via IPC.
 */

import { spawn, ChildProcess } from 'child_process';
import path from 'path';
import fs from 'fs';
import os from 'os';
import { RenderJob, WorkerMessage, MainMessage } from '../types/renderJob.js';
import { TEMP_DIR } from '../utils/index.js';

const WORKER_ID = process.env.WORKER_ID || `worker_${process.pid}`;

// Memory monitoring
const MEMORY_WARNING_THRESHOLD = 1.5 * 1024 * 1024 * 1024; // 1.5GB
const MEMORY_CHECK_INTERVAL = 5000;
const HEARTBEAT_INTERVAL = 5000;

let currentJob: RenderJob | null = null;
let ffmpegProcess: ChildProcess | null = null;
let heartbeatTimer: NodeJS.Timeout | null = null;
let memoryCheckTimer: NodeJS.Timeout | null = null;
let isCancelled = false;

/**
 * Send message to parent process
 */
function sendMessage(msg: Omit<WorkerMessage, 'workerId' | 'timestamp'>): void {
  const fullMessage: WorkerMessage = {
    ...msg,
    workerId: WORKER_ID,
    timestamp: Date.now(),
  };

  if (process.send) {
    process.send(fullMessage);
  }
}

/**
 * Start heartbeat timer
 */
function startHeartbeat(): void {
  if (heartbeatTimer) {
    clearInterval(heartbeatTimer);
  }

  heartbeatTimer = setInterval(() => {
    if (currentJob) {
      sendMessage({
        type: 'HEARTBEAT',
        jobId: currentJob.jobId,
      });
    }
  }, HEARTBEAT_INTERVAL);
}

/**
 * Start memory monitoring
 */
function startMemoryMonitoring(): void {
  if (memoryCheckTimer) {
    clearInterval(memoryCheckTimer);
  }

  memoryCheckTimer = setInterval(() => {
    const usage = process.memoryUsage();
    if (usage.heapUsed > MEMORY_WARNING_THRESHOLD) {
      sendMessage({
        type: 'MEMORY_WARNING',
        jobId: currentJob?.jobId || 'unknown',
        data: {
          memoryUsage: usage.heapUsed,
        },
      });
    }
  }, MEMORY_CHECK_INTERVAL);
}

/**
 * Stop all timers
 */
function stopTimers(): void {
  if (heartbeatTimer) {
    clearInterval(heartbeatTimer);
    heartbeatTimer = null;
  }
  if (memoryCheckTimer) {
    clearInterval(memoryCheckTimer);
    memoryCheckTimer = null;
  }
}

/**
 * Parse FFmpeg progress from stderr
 */
function parseFFmpegProgress(
  line: string,
  totalFrames: number
): { frame: number; speed: string } | null {
  // FFmpeg outputs: frame=  123 fps= 25 q=28.0 size=    1024kB time=00:00:05.00 bitrate=1677.5kbits/s speed=1.5x
  const frameMatch = line.match(/frame=\s*(\d+)/);
  const speedMatch = line.match(/speed=\s*([\d.]+)x/);

  if (frameMatch) {
    return {
      frame: parseInt(frameMatch[1] ?? '0', 10),
      speed: speedMatch ? (speedMatch[1] ?? '0') + 'x' : 'N/A',
    };
  }

  return null;
}

/**
 * Build FFmpeg arguments
 */
function buildFFmpegArgs(job: RenderJob, sessionDir: string): string[] {
  const { config } = job;
  const encoder = config.encoder;

  const inputPattern = path.join(sessionDir, 'frame%06d.jpg');
  const audioPath = path.join(sessionDir, 'audio.mp3');
  const outputPath = path.join(sessionDir, 'output.mp4');

  const args: string[] = [
    '-framerate', String(config.fps),
    '-i', inputPattern,
  ];

  // Add audio if exists
  if (fs.existsSync(audioPath)) {
    args.push('-i', audioPath);
  }

  // Video encoder
  args.push('-c:v', encoder);

  // Encoder-specific settings (aligned with encoderStrategy.ts ENCODING_SPEC)
  const quality = config.quality || 21;
  switch (encoder) {
    case 'h264_nvenc':
      args.push(
        '-preset', 'p4',
        '-rc', 'vbr',
        '-cq', String(quality),
        '-b:v', '8M',
        '-maxrate', '12M',
        '-bufsize', '16M'
      );
      break;
    case 'h264_qsv':
      args.push(
        '-preset', 'medium',
        '-global_quality', String(quality),
        '-look_ahead', '1'
      );
      break;
    case 'h264_amf':
      args.push(
        '-quality', 'quality',
        '-rc', 'vbr_latency',
        '-qp_i', String(quality),
        '-qp_p', String(quality + 2)
      );
      break;
    case 'libx264':
    default: {
      const cpuCount = Math.max(2, os.cpus().length - 2);
      args.push(
        '-preset', 'fast',
        '-crf', String(quality),
        '-tune', 'film',
        '-threads', String(cpuCount)
      );
      break;
    }
  }

  // Color space standardization (BT.709)
  args.push(
    '-colorspace', 'bt709',
    '-color_primaries', 'bt709',
    '-color_trc', 'bt709',
    '-pix_fmt', 'yuv420p'
  );

  // Audio settings
  if (fs.existsSync(audioPath)) {
    args.push(
      '-c:a', 'aac',
      '-b:a', '256k',
      '-shortest'
    );
  }

  // Output settings
  args.push(
    '-movflags', '+faststart',
    '-y',
    outputPath
  );

  return args;
}

/**
 * Process a render job
 */
async function processJob(job: RenderJob): Promise<void> {
  currentJob = job;
  isCancelled = false;

  const sessionDir = path.join(TEMP_DIR, job.sessionId);
  const outputPath = path.join(sessionDir, 'output.mp4');
  const totalFrames = job.frameManifest.totalFrames;

  console.log(`[${WORKER_ID}] Processing job ${job.jobId} (${totalFrames} frames)`);

  sendMessage({
    type: 'STARTED',
    jobId: job.jobId,
  });

  startHeartbeat();
  startMemoryMonitoring();

  try {
    // Validate session directory
    if (!fs.existsSync(sessionDir)) {
      throw new Error(`Session directory not found: ${sessionDir}`);
    }

    // Build FFmpeg command
    const ffmpegArgs = buildFFmpegArgs(job, sessionDir);
    console.log(`[${WORKER_ID}] FFmpeg args:`, ffmpegArgs.join(' '));

    // Spawn FFmpeg
    ffmpegProcess = spawn('ffmpeg', ffmpegArgs, {
      stdio: ['pipe', 'pipe', 'pipe'],
    });

    let lastProgress = 0;

    // Handle stderr (FFmpeg outputs progress here)
    ffmpegProcess.stderr?.on('data', (data: Buffer) => {
      const line = data.toString();

      // Parse progress
      const progress = parseFFmpegProgress(line, totalFrames);
      if (progress) {
        const percent = Math.min(99, Math.round((progress.frame / totalFrames) * 100));

        // Only send progress updates every 1%
        if (percent > lastProgress) {
          lastProgress = percent;
          sendMessage({
            type: 'PROGRESS',
            jobId: job.jobId,
            data: {
              progress: percent,
              currentFrame: progress.frame,
              totalFrames,
              encodingSpeed: progress.speed,
            },
          });
        }
      }

      // Log errors
      if (line.toLowerCase().includes('error')) {
        console.error(`[${WORKER_ID}] FFmpeg error:`, line.trim());
      }
    });

    // Wait for FFmpeg to complete
    await new Promise<void>((resolve, reject) => {
      if (!ffmpegProcess) {
        reject(new Error('FFmpeg process not started'));
        return;
      }

      ffmpegProcess.on('close', (code) => {
        if (isCancelled) {
          reject(new Error('Job cancelled'));
        } else if (code === 0) {
          resolve();
        } else {
          reject(new Error(`FFmpeg exited with code ${code}`));
        }
      });

      ffmpegProcess.on('error', (err) => {
        reject(err);
      });
    });

    // Verify output
    if (!fs.existsSync(outputPath)) {
      throw new Error('Output file not created');
    }

    const stats = fs.statSync(outputPath);
    console.log(
      `[${WORKER_ID}] Job ${job.jobId} complete - output size: ${(stats.size / 1024 / 1024).toFixed(2)}MB`
    );

    sendMessage({
      type: 'COMPLETE',
      jobId: job.jobId,
      data: {
        outputPath,
        outputSize: stats.size,
        progress: 100,
      },
    });
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : 'Unknown error';
    console.error(`[${WORKER_ID}] Job ${job.jobId} failed:`, errorMessage);

    sendMessage({
      type: 'ERROR',
      jobId: job.jobId,
      data: {
        error: errorMessage,
      },
    });
  } finally {
    stopTimers();
    ffmpegProcess = null;
    currentJob = null;
  }
}

/**
 * Cancel current job
 */
function cancelJob(): void {
  if (ffmpegProcess && !isCancelled) {
    console.log(`[${WORKER_ID}] Cancelling job ${currentJob?.jobId}`);
    isCancelled = true;
    ffmpegProcess.kill('SIGTERM');
  }
}

/**
 * Handle shutdown
 */
function shutdown(): void {
  console.log(`[${WORKER_ID}] Shutting down...`);
  stopTimers();
  cancelJob();

  // Give time for cleanup
  setTimeout(() => {
    process.exit(0);
  }, 1000);
}

// Handle messages from parent
process.on('message', (msg: MainMessage) => {
  switch (msg.type) {
    case 'START_JOB':
      if (msg.job) {
        processJob(msg.job).catch((err) => {
          console.error(`[${WORKER_ID}] Unhandled error:`, err);
        });
      }
      break;

    case 'CANCEL_JOB':
      cancelJob();
      break;

    case 'SHUTDOWN':
      shutdown();
      break;
  }
});

// Handle process signals
process.on('SIGTERM', shutdown);
process.on('SIGINT', shutdown);

// Handle uncaught errors
process.on('uncaughtException', (err) => {
  console.error(`[${WORKER_ID}] Uncaught exception:`, err);
  if (currentJob) {
    sendMessage({
      type: 'ERROR',
      jobId: currentJob.jobId,
      data: {
        error: `Uncaught exception: ${err.message}`,
      },
    });
  }
  process.exit(1);
});

process.on('unhandledRejection', (reason) => {
  console.error(`[${WORKER_ID}] Unhandled rejection:`, reason);
});

console.log(`[${WORKER_ID}] Worker started (PID: ${process.pid})`);
````

## File: packages/server/workers/workerPool.ts
````typescript
/**
 * Worker Pool Manager
 *
 * Manages a pool of FFmpeg worker processes for isolated video encoding.
 * Each worker runs in its own process with memory limits.
 */

import { fork, ChildProcess } from 'child_process';
import path from 'path';
import { fileURLToPath } from 'url';
import { createLogger } from '@studio/shared/src/services/logger.js';
import { RenderJob, WorkerMessage, MainMessage } from '../types/renderJob.js';

const log = createLogger('WorkerPool');

const __dirname = path.dirname(fileURLToPath(import.meta.url));

// Configuration
const MAX_WORKERS = 4; // Maximum concurrent worker processes
const WORKER_MEMORY_LIMIT_MB = 8192; // 8GB per worker
const WORKER_RESTART_DELAY_MS = 1000;

export type WorkerEventCallback = (message: WorkerMessage) => void;

interface WorkerInfo {
  process: ChildProcess;
  workerId: string;
  currentJobId: string | null;
  startedAt: number;
  memoryUsage: number;
  isHealthy: boolean;
}

export class WorkerPool {
  private workers: Map<string, WorkerInfo> = new Map();
  private pendingJobs: RenderJob[] = [];
  private onWorkerMessage: WorkerEventCallback | null = null;
  private workerScript: string;
  private isShuttingDown = false;

  constructor() {
    // Worker script path (TypeScript, executed via tsx)
    this.workerScript = path.join(__dirname, 'ffmpegWorker.ts');
  }

  /**
   * Set callback for worker messages
   */
  setMessageHandler(handler: WorkerEventCallback): void {
    this.onWorkerMessage = handler;
  }

  /**
   * Initialize the worker pool
   */
  async initialize(): Promise<void> {
    log.info(`Worker pool initializing (max ${MAX_WORKERS} workers, ${WORKER_MEMORY_LIMIT_MB}MB limit each)`);

    // Pre-spawn one worker for faster first job
    await this.spawnWorker();
  }

  /**
   * Spawn a new worker process
   */
  private async spawnWorker(): Promise<WorkerInfo | null> {
    if (this.isShuttingDown) return null;
    if (this.workers.size >= MAX_WORKERS) {
      log.debug('Worker pool at capacity');
      return null;
    }

    const workerId = `worker_${Date.now()}_${Math.random().toString(36).substring(2, 6)}`;

    try {
      const worker = fork(this.workerScript, [], {
        execArgv: ['--import', 'tsx', `--max-old-space-size=${WORKER_MEMORY_LIMIT_MB}`],
        env: {
          ...process.env,
          WORKER_ID: workerId,
        },
        stdio: ['pipe', 'pipe', 'pipe', 'ipc'],
      });

      const workerInfo: WorkerInfo = {
        process: worker,
        workerId,
        currentJobId: null,
        startedAt: Date.now(),
        memoryUsage: 0,
        isHealthy: true,
      };

      // Handle worker messages
      worker.on('message', (msg: WorkerMessage) => {
        this.handleWorkerMessage(workerId, msg);
      });

      // Handle worker stdout/stderr
      worker.stdout?.on('data', (data) => {
        log.debug(`[${workerId}] ${data.toString().trim()}`);
      });

      worker.stderr?.on('data', (data) => {
        log.warn(`[${workerId}] stderr: ${data.toString().trim()}`);
      });

      // Handle worker exit
      worker.on('exit', (code, signal) => {
        this.handleWorkerExit(workerId, code, signal);
      });

      // Handle worker errors
      worker.on('error', (err) => {
        log.error(`Worker ${workerId} error:`, err);
        workerInfo.isHealthy = false;
      });

      this.workers.set(workerId, workerInfo);
      log.info(`Spawned worker ${workerId} (${this.workers.size}/${MAX_WORKERS})`);

      return workerInfo;
    } catch (error) {
      log.error('Failed to spawn worker:', error);
      return null;
    }
  }

  /**
   * Handle messages from workers
   */
  private handleWorkerMessage(workerId: string, msg: WorkerMessage): void {
    const worker = this.workers.get(workerId);
    if (!worker) return;

    // Update worker state
    switch (msg.type) {
      case 'STARTED':
        worker.currentJobId = msg.jobId;
        break;

      case 'PROGRESS':
      case 'HEARTBEAT':
        // Worker is alive
        break;

      case 'MEMORY_WARNING':
        if (msg.data?.memoryUsage) {
          worker.memoryUsage = msg.data.memoryUsage;
          log.warn(
            `Worker ${workerId} memory warning: ${Math.round(worker.memoryUsage / 1024 / 1024)}MB`
          );
        }
        break;

      case 'COMPLETE':
        worker.currentJobId = null;
        // Worker is now free for next job
        this.processNextPendingJob();
        break;

      case 'ERROR':
        worker.currentJobId = null;
        worker.isHealthy = false;
        // Worker may need restart
        this.processNextPendingJob();
        break;
    }

    // Forward to external handler
    if (this.onWorkerMessage) {
      this.onWorkerMessage(msg);
    }
  }

  /**
   * Handle worker process exit
   */
  private handleWorkerExit(
    workerId: string,
    code: number | null,
    signal: NodeJS.Signals | null
  ): void {
    const worker = this.workers.get(workerId);
    const jobId = worker?.currentJobId;

    this.workers.delete(workerId);
    log.warn(
      `Worker ${workerId} exited (code=${code}, signal=${signal}), ${this.workers.size} workers remaining`
    );

    // If worker was processing a job, notify error
    if (jobId && this.onWorkerMessage) {
      this.onWorkerMessage({
        type: 'ERROR',
        jobId,
        workerId,
        timestamp: Date.now(),
        data: {
          error: `Worker process terminated unexpectedly (code=${code}, signal=${signal})`,
        },
      });
    }

    // Restart worker if not shutting down
    if (!this.isShuttingDown) {
      setTimeout(() => {
        this.spawnWorker().then(() => {
          this.processNextPendingJob();
        });
      }, WORKER_RESTART_DELAY_MS);
    }
  }

  /**
   * Submit a job to an available worker
   */
  async submitJob(job: RenderJob): Promise<boolean> {
    // Find available worker
    let availableWorker: WorkerInfo | null = null;

    for (const worker of this.workers.values()) {
      if (!worker.currentJobId && worker.isHealthy) {
        availableWorker = worker;
        break;
      }
    }

    // Spawn new worker if needed
    if (!availableWorker && this.workers.size < MAX_WORKERS) {
      availableWorker = await this.spawnWorker();
    }

    // If no worker available, queue the job
    if (!availableWorker) {
      this.pendingJobs.push(job);
      log.debug(`Job ${job.jobId} queued - no workers available`);
      return true;
    }

    // Send job to worker
    return this.sendJobToWorker(availableWorker, job);
  }

  /**
   * Send a job to a specific worker
   */
  private sendJobToWorker(worker: WorkerInfo, job: RenderJob): boolean {
    const message: MainMessage = {
      type: 'START_JOB',
      job,
    };

    try {
      worker.process.send(message);
      worker.currentJobId = job.jobId;
      log.info(`Job ${job.jobId} sent to worker ${worker.workerId}`);
      return true;
    } catch (error) {
      log.error(`Failed to send job to worker ${worker.workerId}:`, error);
      worker.isHealthy = false;
      return false;
    }
  }

  /**
   * Process the next pending job if a worker is available
   */
  private processNextPendingJob(): void {
    if (this.pendingJobs.length === 0) return;

    // Find available worker
    for (const worker of this.workers.values()) {
      if (!worker.currentJobId && worker.isHealthy) {
        const job = this.pendingJobs.shift();
        if (job) {
          this.sendJobToWorker(worker, job);
        }
        break;
      }
    }
  }

  /**
   * Cancel a job
   */
  cancelJob(jobId: string): boolean {
    // Remove from pending queue
    const pendingIndex = this.pendingJobs.findIndex((j) => j.jobId === jobId);
    if (pendingIndex !== -1) {
      this.pendingJobs.splice(pendingIndex, 1);
      return true;
    }

    // Find worker processing this job
    for (const worker of this.workers.values()) {
      if (worker.currentJobId === jobId) {
        const message: MainMessage = { type: 'CANCEL_JOB' };
        try {
          worker.process.send(message);
          return true;
        } catch {
          return false;
        }
      }
    }

    return false;
  }

  /**
   * Get pool statistics
   */
  getStats(): {
    totalWorkers: number;
    activeWorkers: number;
    idleWorkers: number;
    pendingJobs: number;
    unhealthyWorkers: number;
  } {
    let active = 0;
    let idle = 0;
    let unhealthy = 0;

    for (const worker of this.workers.values()) {
      if (!worker.isHealthy) {
        unhealthy++;
      } else if (worker.currentJobId) {
        active++;
      } else {
        idle++;
      }
    }

    return {
      totalWorkers: this.workers.size,
      activeWorkers: active,
      idleWorkers: idle,
      pendingJobs: this.pendingJobs.length,
      unhealthyWorkers: unhealthy,
    };
  }

  /**
   * Shutdown the worker pool
   */
  async shutdown(): Promise<void> {
    this.isShuttingDown = true;
    log.info('Shutting down worker pool...');

    const shutdownPromises: Promise<void>[] = [];

    for (const worker of this.workers.values()) {
      shutdownPromises.push(
        new Promise((resolve) => {
          // Send shutdown message
          try {
            const message: MainMessage = { type: 'SHUTDOWN' };
            worker.process.send(message);
          } catch {
            // Ignore send errors
          }

          // Give worker time to gracefully exit
          const timeout = setTimeout(() => {
            worker.process.kill('SIGTERM');
            resolve();
          }, 5000);

          worker.process.once('exit', () => {
            clearTimeout(timeout);
            resolve();
          });
        })
      );
    }

    await Promise.all(shutdownPromises);
    this.workers.clear();
    this.pendingJobs = [];
    log.info('Worker pool shutdown complete');
  }
}

// Export singleton instance
export const workerPool = new WorkerPool();
````

## File: packages/shared/src/constants/index.ts
````typescript
/**
 * Central export point for all application constants.
 * Import from this file for convenient access to all constants.
 *
 * @example
 * import { ART_STYLES, LANGUAGES, VIDEO_PURPOSES } from './constants';
 */

// Style constants
export {
  ART_STYLES,
  IMAGE_STYLE_MODIFIERS,
  VIDEO_STYLE_MODIFIERS,
  type ArtStyle,
} from "./styles";

// Language constants
export { LANGUAGES, CONTENT_LANGUAGES, getLanguageName, type Language, type LanguageOption, type LanguageCode } from "./languages";

// Video constants
export {
  VIDEO_PURPOSES,
  CAMERA_ANGLES,
  LIGHTING_MOODS,
  DEFAULT_NEGATIVE_CONSTRAINTS,
  type VideoPurposeOption,
  type VideoPurpose,
  type CameraAngle,
  type LightingMood,
  type NegativeConstraint,
} from "./video";
````

## File: packages/shared/src/constants/languages.ts
````typescript
/**
 * Supported languages for content generation and subtitles.
 */
export interface LanguageOption {
  code: string;
  label: string;
  nativeLabel: string;
  direction: "ltr" | "rtl";
}

export const CONTENT_LANGUAGES: LanguageOption[] = [
  { code: "auto", label: "Auto-detect", nativeLabel: "Auto", direction: "ltr" },
  { code: "en", label: "English", nativeLabel: "English", direction: "ltr" },
  { code: "ar", label: "Arabic", nativeLabel: "العربية", direction: "rtl" },
  { code: "es", label: "Spanish", nativeLabel: "Español", direction: "ltr" },
  { code: "fr", label: "French", nativeLabel: "Français", direction: "ltr" },
  { code: "de", label: "German", nativeLabel: "Deutsch", direction: "ltr" },
  { code: "pt", label: "Portuguese", nativeLabel: "Português", direction: "ltr" },
  { code: "ru", label: "Russian", nativeLabel: "Русский", direction: "ltr" },
  { code: "zh", label: "Chinese", nativeLabel: "中文", direction: "ltr" },
  { code: "ja", label: "Japanese", nativeLabel: "日本語", direction: "ltr" },
  { code: "ko", label: "Korean", nativeLabel: "한국어", direction: "ltr" },
  { code: "hi", label: "Hindi", nativeLabel: "हिन्दी", direction: "ltr" },
  { code: "tr", label: "Turkish", nativeLabel: "Türkçe", direction: "ltr" },
  { code: "it", label: "Italian", nativeLabel: "Italiano", direction: "ltr" },
  { code: "nl", label: "Dutch", nativeLabel: "Nederlands", direction: "ltr" },
  { code: "pl", label: "Polish", nativeLabel: "Polski", direction: "ltr" },
  { code: "id", label: "Indonesian", nativeLabel: "Bahasa Indonesia", direction: "ltr" },
  { code: "th", label: "Thai", nativeLabel: "ไทย", direction: "ltr" },
  { code: "vi", label: "Vietnamese", nativeLabel: "Tiếng Việt", direction: "ltr" },
  { code: "ur", label: "Urdu", nativeLabel: "اردو", direction: "rtl" },
  { code: "fa", label: "Persian", nativeLabel: "فارسی", direction: "rtl" },
  { code: "he", label: "Hebrew", nativeLabel: "עברית", direction: "rtl" },
];

export type LanguageCode = typeof CONTENT_LANGUAGES[number]["code"];

/**
 * Legacy: Supported languages for subtitle translation.
 * @deprecated Use CONTENT_LANGUAGES instead
 */
export const LANGUAGES = [
  "Spanish",
  "French",
  "German",
  "Japanese",
  "Korean",
  "Chinese",
  "Hindi",
  "Italian",
  "Portuguese",
  "English",
  "Arabic",
] as const;

export type Language = (typeof LANGUAGES)[number];

/**
 * Get the full language name from a language code.
 * Used for explicit language directives in AI prompts.
 */
export function getLanguageName(code: LanguageCode): string {
  const lang = CONTENT_LANGUAGES.find(l => l.code === code);
  return lang?.label || "the same language as the input";
}
````

## File: packages/shared/src/constants/layout.ts
````typescript
/**
 * Layout Constants
 * Defines zone-based layout system for video rendering
 * Ensures clear visual hierarchy with non-overlapping elements
 */

import { LayoutConfig } from "../types";

/**
 * Layout presets for different video orientations
 * Each zone uses normalized coordinates (0-1) for responsive scaling
 */
export const LAYOUT_PRESETS: Record<string, LayoutConfig> = {
    landscape: {
        orientation: "landscape",
        zones: {
            background: {
                name: "background",
                x: 0,
                y: 0,
                width: 1,
                height: 1,
                zIndex: 0,
            },
            visualizer: {
                name: "visualizer",
                x: 0,
                y: 0.75,
                width: 1,
                height: 0.25,
                zIndex: 1,
            },
            text: {
                name: "text",
                x: 0.05,
                y: 0.72,
                width: 0.9,
                height: 0.22,
                zIndex: 10,
            },
            translation: {
                name: "translation",
                x: 0.1,
                y: 0.92,
                width: 0.8,
                height: 0.06,
                zIndex: 11,
            },
        },
    },
    portrait: {
        orientation: "portrait",
        zones: {
            background: {
                name: "background",
                x: 0,
                y: 0,
                width: 1,
                height: 1,
                zIndex: 0,
            },
            visualizer: {
                name: "visualizer",
                x: 0,
                y: 0.85,
                width: 1,
                height: 0.15,
                zIndex: 1,
            },
            text: {
                name: "text",
                x: 0.05,
                y: 0.70,
                width: 0.9,
                height: 0.20,
                zIndex: 10,
            },
            translation: {
                name: "translation",
                x: 0.05,
                y: 0.88,
                width: 0.9,
                height: 0.08,
                zIndex: 11,
            },
        },
    },
};

/**
 * Get layout preset for a given orientation
 */
export function getLayoutPreset(orientation: "landscape" | "portrait"): LayoutConfig {
    const preset = LAYOUT_PRESETS[orientation];
    if (!preset) return LAYOUT_PRESETS["landscape"]!; // Fallback
    return preset;
}

/**
 * Default visualizer configuration
 * Provides balanced settings for most use cases
 */
export const DEFAULT_VISUALIZER_CONFIG = {
    enabled: true,
    opacity: 0.15,
    maxHeightRatio: 0.25,
    zIndex: 1,
    barWidth: 3,
    barGap: 2,
    colorScheme: "cyan-purple" as const,
};

/**
 * Default text animation configuration
 * Provides smooth directional reveal effects
 */
export const DEFAULT_TEXT_ANIMATION_CONFIG = {
    revealDirection: "ltr" as const,
    revealDuration: 0.3, // 300ms per word
    wordReveal: true,
};
````

## File: packages/shared/src/constants/styles.ts
````typescript
/**
 * Art styles available for image and video generation.
 * Used in the sidebar style selector and passed to generation services.
 */
export const ART_STYLES = [
  "Cinematic",
  "Anime / Manga",
  "Cyberpunk",
  "Watercolor",
  "Oil Painting",
  "Pixel Art",
  "Surrealist",
  "Dark Fantasy",
  "Commercial / Ad",
  "Minimalist / Tutorial",
  "Comic Book",
  "Corporate / Brand",
  "Photorealistic",
] as const;

export type ArtStyle = (typeof ART_STYLES)[number];

/**
 * Style modifiers for image generation.
 * These are appended to prompts to achieve the desired visual style.
 */
export const IMAGE_STYLE_MODIFIERS: Record<string, string> = {
  Cinematic:
    "Cinematic movie still, 35mm film grain, anamorphic lens flare, hyper-realistic, dramatic lighting, 8k resolution",
  "Anime / Manga":
    "High-quality Anime style, Studio Ghibli aesthetic, vibrant colors, detailed backgrounds, cel shaded, expressive",
  Cyberpunk:
    "Futuristic cyberpunk city style, neon lights, rain-slicked streets, high contrast, blade runner vibe, technological",
  Watercolor:
    "Soft watercolor painting, artistic brush strokes, paper texture, bleeding colors, dreamy atmosphere",
  "Oil Painting":
    "Classic oil painting, thick impasto, visible brushwork, texture, rich colors, classical composition",
  "Pixel Art":
    "High quality pixel art, 16-bit retro game style, dithering, vibrant colors",
  Surrealist:
    "Surrealist art style, dreamlike, Dali-esque, impossible geometry, symbolic, mysterious",
  "Dark Fantasy":
    "Dark fantasy art, grimdark, gothic atmosphere, misty, detailed textures, eldritch",
  "Commercial / Ad":
    "Professional product photography, studio lighting, clean background, macro details, commercial aesthetic, 4k, sharp focus, advertising standard",
  "Minimalist / Tutorial":
    "Clean vector illustration, flat design, isometric perspective, white background, educational style, clear visibility, infographic aesthetic",
  "Comic Book":
    "American comic book style, dynamic action lines, bold ink outlines, halftone patterns, vibrant superhero colors, expressive",
  "Corporate / Brand":
    "Modern corporate memphis style, flat vector, clean lines, professional, trustworthy, blue and white color palette, tech startup aesthetic",
  Photorealistic:
    "Raw photo, hyper-realistic, DSLR, 50mm lens, depth of field, natural lighting, unedited footage style",
};

/**
 * Style modifiers for video generation.
 * These are optimized for motion and animation effects.
 */
export const VIDEO_STYLE_MODIFIERS: Record<string, string> = {
  Cinematic:
    "Cinematic movie shot, slow camera movement, 35mm film grain, hyper-realistic, dramatic lighting, 8k resolution",
  "Anime / Manga":
    "High-quality Anime animation, Studio Ghibli style, moving clouds, wind effects, vibrant colors",
  Cyberpunk:
    "Futuristic cyberpunk city, neon lights flickering, rain falling, flying cars, high contrast",
  Watercolor:
    "Animated watercolor painting, flowing paint, artistic brush strokes, paper texture, bleeding colors",
  "Oil Painting":
    "Living oil painting, shifting textures, visible brushwork, classical composition",
  "Pixel Art":
    "Animated pixel art, 16-bit retro game loop, dithering, vibrant colors",
  Surrealist:
    "Surrealist dreamscape, morphing shapes, impossible geometry, mysterious atmosphere",
  "Dark Fantasy":
    "Dark fantasy atmosphere, rolling fog, flickering torches, grimdark, detailed textures",
  "Commercial / Ad":
    "Professional product b-roll, smooth slider shot, studio lighting, clean background, 4k",
  "Minimalist / Tutorial":
    "Clean motion graphics, animated vector illustration, flat design, smooth transitions",
  "Comic Book":
    "Motion comic style, dynamic action, bold ink outlines, halftone patterns",
  "Corporate / Brand":
    "Modern corporate motion graphics, kinetic typography background, clean lines, professional",
  Photorealistic:
    "Raw video footage, handheld camera, natural lighting, unedited style",
};
````

## File: packages/shared/src/constants/video.ts
````typescript
/**
 * Video purpose options for the sidebar selector.
 * Each purpose affects visual style and pacing of generated content.
 */
export interface VideoPurposeOption {
  value: string;
  label: string;
  description: string;
  icon?: string;
}

export const VIDEO_PURPOSES: VideoPurposeOption[] = [
  {
    value: "music_video",
    label: "Music Video",
    description: "Cinematic, emotional, dramatic scenes",
    icon: "🎵",
  },
  {
    value: "social_short",
    label: "Social Short",
    description: "TikTok/Reels - bold, fast-paced",
    icon: "📱",
  },
  {
    value: "documentary",
    label: "Documentary",
    description: "Realistic, informative visuals",
    icon: "🎬",
  },
  {
    value: "commercial",
    label: "Commercial/Ad",
    description: "Clean, product-focused, persuasive",
    icon: "📺",
  },
  {
    value: "podcast_visual",
    label: "Podcast Visual",
    description: "Ambient, non-distracting backgrounds",
    icon: "🎙️",
  },
  {
    value: "lyric_video",
    label: "Lyric Video",
    description: "Space for text overlays",
    icon: "🎤",
  },
  {
    value: "storytelling",
    label: "Storytelling",
    description: "Narrative-driven, folklore, tales",
    icon: "📖",
  },
  {
    value: "educational",
    label: "Educational",
    description: "Clear explanations, diagrams, tutorials",
    icon: "🎓",
  },
  {
    value: "horror_mystery",
    label: "Horror/Mystery",
    description: "Dark, suspenseful, atmospheric",
    icon: "👻",
  },
  {
    value: "travel",
    label: "Travel/Nature",
    description: "Scenic landscapes, exploration",
    icon: "🌍",
  },
  {
    value: "motivational",
    label: "Motivational",
    description: "Inspiring, uplifting, empowering",
    icon: "💪",
  },
  {
    value: "news_report",
    label: "News Report",
    description: "Factual, journalistic, current events",
    icon: "📰",
  },
];

/**
 * Video purpose type for type-safe usage.
 */
export type VideoPurpose =
  | "music_video"
  | "social_short"
  | "documentary"
  | "commercial"
  | "podcast_visual"
  | "lyric_video"
  | "storytelling"
  | "educational"
  | "horror_mystery"
  | "travel"
  | "motivational"
  | "news_report"
  // Story Mode genre-specific purposes
  | "story_drama"
  | "story_comedy"
  | "story_thriller"
  | "story_scifi"
  | "story_action"
  | "story_fantasy"
  | "story_romance"
  | "story_historical"
  | "story_animation";

/**
 * Camera angles for visual variety in prompt generation.
 * Used to ensure diverse compositions across scenes.
 */
export const CAMERA_ANGLES = [
  "wide establishing shot",
  "medium shot",
  "close-up",
  "extreme close-up on details",
  "low angle looking up",
  "high angle looking down",
  "over-the-shoulder",
  "dutch angle",
  "tracking shot",
  "aerial/drone view",
] as const;

export type CameraAngle = (typeof CAMERA_ANGLES)[number];

/**
 * Lighting moods for emotional progression in scenes.
 * Used to create visual variety and emotional arc.
 */
export const LIGHTING_MOODS = [
  "golden hour warm lighting",
  "cool blue moonlight",
  "dramatic chiaroscuro shadows",
  "soft diffused overcast",
  "neon-lit urban glow",
  "harsh midday sun",
  "candlelit intimate warmth",
  "silhouette backlighting",
  "foggy atmospheric haze",
  "studio three-point lighting",
] as const;

export type LightingMood = (typeof LIGHTING_MOODS)[number];

/**
 * Default negative constraints for image/video generation.
 * These are appended to prompts to avoid common generation issues.
 * Enhanced with aggressive artifact filtering for cinematic quality.
 */
export const DEFAULT_NEGATIVE_CONSTRAINTS = [
  "no text",
  "no subtitles",
  "no watermark",
  "no logo",
  "no brand names",
  "no split-screen",
  "no collage",
  "no UI elements",
  "no distorted anatomy",
  "no extra limbs",
  "no deformed hands",
  "no blurry face",
  "no melted faces",
  // Enhanced quality filters
  "no blurry backgrounds",
  "no low resolution",
  "no jpeg artifacts",
  "no cartoon style (unless specified)",
  "no illustration style (unless specified)",
  "no generic stock photo look",
  "no dull lighting",
  "no flat composition",
] as const;

export type NegativeConstraint = (typeof DEFAULT_NEGATIVE_CONSTRAINTS)[number];
````

## File: packages/shared/src/constants/visualStyles.ts
````typescript
/**
 * Visual Styles for Story Mode
 *
 * Comprehensive style definitions with prompt modifiers for image generation.
 * Each style includes aesthetic descriptions and generation parameters.
 */

export interface VisualStyle {
    id: string;
    name: string;
    description: string;
    promptSuffix: string;
    negativePrompt: string;
    aspectRatios: ('16:9' | '4:3' | '2.35:1' | '9:16' | '1:1')[];
    sampleImage?: string;
    category: 'cinematic' | 'artistic' | 'stylized' | 'modern';
}

export const VISUAL_STYLES: Record<string, VisualStyle> = {
    CINEMATIC: {
        id: 'CINEMATIC',
        name: 'Cinematic',
        description: 'Film-quality realism with dramatic lighting',
        promptSuffix: ', cinematic lighting, film grain, 35mm lens, dramatic shadows, color graded, anamorphic lens flare, movie still, 8k resolution',
        negativePrompt: 'cartoon, illustration, 3d render, anime, low quality, blurry',
        aspectRatios: ['16:9', '2.35:1', '4:3'],
        sampleImage: '/samples/cinematic.jpg',
        category: 'cinematic',
    },

    NOIR: {
        id: 'NOIR',
        name: 'Film Noir',
        description: 'High-contrast black and white with dramatic shadows',
        promptSuffix: ', film noir, black and white, high contrast, dramatic shadows, venetian blind lighting, 1940s detective movie aesthetic, rain-soaked streets, moody atmosphere',
        negativePrompt: 'color, bright, cheerful, cartoon, anime, low quality',
        aspectRatios: ['16:9', '4:3', '2.35:1'],
        sampleImage: '/samples/noir.jpg',
        category: 'cinematic',
    },

    COMIC: {
        id: 'COMIC',
        name: 'Comic Book',
        description: 'Bold ink outlines with halftone patterns',
        promptSuffix: ', comic book style, bold ink outlines, halftone patterns, vibrant superhero colors, dynamic action pose, speech bubble ready, Marvel/DC aesthetic',
        negativePrompt: 'photorealistic, 3d render, soft edges, blurry, muted colors',
        aspectRatios: ['16:9', '4:3', '1:1'],
        sampleImage: '/samples/comic.jpg',
        category: 'stylized',
    },

    ANIME: {
        id: 'ANIME',
        name: 'Anime',
        description: 'Japanese animation style with expressive characters',
        promptSuffix: ', anime style, Studio Ghibli aesthetic, cel shaded, vibrant colors, detailed backgrounds, expressive eyes, Japanese animation, key visual quality',
        negativePrompt: 'photorealistic, western cartoon, 3d render, low quality, sketch',
        aspectRatios: ['16:9', '4:3'],
        sampleImage: '/samples/anime.jpg',
        category: 'stylized',
    },

    WATERCOLOR: {
        id: 'WATERCOLOR',
        name: 'Watercolor',
        description: 'Soft brushstrokes with bleeding colors',
        promptSuffix: ', watercolor painting, soft brushstrokes, paper texture, bleeding colors, dreamy atmosphere, artistic, impressionistic, delicate',
        negativePrompt: 'photorealistic, sharp edges, digital, 3d render, harsh lighting',
        aspectRatios: ['16:9', '4:3', '1:1'],
        sampleImage: '/samples/watercolor.jpg',
        category: 'artistic',
    },

    OIL_PAINTING: {
        id: 'OIL_PAINTING',
        name: 'Oil Painting',
        description: 'Classical texture with visible brushwork',
        promptSuffix: ', oil painting, thick impasto, visible brushwork, rich colors, classical composition, museum quality, canvas texture, Rembrandt lighting',
        negativePrompt: 'digital art, smooth, photorealistic, flat colors, anime',
        aspectRatios: ['16:9', '4:3', '1:1'],
        sampleImage: '/samples/oil-painting.jpg',
        category: 'artistic',
    },

    CYBERPUNK: {
        id: 'CYBERPUNK',
        name: 'Cyberpunk',
        description: 'Neon-lit futuristic cityscapes',
        promptSuffix: ', cyberpunk style, neon lights, rain-slicked streets, holographic ads, futuristic technology, Blade Runner aesthetic, high contrast, purple and cyan',
        negativePrompt: 'natural, pastoral, historical, bright daylight, cartoon',
        aspectRatios: ['16:9', '2.35:1'],
        sampleImage: '/samples/cyberpunk.jpg',
        category: 'modern',
    },

    DARK_FANTASY: {
        id: 'DARK_FANTASY',
        name: 'Dark Fantasy',
        description: 'Gothic atmosphere with eldritch elements',
        promptSuffix: ', dark fantasy, grimdark, gothic atmosphere, misty, detailed textures, eldritch horror, medieval fantasy, dramatic lighting, moody',
        negativePrompt: 'bright, cheerful, modern, cartoon, anime, low quality',
        aspectRatios: ['16:9', '4:3', '2.35:1'],
        sampleImage: '/samples/dark-fantasy.jpg',
        category: 'stylized',
    },

    PHOTOREALISTIC: {
        id: 'PHOTOREALISTIC',
        name: 'Photorealistic',
        description: 'Raw photography with natural lighting',
        promptSuffix: ', raw photo, hyperrealistic, DSLR, 50mm lens, natural lighting, depth of field, unedited, documentary style, National Geographic quality',
        negativePrompt: 'illustration, painting, cartoon, anime, artistic, stylized',
        aspectRatios: ['16:9', '4:3', '2.35:1'],
        sampleImage: '/samples/photorealistic.jpg',
        category: 'cinematic',
    },

    PIXEL_ART: {
        id: 'PIXEL_ART',
        name: 'Pixel Art',
        description: 'Retro 16-bit game aesthetic',
        promptSuffix: ', pixel art, 16-bit, retro game style, dithering, vibrant colors, nostalgic, SNES aesthetic, limited color palette',
        negativePrompt: 'photorealistic, smooth, high resolution, 3d render, modern',
        aspectRatios: ['16:9', '4:3', '1:1'],
        sampleImage: '/samples/pixel-art.jpg',
        category: 'stylized',
    },

    MINIMALIST: {
        id: 'MINIMALIST',
        name: 'Minimalist',
        description: 'Clean vectors with flat design',
        promptSuffix: ', minimalist, flat design, vector illustration, clean lines, limited color palette, modern, geometric, professional, simple shapes',
        negativePrompt: 'detailed, photorealistic, complex, busy, textured, realistic',
        aspectRatios: ['16:9', '1:1', '4:3'],
        sampleImage: '/samples/minimalist.jpg',
        category: 'modern',
    },

    SURREALIST: {
        id: 'SURREALIST',
        name: 'Surrealist',
        description: 'Dreamlike imagery with impossible geometry',
        promptSuffix: ', surrealist art, dreamlike, Salvador Dali style, impossible geometry, melting objects, symbolic imagery, mysterious atmosphere, subconscious',
        negativePrompt: 'realistic, normal, mundane, photographic, simple',
        aspectRatios: ['16:9', '4:3', '1:1'],
        sampleImage: '/samples/surrealist.jpg',
        category: 'artistic',
    },
} as const;

export type VisualStyleKey = keyof typeof VISUAL_STYLES;

/**
 * Get all visual styles as array
 */
export function getVisualStylesArray(): VisualStyle[] {
    return Object.values(VISUAL_STYLES);
}

/**
 * Get styles by category
 */
export function getStylesByCategory(category: VisualStyle['category']): VisualStyle[] {
    return Object.values(VISUAL_STYLES).filter(style => style.category === category);
}

/**
 * Get prompt suffix for a style
 */
export function getStylePromptSuffix(styleId: string): string {
    const style = VISUAL_STYLES[styleId as VisualStyleKey];
    return style?.promptSuffix || '';
}

/**
 * Get negative prompt for a style
 */
export function getStyleNegativePrompt(styleId: string): string {
    const style = VISUAL_STYLES[styleId as VisualStyleKey];
    return style?.negativePrompt || '';
}

/**
 * Available aspect ratios
 */
export const ASPECT_RATIOS = [
    { id: '16:9', label: '16:9', description: 'Standard widescreen' },
    { id: '4:3', label: '4:3', description: 'Classic TV format' },
    { id: '2.35:1', label: '2.35:1', description: 'Cinematic widescreen' },
    { id: '9:16', label: '9:16', description: 'Vertical/Mobile' },
    { id: '1:1', label: '1:1', description: 'Square' },
] as const;

export type AspectRatioId = (typeof ASPECT_RATIOS)[number]['id'];
````

## File: packages/shared/src/lib/cinematicMotion.ts
````typescript
/**
 * Motion Library — "Precision Protocol"
 * Fast, snappy Framer Motion variants.
 * duration-200 ease-out — no slow, dramatic fades.
 */

import type { Variants, Transition } from "framer-motion";

// ==========================================
// CORE EASING CURVES
// ==========================================

export const cinematicEasing = {
  /** Default snappy ease-out */
  cinematic: [0.0, 0.0, 0.2, 1] as const,
  /** Quick deceleration */
  dramatic: [0.0, 0.0, 0.2, 1] as const,
  /** Sharp curtain */
  curtain: [0.4, 0, 0.2, 1] as const,
  /** Smooth utility */
  smooth: [0.4, 0, 0.2, 1] as const,
};

// ==========================================
// PAGE & SECTION TRANSITIONS
// ==========================================

/** Quick fade-in */
export const fadeFromBlack: Variants = {
  initial: { opacity: 0 },
  animate: {
    opacity: 1,
    transition: { duration: 0.2, ease: cinematicEasing.cinematic },
  },
  exit: {
    opacity: 0,
    transition: { duration: 0.15 },
  },
};

/** Curtain rise — fast vertical reveal */
export const curtainRise: Variants = {
  initial: { y: 24, opacity: 0 },
  animate: {
    y: 0,
    opacity: 1,
    transition: { duration: 0.2, ease: cinematicEasing.curtain },
  },
  exit: {
    y: -12,
    opacity: 0,
    transition: { duration: 0.15 },
  },
};

/** Spotlight reveal — fast scale */
export const spotlightReveal: Variants = {
  initial: { scale: 0.98, opacity: 0 },
  animate: {
    scale: 1,
    opacity: 1,
    transition: { duration: 0.2, ease: cinematicEasing.dramatic },
  },
  exit: {
    scale: 1.01,
    opacity: 0,
    transition: { duration: 0.15 },
  },
};

/** Step transition — horizontal slide */
export const stepTransition: Variants = {
  initial: { opacity: 0, x: 32 },
  animate: {
    opacity: 1,
    x: 0,
    transition: { duration: 0.2, ease: cinematicEasing.cinematic },
  },
  exit: {
    opacity: 0,
    x: -32,
    transition: { duration: 0.15 },
  },
};

// ==========================================
// LIST & GRID ANIMATIONS
// ==========================================

/** Stagger container */
export const staggerContainer: Variants = {
  initial: {},
  animate: {
    transition: {
      staggerChildren: 0.04,
      delayChildren: 0.05,
    },
  },
  exit: {
    transition: {
      staggerChildren: 0.03,
      staggerDirection: -1,
    },
  },
};

/** Stagger item */
export const staggerItem: Variants = {
  initial: { opacity: 0, y: 8 },
  animate: {
    opacity: 1,
    y: 0,
    transition: { duration: 0.2, ease: cinematicEasing.dramatic },
  },
  exit: {
    opacity: 0,
    y: -4,
    transition: { duration: 0.1 },
  },
};

/** Card flip animation */
export const cardFlip: Variants = {
  initial: { rotateY: -90, opacity: 0, transformPerspective: 1200 },
  animate: {
    rotateY: 0,
    opacity: 1,
    transition: { duration: 0.25, ease: cinematicEasing.dramatic },
  },
  exit: {
    rotateY: 90,
    opacity: 0,
    transition: { duration: 0.15 },
  },
};

// ==========================================
// ROUTE & PAGE TRANSITIONS
// ==========================================

/** Route transition */
export const routeTransition: Variants = {
  initial: { opacity: 0, y: 8 },
  animate: {
    opacity: 1,
    y: 0,
    transition: { duration: 0.2, ease: cinematicEasing.cinematic },
  },
  exit: {
    opacity: 0,
    y: -4,
    transition: { duration: 0.15 },
  },
};

// ==========================================
// INTERACTIVE ELEMENTS
// ==========================================

export const cardHover = {
  whileHover: {
    scale: 1.01,
    y: -2,
    transition: { duration: 0.15, ease: cinematicEasing.smooth },
  },
  whileTap: { scale: 0.99, transition: { duration: 0.05 } },
};

export const cardLift = {
  whileHover: {
    y: -3,
    transition: { duration: 0.15, ease: cinematicEasing.smooth },
  },
  whileTap: { scale: 0.99, transition: { duration: 0.05 } },
};

export const buttonPress = {
  whileHover: { scale: 1.01, transition: { duration: 0.1 } },
  whileTap: { scale: 0.98, transition: { duration: 0.05 } },
};

export const glowHover = {
  whileHover: {
    boxShadow: "0 0 16px rgba(59,130,246,0.3)",
    transition: { duration: 0.15 },
  },
};

// ==========================================
// LOADING ANIMATIONS
// ==========================================

export const filmReelSpin: Variants = {
  animate: {
    rotate: 360,
    transition: { duration: 1.5, repeat: Infinity, ease: "linear" },
  },
};

export const dualReelSpin = {
  left: {
    animate: {
      rotate: 360,
      transition: { duration: 2, repeat: Infinity, ease: "linear" },
    },
  },
  right: {
    animate: {
      rotate: -360,
      transition: { duration: 1.5, repeat: Infinity, ease: "linear" },
    },
  },
};

export const progressStage: Variants = {
  pending: { opacity: 0.4, x: -4 },
  active: {
    opacity: 1,
    x: 0,
    transition: { duration: 0.15, ease: cinematicEasing.dramatic },
  },
  complete: { opacity: 1, x: 0, scale: 1 },
};

export const shimmerEffect: Variants = {
  animate: {
    x: ["0%", "100%"],
    transition: { duration: 1.2, repeat: Infinity, ease: "linear" },
  },
};

export const pulseGlow: Variants = {
  animate: {
    opacity: [0.6, 1, 0.6],
    transition: { duration: 1.5, repeat: Infinity, ease: "easeInOut" },
  },
};

// ==========================================
// MODAL & OVERLAY ANIMATIONS
// ==========================================

export const backdropFade: Variants = {
  initial: { opacity: 0 },
  animate: { opacity: 1, transition: { duration: 0.15 } },
  exit: { opacity: 0, transition: { duration: 0.1 } },
};

export const modalScale: Variants = {
  initial: { opacity: 0, scale: 0.97, y: 4 },
  animate: {
    opacity: 1,
    scale: 1,
    y: 0,
    transition: { duration: 0.2, ease: cinematicEasing.dramatic },
  },
  exit: {
    opacity: 0,
    scale: 0.97,
    y: 4,
    transition: { duration: 0.15 },
  },
};

// ==========================================
// TIMELINE ANIMATIONS
// ==========================================

export const thumbnailSelect: Variants = {
  selected: {
    scale: 1.03,
    boxShadow: "0 0 12px rgba(59,130,246,0.4)",
    transition: { duration: 0.15 },
  },
  unselected: {
    scale: 1,
    boxShadow: "none",
    transition: { duration: 0.1 },
  },
};

export const playhead: Variants = {
  animate: {
    scaleY: [1, 1.1, 1],
    transition: { duration: 0.4, repeat: Infinity, ease: "easeInOut" },
  },
};

// ==========================================
// UTILITY FUNCTIONS
// ==========================================

export const getStaggerDelay = (index: number, baseDelay = 0.04): Transition => ({
  delay: index * baseDelay,
  duration: 0.2,
  ease: cinematicEasing.dramatic,
});

export const cinematicWipe = (progress: number): Variants => ({
  animate: {
    scaleX: progress / 100,
    transition: { duration: 0.15, ease: cinematicEasing.cinematic },
  },
});

export const reducedMotion: Variants = {
  initial: { opacity: 0 },
  animate: { opacity: 1, transition: { duration: 0.01 } },
  exit: { opacity: 0, transition: { duration: 0.01 } },
};
````

## File: packages/shared/src/lib/utils.ts
````typescript
import { clsx, type ClassValue } from "clsx";
import { twMerge } from "tailwind-merge";
import type { CSSProperties } from "react";
import ArabicReshaper from "arabic-reshaper";

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs));
}

/**
 * Reshapes Arabic text for proper rendering in Canvas.
 * Arabic letters have different forms (initial, medial, final, isolated)
 * that must be connected properly. Canvas doesn't do this natively.
 */
export function reshapeArabicText(text: string): string {
  if (!text || !isRTL(text)) return text;
  try {
    return ArabicReshaper.convertArabic(text);
  } catch (error) {
    console.warn('[reshapeArabicText] Failed to reshape Arabic text:', error);
    return text;
  }
}

/**
 * RTL Unicode ranges:
 * - Arabic: \u0600-\u06FF, \u0750-\u077F, \u08A0-\u08FF, \uFB50-\uFDFF, \uFE70-\uFEFF
 * - Hebrew: \u0590-\u05FF, \uFB1D-\uFB4F
 * - Syriac: \u0700-\u074F
 * - Thaana (Maldivian): \u0780-\u07BF
 * - N'Ko: \u07C0-\u07FF
 */
const RTL_REGEX =
  /[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF\u0590-\u05FF\uFB1D-\uFB4F\u0700-\u074F\u0780-\u07BF\u07C0-\u07FF]/;

/**
 * Detects if a string contains RTL (right-to-left) characters.
 * Supports Arabic, Hebrew, Farsi, Urdu, Syriac, Thaana, and N'Ko scripts.
 */
export function isRTL(text: string): boolean {
  if (!text) return false;
  return RTL_REGEX.test(text);
}

/**
 * Returns the appropriate text direction ('rtl' or 'ltr') for the given text.
 */
export function getTextDirection(text: string): "rtl" | "ltr" {
  return isRTL(text) ? "rtl" : "ltr";
}

/**
 * Returns CSS properties for proper RTL/LTR text rendering.
 */
export function getDirectionStyles(text: string): CSSProperties {
  const dir = getTextDirection(text);
  return {
    direction: dir,
    textAlign: dir === "rtl" ? "right" : "left",
    unicodeBidi: "isolate",
  };
}

export function applyPolyfills(ctx: CanvasRenderingContext2D) {
  if (!ctx.roundRect) {
    ctx.roundRect = function (
      x: number,
      y: number,
      w: number,
      h: number,
      r: number,
    ) {
      if (typeof r === "number") {
        if (w < 2 * r) r = w / 2;
        if (h < 2 * r) r = h / 2;
        this.beginPath();
        this.moveTo(x + r, y);
        this.arcTo(x + w, y, x + w, y + h, r);
        this.arcTo(x + w, y + h, x, y + h, r);
        this.arcTo(x, y + h, x, y, r);
        this.arcTo(x, y, x + w, y, r);
        this.closePath();
      } else {
        this.rect(x, y, w, h);
      }
    };
  }
}
````

## File: packages/shared/src/services/agent/agentLogger.ts
````typescript
/**
 * Agent Logger
 * 
 * Logging infrastructure for the Agent Director Service.
 * Extracted from agentDirectorService.ts for modularity.
 */

import { type ParseError, type ExtractionSuccess, type FallbackNotification } from '../jsonExtractor';

/**
 * Logging levels for the Agent Director Service.
 */
export enum LogLevel {
    DEBUG = 'debug',
    INFO = 'info',
    WARN = 'warn',
    ERROR = 'error'
}

/**
 * Log entry structure for detailed logging.
 */
export interface LogEntry {
    timestamp: string;
    level: LogLevel;
    message: string;
    context?: Record<string, unknown>;
    duration?: number;
}

/**
 * Logger for the Agent Director Service.
 */
class AgentDirectorLogger {
    private logs: LogEntry[] = [];
    private maxLogs: number = 1000;
    private enabled: boolean = true;

    log(level: LogLevel, message: string, context?: Record<string, unknown>): void {
        if (!this.enabled) return;

        const entry: LogEntry = {
            timestamp: new Date().toISOString(),
            level,
            message,
            context
        };

        this.logs.push(entry);

        // Trim old logs if necessary
        if (this.logs.length > this.maxLogs) {
            this.logs = this.logs.slice(-this.maxLogs);
        }

        // Also output to console
        const prefix = `[AgentDirector] [${level.toUpperCase()}]`;
        switch (level) {
            case LogLevel.DEBUG:
                console.debug(prefix, message, context || '');
                break;
            case LogLevel.INFO:
                console.log(prefix, message, context || '');
                break;
            case LogLevel.WARN:
                console.warn(prefix, message, context || '');
                break;
            case LogLevel.ERROR:
                console.error(prefix, message, context || '');
                break;
        }
    }

    debug(message: string, context?: Record<string, unknown>): void {
        this.log(LogLevel.DEBUG, message, context);
    }

    info(message: string, context?: Record<string, unknown>): void {
        this.log(LogLevel.INFO, message, context);
    }

    warn(message: string, context?: Record<string, unknown>): void {
        this.log(LogLevel.WARN, message, context);
    }

    error(message: string, context?: Record<string, unknown>): void {
        this.log(LogLevel.ERROR, message, context);
    }

    /**
     * Log detailed error information for JSON extraction failures.
     */
    logExtractionError(parseError: ParseError): void {
        this.error('JSON extraction failed', {
            type: parseError.type,
            message: parseError.message,
            contentLength: parseError.contentLength,
            contentPreview: parseError.originalContent.substring(0, 500),
            attemptedMethods: parseError.attemptedMethods,
            failureReasons: parseError.failureReasons,
            suggestions: parseError.suggestions
        });
    }

    /**
     * Log successful extraction with method tracking.
     */
    logExtractionSuccess(success: ExtractionSuccess): void {
        this.info('JSON extraction succeeded', {
            method: success.method,
            confidence: success.confidence,
            retryCount: success.retryCount,
            processingTimeMs: success.processingTimeMs
        });
    }

    /**
     * Log fallback processing usage.
     */
    logFallbackUsage(notification: FallbackNotification): void {
        this.warn('Fallback processing used', {
            message: notification.message,
            extractedPromptCount: notification.extractedPromptCount,
            reducedFunctionality: notification.reducedFunctionality
        });
    }

    getLogs(): LogEntry[] {
        return [...this.logs];
    }

    getRecentLogs(count: number = 50): LogEntry[] {
        return this.logs.slice(-count);
    }

    clearLogs(): void {
        this.logs = [];
    }

    setEnabled(enabled: boolean): void {
        this.enabled = enabled;
    }
}

// Singleton instance
export const agentDirectorLogger = new AgentDirectorLogger();
````

## File: packages/shared/src/services/agent/agentMetrics.ts
````typescript
/**
 * Agent Metrics
 * 
 * Performance metrics collection for the Agent Director Service.
 * Extracted from agentDirectorService.ts for modularity.
 */

import { ExtractionMethod } from '../jsonExtractor';

/**
 * Performance metrics for the Agent Director Service.
 */
export interface AgentDirectorMetrics {
    totalRequests: number;
    successfulRequests: number;
    failedRequests: number;
    averageProcessingTimeMs: number;
    jsonExtractionSuccessRate: number;
    fallbackUsageRate: number;
    lastRequestTimestamp: string | null;
    extractionMethodBreakdown: Map<ExtractionMethod, number>;
}

/**
 * Metrics collector for the Agent Director Service.
 */
class AgentDirectorMetricsCollector {
    private metrics: AgentDirectorMetrics;

    constructor() {
        this.metrics = this.initializeMetrics();
    }

    private initializeMetrics(): AgentDirectorMetrics {
        return {
            totalRequests: 0,
            successfulRequests: 0,
            failedRequests: 0,
            averageProcessingTimeMs: 0,
            jsonExtractionSuccessRate: 0,
            fallbackUsageRate: 0,
            lastRequestTimestamp: null,
            extractionMethodBreakdown: new Map()
        };
    }

    recordRequest(success: boolean, processingTimeMs: number): void {
        this.metrics.totalRequests++;
        this.metrics.lastRequestTimestamp = new Date().toISOString();

        if (success) {
            this.metrics.successfulRequests++;
        } else {
            this.metrics.failedRequests++;
        }

        // Update average processing time
        const totalTime = this.metrics.averageProcessingTimeMs * (this.metrics.totalRequests - 1) + processingTimeMs;
        this.metrics.averageProcessingTimeMs = totalTime / this.metrics.totalRequests;
    }

    recordExtractionMethod(method: ExtractionMethod): void {
        const current = this.metrics.extractionMethodBreakdown.get(method) || 0;
        this.metrics.extractionMethodBreakdown.set(method, current + 1);
    }

    recordFallbackUsage(): void {
        // Update fallback usage rate
        const totalExtractions = Array.from(this.metrics.extractionMethodBreakdown.values())
            .reduce((sum, count) => sum + count, 0);
        const fallbackCount = this.metrics.extractionMethodBreakdown.get(ExtractionMethod.FALLBACK_TEXT) || 0;

        if (totalExtractions > 0) {
            this.metrics.fallbackUsageRate = fallbackCount / totalExtractions;
        }
    }

    updateExtractionSuccessRate(successCount: number, totalCount: number): void {
        if (totalCount > 0) {
            this.metrics.jsonExtractionSuccessRate = successCount / totalCount;
        }
    }

    getMetrics(): AgentDirectorMetrics {
        return {
            ...this.metrics,
            extractionMethodBreakdown: new Map(this.metrics.extractionMethodBreakdown)
        };
    }

    getMetricsSummary(): {
        successRate: number;
        averageTimeMs: number;
        fallbackRate: number;
        mostUsedMethod: ExtractionMethod | null;
    } {
        let mostUsedMethod: ExtractionMethod | null = null;
        let maxCount = 0;

        for (const [method, count] of this.metrics.extractionMethodBreakdown) {
            if (count > maxCount) {
                maxCount = count;
                mostUsedMethod = method;
            }
        }

        return {
            successRate: this.metrics.totalRequests > 0
                ? this.metrics.successfulRequests / this.metrics.totalRequests
                : 0,
            averageTimeMs: this.metrics.averageProcessingTimeMs,
            fallbackRate: this.metrics.fallbackUsageRate,
            mostUsedMethod
        };
    }

    reset(): void {
        this.metrics = this.initializeMetrics();
    }
}

// Singleton instance
export const agentMetrics = new AgentDirectorMetricsCollector();
````

## File: packages/shared/src/services/agent/agentTools.ts
````typescript
/**
 * Agent Tools
 * 
 * LangChain tool definitions for the Agent Director Service.
 * Extracted from agentDirectorService.ts for modularity.
 */

import { tool } from "@langchain/core/tools";
import { z } from "zod";
import { lintPrompt, getSystemPersona, refineImagePrompt } from "../promptService";
import { type AnalysisOutput, type StoryboardOutput, runAnalyzer, runStoryboarder } from "../directorService";
import { JSONExtractor, FallbackProcessor, ExtractionMethod, type StoryboardData, type FallbackNotification } from "../jsonExtractor";
import { VideoPurpose } from "../../constants";
import { agentDirectorLogger as agentLogger } from "./agentLogger";
import { agentMetrics } from "./agentMetrics";
import { needsFormatCorrection, preprocessFormatCorrection } from "../promptFormatService";

// Create instances for tool usage
const jsonExtractor = new JSONExtractor();
const fallbackProcessor = new FallbackProcessor();

// Register fallback notification callback
fallbackProcessor.registerNotificationCallback((notification: FallbackNotification) => {
    agentLogger.logFallbackUsage(notification);
    agentMetrics.recordFallbackUsage();
});

// Export for external use
export { jsonExtractor, fallbackProcessor };

/**
 * Sanitizes a JSON string by replacing unescaped control characters.
 */
export function sanitizeJsonString(jsonStr: string): string {
    try {
        JSON.parse(jsonStr);
        return jsonStr;
    } catch {
        // Continue with sanitization
    }

    if (needsFormatCorrection(jsonStr)) {
        const correctionResult = preprocessFormatCorrection(jsonStr);
        if (correctionResult.wasModified) {
            try {
                JSON.parse(correctionResult.corrected);
                agentLogger.debug('Format correction applied successfully', {
                    appliedCorrections: correctionResult.appliedCorrections
                });
                return correctionResult.corrected;
            } catch {
                // Continue with manual sanitization
            }
        }
    }

    return jsonStr
        .replace(/[\x00-\x1F\x7F]/g, (char) => {
            switch (char) {
                case '\n': return '\\n';
                case '\r': return '\\r';
                case '\t': return '\\t';
                default: return '';
            }
        });
}

// --- Visual References Helper ---

export function getVisualReferences(query: string, style: string): {
    cameraAngles: string[];
    lighting: string[];
    composition: string[];
    colorPalette: string[];
} {
    const queryLower = query.toLowerCase();

    const moodSuggestions: Record<string, { camera: string[]; lighting: string[]; colors: string[] }> = {
        melancholic: {
            camera: ["slow dolly out", "static wide shot", "low angle looking up"],
            lighting: ["blue hour", "overcast diffused", "single source dramatic"],
            colors: ["desaturated blues", "muted grays", "cold tones"],
        },
        energetic: {
            camera: ["dynamic tracking", "quick cuts", "dutch angle", "crane shot"],
            lighting: ["high contrast", "strobe effects", "rim lighting"],
            colors: ["vibrant saturated", "warm oranges", "electric blues"],
        },
        romantic: {
            camera: ["soft focus close-up", "two-shot", "slow pan"],
            lighting: ["golden hour", "candlelight", "soft diffused"],
            colors: ["warm pastels", "rose gold", "soft pinks"],
        },
        mysterious: {
            camera: ["obscured framing", "silhouette shot", "slow reveal"],
            lighting: ["chiaroscuro", "backlit", "fog/haze"],
            colors: ["deep shadows", "teal and orange", "noir palette"],
        },
        triumphant: {
            camera: ["hero shot low angle", "crane up", "epic wide"],
            lighting: ["dramatic rim light", "god rays", "golden backlight"],
            colors: ["rich golds", "deep reds", "royal blues"],
        },
    };

    const styleComposition: Record<string, string[]> = {
        cinematic: ["rule of thirds", "leading lines", "depth layers", "negative space"],
        anime: ["dynamic poses", "speed lines", "dramatic angles", "expressive lighting"],
        "film noir": ["high contrast shadows", "venetian blind lighting", "dutch angles"],
        watercolor: ["soft edges", "color bleeding", "organic shapes"],
        documentary: ["natural framing", "candid composition", "environmental context"],
    };

    let selectedMood = "energetic";
    for (const mood of Object.keys(moodSuggestions)) {
        if (queryLower.includes(mood) || queryLower.includes(mood.slice(0, 5))) {
            selectedMood = mood;
            break;
        }
    }

    const moodData = moodSuggestions[selectedMood] || moodSuggestions.energetic;
    const styleLower = style.toLowerCase();
    const composition = styleComposition[styleLower] || styleComposition.cinematic;

    return {
        cameraAngles: moodData?.camera ?? [],
        lighting: moodData?.lighting ?? [],
        composition: composition ?? [],
        colorPalette: moodData?.colors ?? [],
    };
}

// --- Storyboard Critique Helper ---

export function critiqueStoryboard(
    storyboard: StoryboardOutput,
    globalSubject: string
): {
    overallScore: number;
    promptCount: number;
    issues: Array<{ promptIndex: number; code: string; message: string }>;
    strengths: string[];
    recommendations: string[];
} {
    const issues: Array<{ promptIndex: number; code: string; message: string }> = [];
    const strengths: string[] = [];
    let totalScore = 100;

    const prompts = storyboard.prompts || [];

    if (prompts.length < 8) {
        issues.push({ promptIndex: -1, code: "too_few_prompts", message: `Only ${prompts.length} prompts` });
        totalScore -= 15;
    } else if (prompts.length >= 10) {
        strengths.push("Complete set of 10 prompts");
    }

    const previousPrompts: string[] = [];
    const moods = new Set<string>();

    prompts.forEach((prompt: { text: string; mood?: string }, index: number) => {
        const lintIssues = lintPrompt({
            promptText: prompt.text,
            globalSubject,
            previousPrompts,
        });

        lintIssues.forEach((issue: { code: string; message: string; severity: string }) => {
            issues.push({ promptIndex: index, code: issue.code, message: issue.message });
            totalScore -= issue.severity === "error" ? 5 : 2;
        });

        previousPrompts.push(prompt.text);
        moods.add(prompt.mood?.toLowerCase() || "unknown");
    });

    if (moods.size >= 4) {
        strengths.push(`Good emotional variety with ${moods.size} moods`);
    } else if (moods.size < 3) {
        issues.push({ promptIndex: -1, code: "low_variety", message: "Limited mood variety" });
        totalScore -= 10;
    }

    const recommendations: string[] = [];
    const errorPrompts = issues.filter(i => i.promptIndex >= 0);

    if (errorPrompts.length > 0) {
        const uniqueIndices = Array.from(new Set(errorPrompts.map(i => i.promptIndex)));
        recommendations.push(`Refine prompts at indices: ${uniqueIndices.join(", ")}`);
    }

    return {
        overallScore: Math.max(0, Math.min(100, totalScore)),
        promptCount: prompts.length,
        issues,
        strengths,
        recommendations,
    };
}

// --- Tool Definitions ---

/**
 * Tool: Analyze Content
 */
export const analyzeContentTool = tool(
    async ({ content, contentType }: { content: string; contentType: "lyrics" | "story" }) => {
        try {
            console.log("[AgentDirector] Running content analysis...");
            const analysis = await runAnalyzer(content, contentType);
            return JSON.stringify(analysis, null, 2);
        } catch (error) {
            return `Analysis failed: ${error instanceof Error ? error.message : String(error)}`;
        }
    },
    {
        name: "analyze_content",
        description: `Analyze lyrics or story content to identify structure, emotional arc, themes, and art-directed visual scenes.
Use this FIRST before generating storyboards.
RETURNS: A JSON object with sections, emotionalArc, themes, motifs, and visualScenes (full cinematic prompts).
IMPORTANT: Copy the EXACT output of this tool and pass it directly to generate_storyboard's analysisJson parameter.`,
        schema: z.object({
            content: z.string().describe("The SRT/lyrics/story content to analyze"),
            contentType: z.enum(["lyrics", "story"]).describe("Type of content"),
        }),
    }
);

/**
 * Tool: Search Visual References
 */
export const searchVisualReferencesTool = tool(
    async ({ query, style }: { query: string; style: string }) => {
        const references = getVisualReferences(query, style);
        return JSON.stringify(references, null, 2);
    },
    {
        name: "search_visual_references",
        description: `Search for visual references, cinematography techniques, and art style guidance.`,
        schema: z.object({
            query: z.string().describe("What to search for (e.g., 'melancholic night scene')"),
            style: z.string().describe("The art style context"),
        }),
    }
);

/**
 * Tool: Analyze and Generate Storyboard (Combined)
 */
export const analyzeAndGenerateStoryboardTool = tool(
    async ({
        content,
        contentType,
        style,
        videoPurpose,
        globalSubject,
        targetAssetCount,
    }: {
        content: string;
        contentType: "lyrics" | "story";
        style: string;
        videoPurpose: string;
        globalSubject: string;
        targetAssetCount?: number;
    }) => {
        try {
            console.log("[AgentDirector] Running combined analysis + storyboard generation...");

            // Step 1: Analyze content
            console.log("[AgentDirector] Step 1: Analyzing content...");
            const analysis = await runAnalyzer(content, contentType);
            console.log(`[AgentDirector] Analysis complete: ${analysis.sections.length} sections, ${analysis.visualScenes?.length || 0} visual scenes`);

            // Step 2: Generate storyboard from analysis
            console.log("[AgentDirector] Step 2: Generating storyboard...");
            const storyboard = await runStoryboarder(
                analysis,
                style,
                videoPurpose as VideoPurpose,
                globalSubject,
                {
                    targetAssetCount: typeof targetAssetCount === "number" ? targetAssetCount : 10,
                }
            );

            if (!storyboard || !storyboard.prompts || !Array.isArray(storyboard.prompts)) {
                throw new Error("Invalid storyboard structure: missing prompts array");
            }

            if (storyboard.prompts.length === 0) {
                throw new Error("Storyboard contains no prompts");
            }

            console.log(`[AgentDirector] Storyboard generated: ${storyboard.prompts.length} prompts`);

            return JSON.stringify({
                analysis: {
                    sectionCount: analysis.sections.length,
                    themes: analysis.themes,
                    emotionalArc: analysis.emotionalArc,
                    visualScenes: analysis.visualScenes,
                },
                storyboard: storyboard,
            }, null, 2);
        } catch (error) {
            const errorMsg = `Combined analysis + storyboard failed: ${error instanceof Error ? error.message : String(error)}`;
            console.error("[AgentDirector]", errorMsg);
            return errorMsg;
        }
    },
    {
        name: "analyze_and_generate_storyboard",
        description: `RECOMMENDED: Analyze content AND generate storyboard in one step.
This is the most reliable way to create a storyboard - use this instead of calling analyze_content and generate_storyboard separately.
Returns both the analysis summary and the complete storyboard with all prompts.`,
        schema: z.object({
            content: z.string().describe("The SRT/lyrics/story content to analyze"),
            contentType: z.enum(["lyrics", "story"]).describe("Type of content"),
            style: z.string().describe("Art style (e.g., 'Cinematic', 'Anime', 'Watercolor')"),
            videoPurpose: z.string().describe("Video purpose (e.g., 'music_video', 'commercial', 'documentary')"),
            globalSubject: z.string().default("").describe("Main subject for visual consistency (optional)"),
            targetAssetCount: z.number().optional().describe("Target number of prompts to generate (default: 10)"),
        }),
    }
);

/**
 * Tool: Generate Storyboard
 */
export const generateStoryboardTool = tool(
    async ({
        analysisJson,
        style,
        videoPurpose,
        globalSubject,
        targetAssetCount
    }: {
        analysisJson: string;
        style: string;
        videoPurpose: string;
        globalSubject: string;
        targetAssetCount?: number;
    }) => {
        try {
            agentLogger.info('Generating storyboard', { style, videoPurpose, targetAssetCount });

            // Robust JSON extraction for analysis input
            const extracted = await jsonExtractor.extractJSON(analysisJson);
            if (!extracted) throw new Error("Failed to extract analysis JSON");
            
            const data = extracted.data as any;
            const analysis = data.sections ? data : (data.result || data);

            const storyboard = await runStoryboarder(
                analysis as AnalysisOutput,
                style,
                videoPurpose as VideoPurpose,
                globalSubject,
                {
                    targetAssetCount: typeof targetAssetCount === "number" ? targetAssetCount : 10,
                }
            );

            return JSON.stringify(storyboard, null, 2);
        } catch (error) {
            const errorMsg = `Storyboard generation failed: ${error instanceof Error ? error.message : String(error)}`;
            agentLogger.error('Storyboard generation failed', { error: errorMsg });

            const fallbackStoryboard = fallbackProcessor.processWithFallback(
                analysisJson,
                error instanceof Error ? error.message : String(error)
            );

            if (fallbackStoryboard && fallbackStoryboard.prompts.length > 0) {
                agentMetrics.recordExtractionMethod(ExtractionMethod.FALLBACK_TEXT);
                return JSON.stringify(fallbackStoryboard, null, 2);
            }

            return errorMsg;
        }
    },
    {
        name: "generate_storyboard",
        description: `Generate a visual storyboard with detailed image prompts based on content analysis.
CRITICAL: All prompts must share the same visual universe.
Use this AFTER analyze_content.`,
        schema: z.object({
            analysisJson: z.string().describe("Pass the EXACT raw JSON string output from analyze_content here."),
            style: z.string().describe("Art style (e.g., 'Cinematic', 'Anime', 'Watercolor')"),
            videoPurpose: z.string().describe("Video purpose (e.g., 'music_video', 'commercial', 'documentary')"),
            globalSubject: z.string().default("").describe("Main subject for consistency (can be empty string)"),
            targetAssetCount: z.number().optional().describe("Target number of prompts to generate (default: 10)"),
        }),
    }
);

/**
 * Tool: Refine Prompt
 */
export const refinePromptTool = tool(
    async ({
        promptText,
        style,
        globalSubject,
        previousPrompts
    }: {
        promptText: string;
        style: string;
        globalSubject: string;
        previousPrompts: string[];
    }) => {
        try {
            console.log("[AgentDirector] Refining prompt...");
            const result = await refineImagePrompt({
                promptText,
                style,
                globalSubject,
                intent: "auto",
                previousPrompts,
            });
            return JSON.stringify({
                refinedPrompt: result.refinedPrompt,
                issues: result.issues.map((i: { code: string; message: string }) => ({ code: i.code, message: i.message })),
                wasRefined: result.refinedPrompt !== promptText,
            }, null, 2);
        } catch (error) {
            return `Refinement failed: ${error instanceof Error ? error.message : String(error)}`;
        }
    },
    {
        name: "refine_prompt",
        description: `Refine and improve a single image prompt.`,
        schema: z.object({
            promptText: z.string().describe("The prompt text to refine"),
            style: z.string().describe("Art style for context"),
            globalSubject: z.string().default("").describe("Main subject (can be empty)"),
            previousPrompts: z.array(z.string()).default([]).describe("Previous prompts to avoid repetition (optional)"),
        }),
    }
);

/**
 * Tool: Critique Storyboard
 */
export const critiqueStoryboardTool = tool(
    async ({
        storyboardJson,
        globalSubject
    }: {
        storyboardJson: string;
        globalSubject: string;
    }) => {
        try {
            console.log("[AgentDirector] Critiquing storyboard...");
            const sanitizedJson = sanitizeJsonString(storyboardJson);
            const storyboard: StoryboardOutput = JSON.parse(sanitizedJson);
            const critique = critiqueStoryboard(storyboard, globalSubject);
            return JSON.stringify(critique, null, 2);
        } catch (error) {
            return `Critique failed: ${error instanceof Error ? error.message : String(error)}`;
        }
    },
    {
        name: "critique_storyboard",
        description: `Evaluate the quality of a generated storyboard.
Returns a quality score and list of issues to fix.`,
        schema: z.object({
            storyboardJson: z.string().describe("The JSON output from generate_storyboard tool"),
            globalSubject: z.string().default("").describe("Main subject (can be empty)"),
        }),
    }
);

// All tools for binding
export const allTools = [
    analyzeAndGenerateStoryboardTool,
    analyzeContentTool,
    searchVisualReferencesTool,
    generateStoryboardTool,
    refinePromptTool,
    critiqueStoryboardTool,
];

/**
 * Execute a tool call by name.
 */
export async function executeToolCall(
    toolCall: { name: string; args: Record<string, unknown> }
): Promise<string> {
    const { name, args } = toolCall;

    // Sanitize args to handle null/undefined values
    const sanitizedArgs = { ...args };
    for (const [key, value] of Object.entries(sanitizedArgs)) {
        if (value === null || value === undefined) {
            if (key === 'previousPrompts') {
                sanitizedArgs[key] = [];
            } else {
                sanitizedArgs[key] = '';
            }
        }
    }

    switch (name) {
        case "analyze_and_generate_storyboard":
            return analyzeAndGenerateStoryboardTool.invoke(sanitizedArgs as {
                content: string;
                contentType: "lyrics" | "story";
                style: string;
                videoPurpose: string;
                globalSubject: string;
                targetAssetCount?: number;
            });
        case "analyze_content":
            return analyzeContentTool.invoke(sanitizedArgs as { content: string; contentType: "lyrics" | "story" });
        case "search_visual_references":
            return searchVisualReferencesTool.invoke(sanitizedArgs as { query: string; style: string });
        case "generate_storyboard":
            return generateStoryboardTool.invoke(sanitizedArgs as { analysisJson: string; style: string; videoPurpose: string; globalSubject: string });
        case "refine_prompt":
            return refinePromptTool.invoke(sanitizedArgs as { promptText: string; style: string; globalSubject: string; previousPrompts: string[] });
        case "critique_storyboard":
            return critiqueStoryboardTool.invoke(sanitizedArgs as { storyboardJson: string; globalSubject: string });
        default:
            return `Unknown tool: ${name}`;
    }
}
````

## File: packages/shared/src/services/agent/audioMixingTools.ts
````typescript
/**
 * Audio Mixing Tools - LangChain tools for audio mixing
 *
 * Provides tools for mixing multiple audio tracks:
 * - mix_audio_tracks: Combine narration, music, SFX, and Veo video audio with volume control and ducking
 *
 * Requirements: 3.1, 3.2, 3.3, 3.4, 3.5
 */

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { mixAudioWithSFX, MixConfig, SceneAudioInfo } from "../audioMixerService";
import { VideoSFXPlan } from "../sfxService";
import { productionStore } from "../ai/production/store";
import { concatenateNarrationSegments } from "./audioUtils";
import {
  extractAudioFromVideos,
  mixVideoAudioWithNarration,
  type ExtractedVideoAudio,
} from "../../services/ffmpeg/videoAudioExtractor";

// --- Types ---

/**
 * Result of audio mixing operation
 */
export interface MixedAudioResult {
  /** Mixed audio as a Blob */
  audioBlob: Blob;
  /** Total duration in seconds */
  duration: number;
  /** Track presence and volume information */
  tracks: {
    narration: { volume: number; present: boolean };
    music: { volume: number; present: boolean };
    sfx: { volume: number; present: boolean };
    videoAudio: { volume: number; present: boolean; sceneCount: number };
  };
  /** Whether ducking was applied to background music */
  duckingApplied: boolean;
}

/**
 * Session storage for mixed audio results
 */
const mixedAudioStore = new Map<string, MixedAudioResult>();

/**
 * Get mixed audio result for a session
 */
export function getMixedAudio(sessionId: string): MixedAudioResult | undefined {
  return mixedAudioStore.get(sessionId);
}

/**
 * Store mixed audio result for a session
 */
export function setMixedAudio(sessionId: string, result: MixedAudioResult): void {
  mixedAudioStore.set(sessionId, result);
}

/**
 * Clear mixed audio for a session
 */
export function clearMixedAudio(sessionId: string): boolean {
  return mixedAudioStore.delete(sessionId);
}

// --- Tool Schema ---

/**
 * Schema for mix_audio_tracks tool
 */
const MixAudioTracksSchema = z.object({
  contentPlanId: z.string().describe("Session ID containing the content plan and audio assets"),
  narrationUrl: z.string().optional().describe("Optional URL to the narration audio file. If not provided, will be automatically retrieved from session narration segments."),
  narrationVolume: z.number().min(0).max(1).default(1.0).describe("Volume level for narration (0-1, default 1.0)"),
  musicVolume: z.number().min(0).max(1).default(0.3).describe("Volume level for background music (0-1, default 0.3)"),
  sfxVolume: z.number().min(0).max(1).default(0.5).describe("Volume level for sound effects (0-1, default 0.5)"),
  videoAudioVolume: z.number().min(0).max(1).default(0.3).describe("Volume level for Veo video native audio (0-1, default 0.3)"),
  includeVideoAudio: z.boolean().default(true).describe("Whether to extract and mix native audio from Veo video assets (default true)"),
  duckingEnabled: z.boolean().default(true).describe("Whether to duck background music during narration (default true)"),
  sfxPlan: z.any().optional().describe("Optional SFX plan with audio URLs for ambient sounds and music"),
  scenes: z.array(z.object({
    sceneId: z.string(),
    startTime: z.number(),
    duration: z.number(),
  })).optional().describe("Optional scene timing information for SFX placement"),
});

// --- Helper Functions ---

/**
 * Validate that a URL is accessible
 * Note: Blob URLs don't support HEAD requests, so we skip validation for them
 */
async function validateAudioUrl(url: string): Promise<boolean> {
  // Blob URLs are always valid if they exist - HEAD requests don't work on them
  if (url.startsWith('blob:')) {
    return true;
  }
  
  try {
    const response = await fetch(url, { method: "HEAD" });
    return response.ok;
  } catch {
    return false;
  }
}

/**
 * Check if SFX plan has any audio content
 */
function hasSFXContent(sfxPlan: VideoSFXPlan | null | undefined): boolean {
  if (!sfxPlan) return false;
  
  // Check for background music
  if (sfxPlan.backgroundMusic?.audioUrl) return true;
  
  // Check for scene ambient tracks
  if (sfxPlan.scenes?.some((s: any) => s.ambientTrack?.audioUrl)) return true;
  
  return false;
}

/**
 * Check if SFX plan has background music
 */
function hasBackgroundMusic(sfxPlan: VideoSFXPlan | null | undefined): boolean {
  return !!(sfxPlan?.backgroundMusic?.audioUrl);
}

// --- Tool Implementation ---

/**
 * Mix Audio Tracks Tool
 * 
 * Combines narration, background music, and SFX into a single audio output.
 * Supports volume control for each track type and automatic ducking.
 * 
 * Requirements: 3.1, 3.2, 3.3, 3.4, 3.5
 */
export const mixAudioTracksTool = tool(
  async ({
    contentPlanId,
    narrationUrl,
    narrationVolume = 1.0,
    musicVolume = 0.3,
    sfxVolume = 0.5,
    videoAudioVolume = 0.3,
    includeVideoAudio = true,
    duckingEnabled = true,
    sfxPlan,
    scenes,
  }) => {
    console.log(`[AudioMixingTools] Mixing audio for session: ${contentPlanId}`);
    console.log(`[AudioMixingTools] Volumes - Narration: ${narrationVolume}, Music: ${musicVolume}, SFX: ${sfxVolume}, VideoAudio: ${videoAudioVolume}`);
    console.log(`[AudioMixingTools] Ducking enabled: ${duckingEnabled}, Include video audio: ${includeVideoAudio}`);

    // Get session state
    const state = productionStore.get(contentPlanId);

    // Get narration URL from session if not provided
    let finalNarrationUrl = narrationUrl;
    if (!finalNarrationUrl) {
      if (!state || !state.narrationSegments || state.narrationSegments.length === 0) {
        return JSON.stringify({
          success: false,
          error: "No narration found in session and no narration URL provided",
          suggestion: "Run narrate_scenes first or provide a narrationUrl parameter",
        });
      }

      // Concatenate all narration segments into a single audio blob
      try {
        console.log(`[AudioMixingTools] Concatenating ${state.narrationSegments.length} narration segments`);
        const concatenatedBlob = await concatenateNarrationSegments(state.narrationSegments);
        finalNarrationUrl = URL.createObjectURL(concatenatedBlob);
        console.log(`[AudioMixingTools] Created narration URL from concatenated audio (${Math.round(concatenatedBlob.size / 1024)}KB)`);
      } catch (error) {
        return JSON.stringify({
          success: false,
          error: `Failed to concatenate narration segments: ${error instanceof Error ? error.message : String(error)}`,
          suggestion: "Check that all narration segments have valid audio blobs",
        });
      }
    }

    // Check if narration is accessible
    const narrationValid = await validateAudioUrl(finalNarrationUrl);
    if (!narrationValid) {
      return JSON.stringify({
        success: false,
        error: "Narration audio URL is not accessible",
        suggestion: "Check that the narration URL is valid and the audio format is supported",
      });
    }

    // Check for Veo video assets with native audio
    let videoAudioTracks: ExtractedVideoAudio[] = [];
    let hasVideoAudio = false;

    if (includeVideoAudio && state?.visuals) {
      const videoAssets = state.visuals.filter(v => v.type === "video");
      if (videoAssets.length > 0) {
        console.log(`[AudioMixingTools] Found ${videoAssets.length} Veo video assets, extracting audio...`);

        // Build scene timings for video assets
        const sceneList = state.contentPlan?.scenes || [];
        const videoInfos = videoAssets.map((visual, _index) => {
          const sceneIndex = sceneList.findIndex(s => s.id === visual.promptId);
          let startTime = 0;
          if (sceneIndex >= 0) {
            for (let i = 0; i < sceneIndex; i++) {
              const scene = sceneList[i];
              if (scene) {
                startTime += scene.duration || 0;
              }
            }
          }
          return {
            sceneId: visual.promptId,
            videoUrl: visual.imageUrl,
            startTime,
          };
        });

        try {
          const extractionResult = await extractAudioFromVideos(videoInfos);
          videoAudioTracks = extractionResult.audioTracks;
          hasVideoAudio = videoAudioTracks.length > 0;

          if (hasVideoAudio) {
            console.log(`[AudioMixingTools] Extracted audio from ${videoAudioTracks.length}/${videoAssets.length} Veo videos`);
          } else {
            console.log(`[AudioMixingTools] No audio found in Veo videos (may be silent or audio-free)`);
          }

          if (extractionResult.failedScenes.length > 0) {
            console.log(`[AudioMixingTools] Video audio extraction failed for scenes: ${extractionResult.failedScenes.join(", ")}`);
          }
        } catch (error) {
          console.warn(`[AudioMixingTools] Video audio extraction failed (non-fatal):`, error);
        }
      }
    }

    // Determine what tracks are present
    const hasNarration = true; // Required
    const hasMusic = hasBackgroundMusic(sfxPlan as VideoSFXPlan | null);
    const hasSfx = hasSFXContent(sfxPlan as VideoSFXPlan | null);

    console.log(`[AudioMixingTools] Tracks present - Narration: ${hasNarration}, Music: ${hasMusic}, SFX: ${hasSfx}, VideoAudio: ${hasVideoAudio} (${videoAudioTracks.length} tracks)`);

    // Build scene audio info if scenes provided
    const sceneAudioInfo: SceneAudioInfo[] = scenes?.map(s => ({
      sceneId: s.sceneId,
      startTime: s.startTime,
      duration: s.duration,
    })) || [];

    try {
      // Build mix config
      const mixConfig: MixConfig = {
        narrationUrl: finalNarrationUrl,
        sfxPlan: sfxPlan as VideoSFXPlan | null,
        scenes: sceneAudioInfo,
        sfxMasterVolume: sfxVolume,
        musicMasterVolume: musicVolume,
        sampleRate: 44100,
      };

      // Perform the base mix (narration + SFX + music)
      let audioBlob = await mixAudioWithSFX(mixConfig);

      // Mix in Veo video audio if present
      if (hasVideoAudio && videoAudioTracks.length > 0) {
        console.log(`[AudioMixingTools] Mixing in ${videoAudioTracks.length} Veo audio tracks at volume ${videoAudioVolume}`);
        try {
          audioBlob = await mixVideoAudioWithNarration(
            audioBlob,
            videoAudioTracks,
            videoAudioVolume
          );
          console.log(`[AudioMixingTools] Veo audio mixed successfully`);
        } catch (error) {
          console.warn(`[AudioMixingTools] Veo audio mixing failed (non-fatal):`, error);
          // Continue with the base mix without video audio
        }
      }

      // Calculate duration from blob (approximate based on WAV format)
      // WAV header is 44 bytes, 16-bit stereo at 44100 Hz = 176400 bytes/sec
      const duration = (audioBlob.size - 44) / (44100 * 4);

      // Create result object
      const result: MixedAudioResult = {
        audioBlob,
        duration,
        tracks: {
          narration: { volume: narrationVolume, present: hasNarration },
          music: { volume: musicVolume, present: hasMusic },
          sfx: { volume: sfxVolume, present: hasSfx },
          videoAudio: { volume: videoAudioVolume, present: hasVideoAudio, sceneCount: videoAudioTracks.length },
        },
        duckingApplied: duckingEnabled && hasMusic,
      };

      // Store the result
      setMixedAudio(contentPlanId, result);

      // Also update production state with mixed audio info
      if (state) {
        state.mixedAudio = result;
        productionStore.set(contentPlanId, state);
      }

      // Create a blob URL for the mixed audio
      const audioUrl = URL.createObjectURL(audioBlob);

      return JSON.stringify({
        success: true,
        sessionId: contentPlanId,
        audioUrl,
        duration: Math.round(duration * 100) / 100,
        fileSizeKB: Math.round(audioBlob.size / 1024),
        tracks: {
          narration: { volume: narrationVolume, present: hasNarration },
          music: { volume: musicVolume, present: hasMusic },
          sfx: { volume: sfxVolume, present: hasSfx },
          videoAudio: { volume: videoAudioVolume, present: hasVideoAudio, sceneCount: videoAudioTracks.length },
        },
        duckingApplied: duckingEnabled && hasMusic,
        message: `Successfully mixed audio (${Math.round(duration)}s, ${Math.round(audioBlob.size / 1024)}KB)${hasVideoAudio ? ` including ${videoAudioTracks.length} Veo audio tracks` : ""}`,
      });

    } catch (error) {
      console.error("[AudioMixingTools] Mix error:", error);
      
      const errorMessage = error instanceof Error ? error.message : String(error);
      
      // Check for specific error types
      if (errorMessage.includes("Failed to load narration")) {
        return JSON.stringify({
          success: false,
          error: "Failed to load narration audio",
          suggestion: "Check that the narration URL is valid and the audio format is supported (MP3, WAV, OGG)",
        });
      }

      return JSON.stringify({
        success: false,
        error: errorMessage,
        suggestion: "Check that all audio URLs are valid and accessible",
      });
    }
  },
  {
    name: "mix_audio_tracks",
    description: "Mix multiple audio tracks (narration, background music, SFX, and Veo video native audio) into a single audio output. Narration is automatically fetched from the session - you only need to provide contentPlanId. Veo video native audio is automatically extracted and mixed when includeVideoAudio=true (default). Optionally specify volume levels (narrationVolume, musicVolume, sfxVolume, videoAudioVolume) or duckingEnabled. Missing tracks are handled gracefully - the mix will proceed with available tracks.",
    schema: MixAudioTracksSchema,
  }
);

// --- Export all audio mixing tools ---

export const audioMixingTools = [
  mixAudioTracksTool,
];
````

## File: packages/shared/src/services/agent/audioUtils.ts
````typescript
/**
 * Audio Utilities for Agent Tools
 *
 * Provides utilities for concatenating and managing audio blobs
 */

import { NarrationSegment } from "../../types";

/**
 * Concatenate multiple WAV audio blobs into a single WAV blob
 *
 * IMPORTANT: This assumes all blobs are WAV files with the same format
 * (sample rate, bit depth, channels). Gemini TTS produces 24kHz, 16-bit, mono WAV.
 *
 * @param audioBlobs - Array of WAV audio blobs to concatenate
 * @returns Single concatenated WAV blob
 */
export async function concatenateWavBlobs(audioBlobs: Blob[]): Promise<Blob> {
  if (audioBlobs.length === 0) {
    throw new Error("No audio blobs to concatenate");
  }

  if (audioBlobs.length === 1) {
    const firstBlob = audioBlobs[0];
    if (!firstBlob) {
      throw new Error("Audio blob is missing");
    }
    return firstBlob;
  }

  // WAV file structure:
  // - 44 bytes header (RIFF, fmt, data chunks)
  // - Remaining bytes are PCM audio data

  const WAV_HEADER_SIZE = 44;

  // Read all PCM data (skip headers)
  const pcmDataArrays: Uint8Array[] = [];
  let totalPcmSize = 0;

  for (const blob of audioBlobs) {
    const arrayBuffer = await blob.arrayBuffer();
    const fullData = new Uint8Array(arrayBuffer);

    // Extract PCM data (skip 44-byte header)
    const pcmData = fullData.slice(WAV_HEADER_SIZE);
    pcmDataArrays.push(pcmData);
    totalPcmSize += pcmData.length;
  }

  // Concatenate all PCM data
  const concatenatedPcm = new Uint8Array(totalPcmSize);
  let offset = 0;
  for (const pcmData of pcmDataArrays) {
    concatenatedPcm.set(pcmData, offset);
    offset += pcmData.length;
  }

  // Create new WAV header for the concatenated audio
  const newWavHeader = createWavHeader(
    concatenatedPcm.length,
    24000, // Gemini TTS sample rate
    1,     // Mono
    16     // 16-bit
  );

  // Combine header and PCM data
  const finalWav = new Uint8Array(newWavHeader.length + concatenatedPcm.length);
  finalWav.set(newWavHeader, 0);
  finalWav.set(concatenatedPcm, newWavHeader.length);

  return new Blob([finalWav], { type: 'audio/wav' });
}

/**
 * Create a WAV header for PCM audio data
 *
 * @param pcmDataLength - Length of PCM data in bytes
 * @param sampleRate - Sample rate (default 24000 for Gemini TTS)
 * @param numChannels - Number of channels (default 1 for mono)
 * @param bitsPerSample - Bits per sample (default 16)
 * @returns WAV header as Uint8Array
 */
function createWavHeader(
  pcmDataLength: number,
  sampleRate: number = 24000,
  numChannels: number = 1,
  bitsPerSample: number = 16
): Uint8Array {
  const byteRate = (sampleRate * numChannels * bitsPerSample) / 8;
  const blockAlign = (numChannels * bitsPerSample) / 8;
  const fileSize = pcmDataLength + 36; // 44 - 8 bytes

  const header = new ArrayBuffer(44);
  const view = new DataView(header);

  // RIFF chunk descriptor
  writeString(view, 0, 'RIFF');
  view.setUint32(4, fileSize, true);
  writeString(view, 8, 'WAVE');

  // fmt sub-chunk
  writeString(view, 12, 'fmt ');
  view.setUint32(16, 16, true); // Subchunk1Size (16 for PCM)
  view.setUint16(20, 1, true); // AudioFormat (1 for PCM)
  view.setUint16(22, numChannels, true);
  view.setUint32(24, sampleRate, true);
  view.setUint32(28, byteRate, true);
  view.setUint16(32, blockAlign, true);
  view.setUint16(34, bitsPerSample, true);

  // data sub-chunk
  writeString(view, 36, 'data');
  view.setUint32(40, pcmDataLength, true);

  return new Uint8Array(header);
}

/**
 * Write string to DataView
 */
function writeString(view: DataView, offset: number, str: string): void {
  for (let i = 0; i < str.length; i++) {
    view.setUint8(offset + i, str.charCodeAt(i));
  }
}

/**
 * Concatenate narration segments into a single audio blob
 *
 * @param segments - Array of narration segments
 * @returns Single concatenated audio blob
 */
export async function concatenateNarrationSegments(
  segments: NarrationSegment[]
): Promise<Blob> {
  if (segments.length === 0) {
    throw new Error("No narration segments to concatenate");
  }

  const audioBlobs = segments.map(seg => seg.audioBlob);
  return concatenateWavBlobs(audioBlobs);
}
````

## File: packages/shared/src/services/agent/cloudStorageTools.ts
````typescript
/**
 * Cloud Storage Tools - LangChain tools for uploading production outputs to GCS
 *
 * Provides tools for uploading complete production bundles to Google Cloud Storage:
 * - upload_production_to_cloud: Upload all production outputs to GCS bucket
 *
 * Requirements: Production asset storage and archival
 *
 * NOTE: These tools are only available in Node.js environment (server-side).
 * In browser builds, stub functions are used to prevent bundling GCS dependencies.
 */

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import type { ProductionBundle } from "../cloudStorageService";

// Check if we're in Node.js environment
const isNode = typeof window === 'undefined';

// Conditional imports - only load GCS in Node.js
let uploadProductionBundle: any = null;
let generateProductionFolder: any = null;
let getMixedAudio: any = null;
let concatenateNarrationSegments: any = null;
let productionStore: any = null;

if (isNode) {
  try {
    const cloudStorage = require("../cloudStorageService");
    uploadProductionBundle = cloudStorage.uploadProductionBundle;
    generateProductionFolder = cloudStorage.generateProductionFolder;

    const audioMixing = require("./audioMixingTools");
    getMixedAudio = audioMixing.getMixedAudio;

    const audioUtils = require("./audioUtils");
    concatenateNarrationSegments = audioUtils.concatenateNarrationSegments;

    const prodAgent = require("../ai/productionAgent");
    productionStore = prodAgent.productionStore;
  } catch (error) {
    console.warn("[CloudStorageTools] Failed to load dependencies:", error);
  }
}

// --- Types ---

/**
 * Result of cloud storage upload operation
 */
export interface CloudUploadResult {
  /** GCS folder name (YYYY-MM-DD_HH-mm-ss format) */
  folderName: string;
  /** GCS bucket path */
  bucketPath: string;
  /** Number of files uploaded successfully */
  filesUploaded: number;
  /** Total number of files attempted */
  totalFiles: number;
  /** Total size uploaded in MB */
  totalSizeMB: number;
  /** Public URLs if makePublic was true */
  publicUrls?: Record<string, string>;
  /** Upload errors if any */
  errors?: string[];
}

// --- Tool Schema ---

/**
 * Schema for upload_production_to_cloud tool
 */
const UploadProductionSchema = z.object({
  contentPlanId: z.string().describe("Session ID containing the complete production to upload"),
  makePublic: z.boolean().default(false).describe("Whether to make uploaded files publicly accessible (default: false)"),
  includeNarrationAudio: z.boolean().default(true).describe("Include separate narration audio file (default: true)"),
  includeMixedAudio: z.boolean().default(true).describe("Include mixed audio file if available (default: true)"),
  includeVisuals: z.boolean().default(true).describe("Include scene visuals (default: true)"),
  includeSubtitles: z.boolean().default(true).describe("Include subtitle files (default: true)"),
});

// --- Helper Functions ---

/**
 * Generate SRT subtitle content from narration segments
 */
function generateSRTFromNarration(segments: any[]): string {
  let srtContent = '';
  let index = 1;
  let currentTime = 0;

  for (const segment of segments) {
    const startTime = currentTime;
    const endTime = currentTime + segment.audioDuration;

    // Format times as SRT format (HH:MM:SS,mmm)
    const formatTime = (seconds: number): string => {
      const hours = Math.floor(seconds / 3600);
      const minutes = Math.floor((seconds % 3600) / 60);
      const secs = Math.floor(seconds % 60);
      const millis = Math.floor((seconds % 1) * 1000);

      return `${String(hours).padStart(2, '0')}:${String(minutes).padStart(2, '0')}:${String(secs).padStart(2, '0')},${String(millis).padStart(3, '0')}`;
    };

    srtContent += `${index}\n`;
    srtContent += `${formatTime(startTime)} --> ${formatTime(endTime)}\n`;
    srtContent += `${segment.transcript}\n\n`;

    index++;
    currentTime = endTime;
  }

  return srtContent;
}

/**
 * Generate VTT subtitle content from narration segments
 */
function generateVTTFromNarration(segments: any[]): string {
  let vttContent = 'WEBVTT\n\n';
  let currentTime = 0;

  for (const segment of segments) {
    const startTime = currentTime;
    const endTime = currentTime + segment.audioDuration;

    // Format times as VTT format (HH:MM:SS.mmm)
    const formatTime = (seconds: number): string => {
      const hours = Math.floor(seconds / 3600);
      const minutes = Math.floor((seconds % 3600) / 60);
      const secs = Math.floor(seconds % 60);
      const millis = Math.floor((seconds % 1) * 1000);

      return `${String(hours).padStart(2, '0')}:${String(minutes).padStart(2, '0')}:${String(secs).padStart(2, '0')}.${String(millis).padStart(3, '0')}`;
    };

    vttContent += `${formatTime(startTime)} --> ${formatTime(endTime)}\n`;
    vttContent += `${segment.transcript}\n\n`;

    currentTime = endTime;
  }

  return vttContent;
}

// --- Tool Implementation ---

/**
 * Upload Production to Cloud Tool
 *
 * Uploads all production outputs to Google Cloud Storage bucket with organized structure.
 * Auto-fetches all assets from production session state.
 *
 * Creates folder structure:
 * gs://aisoul-studio-storage/YYYY-MM-DD_HH-mm-ss/
 *   ├── final-video.mp4 (or .webm)
 *   ├── narration.wav
 *   ├── mixed-audio.wav
 *   ├── background-music.mp3
 *   ├── visuals/scene-1.png
 *   ├── visuals/scene-2.png
 *   ├── subtitles.srt
 *   ├── subtitles.vtt
 *   ├── production.log
 *   └── metadata.json
 */
export const uploadProductionTool = tool(
  async ({
    contentPlanId,
    makePublic = false,
    includeNarrationAudio = true,
    includeMixedAudio = true,
    includeVisuals = true,
    includeSubtitles = true,
  }) => {
    console.log(`[CloudStorageTools] Starting upload for session: ${contentPlanId}`);

    // Check if running in browser
    if (typeof window !== 'undefined') {
      return JSON.stringify({
        success: false,
        error: "Cloud Storage upload is not supported in browser environment",
        suggestion: "This feature requires server-side execution. The production agent must run in a Node.js environment with proper GCS credentials configured.",
      });
    }

    // Get production state
    const state = productionStore.get(contentPlanId);
    if (!state) {
      return JSON.stringify({
        success: false,
        error: "Production session not found",
        suggestion: "Ensure the contentPlanId is correct and the production has been run",
      });
    }

    // Build production bundle
    const bundle: ProductionBundle = {
      metadata: {
        topic: state.contentPlan?.topic || "Unknown",
        duration: state.narrationSegments?.reduce((sum: number, seg: { audioDuration: number }) => sum + seg.audioDuration, 0) || 0,
        language: state.contentPlan?.language || "en",
        sceneCount: state.contentPlan?.scenes.length || 0,
        productionId: contentPlanId,
      },
    };

    const logs: string[] = [`Production upload started at ${new Date().toISOString()}`];

    // Add video if available
    if (state.exportedVideo) {
      bundle.video = state.exportedVideo;
      logs.push(`✓ Video ready for upload (${Math.round(state.exportedVideo.size / 1024 / 1024)}MB)`);
    } else {
      logs.push(`⚠ No exported video found - run export_final_video first`);
    }

    // Add narration audio (concatenated)
    if (includeNarrationAudio && state.narrationSegments && state.narrationSegments.length > 0) {
      try {
        const narrationBlob = await concatenateNarrationSegments(state.narrationSegments);
        bundle.narrationAudio = narrationBlob;
        logs.push(`✓ Narration audio ready (${Math.round(narrationBlob.size / 1024)}KB, ${state.narrationSegments.length} segments)`);
      } catch (error) {
        logs.push(`✗ Failed to concatenate narration: ${error instanceof Error ? error.message : String(error)}`);
      }
    }

    // Add mixed audio if available
    if (includeMixedAudio) {
      const mixedAudio = getMixedAudio(contentPlanId);
      if (mixedAudio) {
        bundle.mixedAudio = mixedAudio.audioBlob;
        logs.push(`✓ Mixed audio ready (${Math.round(mixedAudio.audioBlob.size / 1024)}KB)`);
      }
    }

    // Add background music if available
    if (state.sfxPlan?.backgroundMusic?.audioUrl) {
      try {
        const response = await fetch(state.sfxPlan.backgroundMusic.audioUrl);
        if (response.ok) {
          bundle.backgroundMusic = await response.blob();
          logs.push(`✓ Background music ready`);
        }
      } catch (error) {
        logs.push(`⚠ Could not fetch background music: ${error instanceof Error ? error.message : String(error)}`);
      }
    }

    // Add visuals
    if (includeVisuals && state.visuals && state.visuals.length > 0) {
      bundle.visuals = [];
      for (const visual of state.visuals) {
        try {
          const response = await fetch(visual.imageUrl);
          if (response.ok) {
            const blob = await response.blob();
            bundle.visuals.push({
              sceneId: visual.promptId,
              blob,
              type: visual.type === 'video' ? 'video' : 'image',
            });
          }
        } catch (error) {
          logs.push(`⚠ Could not fetch visual ${visual.promptId}: ${error instanceof Error ? error.message : String(error)}`);
        }
      }
      logs.push(`✓ ${bundle.visuals.length}/${state.visuals.length} visuals ready`);
    }

    // Add subtitles
    if (includeSubtitles && state.narrationSegments && state.narrationSegments.length > 0) {
      try {
        const srtContent = generateSRTFromNarration(state.narrationSegments);
        const vttContent = generateVTTFromNarration(state.narrationSegments);

        bundle.subtitles = [
          { format: 'srt', content: srtContent },
          { format: 'vtt', content: vttContent },
        ];
        logs.push(`✓ Subtitles generated (SRT and VTT)`);
      } catch (error) {
        logs.push(`⚠ Could not generate subtitles: ${error instanceof Error ? error.message : String(error)}`);
      }
    }

    // Add production logs
    bundle.logs = logs;

    try {
      // Upload to GCS
      logs.push(`Uploading to Google Cloud Storage...`);
      const uploadResult = await uploadProductionBundle(bundle, makePublic);

      const successCount = uploadResult.results.filter((r: { success: boolean }) => r.success).length;
      const totalSize = uploadResult.results.reduce((sum: number, r: { size: number }) => sum + r.size, 0);

      const result: CloudUploadResult = {
        folderName: uploadResult.folderName,
        bucketPath: `gs://aisoul-studio-storage/${uploadResult.folderName}`,
        filesUploaded: successCount,
        totalFiles: uploadResult.results.length,
        totalSizeMB: Math.round(totalSize / 1024 / 1024 * 100) / 100,
        publicUrls: makePublic ? uploadResult.publicUrls : undefined,
        errors: uploadResult.errors.length > 0 ? uploadResult.errors : undefined,
      };

      console.log(`[CloudStorageTools] Upload complete: ${successCount}/${uploadResult.results.length} files`);

      return JSON.stringify({
        success: true,
        ...result,
        message: `Successfully uploaded ${successCount}/${uploadResult.results.length} files (${result.totalSizeMB}MB) to ${result.bucketPath}`,
      });

    } catch (error) {
      console.error("[CloudStorageTools] Upload error:", error);

      const errorMessage = error instanceof Error ? error.message : String(error);

      return JSON.stringify({
        success: false,
        error: errorMessage,
        suggestion: "Check GCS credentials and bucket permissions. Ensure GOOGLE_CLOUD_PROJECT env var is set or run 'gcloud auth application-default login'",
      });
    }
  },
  {
    name: "upload_production_to_cloud",
    description: "Upload all production outputs (video, audio, visuals, subtitles, logs) to Google Cloud Storage bucket. Creates organized folder with date/time naming (YYYY-MM-DD_HH-mm-ss). All assets are automatically fetched from session state - just provide contentPlanId. Use this after export_final_video to archive the complete production.",
    schema: UploadProductionSchema,
  }
);

// --- Export all cloud storage tools ---

// Only export tools in Node.js environment
// In browser, export empty array to prevent bundling errors
export const cloudStorageTools = isNode ? [
  uploadProductionTool,
] : [];
````

## File: packages/shared/src/services/agent/enhancementTools.ts
````typescript
/**
 * Enhancement Tools - LangChain tools for visual enhancement
 * 
 * Provides tools for enhancing generated visuals:
 * - Background removal (transparent PNG output)
 * - Style transfer (Anime, Watercolor, Oil Painting, etc.)
 * 
 * Requirements: 2.1, 2.2, 2.4, 2.5
 */

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { ai, MODELS, withRetry } from "../shared/apiClient";

// --- Types ---

/**
 * Result of background removal operation
 */
export interface BackgroundRemovalResult {
  success: boolean;
  imageBase64?: string;
  originalImageBase64?: string;
  error?: string;
  sceneIndex?: number;
}

/**
 * Result of style transfer operation
 */
export interface StyleTransferResult {
  success: boolean;
  imageBase64?: string;
  originalImageBase64?: string;
  appliedStyle?: string;
  error?: string;
  sceneIndex?: number;
  suggestedStyles?: string[];
}

/**
 * Enhanced image stored in session
 */
export interface EnhancedImage {
  sceneIndex: number;
  originalBase64: string;
  enhancedBase64: string;
  enhancementType: "background_removal" | "style_transfer";
  style?: string;
  timestamp: number;
}

// --- Session State ---

const enhancedImagesStore: Map<string, EnhancedImage[]> = new Map();

/**
 * Get enhanced images for a session
 */
export function getEnhancedImages(sessionId: string): EnhancedImage[] {
  return enhancedImagesStore.get(sessionId) || [];
}

/**
 * Add an enhanced image to a session
 */
export function addEnhancedImage(sessionId: string, image: EnhancedImage): void {
  const existing = enhancedImagesStore.get(sessionId) || [];
  existing.push(image);
  enhancedImagesStore.set(sessionId, existing);
}

/**
 * Clear enhanced images for a session
 */
export function clearEnhancedImages(sessionId: string): void {
  enhancedImagesStore.delete(sessionId);
}

// --- Available Styles ---

/**
 * Supported style options for style transfer
 */
export const AVAILABLE_STYLES = [
  "Anime",
  "Watercolor",
  "Oil Painting",
  "Pencil Sketch",
  "Pop Art",
  "Impressionist",
  "Cyberpunk",
  "Vintage Film",
  "Comic Book",
  "Minimalist",
  "Surrealist",
  "Art Nouveau",
  "Pixel Art",
  "Neon Glow",
  "Pastel Dream",
] as const;

export type StyleOption = typeof AVAILABLE_STYLES[number];

/**
 * Check if a style is recognized
 */
export function isRecognizedStyle(style: string): boolean {
  return AVAILABLE_STYLES.some(
    s => s.toLowerCase() === style.toLowerCase()
  );
}

/**
 * Find closest matching style
 */
export function findClosestStyle(input: string): string | null {
  const inputLower = input.toLowerCase();
  
  // Exact match
  const exact = AVAILABLE_STYLES.find(s => s.toLowerCase() === inputLower);
  if (exact) return exact;
  
  // Partial match
  const partial = AVAILABLE_STYLES.find(s => 
    s.toLowerCase().includes(inputLower) || inputLower.includes(s.toLowerCase())
  );
  if (partial) return partial;
  
  return null;
}

/**
 * Get style suggestions based on partial input
 */
export function getStyleSuggestions(input: string): string[] {
  const inputLower = input.toLowerCase();
  return AVAILABLE_STYLES.filter(s => 
    s.toLowerCase().includes(inputLower) || 
    inputLower.split(' ').some(word => s.toLowerCase().includes(word))
  );
}

// --- Tool Schemas ---

/**
 * Schema for background removal tool
 */
const RemoveBackgroundSchema = z.object({
  contentPlanId: z.string().describe("Session ID containing the visuals"),
  sceneIndex: z.number().describe("Index of the scene to process (0-based)"),
  imageBase64: z.string().describe("Base64-encoded image data (with or without data URI prefix)"),
});

/**
 * Schema for style transfer tool
 */
const RestyleImageSchema = z.object({
  contentPlanId: z.string().describe("Session ID containing the visuals"),
  sceneIndex: z.number().describe("Index of the scene to process (0-based)"),
  imageBase64: z.string().describe("Base64-encoded image data (with or without data URI prefix)"),
  targetStyle: z.string().describe("Target style (e.g., 'Anime', 'Watercolor', 'Oil Painting')"),
});

// --- Helper Functions ---

/**
 * Extract base64 data from a data URI or return as-is
 */
function extractBase64Data(input: string): { data: string; mimeType: string } {
  const dataUriMatch = input.match(/^data:([^;]+);base64,(.+)$/);
  if (dataUriMatch && dataUriMatch[1] && dataUriMatch[2]) {
    return {
      mimeType: dataUriMatch[1],
      data: dataUriMatch[2],
    };
  }
  // Assume PNG if no prefix
  return {
    mimeType: "image/png",
    data: input,
  };
}

/**
 * Create a data URI from base64 data
 */
function createDataUri(base64: string, mimeType: string = "image/png"): string {
  return `data:${mimeType};base64,${base64}`;
}

// --- Core Enhancement Functions ---

/**
 * Remove background from an image using Gemini
 * Returns transparent PNG
 */
async function removeBackgroundWithGemini(
  imageBase64: string,
  mimeType: string
): Promise<string> {
  console.log("[EnhancementTools] Removing background with Gemini...");

  const prompt = `You are an expert image editor. Remove the background from this image completely.
Keep only the main subject(s) in the foreground.
The background should be completely transparent.
Maintain the quality and details of the foreground subject.
Output the result as a PNG image with transparent background.`;

  const response = await withRetry(async () => {
    return ai.models.generateContent({
      model: MODELS.TEXT,
      contents: [
        {
          role: "user",
          parts: [
            { text: prompt },
            {
              inlineData: {
                mimeType,
                data: imageBase64,
              },
            },
          ],
        },
      ],
      config: {
        // @ts-ignore - responseModalities may not be in types
        responseModalities: ["IMAGE", "TEXT"],
      },
    });
  });

  // Extract image from response
  for (const part of response.candidates?.[0]?.content?.parts || []) {
    if (part.inlineData?.data) {
      return part.inlineData.data;
    }
  }

  throw new Error("No image data in response. Background removal may not be supported by the current model.");
}

/**
 * Apply style transfer to an image using Gemini
 */
async function applyStyleTransferWithGemini(
  imageBase64: string,
  mimeType: string,
  targetStyle: string
): Promise<string> {
  console.log(`[EnhancementTools] Applying ${targetStyle} style with Gemini...`);

  const stylePrompts: Record<string, string> = {
    "anime": "Transform this image into anime/manga art style with bold outlines, vibrant colors, and characteristic anime aesthetics.",
    "watercolor": "Transform this image into a beautiful watercolor painting with soft edges, color bleeding, and organic brush strokes.",
    "oil painting": "Transform this image into a classic oil painting with visible brush strokes, rich textures, and painterly qualities.",
    "pencil sketch": "Transform this image into a detailed pencil sketch with fine lines, shading, and artistic hatching.",
    "pop art": "Transform this image into bold pop art style with bright colors, halftone dots, and graphic design elements.",
    "impressionist": "Transform this image into an impressionist painting with visible brush strokes, light effects, and soft color blending.",
    "cyberpunk": "Transform this image into cyberpunk aesthetic with neon colors, futuristic elements, and high-tech atmosphere.",
    "vintage film": "Transform this image into vintage film photography style with grain, faded colors, and nostalgic atmosphere.",
    "comic book": "Transform this image into comic book art with bold outlines, halftone shading, and dynamic composition.",
    "minimalist": "Transform this image into minimalist art with simplified shapes, limited color palette, and clean design.",
    "surrealist": "Transform this image into surrealist art with dreamlike elements, unexpected combinations, and artistic distortions.",
    "art nouveau": "Transform this image into Art Nouveau style with organic curves, decorative elements, and elegant flowing lines.",
    "pixel art": "Transform this image into pixel art with visible pixels, limited color palette, and retro game aesthetics.",
    "neon glow": "Transform this image with neon glow effects, vibrant glowing colors, and dark background contrast.",
    "pastel dream": "Transform this image into soft pastel art with gentle colors, dreamy atmosphere, and ethereal quality.",
  };

  const styleLower = targetStyle.toLowerCase();
  const stylePrompt = stylePrompts[styleLower] || 
    `Transform this image into ${targetStyle} art style while maintaining the subject and composition.`;

  const prompt = `You are an expert digital artist specializing in style transfer.
${stylePrompt}
Maintain the core subject and composition of the original image.
Ensure high quality output with the distinctive characteristics of the ${targetStyle} style.
Output the result as a high-quality image.`;

  const response = await withRetry(async () => {
    return ai.models.generateContent({
      model: MODELS.TEXT,
      contents: [
        {
          role: "user",
          parts: [
            { text: prompt },
            {
              inlineData: {
                mimeType,
                data: imageBase64,
              },
            },
          ],
        },
      ],
      config: {
        // @ts-ignore - responseModalities may not be in types
        responseModalities: ["IMAGE", "TEXT"],
      },
    });
  });

  // Extract image from response
  for (const part of response.candidates?.[0]?.content?.parts || []) {
    if (part.inlineData?.data) {
      return part.inlineData.data;
    }
  }

  throw new Error("No image data in response. Style transfer may not be supported by the current model.");
}

// --- Tool Implementations ---

/**
 * Background Removal Tool
 * 
 * Removes background from an image, returning transparent PNG.
 * Requirements: 2.1, 2.4
 */
export const removeBackgroundTool = tool(
  async ({ contentPlanId, sceneIndex, imageBase64 }) => {
    console.log(`[EnhancementTools] Removing background for scene ${sceneIndex} in session ${contentPlanId}`);

    try {
      // Extract base64 data
      const { data, mimeType } = extractBase64Data(imageBase64);

      // Attempt background removal
      const resultBase64 = await removeBackgroundWithGemini(data, mimeType);

      // Store the enhanced image
      addEnhancedImage(contentPlanId, {
        sceneIndex,
        originalBase64: data,
        enhancedBase64: resultBase64,
        enhancementType: "background_removal",
        timestamp: Date.now(),
      });

      return JSON.stringify({
        success: true,
        sceneIndex,
        imageBase64: createDataUri(resultBase64, "image/png"),
        message: `Successfully removed background from scene ${sceneIndex}`,
      });

    } catch (error) {
      console.error("[EnhancementTools] Background removal error:", error);
      const errorMessage = error instanceof Error ? error.message : String(error);

      // Return original image on failure (graceful degradation per Requirement 2.4)
      return JSON.stringify({
        success: false,
        sceneIndex,
        error: errorMessage,
        originalImagePreserved: true,
        message: `Background removal failed for scene ${sceneIndex}. Original image preserved.`,
        suggestion: "The image will be used as-is. You can try again or proceed with the original.",
      });
    }
  },
  {
    name: "remove_background",
    description: "Remove the background from an image, returning a transparent PNG. If removal fails, the original image is preserved and production continues.",
    schema: RemoveBackgroundSchema,
  }
);

/**
 * Style Transfer Tool
 * 
 * Applies artistic style transfer to an image.
 * Requirements: 2.2, 2.5
 */
export const restyleImageTool = tool(
  async ({ contentPlanId, sceneIndex, imageBase64, targetStyle }) => {
    console.log(`[EnhancementTools] Applying ${targetStyle} style to scene ${sceneIndex} in session ${contentPlanId}`);

    // Check if style is recognized
    if (!isRecognizedStyle(targetStyle)) {
      const closestMatch = findClosestStyle(targetStyle);
      const suggestions = getStyleSuggestions(targetStyle);
      
      if (closestMatch) {
        console.log(`[EnhancementTools] Using closest match: ${closestMatch} for input: ${targetStyle}`);
        targetStyle = closestMatch;
      } else {
        return JSON.stringify({
          success: false,
          sceneIndex,
          error: `Style "${targetStyle}" is not recognized`,
          availableStyles: AVAILABLE_STYLES,
          suggestions: suggestions.length > 0 ? suggestions : AVAILABLE_STYLES.slice(0, 5),
          message: `Please choose from the available styles: ${AVAILABLE_STYLES.join(", ")}`,
        });
      }
    }

    try {
      // Extract base64 data
      const { data, mimeType } = extractBase64Data(imageBase64);

      // Apply style transfer
      const resultBase64 = await applyStyleTransferWithGemini(data, mimeType, targetStyle);

      // Store the enhanced image
      addEnhancedImage(contentPlanId, {
        sceneIndex,
        originalBase64: data,
        enhancedBase64: resultBase64,
        enhancementType: "style_transfer",
        style: targetStyle,
        timestamp: Date.now(),
      });

      return JSON.stringify({
        success: true,
        sceneIndex,
        appliedStyle: targetStyle,
        imageBase64: createDataUri(resultBase64, "image/png"),
        message: `Successfully applied ${targetStyle} style to scene ${sceneIndex}`,
      });

    } catch (error) {
      console.error("[EnhancementTools] Style transfer error:", error);
      const errorMessage = error instanceof Error ? error.message : String(error);

      // Return available styles on failure (per Requirement 2.5)
      return JSON.stringify({
        success: false,
        sceneIndex,
        error: errorMessage,
        requestedStyle: targetStyle,
        availableStyles: AVAILABLE_STYLES,
        message: `Style transfer failed for scene ${sceneIndex}. Try a different style or proceed with the original image.`,
        suggestion: `Available styles: ${AVAILABLE_STYLES.slice(0, 5).join(", ")}...`,
      });
    }
  },
  {
    name: "restyle_image",
    description: "Apply artistic style transfer to an image. Supports styles like Anime, Watercolor, Oil Painting, etc. If the style is not recognized, suggests available options.",
    schema: RestyleImageSchema,
  }
);

// --- Export all enhancement tools ---

export const enhancementTools = [
  removeBackgroundTool,
  restyleImageTool,
];
````

## File: packages/shared/src/services/agent/errorRecovery.ts
````typescript
/**
 * Error Recovery System for Production Agent
 * 
 * Provides:
 * - Retry logic with exponential backoff for transient failures
 * - Fallback behaviors for each tool
 * - Partial success tracking and reporting
 * - Error categorization (transient, recoverable, fatal)
 * 
 * Requirements: 6.1, 6.2, 6.3, 6.4, 6.5
 */

// --- Error Categories ---

export type ErrorCategory = 'transient' | 'recoverable' | 'fatal';

/**
 * Structured tool error with recovery information.
 * 
 * Requirement 6.4: Track errors in session state
 */
export interface ToolError {
    /** Name of the tool that failed */
    tool: string;
    /** Scene index if applicable (for per-scene operations) */
    sceneIndex?: number;
    /** Human-readable error message */
    error: string;
    /** Error category for recovery decision */
    category: ErrorCategory;
    /** Timestamp when error occurred */
    timestamp: number;
    /** Number of retry attempts made */
    retryCount: number;
    /** Whether this error is recoverable */
    recoverable: boolean;
    /** Fallback action taken (if any) */
    fallbackApplied?: string;
}

/**
 * Recovery strategy configuration for a tool.
 */
export interface RecoveryStrategy {
    /** Tool name this strategy applies to */
    tool: string;
    /** Maximum number of retries before giving up */
    maxRetries: number;
    /** Initial delay in ms for exponential backoff */
    initialDelayMs: number;
    /** Backoff multiplier (e.g., 2 = double delay each retry) */
    backoffFactor: number;
    /** Maximum delay in ms (cap for exponential backoff) */
    maxDelayMs: number;
    /** Fallback action to take on failure (null = no fallback) */
    fallbackAction: string | null;
    /** Whether to continue production if this tool fails */
    continueOnFailure: boolean;
    /** Custom error classification function */
    classifyError?: (error: Error) => ErrorCategory;
}

/**
 * Result of a tool execution with retry and recovery.
 */
export interface ToolExecutionResult<T = string> {
    /** Whether the tool succeeded */
    success: boolean;
    /** Result data if successful */
    data?: T;
    /** Error information if failed */
    error?: ToolError;
    /** Whether a fallback was applied */
    fallbackApplied: boolean;
    /** Fallback action name if applied */
    fallbackAction?: string;
    /** Number of retries attempted */
    retryCount: number;
}

/**
 * Partial success report for production run.
 * 
 * Requirement 6.4: Report partial success with details
 */
export interface PartialSuccessReport {
    /** Total number of tools/operations attempted */
    totalAttempted: number;
    /** Number of successful operations */
    succeeded: number;
    /** Number of operations that failed but continued with fallback */
    fallbackApplied: number;
    /** Number of operations that failed permanently */
    failed: number;
    /** Detailed list of all errors encountered */
    errors: ToolError[];
    /** Summary message for user */
    summary: string;
    /** Whether the production is usable despite errors */
    isUsable: boolean;
}

// --- Recovery Strategies ---

/**
 * Pre-configured recovery strategies for each tool.
 * 
 * Requirements: 6.1, 6.2, 6.3, 9.5
 */
export const RECOVERY_STRATEGIES: Record<string, RecoveryStrategy> = {
    // --- CONTENT Group ---
    plan_video: {
        tool: 'plan_video',
        maxRetries: 3,
        initialDelayMs: 1000,
        backoffFactor: 2,
        maxDelayMs: 10000,
        fallbackAction: null, // No fallback - this is required
        continueOnFailure: false,
    },
    narrate_scenes: {
        tool: 'narrate_scenes',
        maxRetries: 3,
        initialDelayMs: 1000,
        backoffFactor: 2,
        maxDelayMs: 10000,
        fallbackAction: null, // No fallback - narration is required
        continueOnFailure: false,
    },
    validate_plan: {
        tool: 'validate_plan',
        maxRetries: 2,
        initialDelayMs: 500,
        backoffFactor: 2,
        maxDelayMs: 5000,
        fallbackAction: 'assume_valid',
        continueOnFailure: true,
    },

    // --- MEDIA Group ---
    generate_visuals: {
        tool: 'generate_visuals',
        maxRetries: 3,
        initialDelayMs: 2000,
        backoffFactor: 2,
        maxDelayMs: 15000,
        fallbackAction: 'use_placeholder',
        continueOnFailure: true, // Requirement 6.1
    },
    animate_image: {
        tool: 'animate_image',
        maxRetries: 2,
        initialDelayMs: 1000,
        backoffFactor: 2,
        maxDelayMs: 10000,
        fallbackAction: 'use_static_image',
        continueOnFailure: true, // Requirement 6.2
    },
    generate_music: {
        tool: 'generate_music',
        maxRetries: 2,
        initialDelayMs: 2000,
        backoffFactor: 2,
        maxDelayMs: 15000,
        fallbackAction: null, // Continue without music
        continueOnFailure: true, // Requirement 6.3
    },
    plan_sfx: {
        tool: 'plan_sfx',
        maxRetries: 2,
        initialDelayMs: 1000,
        backoffFactor: 2,
        maxDelayMs: 8000,
        fallbackAction: null, // Continue without SFX
        continueOnFailure: true,
    },

    // --- ENHANCEMENT Group ---
    remove_background: {
        tool: 'remove_background',
        maxRetries: 2,
        initialDelayMs: 1000,
        backoffFactor: 2,
        maxDelayMs: 8000,
        fallbackAction: 'keep_original_image',
        continueOnFailure: true,
    },
    restyle_image: {
        tool: 'restyle_image',
        maxRetries: 2,
        initialDelayMs: 1000,
        backoffFactor: 2,
        maxDelayMs: 8000,
        fallbackAction: 'keep_original_image',
        continueOnFailure: true,
    },
    mix_audio_tracks: {
        tool: 'mix_audio_tracks',
        maxRetries: 2,
        initialDelayMs: 1000,
        backoffFactor: 2,
        maxDelayMs: 8000,
        fallbackAction: 'use_narration_only',
        continueOnFailure: true,
    },

    // --- EXPORT Group ---
    generate_subtitles: {
        tool: 'generate_subtitles',
        maxRetries: 2,
        initialDelayMs: 500,
        backoffFactor: 2,
        maxDelayMs: 5000,
        fallbackAction: 'skip_subtitles',
        continueOnFailure: true,
    },
    export_final_video: {
        tool: 'export_final_video',
        maxRetries: 2,
        initialDelayMs: 2000,
        backoffFactor: 2,
        maxDelayMs: 15000,
        fallbackAction: 'provide_asset_bundle', // Requirement 9.5
        continueOnFailure: false, // But we provide asset bundle as fallback
    },

    // --- IMPORT Group ---
    import_youtube_content: {
        tool: 'import_youtube_content',
        maxRetries: 3,
        initialDelayMs: 2000,
        backoffFactor: 2,
        maxDelayMs: 15000,
        fallbackAction: null,
        continueOnFailure: false,
    },
    transcribe_audio_file: {
        tool: 'transcribe_audio_file',
        maxRetries: 3,
        initialDelayMs: 1000,
        backoffFactor: 2,
        maxDelayMs: 10000,
        fallbackAction: null,
        continueOnFailure: false,
    },
};

/**
 * Get the recovery strategy for a tool.
 * Returns default strategy if not configured.
 */
export function getRecoveryStrategy(toolName: string): RecoveryStrategy {
    return RECOVERY_STRATEGIES[toolName] || {
        tool: toolName,
        maxRetries: 3,
        initialDelayMs: 1000,
        backoffFactor: 2,
        maxDelayMs: 10000,
        fallbackAction: null,
        continueOnFailure: true,
    };
}

// --- Error Classification ---

/**
 * Classify an error into categories for recovery decisions.
 * 
 * Transient: Network issues, rate limits - worth retrying
 * Recoverable: Tool-specific failure - apply fallback
 * Fatal: Configuration/auth issues - cannot recover
 */
export function classifyError(error: Error, toolName?: string): ErrorCategory {
    const message = error.message?.toLowerCase() || '';
    const statusCode = (error as any).status || (error as any).statusCode;

    // Transient errors - retry with backoff
    if (
        statusCode === 429 ||  // Rate limit
        statusCode === 500 ||  // Internal server error
        statusCode === 503 ||  // Service unavailable
        statusCode === 504 ||  // Gateway timeout
        message.includes('timeout') ||
        message.includes('network') ||
        message.includes('fetch failed') ||
        message.includes('econnrefused') ||
        message.includes('enotfound') ||
        message.includes('internal') ||
        message.includes('temporarily unavailable') ||
        message.includes('rate limit') ||
        message.includes('quota exceeded')
    ) {
        return 'transient';
    }

    // Fatal errors - cannot recover
    if (
        statusCode === 401 ||  // Unauthorized
        statusCode === 403 ||  // Forbidden
        message.includes('api key') ||
        message.includes('authentication') ||
        message.includes('not configured') ||
        message.includes('missing required') ||
        message.includes('invalid session')
    ) {
        return 'fatal';
    }

    // Recoverable - tool-specific failures
    return 'recoverable';
}

/**
 * Check if an error is retryable based on its category.
 */
export function isRetryableError(error: Error): boolean {
    const category = classifyError(error);
    return category === 'transient';
}

// --- Retry Logic ---

/**
 * Sleep for a specified duration.
 */
function sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
}

/**
 * Execute a function with retry logic and exponential backoff.
 * 
 * Requirement 6.5: Retry transient failures up to 3 times
 * 
 * @param fn Function to execute
 * @param strategy Recovery strategy to use
 * @param onRetry Optional callback for retry events
 * @returns Execution result with retry information
 */
export async function executeWithRetry<T>(
    fn: () => Promise<T>,
    strategy: RecoveryStrategy,
    onRetry?: (attempt: number, error: Error, nextDelayMs: number) => void
): Promise<ToolExecutionResult<T>> {
    let lastError: Error | null = null;
    let retryCount = 0;
    let currentDelay = strategy.initialDelayMs;

    for (let attempt = 0; attempt <= strategy.maxRetries; attempt++) {
        try {
            const result = await fn();
            return {
                success: true,
                data: result,
                fallbackApplied: false,
                retryCount,
            };
        } catch (error) {
            lastError = error instanceof Error ? error : new Error(String(error));
            const category = classifyError(lastError, strategy.tool);

            // Log the attempt
            console.warn(
                `[ErrorRecovery] ${strategy.tool} attempt ${attempt + 1}/${strategy.maxRetries + 1} failed:`,
                lastError.message,
                `(category: ${category})`
            );

            // Don't retry fatal errors
            if (category === 'fatal') {
                break;
            }

            // Don't retry if we've exhausted attempts
            if (attempt >= strategy.maxRetries) {
                break;
            }

            // Only retry transient errors
            if (category === 'transient') {
                retryCount++;

                // Notify about retry
                onRetry?.(attempt + 1, lastError, currentDelay);

                // Wait with exponential backoff
                console.log(`[ErrorRecovery] Retrying ${strategy.tool} in ${currentDelay}ms...`);
                await sleep(currentDelay);

                // Increase delay for next attempt (capped at maxDelay)
                currentDelay = Math.min(
                    currentDelay * strategy.backoffFactor,
                    strategy.maxDelayMs
                );
            } else {
                // Recoverable but not transient - don't retry, apply fallback
                break;
            }
        }
    }

    // All attempts failed - return error result
    const category = classifyError(lastError!, strategy.tool);

    return {
        success: false,
        error: {
            tool: strategy.tool,
            error: lastError!.message,
            category,
            timestamp: Date.now(),
            retryCount,
            recoverable: category !== 'fatal' && strategy.continueOnFailure,
        },
        fallbackApplied: false,
        retryCount,
    };
}

// --- Fallback Behaviors ---

/**
 * Fallback handlers for each tool.
 * Returns the fallback result or null if no fallback available.
 */
export type FallbackHandler<T = any> = (
    error: ToolError,
    context: any
) => Promise<T | null>;

const fallbackHandlers: Record<string, FallbackHandler> = {
    /**
     * Requirement 6.1: Use placeholder for failed visual generation
     */
    use_placeholder: async (_error, context) => {
        console.log('[ErrorRecovery] Applying fallback: use_placeholder');
        // Return a placeholder visual result
        return {
            success: true,
            isPlaceholder: true,
            message: 'Using placeholder image due to generation failure',
            imageUrl: null, // UI should show a placeholder
            sceneIndex: context?.sceneIndex,
        };
    },

    /**
     * Requirement 6.2: Keep static image when animation fails
     */
    use_static_image: async (_error, context) => {
        console.log('[ErrorRecovery] Applying fallback: use_static_image');
        return {
            success: true,
            isStatic: true,
            message: `Scene ${context?.sceneIndex ?? 'unknown'} will use static image (animation failed)`,
            sceneIndex: context?.sceneIndex,
        };
    },

    /**
     * Keep original image when enhancement fails
     */
    keep_original_image: async (_error, context) => {
        console.log('[ErrorRecovery] Applying fallback: keep_original_image');
        return {
            success: true,
            unchanged: true,
            message: `Keeping original image for scene ${context?.sceneIndex ?? 'unknown'}`,
            sceneIndex: context?.sceneIndex,
        };
    },

    /**
     * Use narration only when audio mixing fails
     */
    use_narration_only: async (_error, _context) => {
        console.log('[ErrorRecovery] Applying fallback: use_narration_only');
        return {
            success: true,
            narrationOnly: true,
            message: 'Using narration audio only (mixing failed)',
        };
    },

    /**
     * Skip subtitles when generation fails
     */
    skip_subtitles: async (_error, _context) => {
        console.log('[ErrorRecovery] Applying fallback: skip_subtitles');
        return {
            success: true,
            skipped: true,
            message: 'Subtitles skipped due to generation failure',
        };
    },

    /**
     * Requirement 9.5: Provide asset bundle when export fails
     */
    provide_asset_bundle: async (_error, context) => {
        console.log('[ErrorRecovery] Applying fallback: provide_asset_bundle');
        return {
            success: true,
            isAssetBundle: true,
            message: 'Video export failed. Providing asset bundle for manual assembly.',
            assets: {
                visuals: context?.visuals || [],
                narration: context?.narrationSegments || [],
                music: context?.musicUrl || null,
                sfx: context?.sfxPlan || null,
                subtitles: context?.subtitles || null,
            },
        };
    },

    /**
     * Assume valid when validation fails
     */
    assume_valid: async (_error, _context) => {
        console.log('[ErrorRecovery] Applying fallback: assume_valid');
        return {
            success: true,
            assumed: true,
            score: 70, // Assume moderate quality
            message: 'Validation failed - assuming plan is acceptable',
        };
    },
};

/**
 * Apply a fallback action for a failed tool.
 */
export async function applyFallback(
    fallbackAction: string,
    error: ToolError,
    context: any
): Promise<any> {
    const handler = fallbackHandlers[fallbackAction];
    if (!handler) {
        console.warn(`[ErrorRecovery] No fallback handler for: ${fallbackAction}`);
        return null;
    }

    try {
        const result = await handler(error, context);
        if (result) {
            error.fallbackApplied = fallbackAction;
        }
        return result;
    } catch (fallbackError) {
        console.error(
            `[ErrorRecovery] Fallback ${fallbackAction} failed:`,
            fallbackError
        );
        return null;
    }
}

// --- Partial Success Tracking ---

/**
 * Error tracker for a production session.
 * Collects errors and generates partial success reports.
 * 
 * Requirement 6.4: Track errors and report partial success
 */
export class ErrorTracker {
    private errors: ToolError[] = [];
    private totalAttempted = 0;
    private succeeded = 0;
    private fallbackApplied = 0;

    /**
     * Record a successful operation.
     */
    recordSuccess(): void {
        this.totalAttempted++;
        this.succeeded++;
    }

    /**
     * Record an error with optional fallback.
     */
    recordError(error: ToolError, fallbackApplied = false): void {
        this.totalAttempted++;
        this.errors.push(error);

        if (fallbackApplied) {
            this.fallbackApplied++;
        }
    }

    /**
     * Get all recorded errors.
     */
    getErrors(): ToolError[] {
        return [...this.errors];
    }

    /**
     * Check if there were any errors.
     */
    hasErrors(): boolean {
        return this.errors.length > 0;
    }

    /**
     * Check if there were any fatal (non-recoverable) errors.
     */
    hasFatalErrors(): boolean {
        return this.errors.some((e) => !e.recoverable);
    }

    /**
     * Generate a partial success report.
     */
    generateReport(): PartialSuccessReport {
        const failed = this.errors.filter((e) => !e.fallbackApplied && !e.recoverable).length;
        const isUsable = !this.hasFatalErrors() && this.succeeded > 0;

        // Build summary message
        let summary: string;
        if (this.errors.length === 0) {
            summary = 'All operations completed successfully.';
        } else if (isUsable) {
            summary = `Production completed with ${this.errors.length} issue(s). ` +
                `${this.fallbackApplied} fallback(s) applied. Result is usable.`;
        } else {
            summary = `Production failed with ${failed} critical error(s). ` +
                `Please review the errors and try again.`;
        }

        return {
            totalAttempted: this.totalAttempted,
            succeeded: this.succeeded,
            fallbackApplied: this.fallbackApplied,
            failed,
            errors: this.getErrors(),
            summary,
            isUsable,
        };
    }

    /**
     * Clear all tracked errors.
     */
    clear(): void {
        this.errors = [];
        this.totalAttempted = 0;
        this.succeeded = 0;
        this.fallbackApplied = 0;
    }
}

// --- High-Level Tool Execution ---

/**
 * Execute a tool with full error recovery.
 * Handles retries, fallbacks, and error tracking.
 * 
 * @param toolName Name of the tool
 * @param fn Function to execute the tool
 * @param tracker Error tracker for the session
 * @param context Context for fallback handlers
 * @param onProgress Optional progress callback
 */
export async function executeToolWithRecovery<T>(
    toolName: string,
    fn: () => Promise<T>,
    tracker: ErrorTracker,
    context?: any,
    onProgress?: (message: string) => void
): Promise<ToolExecutionResult<T>> {
    const strategy = getRecoveryStrategy(toolName);

    // Execute with retry
    const result = await executeWithRetry(fn, strategy, (attempt, error, delay) => {
        onProgress?.(
            `${toolName} failed (attempt ${attempt}/${strategy.maxRetries}). ` +
            `Retrying in ${Math.round(delay / 1000)}s... (${error.message})`
        );
    });

    if (result.success) {
        tracker.recordSuccess();
        return result;
    }

    // Record the error
    const error = result.error!;

    // Try to apply fallback
    if (strategy.fallbackAction && strategy.continueOnFailure) {
        onProgress?.(`${toolName} failed. Applying fallback: ${strategy.fallbackAction}`);

        const fallbackResult = await applyFallback(
            strategy.fallbackAction,
            error,
            context
        );

        if (fallbackResult) {
            error.fallbackApplied = strategy.fallbackAction;
            tracker.recordError(error, true);

            return {
                success: true, // Consider fallback as success
                data: fallbackResult as T,
                fallbackApplied: true,
                fallbackAction: strategy.fallbackAction,
                retryCount: result.retryCount,
            };
        }
    }

    // No fallback or fallback failed
    tracker.recordError(error, false);

    return {
        success: false,
        error,
        fallbackApplied: false,
        retryCount: result.retryCount,
    };
}

/**
 * Format errors for display in the final response.
 */
export function formatErrorsForResponse(errors: ToolError[]): string {
    if (errors.length === 0) {
        return '';
    }

    const lines = ['## Errors Encountered:\n'];

    for (const error of errors) {
        const sceneInfo = error.sceneIndex !== undefined ? ` (scene ${error.sceneIndex})` : '';
        const fallbackInfo = error.fallbackApplied ? ` → Fallback: ${error.fallbackApplied}` : '';
        const retryInfo = error.retryCount > 0 ? ` (${error.retryCount} retries)` : '';

        lines.push(`- **${error.tool}**${sceneInfo}: ${error.error}${retryInfo}${fallbackInfo}`);
    }

    return lines.join('\n');
}
````

## File: packages/shared/src/services/agent/errors.ts
````typescript
/**
 * Agent Error Types
 * 
 * Standardized error classes for agent tools to enable
 * better error handling and decision making in the orchestrator.
 */

export class AgentToolError extends Error {
    constructor(message: string, public readonly toolName: string, public readonly code: string = "UNKNOWN_ERROR") {
        super(message);
        this.name = "AgentToolError";
    }
}

export class ValidationError extends AgentToolError {
    constructor(message: string, toolName: string) {
        super(message, toolName, "VALIDATION_ERROR");
        this.name = "ValidationError";
    }
}

export class ServiceError extends AgentToolError {
    constructor(message: string, toolName: string, public readonly serviceName: string) {
        super(message, toolName, "SERVICE_ERROR");
        this.name = "ServiceError";
    }
}

export class RateLimitError extends AgentToolError {
    constructor(message: string, toolName: string, public readonly serviceName: string, public readonly retryAfter?: number) {
        super(message, toolName, "RATE_LIMIT_ERROR");
        this.name = "RateLimitError";
    }
}

export class ResourceNotFoundError extends AgentToolError {
    constructor(message: string, toolName: string, public readonly resourceId: string) {
        super(message, toolName, "RESOURCE_NOT_FOUND");
        this.name = "ResourceNotFoundError";
    }
}
````

## File: packages/shared/src/services/agent/exportTools.ts
````typescript
/**
 * Export Tools - LangChain tools for video export
 * 
 * Provides tools for exporting final video:
 * - export_final_video: Render video with all available assets
 * 
 * Requirements: 9.1, 9.2, 9.3, 9.4, 9.5
 */

import { tool } from "@langchain/core/tools";
import { ExportFinalVideoSchema, ValidateExportSchema, ListExportPresetsSchema } from "./schemas/exportSchemas";
import { ServiceError, ResourceNotFoundError, ValidationError } from "./errors";
import {
  exportVideoWithFFmpeg,
  exportVideoClientSide,
  isClientSideExportAvailable,
  EXPORT_PRESETS,
  getExportPreset,
  getAllPresetIds,
  type ExportConfig,
  type ExportProgress,
  type ExportPresetId,
} from "../ffmpeg";
import { SongData, GeneratedImage, VideoSFXPlan, SubtitleItem } from "../../types";
import { getSubtitles, type SubtitleResult } from "./subtitleTools";
import { getMixedAudio, type MixedAudioResult } from "./audioMixingTools";
import { productionStore } from "../ai/productionAgent";
import { concatenateNarrationSegments } from "./audioUtils";

// --- Types ---

/**
 * Result of video export operation
 */
export interface ExportResult {
  /** Exported video as a Blob */
  videoBlob: Blob;
  /** Video format (mp4 or webm) */
  format: "mp4" | "webm";
  /** Aspect ratio used */
  aspectRatio: string;
  /** Total duration in seconds */
  duration: number;
  /** File size in bytes */
  fileSize: number;
  /** Blob URL for download */
  downloadUrl: string;
  /** Whether subtitles were included */
  includesSubtitles: boolean;
  /** Assets included in the export */
  includedAssets: {
    visuals: number;
    narration: boolean;
    music: boolean;
    sfx: boolean;
    subtitles: boolean;
  };
}

/**
 * Asset bundle for fallback when export fails
 */
export interface AssetBundle {
  /** Visual assets with URLs */
  visuals: Array<{ sceneId: string; imageUrl: string; type: "image" | "video" }>;
  /** Narration audio URL */
  narrationUrl?: string;
  /** Music audio URL */
  musicUrl?: string;
  /** SFX plan */
  sfxPlan?: VideoSFXPlan;
  /** Subtitles content */
  subtitles?: SubtitleResult;
  /** Mixed audio URL */
  mixedAudioUrl?: string;
}

/**
 * Session storage for export results
 */
const exportStore = new Map<string, ExportResult>();

/**
 * Get export result for a session
 */
export function getExportResult(sessionId: string): ExportResult | undefined {
  return exportStore.get(sessionId);
}

/**
 * Store export result for a session
 */
export function setExportResult(sessionId: string, result: ExportResult): void {
  exportStore.set(sessionId, result);
}

/**
 * Clear export result for a session
 */
export function clearExportResult(sessionId: string): boolean {
  return exportStore.delete(sessionId);
}

// --- Tool Schema ---
// Schemas imported from ./schemas/exportSchemas

// --- Helper Functions ---

/**
 * Map aspect ratio string to orientation
 */
function getOrientation(aspectRatio: string): "landscape" | "portrait" {
  if (aspectRatio === "9:16") return "portrait";
  return "landscape";
}

/**
 * Build SongData structure from export inputs
 */
function buildSongData(
  visuals: Array<{ sceneId: string; imageUrl: string; type: "image" | "video"; startTime?: number; duration?: number }>,
  narrationUrl: string | undefined,
  mixedAudioUrl: string | undefined,
  subtitles: SubtitleResult | undefined,
  useMixedAudio: boolean
): SongData {
  // Determine audio URL - prefer mixed audio if available and requested
  const audioUrl = useMixedAudio && mixedAudioUrl ? mixedAudioUrl : narrationUrl || "";

  // Build generated images array
  const generatedImages: GeneratedImage[] = visuals.map(v => ({
    promptId: v.sceneId,
    imageUrl: v.imageUrl,
    type: v.type,
  }));

  // Build parsed subtitles from subtitle result
  const parsedSubtitles: SubtitleItem[] = subtitles?.items || [];

  return {
    fileName: "production_export",
    audioUrl,
    srtContent: subtitles?.content || "",
    parsedSubtitles,
    prompts: visuals.map(v => ({
      id: v.sceneId,
      text: "",
      mood: "",
      timestampSeconds: v.startTime || 0,
    })),
    generatedImages,
  };
}

/**
 * Build export configuration
 */
function buildExportConfig(
  aspectRatio: string,
  quality: string,
  sfxPlan: VideoSFXPlan | null,
  sceneTimings: Array<{ sceneId: string; startTime: number; duration: number }>
): Partial<ExportConfig> {
  const orientation = getOrientation(aspectRatio);

  // Quality presets
  const qualitySettings: Record<string, Partial<ExportConfig>> = {
    draft: {
      useModernEffects: false,
      transitionDuration: 0.5,
    },
    standard: {
      useModernEffects: true,
      transitionDuration: 1.0,
    },
    high: {
      useModernEffects: true,
      transitionDuration: 1.5,
    },
  };

  return {
    orientation,
    ...qualitySettings[quality] || qualitySettings.standard,
    sfxPlan,
    sceneTimings: sceneTimings.map(s => ({
      sceneId: s.sceneId,
      startTime: s.startTime,
      duration: s.duration,
    })),
    contentMode: "story",
  };
}

/**
 * Create asset bundle for fallback
 */
function createAssetBundle(
  visuals: Array<{ sceneId: string; imageUrl: string; type: "image" | "video" }>,
  narrationUrl: string | undefined,
  musicUrl: string | undefined,
  sfxPlan: VideoSFXPlan | undefined,
  subtitles: SubtitleResult | undefined,
  mixedAudioUrl: string | undefined
): AssetBundle {
  return {
    visuals: visuals.map(v => ({
      sceneId: v.sceneId,
      imageUrl: v.imageUrl,
      type: v.type as "image" | "video",
    })),
    narrationUrl,
    musicUrl,
    sfxPlan,
    subtitles,
    mixedAudioUrl,
  };
}

// --- Tool Implementation ---

/**
 * Export Final Video Tool
 * 
 * Renders the final video with all available assets including visuals,
 * narration, music, SFX, and subtitles.
 * 
 * Requirements: 9.1, 9.2, 9.3, 9.4, 9.5
 */
export const exportFinalVideoTool = tool(
  async ({
    contentPlanId,
    preset,
    format = "mp4",
    aspectRatio = "16:9",
    includeSubtitles = true,
    quality = "standard",
    visuals,
    narrationUrl,
    musicUrl,
    sfxPlan,
    totalDuration,
    useMixedAudio = false,
  }) => {
    // Apply preset if provided
    const finalFormat = format;
    let finalAspectRatio = aspectRatio;
    let finalQuality = quality;
    let presetConfig: Partial<ExportConfig> = {};

    if (preset) {
      const presetData = getExportPreset(preset as ExportPresetId);
      if (presetData) {
        finalAspectRatio = presetData.aspectRatio === "4:5" ? "1:1" : presetData.aspectRatio; // 4:5 not supported, fallback to 1:1
        finalQuality = presetData.quality;
        presetConfig = presetData.config;
        console.log(`[ExportTools] Using preset: ${preset} (${presetData.name})`);
      }
    }

    console.log(`[ExportTools] Exporting video for session: ${contentPlanId}`);
    console.log(`[ExportTools] Format: ${finalFormat}, Aspect: ${finalAspectRatio}, Quality: ${finalQuality}${preset ? `, Preset: ${preset}` : ""}`);

    // Get session state for auto-fetching missing data
    const state = productionStore.get(contentPlanId);
    if (!state) {
      throw new ResourceNotFoundError("Production session not found. Run plan_video first to create a production session.", "export_final_video", contentPlanId);
    }

    // Auto-fetch visuals from session if not provided
    let finalVisuals = visuals;
    if (!finalVisuals || finalVisuals.length === 0) {
      if (!state.visuals || state.visuals.length === 0) {
        throw new ResourceNotFoundError("No visuals found in session and none provided. Run generate_visuals first.", "export_final_video", "visuals");
      }

      // Build visuals array from session state
      // Calculate startTime from cumulative scene durations (Scene type doesn't have startTime)
      const scenes = state.contentPlan?.scenes || [];
      finalVisuals = state.visuals.map((visual, index) => {
        // Calculate start time by summing durations of all previous scenes
        let startTime = 0;
        for (let i = 0; i < index && i < scenes.length; i++) {
          startTime += scenes[i]?.duration || 0;
        }
        return {
          sceneId: visual.promptId,
          imageUrl: visual.imageUrl,
          type: visual.type || "image" as "image" | "video",
          startTime,
          duration: scenes[index]?.duration,
        };
      });
      console.log(`[ExportTools] Auto-fetched ${finalVisuals.length} visuals from session`);
    }

    // Auto-fetch narration URL from session if not provided
    let finalNarrationUrl = narrationUrl;
    if (!finalNarrationUrl && !useMixedAudio) {
      if (!state.narrationSegments || state.narrationSegments.length === 0) {
        throw new ResourceNotFoundError("No narration found in session. Run narrate_scenes first.", "export_final_video", "narration");
      }

      // Concatenate all narration segments into a single audio blob
      try {
        console.log(`[ExportTools] Concatenating ${state.narrationSegments.length} narration segments`);
        const concatenatedBlob = await concatenateNarrationSegments(state.narrationSegments);
        finalNarrationUrl = URL.createObjectURL(concatenatedBlob);
        console.log(`[ExportTools] Auto-fetched narration URL from concatenated audio (${Math.round(concatenatedBlob.size / 1024)}KB)`);
      } catch (error) {
        return JSON.stringify({
          success: false,
          error: `Failed to concatenate narration segments: ${error instanceof Error ? error.message : String(error)}`,
          suggestion: "Check that all narration segments have valid audio blobs",
        });
      }
    }

    // Auto-fetch total duration from content plan if not provided
    let finalDuration = totalDuration;
    if (!finalDuration && state.contentPlan) {
      finalDuration = state.contentPlan.totalDuration;
      console.log(`[ExportTools] Auto-fetched duration from content plan: ${finalDuration}s`);
    }

    if (!finalDuration) {
      throw new ValidationError("No duration provided and content plan has no duration.", "export_final_video");
    }

    // Auto-fetch SFX plan if not provided
    let finalSfxPlan = sfxPlan;
    if (!finalSfxPlan && state.sfxPlan) {
      finalSfxPlan = state.sfxPlan;
      console.log(`[ExportTools] Auto-fetched SFX plan from session`);
    }

    console.log(`[ExportTools] Visuals: ${finalVisuals.length}, Duration: ${finalDuration}s`);

    // Get subtitles if requested
    let subtitles: SubtitleResult | undefined;
    if (includeSubtitles) {
      subtitles = getSubtitles(contentPlanId);
      if (!subtitles) {
        console.log(`[ExportTools] No subtitles found for session, continuing without`);
      }
    }

    // Get mixed audio if requested
    let mixedAudio: MixedAudioResult | undefined;
    let mixedAudioUrl: string | undefined;
    if (useMixedAudio) {
      mixedAudio = getMixedAudio(contentPlanId);
      if (mixedAudio) {
        mixedAudioUrl = URL.createObjectURL(mixedAudio.audioBlob);
        console.log(`[ExportTools] Using pre-mixed audio (${Math.round(mixedAudio.duration)}s)`);
      } else {
        console.log(`[ExportTools] No mixed audio found, falling back to narration`);
        // FALLBACK: If mixed audio requested but not found, try to get narration
        if (!finalNarrationUrl && state.narrationSegments && state.narrationSegments.length > 0) {
          try {
            console.log(`[ExportTools] Fallback: Concatenating ${state.narrationSegments.length} narration segments`);
            const concatenatedBlob = await concatenateNarrationSegments(state.narrationSegments);
            finalNarrationUrl = URL.createObjectURL(concatenatedBlob);
            console.log(`[ExportTools] Fallback: Created narration URL (${Math.round(concatenatedBlob.size / 1024)}KB)`);
          } catch (error) {
            console.error(`[ExportTools] Fallback narration concatenation failed:`, error);
          }
        }
      }
    }

    // Build scene timings from visuals
    const sceneTimings = finalVisuals.map((v, index) => {
      const startTime = v.startTime ?? (index * (finalDuration / finalVisuals.length));
      const duration = v.duration ?? (finalDuration / finalVisuals.length);
      return {
        sceneId: v.sceneId,
        startTime,
        duration,
      };
    });

    // Build SongData for export
    const songData = buildSongData(
      finalVisuals,
      finalNarrationUrl,
      mixedAudioUrl,
      includeSubtitles ? subtitles : undefined,
      useMixedAudio && !!mixedAudioUrl
    );

    // Build export config with preset overrides
    const baseExportConfig = buildExportConfig(
      finalAspectRatio,
      finalQuality,
      finalSfxPlan as VideoSFXPlan | null,
      sceneTimings
    );

    // Merge preset config if using a preset
    const exportConfig = preset ? { ...baseExportConfig, ...presetConfig } : baseExportConfig;

    // Track progress
    const onProgress = (progress: ExportProgress) => {
      console.log(`[ExportTools] Progress: ${progress.stage} - ${progress.progress}% - ${progress.message}`);
    };

    try {
      // Always prefer server-side export for faster encoding
      // Fall back to client-side only if server is unavailable
      const useServerSide = true; // Default to server-side for speed
      console.log(`[ExportTools] Using server-side export for faster encoding`);

      let videoBlob: Blob;

      if (useServerSide) {
        const exportResult = await exportVideoWithFFmpeg(songData, onProgress, exportConfig);
        videoBlob = exportResult.blob;
      } else {
        const exportResult = await exportVideoClientSide(songData, onProgress, exportConfig);
        videoBlob = exportResult.blob;
      }

      // Create download URL
      const downloadUrl = URL.createObjectURL(videoBlob);

      // Build result
      const result: ExportResult = {
        videoBlob,
        format: finalFormat,
        aspectRatio: finalAspectRatio,
        duration: finalDuration,
        fileSize: videoBlob.size,
        downloadUrl,
        includesSubtitles: includeSubtitles && !!subtitles,
        includedAssets: {
          visuals: finalVisuals.length,
          narration: !!finalNarrationUrl || (useMixedAudio && !!mixedAudio),
          music: !!musicUrl || (useMixedAudio && mixedAudio?.tracks.music.present) || false,
          sfx: !!finalSfxPlan || (useMixedAudio && mixedAudio?.tracks.sfx.present) || false,
          subtitles: includeSubtitles && !!subtitles,
        },
      };

      // Store result
      setExportResult(contentPlanId, result);

      // Also store video blob in production state for cloud upload
      const state = productionStore.get(contentPlanId);
      if (state) {
        state.exportedVideo = videoBlob;
        state.exportResult = result;
        productionStore.set(contentPlanId, state);
      }

      return JSON.stringify({
        success: true,
        sessionId: contentPlanId,
        downloadUrl,
        format: finalFormat,
        aspectRatio: finalAspectRatio,
        quality: finalQuality,
        preset: preset || undefined,
        duration: Math.round(finalDuration * 100) / 100,
        fileSizeMB: Math.round(videoBlob.size / (1024 * 1024) * 100) / 100,
        includedAssets: result.includedAssets,
        message: `Successfully exported ${finalFormat.toUpperCase()} video (${Math.round(finalDuration)}s, ${Math.round(videoBlob.size / (1024 * 1024) * 10) / 10}MB)${preset ? ` using ${preset} preset` : ""}`,
      });

    } catch (error) {
      console.error("[ExportTools] Export error:", error);

      const errorMessage = error instanceof Error ? error.message : String(error);

      // Create asset bundle as fallback
      const assetBundle = createAssetBundle(
        finalVisuals,
        finalNarrationUrl,
        musicUrl,
        finalSfxPlan as VideoSFXPlan | undefined,
        subtitles,
        mixedAudioUrl
      );

      // Generate asset bundle download info
      const assetBundleInfo = {
        visualCount: assetBundle.visuals.length,
        hasNarration: !!assetBundle.narrationUrl,
        hasMusic: !!assetBundle.musicUrl,
        hasSfx: !!assetBundle.sfxPlan,
        hasSubtitles: !!assetBundle.subtitles,
        hasMixedAudio: !!assetBundle.mixedAudioUrl,
      };

      return JSON.stringify({
        success: false,
        error: errorMessage,
        fallback: "asset_bundle",
        assetBundle: assetBundleInfo,
        assetBundleData: assetBundle,
        suggestion: "Export failed. An asset bundle has been provided as fallback. You can download individual assets and assemble them manually, or retry the export.",
        message: `Export failed: ${errorMessage}. Asset bundle available with ${assetBundle.visuals.length} visuals.`,
      });
    }
  },
  {
    name: "export_final_video",
    description: "Export the final video with all available assets. All assets (visuals, narration, SFX) are automatically fetched from the session - you only need to provide contentPlanId. Use 'preset' to apply platform-optimized settings (e.g., 'tiktok', 'youtube-shorts', 'instagram-reels'). Or manually specify format (mp4/webm), aspectRatio (16:9/9:16/1:1), quality (draft/standard/high). Presets automatically configure aspect ratio, quality, orientation, and transitions for the target platform. If export fails, provides an asset bundle as fallback for manual assembly.",
    schema: ExportFinalVideoSchema,
  }
);

/**
 * Schema for validate_export tool
 */
// Imported from ./schemas/exportSchemas

/**
 * Export validation result
 */
export interface ExportValidationResult {
  /** Whether export is ready to proceed */
  isReady: boolean;
  /** Estimated duration in seconds */
  estimatedDuration: number;
  /** Estimated file size in MB */
  estimatedFileSizeMB: number;
  /** Asset validation results */
  assets: {
    visuals: {
      ready: boolean;
      count: number;
      videoCount: number;
      imageCount: number;
      missingScenes: string[];
    };
    narration: {
      ready: boolean;
      segmentCount: number;
      totalDuration: number;
    };
    sfx: {
      ready: boolean;
      sceneCount: number;
    };
    subtitles: {
      ready: boolean;
    };
    mixedAudio: {
      ready: boolean;
      duration: number;
    };
  };
  /** Warnings that won't prevent export but should be noted */
  warnings: string[];
  /** Errors that will prevent export */
  errors: string[];
  /** Suggestions for fixing issues */
  suggestions: string[];
}

/**
 * Validate Export Tool
 *
 * Validates export readiness without actually rendering.
 * Checks all assets are present, valid, and consistent.
 */
export const validateExportTool = tool(
  async ({ contentPlanId }) => {
    console.log(`[ExportTools] Validating export readiness for session: ${contentPlanId}`);

    const result: ExportValidationResult = {
      isReady: true,
      estimatedDuration: 0,
      estimatedFileSizeMB: 0,
      assets: {
        visuals: { ready: false, count: 0, videoCount: 0, imageCount: 0, missingScenes: [] },
        narration: { ready: false, segmentCount: 0, totalDuration: 0 },
        sfx: { ready: false, sceneCount: 0 },
        subtitles: { ready: false },
        mixedAudio: { ready: false, duration: 0 },
      },
      warnings: [],
      errors: [],
      suggestions: [],
    };

    // Get session state
    const state = productionStore.get(contentPlanId);
    if (!state) {
      result.isReady = false;
      result.errors.push("Production session not found");
      result.suggestions.push("Run plan_video first to create a production session");
      return JSON.stringify({ success: false, validation: result });
    }

    // Validate content plan
    if (!state.contentPlan) {
      result.isReady = false;
      result.errors.push("No content plan found");
      result.suggestions.push("Run plan_video to create a content plan");
    } else {
      result.estimatedDuration = state.contentPlan.totalDuration || 0;
    }

    const expectedSceneCount = state.contentPlan?.scenes.length || 0;

    // Validate visuals
    if (state.visuals && state.visuals.length > 0) {
      result.assets.visuals.count = state.visuals.length;
      result.assets.visuals.videoCount = state.visuals.filter(v => v.type === "video").length;
      result.assets.visuals.imageCount = state.visuals.filter(v => v.type !== "video").length;

      // Check for missing scenes
      if (state.contentPlan?.scenes) {
        const visualSceneIds = new Set(state.visuals.map(v => v.promptId));
        for (const scene of state.contentPlan.scenes) {
          if (!visualSceneIds.has(scene.id)) {
            result.assets.visuals.missingScenes.push(scene.id);
          }
        }
      }

      if (result.assets.visuals.missingScenes.length > 0) {
        result.warnings.push(`Missing visuals for ${result.assets.visuals.missingScenes.length} scenes`);
      }

      if (result.assets.visuals.count >= expectedSceneCount) {
        result.assets.visuals.ready = true;
      } else {
        result.assets.visuals.ready = result.assets.visuals.count > 0; // Partial is ok
        result.warnings.push(`Only ${result.assets.visuals.count}/${expectedSceneCount} scenes have visuals`);
      }

      // Check video assets for validity
      for (const visual of state.visuals) {
        if (visual.type === "video" && !visual.imageUrl) {
          result.warnings.push(`Video asset ${visual.promptId} has no URL`);
        }
      }

      // Log video asset info
      if (result.assets.visuals.videoCount > 0) {
        console.log(`[ExportTools] Found ${result.assets.visuals.videoCount} Veo video assets`);
      }
    } else {
      result.isReady = false;
      result.errors.push("No visual assets found");
      result.suggestions.push("Run generate_visuals to create visual assets");
    }

    // Validate narration
    if (state.narrationSegments && state.narrationSegments.length > 0) {
      result.assets.narration.segmentCount = state.narrationSegments.length;
      result.assets.narration.totalDuration = state.narrationSegments.reduce(
        (sum, seg) => sum + (seg.audioDuration || 0),
        0
      );
      result.assets.narration.ready = true;

      // Check for segments without audio
      const segmentsWithoutAudio = state.narrationSegments.filter(seg => !seg.audioBlob);
      if (segmentsWithoutAudio.length > 0) {
        result.warnings.push(`${segmentsWithoutAudio.length} narration segments missing audio`);
      }

      // Check duration consistency
      if (Math.abs(result.assets.narration.totalDuration - result.estimatedDuration) > 5) {
        result.warnings.push(
          `Narration duration (${result.assets.narration.totalDuration.toFixed(1)}s) differs from plan duration (${result.estimatedDuration.toFixed(1)}s)`
        );
      }
    } else {
      result.isReady = false;
      result.errors.push("No narration segments found");
      result.suggestions.push("Run narrate_scenes to generate narration");
    }

    // Validate SFX
    if (state.sfxPlan) {
      result.assets.sfx.sceneCount = state.sfxPlan.scenes?.length || 0;
      result.assets.sfx.ready = true;

      // Check for scenes with SFX URLs
      const scenesWithAudio = state.sfxPlan.scenes?.filter(s => s.ambientTrack?.audioUrl) || [];
      if (scenesWithAudio.length < result.assets.sfx.sceneCount) {
        result.warnings.push(
          `Only ${scenesWithAudio.length}/${result.assets.sfx.sceneCount} SFX scenes have audio URLs`
        );
      }
    }

    // Validate subtitles
    const subtitles = getSubtitles(contentPlanId);
    if (subtitles) {
      result.assets.subtitles.ready = true;
    }

    // Validate mixed audio
    const mixedAudio = getMixedAudio(contentPlanId);
    if (mixedAudio) {
      result.assets.mixedAudio.ready = true;
      result.assets.mixedAudio.duration = mixedAudio.duration;
    }

    // Estimate file size (rough estimate: ~1MB per 10 seconds at standard quality)
    result.estimatedFileSizeMB = Math.round(result.estimatedDuration * 0.1 * 10) / 10;
    if (result.assets.visuals.videoCount > 0) {
      // Videos typically result in larger files
      result.estimatedFileSizeMB *= 1.5;
    }

    // Final readiness check
    result.isReady = result.errors.length === 0 && result.assets.visuals.ready && result.assets.narration.ready;

    // Add general suggestions
    if (!result.assets.mixedAudio.ready && result.assets.sfx.ready) {
      result.suggestions.push("Consider running mix_audio_tracks to include SFX in the export");
    }
    if (!result.assets.subtitles.ready) {
      result.suggestions.push("Consider running generate_subtitles for accessibility");
    }

    console.log(`[ExportTools] Validation complete: ${result.isReady ? '✓ Ready' : '✗ Not ready'}`);
    console.log(`[ExportTools] Estimated: ${result.estimatedDuration}s, ~${result.estimatedFileSizeMB}MB`);

    return JSON.stringify({
      success: true,
      validation: result,
      message: result.isReady
        ? `Export ready! ${result.assets.visuals.count} visuals (${result.assets.visuals.videoCount} videos), ${result.assets.narration.segmentCount} narration segments, ~${result.estimatedDuration}s duration`
        : `Export not ready: ${result.errors.join(", ")}`,
    });
  },
  {
    name: "validate_export",
    description:
      "Validate export readiness without rendering. Checks all assets are present, valid, and consistent. Returns detailed validation results including asset counts, warnings, errors, and suggestions. Use before export_final_video to catch issues early.",
    schema: ValidateExportSchema,
  }
);

// --- List Export Presets Tool ---

/**
 * Schema for list_export_presets tool
 */
// Imported from ./schemas/exportSchemas

/**
 * List Export Presets Tool
 *
 * Returns available export presets with their configurations.
 * Useful for helping users choose appropriate export settings.
 */
export const listExportPresetsTool = tool(
  async ({ platform, aspectRatio }) => {
    console.log(`[ExportTools] Listing export presets${platform ? ` for platform: ${platform}` : ""}${aspectRatio ? ` with aspect ratio: ${aspectRatio}` : ""}`);

    let presets = Object.values(EXPORT_PRESETS);

    // Filter by platform if specified
    if (platform) {
      presets = presets.filter(p =>
        p.platform.toLowerCase().includes(platform.toLowerCase()) ||
        p.id.toLowerCase().includes(platform.toLowerCase())
      );
    }

    // Filter by aspect ratio if specified
    if (aspectRatio) {
      presets = presets.filter(p => p.aspectRatio === aspectRatio);
    }

    // Format presets for output
    const formattedPresets = presets.map(p => ({
      id: p.id,
      name: p.name,
      platform: p.platform,
      description: p.description,
      aspectRatio: p.aspectRatio,
      orientation: p.orientation,
      fps: p.fps,
      quality: p.quality,
      maxDuration: p.maxDuration,
      minDuration: p.minDuration,
    }));

    return JSON.stringify({
      success: true,
      presets: formattedPresets,
      count: formattedPresets.length,
      allPresetIds: getAllPresetIds(),
      message: formattedPresets.length > 0
        ? `Found ${formattedPresets.length} export preset(s)${platform ? ` for ${platform}` : ""}${aspectRatio ? ` with ${aspectRatio} aspect ratio` : ""}`
        : "No presets match the specified filters",
    });
  },
  {
    name: "list_export_presets",
    description:
      "List available export presets for different platforms. Presets provide optimized settings for platforms like YouTube, TikTok, Instagram, etc. Optionally filter by platform name or aspect ratio. Use this when the user asks about export options or to recommend appropriate settings.",
    schema: ListExportPresetsSchema,
  }
);

// --- Export all export tools ---

export const exportTools = [
  exportFinalVideoTool,
  validateExportTool,
  listExportPresetsTool,
];
````

## File: packages/shared/src/services/agent/importTools.ts
````typescript
/**
 * Import Tools - LangChain tools for content import
 * 
 * Provides tools for importing content from external sources:
 * - YouTube video import (audio extraction + transcription)
 * - Audio file transcription
 * 
 * Requirements: 1.1, 1.2, 1.3, 1.4, 1.5
 */

import { tool } from "@langchain/core/tools";
import { transcribeAudioWithWordTiming } from "../transcriptionService";
import { ImportYouTubeSchema, TranscribeAudioSchema } from "./schemas/importSchemas";
import { ServiceError, ValidationError, ResourceNotFoundError } from "./errors";
import {
  type ImportedContent,
  type TranscriptResult,
  type TranscriptSegment,
  type WordTimingInfo,
  SUPPORTED_AUDIO_FORMATS,
  getImportedContent,
  setImportedContent,
  clearImportedContent,
  isValidYouTubeUrl,
  extractYouTubeVideoId,
  subtitleItemsToTranscriptResult,
  getServerBaseUrl,
} from "./importUtils";

// Re-export types and functions for consumers
export { 
  type ImportedContent,
  type TranscriptResult,
  type TranscriptSegment,
  type WordTimingInfo,
  getImportedContent,
  setImportedContent,
  clearImportedContent
};

// --- Tool Implementations ---

/**
 * YouTube Import Tool
 * 
 * Downloads audio from YouTube/X videos and prepares for transcription.
 * Requirements: 1.1, 1.4
 */
export const importYouTubeTool = tool(
  async ({ url }: { url: string }) => {
    console.log(`[ImportTools] Importing from YouTube: ${url}`);

    // Validate URL
    if (!isValidYouTubeUrl(url)) {
      throw new ValidationError("Invalid YouTube/X URL. Please provide a valid YouTube (youtube.com, youtu.be) or X (twitter.com, x.com) video URL.", "import_youtube_content");
    }

    try {
      const serverUrl = getServerBaseUrl();
      if (!serverUrl) {
        throw new ServiceError("Server URL configuration missing", "import_youtube_content", "Configuration");
      }
      const response = await fetch(`${serverUrl}/api/import/youtube`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ url }),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({ error: "Unknown error" }));
        throw new ServiceError(errorData.error || `Failed to import from YouTube (HTTP ${response.status})`, "import_youtube_content", "YouTube Import Service");
      }

      // Get the audio blob
      const audioBlob = await response.blob();

      if (audioBlob.size === 0) {
        throw new ServiceError("Downloaded audio is empty", "import_youtube_content", "YouTube Import Service");
      }

      // Convert to base64 for storage
      const arrayBuffer = await audioBlob.arrayBuffer();
      const base64Audio = btoa(
        new Uint8Array(arrayBuffer).reduce((data, byte) => data + String.fromCharCode(byte), "")
      );

      // Generate session ID
      const sessionId = `import_${Date.now()}_${Math.random().toString(36).substring(2, 11)}`;
      const videoId = extractYouTubeVideoId(url);

      // Now transcribe the audio
      console.log(`[ImportTools] Transcribing imported audio...`);
      const subtitleItems = await transcribeAudioWithWordTiming(base64Audio, "audio/mpeg");

      // Convert to transcript result
      const transcript = subtitleItemsToTranscriptResult(subtitleItems, "auto");

      // Calculate duration from transcript
      const duration = subtitleItems.length > 0
        ? subtitleItems[subtitleItems.length - 1]!.endTime
        : 0;

      // Store the imported content
      const importedContent: ImportedContent = {
        source: "youtube",
        sourceUrl: url,
        audioBase64: base64Audio,
        audioMimeType: "audio/mpeg",
        transcript,
        duration,
        metadata: {
          title: videoId ? `YouTube Video ${videoId}` : undefined,
        },
      };

      setImportedContent(sessionId, importedContent);

      return JSON.stringify({
        success: true,
        sessionId,
        source: "youtube",
        sourceUrl: url,
        duration,
        transcriptSegments: transcript.segments.length,
        transcriptPreview: transcript.text.substring(0, 200) + (transcript.text.length > 200 ? "..." : ""),
        message: `Successfully imported audio from YouTube (${Math.round(duration)}s, ${transcript.segments.length} segments)`,
      });

    } catch (error) {
      console.error("[ImportTools] YouTube import error:", error);
      return JSON.stringify({
        success: false,
        error: error instanceof Error ? error.message : String(error),
        suggestion: "Check your network connection and try again. If the problem persists, the video may be unavailable.",
      });
    }
  },
  {
    name: "import_youtube_content",
    description: "Import audio from a YouTube or X (Twitter) video URL. Extracts audio and transcribes it with word-level timing. Returns a sessionId for use with other tools.",
    schema: ImportYouTubeSchema,
  }
);

/**
 * Audio Transcription Tool
 * 
 * Transcribes audio with word-level timing.
 * Requirements: 1.2, 1.3, 1.5
 */
export const transcribeAudioTool = tool(
  async ({ contentPlanId, language }) => {
    console.log(`[ImportTools] Transcribing audio for session: ${contentPlanId}`);

    // Check if we have imported content
    const importedContent = getImportedContent(contentPlanId);

    if (!importedContent) {
      throw new ResourceNotFoundError("No imported content found for this session. Import content first using import_youtube_content.", "transcribe_audio_file", contentPlanId);
    }

    if (!importedContent.audioBase64 || !importedContent.audioMimeType) {
      throw new ValidationError("No audio data available in the imported content.", "transcribe_audio_file");
    }

    // Check if already transcribed
    if (importedContent.transcript && importedContent.transcript.segments.length > 0) {
      return JSON.stringify({
        success: true,
        sessionId: contentPlanId,
        segmentCount: importedContent.transcript.segments.length,
        language: importedContent.transcript.language,
        duration: importedContent.duration,
        transcriptPreview: importedContent.transcript.text.substring(0, 200) +
          (importedContent.transcript.text.length > 200 ? "..." : ""),
        message: `Transcript already exists (${importedContent.transcript.segments.length} segments)`,
      });
    }

    try {
      // Transcribe the audio
      const subtitleItems = await transcribeAudioWithWordTiming(
        importedContent.audioBase64,
        importedContent.audioMimeType
      );

      // Convert to transcript result
      const transcript = subtitleItemsToTranscriptResult(subtitleItems, language || "auto");

      // Update the imported content with transcript
      importedContent.transcript = transcript;
      importedContent.duration = subtitleItems.length > 0
        ? subtitleItems[subtitleItems.length - 1]!.endTime
        : importedContent.duration;

      setImportedContent(contentPlanId, importedContent);

      return JSON.stringify({
        success: true,
        sessionId: contentPlanId,
        segmentCount: transcript.segments.length,
        language: transcript.language,
        duration: importedContent.duration,
        transcriptPreview: transcript.text.substring(0, 200) + (transcript.text.length > 200 ? "..." : ""),
        message: `Transcribed audio with ${transcript.segments.length} segments (~${Math.round(importedContent.duration)}s)`,
      });

    } catch (error) {
      console.error("[ImportTools] Transcription error:", error);

      // Check for unsupported format error
      const errorMessage = error instanceof Error ? error.message : String(error);
      if (errorMessage.toLowerCase().includes("format") || errorMessage.toLowerCase().includes("mime")) {
        return JSON.stringify({
          success: false,
          error: "Unsupported audio format",
          supportedFormats: SUPPORTED_AUDIO_FORMATS,
          suggestion: `Please use one of the supported formats: ${SUPPORTED_AUDIO_FORMATS.join(", ")}`,
        });
      }

      return JSON.stringify({
        success: false,
        error: errorMessage,
        suggestion: "Check if the audio file is valid and not corrupted.",
      });
    }
  },
  {
    name: "transcribe_audio_file",
    description: "Transcribe audio with word-level timing. Use after importing content. Supports language specification for better accuracy.",
    schema: TranscribeAudioSchema,
  }
);

// --- Export all import tools ---

export const importTools = [
  importYouTubeTool,
  transcribeAudioTool,
];
````

## File: packages/shared/src/services/agent/importUtils.ts
````typescript
/**
 * Import Utilities - Pure functions for content import
 * 
 * This module contains pure functions that can be tested without API dependencies.
 * Used by importTools.ts for the actual tool implementations.
 * 
 * Requirements: 1.1, 1.2, 1.3
 */

import { SubtitleItem } from "../../types";

// --- Types ---

/**
 * Imported content from YouTube or audio file
 */
export interface ImportedContent {
  /** Source type */
  source: "youtube" | "audio_file";
  /** Original URL for YouTube imports */
  sourceUrl?: string;
  /** Audio blob (stored as base64 for serialization) */
  audioBase64?: string;
  /** Audio MIME type */
  audioMimeType?: string;
  /** Transcription result */
  transcript: TranscriptResult;
  /** Audio duration in seconds */
  duration: number;
  /** Optional metadata */
  metadata?: {
    title?: string;
    author?: string;
  };
}

/**
 * Transcription result with segments
 */
export interface TranscriptResult {
  /** Full text of the transcript */
  text: string;
  /** Segments with timing */
  segments: TranscriptSegment[];
  /** Detected or specified language */
  language: string;
  /** Confidence score (0-1) */
  confidence: number;
}

/**
 * A segment of the transcript with timing
 */
export interface TranscriptSegment {
  /** Start time in seconds */
  start: number;
  /** End time in seconds */
  end: number;
  /** Text content */
  text: string;
  /** Optional word-level timing */
  words?: WordTimingInfo[];
}

/**
 * Word-level timing information
 */
export interface WordTimingInfo {
  word: string;
  start: number;
  end: number;
}

// --- Supported Formats ---

export const SUPPORTED_AUDIO_FORMATS = ["mp3", "wav", "m4a", "ogg", "webm", "flac", "aac"];
export const SUPPORTED_MIME_TYPES = [
  "audio/mpeg",
  "audio/mp3",
  "audio/wav",
  "audio/x-wav",
  "audio/m4a",
  "audio/x-m4a",
  "audio/ogg",
  "audio/webm",
  "audio/flac",
  "audio/aac",
];

// --- Import Store ---

/**
 * Store for imported content (keyed by session ID)
 * This is shared with productionAgent's productionStore
 */
const importStore: Map<string, ImportedContent> = new Map();

/**
 * Get imported content by session ID
 */
export function getImportedContent(sessionId: string): ImportedContent | undefined {
  return importStore.get(sessionId);
}

/**
 * Store imported content
 */
export function setImportedContent(sessionId: string, content: ImportedContent): void {
  importStore.set(sessionId, content);
}

/**
 * Clear imported content
 */
export function clearImportedContent(sessionId: string): void {
  importStore.delete(sessionId);
}

// --- URL Validation ---

/**
 * Validate if a URL is a valid YouTube or X (Twitter) URL
 */
export function isValidYouTubeUrl(url: string): boolean {
  try {
    const parsed = new URL(url);
    const validHosts = [
      "youtube.com",
      "www.youtube.com",
      "youtu.be",
      "m.youtube.com",
      "twitter.com",
      "www.twitter.com",
      "x.com",
      "www.x.com",
    ];
    return validHosts.some(host => parsed.hostname === host || parsed.hostname.endsWith(`.${host}`));
  } catch {
    return false;
  }
}

/**
 * Extract video ID from YouTube URL for metadata
 */
export function extractYouTubeVideoId(url: string): string | null {
  try {
    const parsed = new URL(url);
    
    // youtu.be/VIDEO_ID
    if (parsed.hostname === "youtu.be") {
      return parsed.pathname.slice(1);
    }
    
    // youtube.com/watch?v=VIDEO_ID
    if (parsed.hostname.includes("youtube.com")) {
      return parsed.searchParams.get("v");
    }
    
    return null;
  } catch {
    return null;
  }
}

// --- Conversion Functions ---

/**
 * Convert SubtitleItem array to TranscriptResult
 * 
 * This is the core conversion function that transforms subtitle items
 * (from transcription) into the transcript format used by the import system.
 */
export function subtitleItemsToTranscriptResult(
  items: SubtitleItem[],
  language: string = "auto"
): TranscriptResult {
  const segments: TranscriptSegment[] = items.map(item => ({
    start: item.startTime,
    end: item.endTime,
    text: item.text,
    words: item.words?.map(w => ({
      word: w.word,
      start: w.startTime,
      end: w.endTime,
    })),
  }));

  const fullText = items.map(item => item.text).join(" ");

  return {
    text: fullText,
    segments,
    language,
    confidence: 0.9, // Gemini transcription is generally high quality
  };
}

/**
 * Get server base URL for API calls
 */
export function getServerBaseUrl(): string {
  // Check for browser environment
  if (typeof window !== "undefined") {
    // In browser, use relative URL or configured server
    // @ts-ignore - Vite injects env at build time
    return (import.meta as any).env?.VITE_SERVER_URL || "http://localhost:3001";
  }
  // In Node.js, use environment variable or default
  return process.env.SERVER_URL || "http://localhost:3001";
}
````

## File: packages/shared/src/services/agent/index.ts
````typescript
/**
 * Agent Module Index
 * 
 * Re-exports all agent-related modules for convenient imports.
 */

export { agentDirectorLogger, LogLevel, type LogEntry } from './agentLogger';
export { agentMetrics, type AgentDirectorMetrics } from './agentMetrics';
export {
    allTools,
    executeToolCall,
    sanitizeJsonString,
    getVisualReferences,
    critiqueStoryboard,
    jsonExtractor,
    fallbackProcessor,
    analyzeContentTool,
    searchVisualReferencesTool,
    analyzeAndGenerateStoryboardTool,
    generateStoryboardTool,
    refinePromptTool,
    critiqueStoryboardTool,
} from './agentTools';

// Tool Registry exports
export {
    ToolGroup,
    TOOL_GROUP_ORDER,
    toolRegistry,
    isValidGroupTransition,
    getNextGroup,
    getGroupDependencyDescription,
    createToolDefinition,
    type ToolDefinition,
} from './toolRegistry';

// Import Tools exports
export {
    importYouTubeTool,
    transcribeAudioTool,
    importTools,
    getImportedContent,
    setImportedContent,
    clearImportedContent,
    type ImportedContent,
    type TranscriptResult,
    type TranscriptSegment,
    type WordTimingInfo,
} from './importTools';

// Audio Mixing Tools exports
export {
    mixAudioTracksTool,
    audioMixingTools,
    getMixedAudio,
    setMixedAudio,
    clearMixedAudio,
    type MixedAudioResult,
} from './audioMixingTools';

// Subtitle Tools exports
export {
    generateSubtitlesTool,
    subtitleTools,
    getSubtitles,
    setSubtitles,
    clearSubtitles,
    isRTLLanguage,
    addRTLMarkers,
    processNarrationToSubtitles,
    type SubtitleResult,
    type NarrationInput,
} from './subtitleTools';

// Enhancement Tools exports
export {
    removeBackgroundTool,
    restyleImageTool,
    enhancementTools,
    getEnhancedImages,
    addEnhancedImage,
    clearEnhancedImages,
    AVAILABLE_STYLES,
    isRecognizedStyle,
    findClosestStyle,
    getStyleSuggestions,
    type BackgroundRemovalResult,
    type StyleTransferResult,
    type EnhancedImage,
    type StyleOption,
} from './enhancementTools';

// Export Tools exports
export {
    exportFinalVideoTool,
    validateExportTool,
    listExportPresetsTool,
    exportTools,
    getExportResult,
    setExportResult,
    clearExportResult,
    type ExportResult,
    type AssetBundle,
    type ExportValidationResult,
} from './exportTools';

// Intent Detection exports
export {
    detectYouTubeUrl,
    detectAudioFile,
    shouldAnimate,
    shouldGenerateMusic,
    extractStyle,
    shouldRemoveBackground,
    shouldGenerateSubtitles,
    analyzeIntent,
    getAvailableStyles,
    generateIntentHint,
    type IntentDetectionResult,
} from './intentDetection';
````

## File: packages/shared/src/services/agent/intentDetection.ts
````typescript
/**
 * Intent Detection for Production Agent Tool Selection
 * 
 * Provides functions to detect user intent from natural language input
 * and determine which tools should be executed.
 * 
 * Requirements: 5.1, 5.2, 5.3, 5.4, 5.5
 */

/**
 * Result of intent detection analysis
 */
export interface IntentDetectionResult {
  /** Whether input contains a YouTube URL */
  hasYouTubeUrl: boolean;
  /** Extracted YouTube URL if present */
  youtubeUrl: string | null;
  /** Whether input contains an audio file path */
  hasAudioFile: boolean;
  /** Extracted audio file path if present */
  audioFilePath: string | null;
  /** Whether user wants animated/motion visuals */
  wantsAnimation: boolean;
  /** Whether user wants background music */
  wantsMusic: boolean;
  /** Detected visual style (or null if not specified) */
  detectedStyle: string | null;
  /** Whether user wants background removal */
  wantsBackgroundRemoval: boolean;
  /** Whether user wants subtitles */
  wantsSubtitles: boolean;
  /** Whether input suggests story-driven content (uses pipeline) */
  isStoryMode: boolean;
  /** First tool that should be called based on input */
  firstTool: 'import_youtube_content' | 'transcribe_audio_file' | 'plan_video' | 'generate_breakdown';
  /** List of optional tools to include based on keywords */
  optionalTools: string[];
}

// --- URL Detection Patterns ---

/**
 * Pattern to detect YouTube URLs in user input.
 * Matches:
 * - youtube.com/watch?v=VIDEO_ID
 * - youtu.be/VIDEO_ID
 * - youtube.com/shorts/VIDEO_ID
 * - youtube.com/embed/VIDEO_ID
 * 
 * Requirement: 5.4
 */
const YOUTUBE_URL_PATTERN = /(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:watch\?v=|shorts\/|embed\/)|youtu\.be\/)([a-zA-Z0-9_-]{11})(?:[?&][^\s]*)?/i;

/**
 * Pattern to detect audio file paths in user input.
 * Matches common audio file extensions.
 */
const AUDIO_FILE_PATTERN = /(?:^|\s)([^\s]+\.(?:mp3|wav|m4a|ogg|flac|aac))(?:\s|$)/i;

// --- Keyword Detection Patterns ---

/**
 * Keywords that indicate user wants animated/motion visuals.
 * Note: "video" alone is too generic since all outputs are videos.
 * We look for explicit animation intent.
 * Requirement: 5.2
 */
const ANIMATION_KEYWORDS = [
  'animated',
  'animation',
  'motion',
  'moving',
  'dynamic',
  'animate',
  'movement',
  'kinetic',
  'live action',
  'motion graphics',
  'video clips',  // More specific than just "video"
  'moving images',
  'video loops',
];

/**
 * Pattern to detect animation keywords.
 * Uses word boundaries to avoid false positives.
 */
const ANIMATION_PATTERN = new RegExp(
  `\\b(${ANIMATION_KEYWORDS.join('|')})\\b`,
  'i'
);

/**
 * Keywords that indicate user wants background music.
 * Requirement: 5.3
 */
const MUSIC_KEYWORDS = [
  'music',
  'background music',
  'soundtrack',
  'bgm',
  'score',
  'musical',
  'audio track',
  'backing track',
  'instrumental',
  'melody',
];

/**
 * Pattern to detect music keywords.
 */
const MUSIC_PATTERN = new RegExp(
  `\\b(${MUSIC_KEYWORDS.join('|').replace(/\s+/g, '\\s+')})\\b`,
  'i'
);

/**
 * Supported visual styles with their variations.
 * Requirement: 5.5
 */
const STYLE_KEYWORDS: Record<string, string[]> = {
  'Cinematic': ['cinematic', 'cinema', 'film', 'movie', 'hollywood'],
  'Anime': ['anime', 'manga', 'japanese animation', 'animated japanese'],
  'Watercolor': ['watercolor', 'watercolour', 'water color', 'aquarelle'],
  'Oil Painting': ['oil painting', 'oil paint', 'painted', 'classical painting'],
  'Documentary': ['documentary', 'docu', 'journalistic', 'news style'],
  'Realistic': ['realistic', 'photorealistic', 'real', 'lifelike', 'natural'],
  'Vintage': ['vintage', 'retro', 'old school', 'classic', 'nostalgic'],
  'Modern': ['modern', 'contemporary', 'sleek', 'minimalist'],
  'Fantasy': ['fantasy', 'magical', 'mythical', 'enchanted'],
  'Sci-Fi': ['sci-fi', 'science fiction', 'futuristic', 'cyberpunk', 'space'],
  'Horror': ['horror', 'dark', 'creepy', 'scary', 'gothic'],
  'Noir': ['noir', 'film noir', 'black and white', 'detective'],
};

/**
 * Keywords that indicate user wants background removal.
 */
const BACKGROUND_REMOVAL_KEYWORDS = [
  'remove background',
  'transparent background',
  'no background',
  'cut out',
  'cutout',
  'isolated',
  'green screen',
];

/**
 * Pattern to detect background removal keywords.
 */
const BACKGROUND_REMOVAL_PATTERN = new RegExp(
  `\\b(${BACKGROUND_REMOVAL_KEYWORDS.join('|').replace(/\s+/g, '\\s+')})\\b`,
  'i'
);

/**
 * Keywords that indicate user wants subtitles.
 */
const SUBTITLE_KEYWORDS = [
  'subtitle',
  'subtitles',
  'caption',
  'captions',
  'closed caption',
  'cc',
  'srt',
  'vtt',
  'accessible',
  'accessibility',
];

/**
 * Pattern to detect subtitle keywords.
 */
const SUBTITLE_PATTERN = new RegExp(
  `\\b(${SUBTITLE_KEYWORDS.join('|')})s?\\b`,
  'i'
);

/**
 * Keywords that indicate story-driven content (uses pipeline).
 * These suggest complex narratives that benefit from the discrete pipeline.
 */
const STORY_KEYWORDS = [
  'story',
  'narrative',
  'screenplay',
  'script',
  'characters',
  'plot',
  'protagonist',
  'antagonist',
  'dialogue',
  'scene',
  'chapter',
  'tale',
  'fiction',
  'drama',
  'قصة',  // Arabic: story
  'سيناريو',  // Arabic: scenario
  'شخصيات',  // Arabic: characters
];

/**
 * Pattern to detect story keywords.
 */
const STORY_PATTERN = new RegExp(
  `\\b(${STORY_KEYWORDS.join('|')})\\b`,
  'i'
);

// --- Detection Functions ---

/**
 * Detect if input contains a YouTube URL.
 * 
 * @param input User input string
 * @returns Object with detection result and extracted URL
 * 
 * Requirement: 5.4
 */
export function detectYouTubeUrl(input: string): { hasUrl: boolean; url: string | null } {
  const match = input.match(YOUTUBE_URL_PATTERN);
  if (match) {
    // Reconstruct the full URL
    const videoId = match[1];
    return {
      hasUrl: true,
      url: `https://www.youtube.com/watch?v=${videoId}`,
    };
  }
  return { hasUrl: false, url: null };
}

/**
 * Detect if input contains an audio file path.
 * 
 * @param input User input string
 * @returns Object with detection result and extracted path
 */
export function detectAudioFile(input: string): { hasFile: boolean; path: string | null } {
  const match = input.match(AUDIO_FILE_PATTERN);
  if (match && match[1]) {
    return {
      hasFile: true,
      path: match[1],
    };
  }
  return { hasFile: false, path: null };
}

/**
 * Detect if user wants animated/motion visuals.
 * 
 * @param input User input string
 * @returns true if animation keywords are detected
 * 
 * Requirement: 5.2
 */
export function shouldAnimate(input: string): boolean {
  return ANIMATION_PATTERN.test(input);
}

/**
 * Detect if user wants background music.
 * 
 * @param input User input string
 * @returns true if music keywords are detected
 * 
 * Requirement: 5.3
 */
export function shouldGenerateMusic(input: string): boolean {
  return MUSIC_PATTERN.test(input);
}

/**
 * Extract visual style from user input.
 * 
 * @param input User input string
 * @returns Detected style name or null if not specified
 * 
 * Requirement: 5.5
 */
export function extractStyle(input: string): string | null {
  const normalizedInput = input.toLowerCase();
  
  for (const [styleName, keywords] of Object.entries(STYLE_KEYWORDS)) {
    for (const keyword of keywords) {
      if (normalizedInput.includes(keyword)) {
        return styleName;
      }
    }
  }
  
  return null;
}

/**
 * Detect if user wants background removal.
 * 
 * @param input User input string
 * @returns true if background removal keywords are detected
 */
export function shouldRemoveBackground(input: string): boolean {
  return BACKGROUND_REMOVAL_PATTERN.test(input);
}

/**
 * Detect if user wants subtitles.
 *
 * @param input User input string
 * @returns true if subtitle keywords are detected
 */
export function shouldGenerateSubtitles(input: string): boolean {
  return SUBTITLE_PATTERN.test(input);
}

/**
 * Detect if input suggests story-driven content.
 * Story mode uses the discrete pipeline to avoid context explosion.
 *
 * @param input User input string
 * @returns true if story keywords are detected
 */
export function isStoryDrivenContent(input: string): boolean {
  // Check for story keywords
  if (STORY_PATTERN.test(input)) {
    return true;
  }

  // Also detect long narrative inputs (>500 chars often indicates a story)
  if (input.length > 500) {
    return true;
  }

  return false;
}

/**
 * Analyze user input and determine which tools should be executed.
 * This is the main entry point for intent-based tool selection.
 * 
 * @param input User input string
 * @returns Complete intent detection result
 * 
 * Requirements: 5.1, 5.2, 5.3, 5.4, 5.5
 */
export function analyzeIntent(input: string): IntentDetectionResult {
  // Detect URLs and file paths
  const youtubeResult = detectYouTubeUrl(input);
  const audioResult = detectAudioFile(input);

  // Detect optional features
  const wantsAnimation = shouldAnimate(input);
  const wantsMusic = shouldGenerateMusic(input);
  const detectedStyle = extractStyle(input);
  const wantsBackgroundRemoval = shouldRemoveBackground(input);
  const wantsSubtitles = shouldGenerateSubtitles(input);
  const isStoryMode = isStoryDrivenContent(input);

  // Determine first tool based on input type
  // Priority: YouTube > Audio File > Story Pipeline > Plan Video
  let firstTool: IntentDetectionResult['firstTool'] = 'plan_video';
  if (youtubeResult.hasUrl) {
    firstTool = 'import_youtube_content';
  } else if (audioResult.hasFile) {
    firstTool = 'transcribe_audio_file';
  } else if (isStoryMode) {
    // Use step-by-step story workflow - starts with breakdown generation
    // User must review and confirm each step before proceeding
    firstTool = 'generate_breakdown';
  }

  // Build list of optional tools
  const optionalTools: string[] = [];
  if (wantsAnimation) {
    optionalTools.push('animate_image');
  }
  if (wantsMusic) {
    optionalTools.push('generate_music');
  }
  if (wantsBackgroundRemoval) {
    optionalTools.push('remove_background');
  }
  if (wantsSubtitles) {
    optionalTools.push('generate_subtitles');
  }

  return {
    hasYouTubeUrl: youtubeResult.hasUrl,
    youtubeUrl: youtubeResult.url,
    hasAudioFile: audioResult.hasFile,
    audioFilePath: audioResult.path,
    wantsAnimation,
    wantsMusic,
    detectedStyle,
    wantsBackgroundRemoval,
    wantsSubtitles,
    isStoryMode,
    firstTool,
    optionalTools,
  };
}

/**
 * Get a list of available visual styles.
 * Useful for suggesting styles when user input is unrecognized.
 * 
 * @returns Array of available style names
 */
export function getAvailableStyles(): string[] {
  return Object.keys(STYLE_KEYWORDS);
}

/**
 * Generate a hint message for the agent based on detected intent.
 * This can be prepended to the system prompt or user message.
 * 
 * @param result Intent detection result
 * @returns Hint message for the agent
 */
export function generateIntentHint(result: IntentDetectionResult): string {
  const hints: string[] = [];

  if (result.hasYouTubeUrl) {
    hints.push(`[DETECTED: YouTube URL - Start with import_youtube_content using URL: ${result.youtubeUrl}]`);
  } else if (result.hasAudioFile) {
    hints.push(`[DETECTED: Audio file - Start with transcribe_audio_file using path: ${result.audioFilePath}]`);
  } else if (result.isStoryMode) {
    hints.push('[DETECTED: Story-driven content - Start with generate_breakdown, then wait for user review before proceeding to next step]');
  }

  if (result.wantsAnimation) {
    hints.push('[DETECTED: Animation requested - Include animate_image for each scene]');
  }

  if (result.wantsMusic) {
    hints.push('[DETECTED: Music requested - Include generate_music]');
  }

  if (result.detectedStyle) {
    hints.push(`[DETECTED: Style "${result.detectedStyle}" - Use this for generate_visuals]`);
  }

  if (result.wantsBackgroundRemoval) {
    hints.push('[DETECTED: Background removal requested - Include remove_background]');
  }

  if (result.wantsSubtitles) {
    hints.push('[DETECTED: Subtitles requested - Include generate_subtitles]');
  }

  return hints.join('\n');
}
````

## File: packages/shared/src/services/agent/schemas/exportSchemas.ts
````typescript
import { z } from "zod";

/**
 * Schema for export_final_video tool
 */
export const ExportFinalVideoSchema = z.object({
    contentPlanId: z.string().describe("Session ID containing all production assets"),
    // Platform preset - overrides format, aspectRatio, and quality if provided
    preset: z.enum([
        "youtube-landscape", "youtube-shorts", "tiktok",
        "instagram-feed", "instagram-reels", "instagram-story",
        "twitter", "linkedin", "draft-preview", "high-quality", "podcast-video"
    ]).optional().describe("Platform preset (e.g., 'tiktok', 'youtube-shorts', 'instagram-reels'). Overrides format, aspectRatio, and quality settings."),
    format: z.enum(["mp4", "webm"]).default("mp4").describe("Output video format (default: mp4)"),
    aspectRatio: z.enum(["16:9", "9:16", "1:1"]).default("16:9").describe("Video aspect ratio (default: 16:9)"),
    includeSubtitles: z.boolean().default(true).describe("Whether to include subtitles in the video (default: true)"),
    quality: z.enum(["draft", "standard", "high"]).default("standard").describe("Export quality preset (default: standard)"),
    // Asset inputs - all optional, will be auto-fetched from session if not provided
    visuals: z.array(z.object({
        sceneId: z.string().describe("Scene ID"),
        imageUrl: z.string().describe("URL to the visual asset"),
        type: z.enum(["image", "video"]).default("image").describe("Asset type"),
        startTime: z.number().optional().describe("Start time in seconds"),
        duration: z.number().optional().describe("Duration in seconds"),
    })).optional().describe("Optional array of visual assets. If not provided, will be auto-fetched from session visuals."),
    narrationUrl: z.string().optional().describe("Optional URL to the narration audio file. If not provided, will be auto-fetched from session narration."),
    musicUrl: z.string().optional().describe("URL to the background music file"),
    sfxPlan: z.any().optional().describe("SFX plan with audio URLs"),
    totalDuration: z.number().optional().describe("Total video duration in seconds. If not provided, will be calculated from content plan."),
    // Optional: use pre-mixed audio instead of individual tracks
    useMixedAudio: z.boolean().default(false).describe("Use pre-mixed audio from mix_audio_tracks instead of individual tracks"),
});

export type ExportFinalVideoInput = z.infer<typeof ExportFinalVideoSchema>;

/**
 * Schema for validate_export tool
 */
export const ValidateExportSchema = z.object({
    contentPlanId: z.string().describe("Session ID to validate export readiness for"),
});

export type ValidateExportInput = z.infer<typeof ValidateExportSchema>;

/**
 * Schema for list_export_presets tool
 */
export const ListExportPresetsSchema = z.object({
    platform: z.string().optional().describe("Optional platform filter (e.g., 'youtube', 'instagram', 'tiktok')"),
    aspectRatio: z.enum(["16:9", "9:16", "1:1", "4:5"]).optional().describe("Optional aspect ratio filter"),
});

export type ListExportPresetsInput = z.infer<typeof ListExportPresetsSchema>;
````

## File: packages/shared/src/services/agent/schemas/importSchemas.ts
````typescript
import { z } from "zod";

/**
 * Schema for YouTube import tool
 */
export const ImportYouTubeSchema = z.object({
    url: z.string().describe("YouTube or X (Twitter) video URL to import audio from"),
});

export type ImportYouTubeInput = z.infer<typeof ImportYouTubeSchema>;

/**
 * Schema for audio transcription tool
 */
export const TranscribeAudioSchema = z.object({
    contentPlanId: z.string().describe("Session ID where audio is stored"),
    language: z.string().optional().describe("Language code for transcription (e.g., 'en', 'ar'). Auto-detected if omitted."),
});

export type TranscribeAudioInput = z.infer<typeof TranscribeAudioSchema>;
````

## File: packages/shared/src/services/agent/schemas/index.ts
````typescript
export * from "./importSchemas";
export * from "./exportSchemas";
````

## File: packages/shared/src/services/agent/subtitleTools.ts
````typescript
/**
 * Subtitle Tools - LangChain tools for subtitle generation
 * 
 * Provides tools for generating subtitles from narration:
 * - generate_subtitles: Create SRT-formatted captions from narration transcripts
 * 
 * Requirements: 4.1, 4.2, 4.3, 4.4, 4.5
 */

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { splitTextIntoSegments } from "../subtitleService";
import { SubtitleItem } from "../../types";
import { productionStore } from "../ai/production/store";

// --- Types ---

/**
 * Result of subtitle generation operation
 */
export interface SubtitleResult {
  /** Subtitle format (srt or vtt) */
  format: "srt" | "vtt";
  /** Generated subtitle content as string */
  content: string;
  /** Language code */
  language: string;
  /** Number of subtitle segments */
  segmentCount: number;
  /** Whether the language is RTL (Arabic, Hebrew) */
  isRTL: boolean;
  /** Parsed subtitle items for programmatic access */
  items: SubtitleItem[];
}

/**
 * Narration segment input for subtitle generation
 */
export interface NarrationInput {
  /** Scene ID this narration belongs to */
  sceneId: string;
  /** Transcript text */
  transcript: string;
  /** Start time in seconds */
  startTime: number;
  /** Duration in seconds */
  duration: number;
}

/**
 * Session storage for subtitle results
 */
const subtitleStore = new Map<string, SubtitleResult>();

/**
 * Get subtitle result for a session
 */
export function getSubtitles(sessionId: string): SubtitleResult | undefined {
  return subtitleStore.get(sessionId);
}

/**
 * Store subtitle result for a session
 */
export function setSubtitles(sessionId: string, result: SubtitleResult): void {
  subtitleStore.set(sessionId, result);
}

/**
 * Clear subtitles for a session
 */
export function clearSubtitles(sessionId: string): boolean {
  return subtitleStore.delete(sessionId);
}

// --- RTL Language Support ---

/**
 * RTL language codes
 */
const RTL_LANGUAGES = ["ar", "he", "fa", "ur", "yi", "ara", "heb", "fas", "urd"];

/**
 * Check if a language code is RTL
 */
export function isRTLLanguage(language: string): boolean {
  const normalizedLang = language.toLowerCase().trim();
  return RTL_LANGUAGES.some(rtl =>
    normalizedLang === rtl ||
    normalizedLang.startsWith(`${rtl}-`) ||
    normalizedLang.startsWith(`${rtl}_`)
  );
}

/**
 * Unicode RTL markers
 */
const RLM = "\u200F"; // Right-to-Left Mark
const _LRM = "\u200E"; // Left-to-Right Mark
const RLE = "\u202B"; // Right-to-Left Embedding
const PDF = "\u202C"; // Pop Directional Formatting

/**
 * Add RTL direction markers to text
 */
export function addRTLMarkers(text: string): string {
  // Wrap text with RTL embedding markers
  return `${RLE}${RLM}${text}${PDF}`;
}

// --- Tool Schema ---

/**
 * Schema for generate_subtitles tool
 */
const GenerateSubtitlesSchema = z.object({
  contentPlanId: z.string().describe("Session ID containing the content plan and narration"),
  language: z.string().default("en").describe("Language code for subtitles (e.g., 'en', 'ar', 'he'). Default is 'en'."),
  format: z.enum(["srt", "vtt"]).default("srt").describe("Output format: 'srt' (SubRip) or 'vtt' (WebVTT). Default is 'srt'."),
  narrationSegments: z.array(z.object({
    sceneId: z.string().describe("Scene ID"),
    transcript: z.string().describe("Narration transcript text"),
    startTime: z.number().describe("Start time in seconds"),
    duration: z.number().describe("Duration in seconds"),
  })).optional().describe("Optional array of narration segments. If not provided, will be auto-fetched from session."),
  maxWordsPerSegment: z.number().min(1).max(20).default(8).describe("Maximum words per subtitle segment (default 8)"),
});

// --- Helper Functions ---

/**
 * Convert seconds to SRT timestamp format (HH:MM:SS,mmm)
 */
function formatSRTTimestamp(seconds: number): string {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  const ms = Math.round((seconds % 1) * 1000);

  return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')},${ms.toString().padStart(3, '0')}`;
}

/**
 * Convert seconds to VTT timestamp format (HH:MM:SS.mmm)
 */
function formatVTTTimestamp(seconds: number): string {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  const ms = Math.round((seconds % 1) * 1000);

  return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}.${ms.toString().padStart(3, '0')}`;
}

/**
 * Generate SRT content from subtitle items
 */
function generateSRTContent(items: SubtitleItem[], isRTL: boolean): string {
  return items.map((item, index) => {
    const id = item.id || index + 1;
    const start = formatSRTTimestamp(item.startTime);
    const end = formatSRTTimestamp(item.endTime);
    const text = isRTL ? addRTLMarkers(item.text) : item.text;
    return `${id}\n${start} --> ${end}\n${text}`;
  }).join('\n\n');
}

/**
 * Generate VTT content from subtitle items
 */
function generateVTTContent(items: SubtitleItem[], isRTL: boolean): string {
  const header = "WEBVTT\n\n";
  const cues = items.map((item, index) => {
    const id = item.id || index + 1;
    const start = formatVTTTimestamp(item.startTime);
    const end = formatVTTTimestamp(item.endTime);
    const text = isRTL ? addRTLMarkers(item.text) : item.text;
    return `${id}\n${start} --> ${end}\n${text}`;
  }).join('\n\n');

  return header + cues;
}

/**
 * Process narration segments into subtitle items with word-level timing
 * 
 * Enhanced to include word-level timing data for karaoke-style highlighting.
 * Each subtitle segment now includes a `words` array with precise timing
 * for each word, enabling smooth word-by-word reveal animations.
 */
export function processNarrationToSubtitles(
  narrationSegments: NarrationInput[],
  maxWordsPerSegment: number = 8
): SubtitleItem[] {
  const allSubtitles: SubtitleItem[] = [];
  let subtitleId = 1;

  for (const segment of narrationSegments) {
    // Use splitTextIntoSegments to break up the transcript
    const textSegments = splitTextIntoSegments(
      segment.transcript,
      segment.duration,
      { maxWordsPerSegment }
    );

    // Calculate timing for each text segment
    let currentTime = segment.startTime;

    for (const textSeg of textSegments) {
      const segmentText = textSeg.text.trim();
      const segmentDuration = textSeg.duration;

      // Calculate word-level timing for karaoke highlighting
      const words = segmentText.split(/\s+/).filter(w => w.length > 0);
      const wordCount = words.length;

      // Distribute time evenly across words (with slight adjustment for natural pacing)
      // Average speaking rate: ~150 words/min = 0.4s per word, but adjust based on actual duration
      const timePerWord = wordCount > 0 ? segmentDuration / wordCount : segmentDuration;

      let wordTime = currentTime;
      const wordTimings: { word: string; startTime: number; endTime: number }[] = [];

      for (let i = 0; i < wordCount; i++) {
        const wordDuration = timePerWord;
        wordTimings.push({
          word: words[i]!,
          startTime: wordTime,
          endTime: wordTime + wordDuration,
        });
        wordTime += wordDuration;
      }

      const subtitleItem: SubtitleItem = {
        id: subtitleId++,
        startTime: currentTime,
        endTime: currentTime + segmentDuration,
        text: segmentText,
        words: wordTimings, // Add word-level timing for karaoke highlighting
      };

      allSubtitles.push(subtitleItem);
      currentTime += segmentDuration;
    }
  }

  return allSubtitles;
}

// --- Tool Implementation ---

/**
 * Generate Subtitles Tool
 * 
 * Creates SRT or VTT formatted subtitles from narration transcripts.
 * Supports RTL languages with proper direction markers.
 * 
 * Requirements: 4.1, 4.2, 4.3, 4.4, 4.5
 */
export const generateSubtitlesTool = tool(
  async ({
    contentPlanId,
    language = "en",
    format = "srt",
    narrationSegments,
    maxWordsPerSegment = 8,
  }) => {
    console.log(`[SubtitleTools] Generating subtitles for session: ${contentPlanId}`);
    console.log(`[SubtitleTools] Language: ${language}, Format: ${format}, Max words: ${maxWordsPerSegment}`);

    // Auto-fetch narration segments from session if not provided
    let finalNarrationSegments = narrationSegments;
    if (!finalNarrationSegments || finalNarrationSegments.length === 0) {
      const state = productionStore.get(contentPlanId);
      if (!state || !state.narrationSegments || state.narrationSegments.length === 0) {
        return JSON.stringify({
          success: false,
          error: "No narration segments found in session and none provided",
          suggestion: "Run narrate_scenes first to generate narration",
        });
      }

      // Build narration input array from session state
      let currentTime = 0;
      finalNarrationSegments = state.narrationSegments.map((segment, _index) => {
        const _scene = state.contentPlan!.scenes[_index];
        const result = {
          sceneId: segment.sceneId,
          transcript: segment.transcript,
          startTime: currentTime,
          duration: segment.audioDuration,
        };
        currentTime += segment.audioDuration;
        return result;
      });
      console.log(`[SubtitleTools] Auto-fetched ${finalNarrationSegments.length} narration segments from session`);
    }

    // Validate each segment has required fields
    for (const seg of finalNarrationSegments) {
      if (!seg || !seg.transcript || seg.transcript.trim().length === 0) {
        return JSON.stringify({
          success: false,
          error: "Narration segment has empty transcript",
          suggestion: "Ensure all narration segments have non-empty transcript text",
        });
      }
      if (typeof seg.startTime !== 'number' || seg.startTime < 0) {
        return JSON.stringify({
          success: false,
          error: "Narration segment has invalid startTime",
          suggestion: "Ensure all narration segments have a valid startTime >= 0",
        });
      }
      if (typeof seg.duration !== 'number' || seg.duration <= 0) {
        return JSON.stringify({
          success: false,
          error: "Narration segment has invalid duration",
          suggestion: "Ensure all narration segments have a valid duration > 0",
        });
      }
    }

    try {
      // Check if language is RTL
      const isRTL = isRTLLanguage(language);
      console.log(`[SubtitleTools] RTL language: ${isRTL}`);

      // Process narration into subtitle items
      const subtitleItems = processNarrationToSubtitles(
        finalNarrationSegments,
        maxWordsPerSegment
      );

      if (subtitleItems.length === 0) {
        return JSON.stringify({
          success: false,
          error: "No subtitle segments generated",
          suggestion: "Check that narration segments contain valid text content",
        });
      }

      // Generate content in requested format
      let content: string;
      if (format === "vtt") {
        content = generateVTTContent(subtitleItems, isRTL);
      } else {
        content = generateSRTContent(subtitleItems, isRTL);
      }

      // Calculate total duration
      const lastItem = subtitleItems[subtitleItems.length - 1];
      const totalDuration = lastItem ? lastItem.endTime : 0;

      // Create result object
      const result: SubtitleResult = {
        format,
        content,
        language,
        segmentCount: subtitleItems.length,
        isRTL,
        items: subtitleItems,
      };

      // Store the result
      setSubtitles(contentPlanId, result);

      return JSON.stringify({
        success: true,
        sessionId: contentPlanId,
        format,
        language,
        segmentCount: subtitleItems.length,
        isRTL,
        totalDuration: Math.round(totalDuration * 100) / 100,
        contentLength: content.length,
        preview: content.substring(0, 300) + (content.length > 300 ? "..." : ""),
        message: `Successfully generated ${subtitleItems.length} subtitle segments in ${format.toUpperCase()} format`,
      });

    } catch (error) {
      console.error("[SubtitleTools] Generation error:", error);

      const errorMessage = error instanceof Error ? error.message : String(error);

      return JSON.stringify({
        success: false,
        error: errorMessage,
        suggestion: "Check that narration segments are properly formatted with valid timing",
      });
    }
  },
  {
    name: "generate_subtitles",
    description: "Generate SRT or VTT formatted subtitles from narration transcripts. Narration segments are automatically fetched from session - you only need to provide contentPlanId. Optionally specify language (default 'en'), format (default 'srt'), or maxWordsPerSegment (default 8). Supports RTL languages (Arabic, Hebrew) with proper direction markers.",
    schema: GenerateSubtitlesSchema,
  }
);

// --- Export all subtitle tools ---

export const subtitleTools = [
  generateSubtitlesTool,
];
````

## File: packages/shared/src/services/agent/toolRegistry.ts
````typescript
/**
 * Tool Registry - Tool Group Management for Production Agent
 * 
 * Provides a centralized registry for organizing LangChain tools into logical groups
 * with dependency ordering for the Enhanced Production Agent.
 * 
 * Tool Group Dependencies:
 * IMPORT → CONTENT → MEDIA → ENHANCEMENT → EXPORT
 * 
 * Requirements: 12.1, 12.2, 12.4
 */

import { StructuredTool } from "@langchain/core/tools";

/**
 * Tool groups for organizing production agent capabilities.
 * Groups follow a dependency order: IMPORT → CONTENT → MEDIA → ENHANCEMENT → EXPORT
 */
export enum ToolGroup {
  /** Import tools for external content (YouTube, audio files) */
  IMPORT = "IMPORT",
  /** Content planning tools (plan, narrate, validate) */
  CONTENT = "CONTENT",
  /** Media generation tools (visuals, animate, music, sfx) */
  MEDIA = "MEDIA",
  /** Enhancement tools (background removal, style transfer, audio mixing) */
  ENHANCEMENT = "ENHANCEMENT",
  /** Export tools (subtitles, video export) */
  EXPORT = "EXPORT",
}

/**
 * Defines the execution order of tool groups.
 * Tools in earlier groups must complete before tools in later groups can execute.
 */
export const TOOL_GROUP_ORDER: ToolGroup[] = [
  ToolGroup.IMPORT,
  ToolGroup.CONTENT,
  ToolGroup.MEDIA,
  ToolGroup.ENHANCEMENT,
  ToolGroup.EXPORT,
];

/**
 * Tool definition with group assignment and optional dependencies.
 */
export interface ToolDefinition {
  /** Unique tool name */
  name: string;
  /** Group this tool belongs to */
  group: ToolGroup;
  /** The LangChain StructuredTool instance */
  tool: StructuredTool;
  /** Optional list of tool names that must run before this tool */
  dependencies?: string[];
  /** Optional description override for system prompt */
  description?: string;
}

/**
 * Registry entry stored internally.
 */
interface RegistryEntry extends ToolDefinition {
  registeredAt: number;
}

/**
 * Tool Registry for managing production agent tools.
 * Provides registration, lookup, and dependency validation.
 */
class ToolRegistry {
  private tools: Map<string, RegistryEntry> = new Map();
  private groupIndex: Map<ToolGroup, Set<string>> = new Map();

  constructor() {
    // Initialize group index
    for (const group of Object.values(ToolGroup)) {
      this.groupIndex.set(group, new Set());
    }
  }

  /**
   * Register a tool with the registry.
   * @param definition Tool definition including name, group, and tool instance
   * @throws Error if tool with same name already exists
   */
  register(definition: ToolDefinition): void {
    if (this.tools.has(definition.name)) {
      throw new Error(`Tool "${definition.name}" is already registered`);
    }

    const entry: RegistryEntry = {
      ...definition,
      registeredAt: Date.now(),
    };

    this.tools.set(definition.name, entry);
    this.groupIndex.get(definition.group)?.add(definition.name);
  }

  /**
   * Register multiple tools at once.
   * @param definitions Array of tool definitions
   */
  registerAll(definitions: ToolDefinition[]): void {
    for (const def of definitions) {
      this.register(def);
    }
  }

  /**
   * Unregister a tool from the registry.
   * @param name Tool name to remove
   * @returns true if tool was removed, false if not found
   */
  unregister(name: string): boolean {
    const entry = this.tools.get(name);
    if (!entry) return false;

    this.groupIndex.get(entry.group)?.delete(name);
    this.tools.delete(name);
    return true;
  }

  /**
   * Get a tool by name.
   * @param name Tool name
   * @returns Tool definition or undefined if not found
   */
  getTool(name: string): ToolDefinition | undefined {
    return this.tools.get(name);
  }

  /**
   * Get the LangChain tool instance by name.
   * @param name Tool name
   * @returns StructuredTool instance or undefined
   */
  getToolInstance(name: string): StructuredTool | undefined {
    return this.tools.get(name)?.tool;
  }

  /**
   * Get all tools in a specific group.
   * @param group Tool group
   * @returns Array of tool definitions in the group
   */
  getToolsByGroup(group: ToolGroup): ToolDefinition[] {
    const toolNames = this.groupIndex.get(group) || new Set();
    return Array.from(toolNames)
      .map(name => this.tools.get(name)!)
      .filter(Boolean);
  }

  /**
   * Get all tool instances in a specific group.
   * @param group Tool group
   * @returns Array of StructuredTool instances
   */
  getToolInstancesByGroup(group: ToolGroup): StructuredTool[] {
    return this.getToolsByGroup(group).map(def => def.tool);
  }

  /**
   * Get all registered tools.
   * @returns Array of all tool definitions
   */
  getAllTools(): ToolDefinition[] {
    return Array.from(this.tools.values());
  }

  /**
   * Get all tool instances for binding to a model.
   * @returns Array of all StructuredTool instances
   */
  getAllToolInstances(): StructuredTool[] {
    return Array.from(this.tools.values()).map(entry => entry.tool);
  }

  /**
   * Get the group a tool belongs to.
   * @param name Tool name
   * @returns ToolGroup or undefined if tool not found
   */
  getToolGroup(name: string): ToolGroup | undefined {
    return this.tools.get(name)?.group;
  }

  /**
   * Check if a tool can execute based on group dependencies.
   * A tool can execute if all tools in preceding groups have completed.
   * 
   * @param toolName Name of the tool to check
   * @param completedGroups Set of groups that have completed execution
   * @returns true if the tool can execute, false otherwise
   */
  canExecute(toolName: string, completedGroups: Set<ToolGroup>): boolean {
    const entry = this.tools.get(toolName);
    if (!entry) return false;

    const toolGroupIndex = TOOL_GROUP_ORDER.indexOf(entry.group);
    
    // Check all preceding groups are complete
    for (let i = 0; i < toolGroupIndex; i++) {
      const precedingGroup = TOOL_GROUP_ORDER[i];
      if (!precedingGroup) continue;
      // Skip IMPORT group if no import tools were used (optional group)
      if (precedingGroup === ToolGroup.IMPORT && this.getToolsByGroup(ToolGroup.IMPORT).length === 0) {
        continue;
      }
      if (!completedGroups.has(precedingGroup)) {
        return false;
      }
    }

    // Check explicit dependencies
    if (entry.dependencies) {
      for (const dep of entry.dependencies) {
        const depEntry = this.tools.get(dep);
        if (depEntry && !completedGroups.has(depEntry.group)) {
          return false;
        }
      }
    }

    return true;
  }

  /**
   * Validate that a sequence of tool names respects group dependencies.
   * 
   * @param toolSequence Array of tool names in execution order
   * @returns Validation result with isValid flag and any violations
   */
  validateExecutionOrder(toolSequence: string[]): {
    isValid: boolean;
    violations: Array<{
      tool: string;
      expectedAfter: ToolGroup[];
      actualPosition: number;
    }>;
  } {
    const violations: Array<{
      tool: string;
      expectedAfter: ToolGroup[];
      actualPosition: number;
    }> = [];

    const seenGroups = new Set<ToolGroup>();
    const groupFirstSeen = new Map<ToolGroup, number>();

    for (let i = 0; i < toolSequence.length; i++) {
      const toolName = toolSequence[i];
      if (!toolName) continue;
      const entry = this.tools.get(toolName);
      
      if (!entry) continue;

      const currentGroupIndex = TOOL_GROUP_ORDER.indexOf(entry.group);
      
      // Track first occurrence of each group
      if (!groupFirstSeen.has(entry.group)) {
        groupFirstSeen.set(entry.group, i);
      }

      // Check if any later group was seen before this tool's group
      for (let j = currentGroupIndex + 1; j < TOOL_GROUP_ORDER.length; j++) {
        const laterGroup = TOOL_GROUP_ORDER[j];
        if (!laterGroup) continue;
        if (seenGroups.has(laterGroup)) {
          violations.push({
            tool: toolName,
            expectedAfter: TOOL_GROUP_ORDER.slice(0, currentGroupIndex),
            actualPosition: i,
          });
          break;
        }
      }

      seenGroups.add(entry.group);
    }

    return {
      isValid: violations.length === 0,
      violations,
    };
  }

  /**
   * Get the required preceding groups for a tool.
   * @param toolName Tool name
   * @returns Array of groups that must complete before this tool
   */
  getRequiredPrecedingGroups(toolName: string): ToolGroup[] {
    const entry = this.tools.get(toolName);
    if (!entry) return [];

    const toolGroupIndex = TOOL_GROUP_ORDER.indexOf(entry.group);
    return TOOL_GROUP_ORDER.slice(0, toolGroupIndex);
  }

  /**
   * Get tools that can execute given the current completed groups.
   * @param completedGroups Set of groups that have completed
   * @returns Array of tool names that can now execute
   */
  getExecutableTools(completedGroups: Set<ToolGroup>): string[] {
    const executable: string[] = [];
    
    for (const [name] of this.tools) {
      if (this.canExecute(name, completedGroups)) {
        executable.push(name);
      }
    }

    return executable;
  }

  /**
   * Get a summary of registered tools by group for logging/debugging.
   * @returns Object mapping group names to tool counts and names
   */
  getSummary(): Record<string, { count: number; tools: string[] }> {
    const summary: Record<string, { count: number; tools: string[] }> = {};

    for (const group of Object.values(ToolGroup)) {
      const tools = Array.from(this.groupIndex.get(group) || []);
      summary[group] = {
        count: tools.length,
        tools,
      };
    }

    return summary;
  }

  /**
   * Clear all registered tools.
   */
  clear(): void {
    this.tools.clear();
    for (const group of Object.values(ToolGroup)) {
      this.groupIndex.set(group, new Set());
    }
  }

  /**
   * Get the total number of registered tools.
   */
  get size(): number {
    return this.tools.size;
  }
}

// Singleton instance for the production agent
export const toolRegistry = new ToolRegistry();

// --- Helper Functions ---

/**
 * Check if a tool group transition is valid (respects dependency order).
 * @param fromGroup Source group (or null if starting)
 * @param toGroup Target group
 * @returns true if transition is valid
 */
export function isValidGroupTransition(
  fromGroup: ToolGroup | null,
  toGroup: ToolGroup
): boolean {
  if (fromGroup === null) {
    // Starting fresh - can only start with IMPORT or CONTENT
    return toGroup === ToolGroup.IMPORT || toGroup === ToolGroup.CONTENT;
  }

  const fromIndex = TOOL_GROUP_ORDER.indexOf(fromGroup);
  const toIndex = TOOL_GROUP_ORDER.indexOf(toGroup);

  // Can stay in same group or move forward, but not backward
  return toIndex >= fromIndex;
}

/**
 * Get the next expected group after the current one.
 * @param currentGroup Current tool group
 * @returns Next group in order, or null if at the end
 */
export function getNextGroup(currentGroup: ToolGroup): ToolGroup | null {
  const currentIndex = TOOL_GROUP_ORDER.indexOf(currentGroup);
  if (currentIndex < 0 || currentIndex >= TOOL_GROUP_ORDER.length - 1) {
    return null;
  }
  return TOOL_GROUP_ORDER[currentIndex + 1] || null;
}

/**
 * Get human-readable description of tool group dependencies.
 * Useful for system prompts and documentation.
 */
export function getGroupDependencyDescription(): string {
  return `Tool Group Dependencies:
- IMPORT: External content import (YouTube, audio files) - Run first if importing
- CONTENT: Content planning (plan_video, narrate_scenes, validate_plan) - Core planning
- MEDIA: Asset generation (generate_visuals, animate_image, generate_music, plan_sfx)
- ENHANCEMENT: Post-processing (remove_background, restyle_image, mix_audio_tracks)
- EXPORT: Final output (generate_subtitles, export_final_video)

Execution Order: IMPORT → CONTENT → MEDIA → ENHANCEMENT → EXPORT
Note: IMPORT is optional - skip directly to CONTENT for topic-based videos.`;
}

/**
 * Create a tool definition helper for cleaner registration.
 */
export function createToolDefinition(
  name: string,
  group: ToolGroup,
  tool: StructuredTool,
  dependencies?: string[]
): ToolDefinition {
  return { name, group, tool, dependencies };
}
````

## File: packages/shared/src/services/agentDirectorService.ts
````typescript
/**
 * Agent Director Service
 * 
 * Orchestrates the AI video creation workflow using LangChain tools.
 * Refactored to separate concerns:
 * - Tools: ./agent/agentTools.ts
 * - Metrics: ./agent/agentMetrics.ts
 * - Logging: ./agent/agentLogger.ts
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { ImagePrompt } from "../types";
import { VideoPurpose } from "../constants";
import {
  getSystemPersona,
  getStyleEnhancement,
} from "./promptService";
import { GEMINI_API_KEY, VERTEX_PROJECT, MODELS } from "./shared/apiClient";
import { parseSRTTimestamp } from "../utils/srtParser";
import { generateCompleteFormatGuidance } from "./promptFormatService";

// Imported from decoupled modules
import { agentDirectorLogger as agentLogger, LogLevel, type LogEntry } from "./agent/agentLogger";
import { agentMetrics } from "./agent/agentMetrics";
import {
  allTools,
  executeToolCall,
  analyzeContentTool,
  searchVisualReferencesTool,
  analyzeAndGenerateStoryboardTool,
  generateStoryboardTool,
  refinePromptTool,
  critiqueStoryboardTool,
  jsonExtractor,
  fallbackProcessor
} from "./agent/agentTools";
import {
  type StoryboardData,
  ExtractionMethod,
} from "./jsonExtractor";
import {
  type AnalysisOutput,
  type StoryboardOutput,
} from "./directorService";

// Use imported singleton instances
export { LogLevel, type LogEntry };

// --- Agent Configuration ---

export interface AgentDirectorConfig {
  model?: string;
  temperature?: number;
  maxIterations?: number;
  qualityThreshold?: number;
  targetAssetCount?: number;
}

const DEFAULT_AGENT_CONFIG: Required<AgentDirectorConfig> = {
  model: MODELS.TEXT,
  temperature: 0.7,
  maxIterations: 2,
  qualityThreshold: 70,
  targetAssetCount: 10,
};

const LANGCHAIN_VERBOSE = process.env.NODE_ENV === "development";

/**
 * Generates a dynamic system prompt based on video purpose.
 */
function getAgentSystemPrompt(purpose: VideoPurpose): string {
  const persona = getSystemPersona(purpose);

  return `You are ${persona.name}, a Visionary Film Director.
  
## Your Identity
${persona.visualPrinciples.map(p => `- ${p}`).join('\n')}

## Core Rule
ATMOSPHERIC RESONANCE: Prioritize the EMOTION over the object. If content mentions 'candle', visualize 'loneliness' or 'fading hope' using lighting and shadows, not just the object itself.

## Capabilities
1. **GENERATE MUSIC**: You can create full musical tracks using Suno AI.
2. **CREATE VIDEOS**: You create cinematic storyboards from text/lyrics.

## Workflow
1. For VIDEO: Use 'analyze_and_generate_storyboard' tool.
2. For MUSIC: Identify prompt/style and use 'generate_music' action.
3. If needed, use 'critique_storyboard' and 'refine_prompt'.
4. Output the final storyboard JSON or music config.

## Quality Standards
- Consistent visual motif in every scene
- Clean visual compositions with focus on subjects
- Varied camera angles`;
}

// --- Main Agent Function ---

export async function generatePromptsWithAgent(
  srtContent: string,
  style: string,
  contentType: "lyrics" | "story",
  videoPurpose: VideoPurpose,
  globalSubject?: string,
  config?: AgentDirectorConfig
): Promise<ImagePrompt[]> {
  const startTime = Date.now();
  const mergedConfig = { ...DEFAULT_AGENT_CONFIG, ...config };

  agentLogger.info('Starting agent workflow', {
    contentType,
    videoPurpose,
    style,
    targetAssetCount: mergedConfig.targetAssetCount
  });

  if (!srtContent || srtContent.trim().length === 0) {
    agentLogger.warn('Empty content provided');
    agentMetrics.recordRequest(false, Date.now() - startTime);
    return [];
  }

  if (!GEMINI_API_KEY && !VERTEX_PROJECT) {
    agentLogger.error('API key not configured');
    throw new Error("API key not configured. Add VITE_GEMINI_API_KEY to .env.local");
  }

  try {
    const model = new ChatGoogleGenerativeAI({
      apiKey: GEMINI_API_KEY, // Can be empty if using Vertex
      model: mergedConfig.model,
      temperature: mergedConfig.temperature,
      verbose: LANGCHAIN_VERBOSE,
    }).bindTools(allTools);

    const systemPrompt = getAgentSystemPrompt(videoPurpose);
    const formatGuidance = generateCompleteFormatGuidance('storyboard');
    const styleData = getStyleEnhancement(style);

    // Simplifed prompt construction
    const taskMessage = `Create a visual storyboard for this ${contentType}.
    
Style: ${style} (${styleData.mediumDescription})
Purpose: ${videoPurpose}
Target Assets: ${mergedConfig.targetAssetCount}

${formatGuidance}

Content:
${srtContent}`;

    const messages: (HumanMessage | AIMessage | ToolMessage)[] = [
      new HumanMessage(systemPrompt + "\n\n" + taskMessage),
    ];

    let finalStoryboard: StoryboardOutput | null = null;
    let iterations = 0;
    const maxIterations = mergedConfig.maxIterations + 3;

    // Agent loop
    while (iterations < maxIterations) {
      iterations++;

      const response = await model.invoke(messages);
      messages.push(response);

      const toolCalls = response.tool_calls || [];

      if (toolCalls.length === 0) {
        // No tool calls, check for result
        const extracted = await extractStoryboardFromContent(response.content as string);
        if (extracted) {
          finalStoryboard = extracted;
        }
        break;
      }

      // Execute tool calls
      for (const toolCall of toolCalls) {
        agentLogger.debug(`Executing tool: ${toolCall.name}`);

        // Inject targetAssetCount if missing
        const toolArgs = { ...toolCall.args as Record<string, unknown> };
        if (['generate_storyboard', 'analyze_and_generate_storyboard'].includes(toolCall.name) && !toolArgs.targetAssetCount) {
          toolArgs.targetAssetCount = mergedConfig.targetAssetCount;
        }

        const result = await executeToolCall({
          name: toolCall.name,
          args: toolArgs,
        });

        messages.push(new ToolMessage({
          content: result,
          tool_call_id: toolCall.id || `call_${Date.now()}`,
        }));

        // Capture storyboard from tool output directly for reliability
        if (['generate_storyboard', 'analyze_and_generate_storyboard'].includes(toolCall.name)) {
          const extracted = await extractStoryboardFromContent(result);
          if (extracted) {
            finalStoryboard = extracted;
            agentLogger.info('Captured storyboard from tool output');
          }
        }
      }

      // Early exit if we have a good storyboard
      if (finalStoryboard) {
        break;
      }
    }

    if (finalStoryboard?.prompts) {
      const prompts = convertToImagePrompts(finalStoryboard.prompts);
      agentMetrics.recordRequest(true, Date.now() - startTime);
      return prompts;
    }

    agentMetrics.recordRequest(false, Date.now() - startTime);
    return [];

  } catch (error) {
    agentMetrics.recordRequest(false, Date.now() - startTime);
    throw error;
  }
}

/**
 * Helper to extract storyboard from text content using jsonExtractor
 */
export async function extractStoryboardFromContent(content: string): Promise<StoryboardOutput | null> {
  const extracted = await jsonExtractor.extractJSON(content);
  if (extracted) {
    const data = extracted.data as any;
    const storyboard = (data.storyboard || data) as StoryboardOutput;
    // Basic validation
    if (storyboard.prompts && Array.isArray(storyboard.prompts)) {
      return storyboard;
    }
  }

  // Fallback
  const fallback = fallbackProcessor.processWithFallback(content, "JSON extraction failed");
  if (fallback) {
    agentMetrics.recordExtractionMethod(ExtractionMethod.FALLBACK_TEXT);
    return {
      prompts: fallback.prompts.map((p, i) => ({
        text: p.prompt || '',
        mood: p.mood || 'neutral',
        timestamp: p.timestamp || '00:00'
      }))
    } as StoryboardOutput;
  }

  return null;
}

export function convertToImagePrompts(prompts: StoryboardOutput["prompts"]): ImagePrompt[] {
  return prompts.map((p, i) => ({
    id: `agent-prompt-${Date.now()}-${i}`,
    text: p.text,
    mood: p.mood,
    timestamp: p.timestamp,
    timestampSeconds: parseSRTTimestamp(p.timestamp) ?? 0,
  }));
}

// Re-export for testing/compatibility
export const agentTools = {
  analyzeContentTool,
  searchVisualReferencesTool,
  analyzeAndGenerateStoryboardTool,
  generateStoryboardTool,
  refinePromptTool,
  critiqueStoryboardTool,
};
export { agentLogger, agentMetrics };
````

## File: packages/shared/src/services/agentOrchestrator.ts
````typescript
/**
 * Agent Orchestrator Service
 *
 * Unified service that runs the complete video production pipeline:
 * 1. ContentPlanner -> generates video plan with scenes
 * 2. Narrator -> generates audio narration for each scene
 * 3. (Visual Generation) -> generates images/videos for each scene (existing)
 * 4. SFX Planning -> generates mood-based ambient sound plan
 * 5. Editor/Critic -> validates and provides feedback
 * 6. FFmpeg -> final assembly
 *
 * ROBUST PATTERNS IMPLEMENTED:
 * - Per-stage timeout protection (prevents hung operations)
 * - Retry with exponential backoff for transient failures
 * - Graceful degradation with fallback visuals/audio
 * - Comprehensive error logging with stage context
 * - AbortController support for cancellation
 * - Concurrent visual generation with rate limiting
 */

import { ContentPlan, NarrationSegment, GeneratedImage, ValidationResult, AppState, VideoSFXPlan, Scene } from "../types";
import { generateContentPlan, ContentPlannerConfig } from "./contentPlannerService";
import { narrateAllScenes, NarratorConfig } from "./narratorService";
import { validateContentPlan, syncDurationsToNarration, EditorConfig } from "./editorService";
import { generateImageFromPrompt } from "./imageService";
import { animateImageWithDeApi, isDeApiConfigured } from "./deapiService";
import { generateProfessionalVideo } from "./videoService";
import { generateMotionPrompt } from "./promptService";
import { generateVideoSFXPlan, generateVideoSFXPlanWithAudio, isSFXAudioAvailable } from "./sfxService";
import { getEffectiveLegacyTone } from "./tripletUtils";
import { traceAsync, isTracingEnabled } from "./tracing";
import {
    withTimeout,
    withRetryBackoff,
    runWithConcurrency,
    createServiceLogger,
} from "./shared/robustUtils";

// Create a service-specific logger
const log = createServiceLogger("Orchestrator");

// --- Timeout Configuration ---
// These timeouts protect against hung operations and enable graceful recovery

const STAGE_TIMEOUTS = {
    /** Content planning stage timeout (2 minutes) */
    CONTENT_PLANNING: 120_000,
    /** Per-scene narration timeout (60 seconds per scene) */
    NARRATION_PER_SCENE: 60_000,
    /** Total narration timeout (10 minutes max) */
    NARRATION_TOTAL: 600_000,
    /** Per-image generation timeout (90 seconds) */
    VISUAL_PER_IMAGE: 90_000,
    /** Per-video animation timeout (3 minutes - video gen is slow) */
    ANIMATION_PER_VIDEO: 180_000,
    /** SFX planning timeout (60 seconds) */
    SFX_PLANNING: 60_000,
    /** Validation timeout (90 seconds) */
    VALIDATION: 90_000,
} as const;

const RETRY_CONFIG = {
    /** Max retries for content planning (critical stage) */
    CONTENT_PLANNING: 3,
    /** Max retries for narration per scene */
    NARRATION: 2,
    /** Max retries for visual generation per image */
    VISUAL: 2,
    /** Max retries for animation per video */
    ANIMATION: 1,
    /** Max retries for SFX planning */
    SFX: 2,
    /** Max retries for validation */
    VALIDATION: 2,
} as const;

// --- Configuration ---

export interface ProductionConfig {
    // Target settings
    targetDuration?: number;
    sceneCount?: number;
    targetAudience?: string;

    // Visual settings
    visualStyle?: string; // Art style (Cinematic, Anime, etc.)
    aspectRatio?: string; // 16:9, 9:16, 1:1
    globalSubject?: string; // Subject to keep consistent

    // Agent configs
    contentPlannerConfig?: ContentPlannerConfig;
    narratorConfig?: NarratorConfig;
    editorConfig?: EditorConfig;

    // Options
    skipNarration?: boolean; // Skip TTS synthesis
    skipVisuals?: boolean; // Skip image generation
    skipValidation?: boolean; // Skip editor validation
    animateVisuals?: boolean; // Animate images to video with DeAPI
    veoVideoCount?: number; // Number of scenes to generate as professional videos
    maxRetries?: number; // Max feedback loop iterations
}

const DEFAULT_CONFIG: Required<Pick<ProductionConfig, "targetDuration" | "sceneCount" | "targetAudience" | "visualStyle" | "aspectRatio" | "skipNarration" | "skipVisuals" | "skipValidation" | "animateVisuals" | "maxRetries">> = {
    targetDuration: 60,
    sceneCount: 5,
    targetAudience: "General audience",
    visualStyle: "Cinematic",
    aspectRatio: "16:9",
    skipNarration: false,
    skipVisuals: false,
    skipValidation: false,
    animateVisuals: false, // Default off - requires DeAPI key
    maxRetries: 2,
};

// --- Progress Tracking ---

export type ProductionStage =
    | "content_planning"
    | "narrating"
    | "generating_visuals"
    | "animating_visuals"
    | "validating"
    | "adjusting"
    | "complete";

export interface ProductionProgress {
    stage: ProductionStage;
    progress: number; // 0-100
    message: string;
    currentScene?: number;
    totalScenes?: number;
}

export type ProgressCallback = (progress: ProductionProgress) => void;

// --- Result Type ---

export interface ProductionResult {
    contentPlan: ContentPlan;
    narrationSegments: NarrationSegment[];
    visuals: GeneratedImage[];
    sfxPlan: VideoSFXPlan | null;
    validation: ValidationResult;
    success: boolean;
    errors?: string[];
}

// --- Error Types ---

export class OrchestratorError extends Error {
    constructor(
        message: string,
        public readonly stage: ProductionStage,
        public readonly originalError?: Error
    ) {
        super(message);
        this.name = "OrchestratorError";
    }
}

// --- Main Pipeline ---

/**
 * Run the complete video production pipeline.
 * 
 * @param input - Either a topic string or existing content
 * @param config - Production configuration
 * @param onProgress - Progress callback
 * @returns Production result with all assets
 */
export const runProductionPipeline = traceAsync(
    async function runProductionPipelineImpl(
        input: string | { topic: string },
        config: ProductionConfig = {},
        onProgress?: ProgressCallback,
        signal?: AbortSignal
    ): Promise<ProductionResult> {
        const topic = typeof input === "string" ? input : input.topic;

        const mergedConfig = { ...DEFAULT_CONFIG, ...config };

        log.info("Starting production pipeline");
        log.info(`Topic: "${topic.substring(0, 50)}..."`);
        if (isTracingEnabled()) {
            log.info("LangSmith tracing is active");
        }

        const result: ProductionResult = {
            contentPlan: null as any,
            narrationSegments: [],
            visuals: [],
            sfxPlan: null,
            validation: { approved: false, score: 0, issues: [], suggestions: [] },
            success: false,
            errors: [],
        };

        // Helper to check abort signal and throw if aborted
        const checkAbort = () => {
            if (signal?.aborted) {
                throw new Error("Production pipeline was cancelled");
            }
        };

        try {
            // --- Stage 1: Content Planning ---
            checkAbort();
            onProgress?.({
                stage: "content_planning",
                progress: 0,
                message: "Analyzing content and creating video plan...",
            });

            log.info("Stage 1: Content Planning (with timeout and retry)");

            // Wrap content planning with timeout and retry for robustness
            result.contentPlan = await withRetryBackoff(
                async () => withTimeout(
                    generateContentPlan(topic, {
                        targetDuration: mergedConfig.targetDuration,
                        sceneCount: mergedConfig.sceneCount,
                        targetAudience: mergedConfig.targetAudience,
                        config: mergedConfig.contentPlannerConfig,
                    }),
                    STAGE_TIMEOUTS.CONTENT_PLANNING,
                    "Content planning timed out"
                ),
                {
                    maxRetries: RETRY_CONFIG.CONTENT_PLANNING,
                    baseDelay: 2000,
                    onRetry: (attempt, error) => {
                        log.warn(`Content planning retry ${attempt}: ${error.message}`);
                        onProgress?.({
                            stage: "content_planning",
                            progress: 10,
                            message: `Retrying content planning (attempt ${attempt + 1})...`,
                        });
                    },
                    signal,
                }
            );

            onProgress?.({
                stage: "content_planning",
                progress: 100,
                message: `Created plan with ${result.contentPlan.scenes.length} scenes`,
            });

            // --- Stage 2: Narration ---
            if (!mergedConfig.skipNarration) {
                checkAbort();
                onProgress?.({
                    stage: "narrating",
                    progress: 0,
                    message: "Generating voice narration...",
                });

                log.info("Stage 2: Narration (with per-scene timeout and retry)");

                // Pass video purpose to narrator for auto-styling
                const narratorConfigWithPurpose: NarratorConfig = {
                    ...mergedConfig.narratorConfig,
                    videoPurpose: mergedConfig.contentPlannerConfig?.videoPurpose,
                };

                // Calculate total timeout based on scene count
                const totalNarrationTimeout = Math.min(
                    STAGE_TIMEOUTS.NARRATION_PER_SCENE * result.contentPlan.scenes.length,
                    STAGE_TIMEOUTS.NARRATION_TOTAL
                );

                try {
                    result.narrationSegments = await withTimeout(
                        narrateAllScenes(
                            result.contentPlan.scenes,
                            narratorConfigWithPurpose,
                            (sceneIndex, totalScenes) => {
                                checkAbort();
                                onProgress?.({
                                    stage: "narrating",
                                    progress: Math.round((sceneIndex / totalScenes) * 100),
                                    message: `Narrating scene ${sceneIndex + 1} of ${totalScenes}`,
                                    currentScene: sceneIndex + 1,
                                    totalScenes,
                                });
                            }
                        ),
                        totalNarrationTimeout,
                        "Narration timed out"
                    );

                    onProgress?.({
                        stage: "narrating",
                        progress: 100,
                        message: `Generated ${result.narrationSegments.length} audio segments`,
                    });
                } catch (narrationError) {
                    log.error("Narration failed, continuing with empty segments", narrationError);
                    result.errors?.push(`Narration failed: ${narrationError instanceof Error ? narrationError.message : String(narrationError)}`);
                    // Continue without narration - graceful degradation
                    result.narrationSegments = [];
                }

                // Sync scene durations to actual narration lengths immediately
                // This prevents timing mismatches from causing validation failures
                if (result.narrationSegments.length > 0) {
                    result.contentPlan = syncDurationsToNarration(
                        result.contentPlan,
                        result.narrationSegments
                    );
                    log.info(`Synced durations to narration. New total: ${result.contentPlan.totalDuration}s`);
                }
            }

            // --- Stage 3: Visual Generation ---
            if (!mergedConfig.skipVisuals) {
                checkAbort();
                onProgress?.({
                    stage: "generating_visuals",
                    progress: 0,
                    message: "Generating visuals for scenes...",
                });

                log.info("Stage 3: Visual Generation (with timeout and retry per image)");

                const totalScenes = result.contentPlan!.scenes.length;
                let completedCount = 0;

                // Generate visuals with concurrency limit to avoid API rate limits
                const visualResults = await runWithConcurrency(
                    result.contentPlan!.scenes,
                    async (scene: Scene, index: number) => {
                        checkAbort();

                        onProgress?.({
                            stage: "generating_visuals",
                            progress: Math.round((completedCount / totalScenes) * 100),
                            message: `Generating visual ${index + 1}/${totalScenes}: ${scene.name}`,
                            currentScene: index + 1,
                            totalScenes,
                        });

                        log.info(`Generating image for scene: ${scene.name}`);

                        try {
                            // Wrap each image generation with timeout and retry
                            const imageUrl = await withRetryBackoff(
                                async () => withTimeout(
                                    generateImageFromPrompt(
                                        scene.visualDescription,
                                        mergedConfig.visualStyle,
                                        config.globalSubject || "",
                                        mergedConfig.aspectRatio,
                                        false // Don't skip refinement
                                    ),
                                    STAGE_TIMEOUTS.VISUAL_PER_IMAGE,
                                    `Image generation for "${scene.name}" timed out`
                                ),
                                {
                                    maxRetries: RETRY_CONFIG.VISUAL,
                                    baseDelay: 1500,
                                    onRetry: (attempt, error) => {
                                        log.warn(`Image retry for "${scene.name}" (attempt ${attempt}): ${error.message}`);
                                    },
                                    signal,
                                }
                            );

                            completedCount++;
                            log.info(`Generated image for scene ${index + 1}`);

                            return {
                                promptId: scene.id,
                                imageUrl,
                                type: "image" as const,
                            };
                        } catch (error) {
                            completedCount++;
                            log.error(`Failed to generate image for scene ${scene.id}:`, error);
                            result.errors?.push(`Visual generation failed for "${scene.name}": ${error instanceof Error ? error.message : String(error)}`);

                            // Return placeholder - graceful degradation
                            return {
                                promptId: scene.id,
                                imageUrl: "", // Empty = placeholder
                                type: "image" as const,
                            };
                        }
                    },
                    2 // Concurrency limit of 2 to avoid rate limits
                );

                // Collect results (filter out errors that were already handled)
                result.visuals = (visualResults as any[])
                    .filter((r): r is GeneratedImage => !(r instanceof Error) && r !== undefined)
                    .sort((a, b) => {
                        // Maintain scene order
                        const aIdx = result.contentPlan!.scenes.findIndex(s => s.id === a.promptId);
                        const bIdx = result.contentPlan!.scenes.findIndex(s => s.id === b.promptId);
                        return aIdx - bIdx;
                    });

                const successCount = result.visuals.filter(v => v.imageUrl).length;
                onProgress?.({
                    stage: "generating_visuals",
                    progress: 100,
                    message: `Generated ${successCount}/${totalScenes} visuals`,
                });
            } else {
                log.info("Stage 3: Visual Generation (skipped)");
                // Create placeholder visuals
                result.visuals = result.contentPlan.scenes.map((scene) => ({
                    promptId: scene.id,
                    prompt: scene.visualDescription,
                    imageUrl: "",
                    type: "image" as const,
                }));
            }

            // --- Stage 3.5: Video Animation (Optional) ---
            if (mergedConfig.animateVisuals && isDeApiConfigured()) {
                checkAbort();
                onProgress?.({
                    stage: "animating_visuals",
                    progress: 0,
                    message: "Animating visuals with AI motion...",
                });

                log.info("Stage 3.5: Video Animation with DeAPI (with timeout per video)");

                const visualsToAnimate = result.visuals.filter(v => v.imageUrl);
                const totalToAnimate = visualsToAnimate.length;
                let animatedCount = 0;

                for (const visual of visualsToAnimate) {
                    checkAbort();
                    const scene = result.contentPlan!.scenes.find(s => s.id === visual.promptId);

                    if (!scene || !visual.imageUrl) continue;

                    onProgress?.({
                        stage: "animating_visuals",
                        progress: Math.round((animatedCount / totalToAnimate) * 100),
                        message: `Animating scene ${animatedCount + 1}/${totalToAnimate}: ${scene.name}`,
                        currentScene: animatedCount + 1,
                        totalScenes: totalToAnimate,
                    });

                    try {
                        log.info(`Generating motion prompt for: ${scene.name}`);

                        // Generate AI-powered motion prompt with timeout
                        const motionResult = await withTimeout(
                            generateMotionPrompt(
                                scene.visualDescription,
                                getEffectiveLegacyTone(scene),
                                config.globalSubject || ""
                            ),
                            30_000,
                            `Motion prompt generation for "${scene.name}" timed out`
                        );

                        log.info(`Animating with prompt: ${motionResult.combined.substring(0, 80)}...`);

                        // Animate the image with timeout and retry
                        const videoUrl = await withRetryBackoff(
                            async () => withTimeout(
                                animateImageWithDeApi(
                                    visual.imageUrl,
                                    motionResult.combined,
                                    (mergedConfig.aspectRatio as "16:9" | "9:16" | "1:1") || "16:9"
                                ),
                                STAGE_TIMEOUTS.ANIMATION_PER_VIDEO,
                                `Animation for "${scene.name}" timed out`
                            ),
                            {
                                maxRetries: RETRY_CONFIG.ANIMATION,
                                baseDelay: 3000,
                                onRetry: (attempt, error) => {
                                    log.warn(`Animation retry for "${scene.name}" (attempt ${attempt}): ${error.message}`);
                                },
                                signal,
                            }
                        );

                        // Update the visual with video URL
                        const visualIndex = result.visuals.findIndex(v => v.promptId === visual.promptId);
                        if (visualIndex !== -1) {
                            (result.visuals[visualIndex] as any).videoUrl = videoUrl;
                            (result.visuals[visualIndex] as any).type = "video";
                            animatedCount++;
                        }

                        log.info(`Animated scene ${animatedCount} successfully`);
                    } catch (error) {
                        const errorMessage = error instanceof Error ? error.message : String(error);
                        log.error(`Failed to animate scene ${scene.id} with DeAPI:`, error);

                        // Check if this is a Cloudflare blocking issue - try Veo as fallback
                        const isCloudflareBlock = errorMessage.includes('Cloudflare') ||
                            errorMessage.includes('blocked') ||
                            errorMessage.includes('503');

                        if (isCloudflareBlock) {
                            log.info(`DeAPI blocked by Cloudflare for "${scene.name}", trying Veo 3.1 with professional prompt...`);
                            onProgress?.({
                                stage: "animating_visuals",
                                progress: Math.round((animatedCount / totalToAnimate) * 100),
                                message: `DeAPI blocked - generating professional Veo 3.1 video for scene ${animatedCount + 1}`,
                                currentScene: animatedCount + 1,
                                totalScenes: totalToAnimate,
                            });

                            try {
                                // Use Veo 3.1 with professional cinematographer-level prompt
                                const videoPurpose = mergedConfig.contentPlannerConfig?.videoPurpose || "documentary";
                                const veoVideoUrl = await withTimeout(
                                    generateProfessionalVideo(
                                        scene.visualDescription,
                                        mergedConfig.visualStyle || "Cinematic",
                                        getEffectiveLegacyTone(scene),
                                        config.globalSubject || "",
                                        videoPurpose,
                                        (mergedConfig.aspectRatio as "16:9" | "9:16") || "16:9",
                                        6, // 6 seconds
                                        true // Use Veo 3.1 Fast
                                    ),
                                    STAGE_TIMEOUTS.ANIMATION_PER_VIDEO,
                                    `Veo fallback for "${scene.name}" timed out`
                                );

                                // Update the visual with Veo video URL
                                const visualIndex = result.visuals.findIndex(v => v.promptId === visual.promptId);
                                if (visualIndex !== -1) {
                                    (result.visuals[visualIndex] as any).videoUrl = veoVideoUrl;
                                    (result.visuals[visualIndex] as any).type = "video";
                                    (result.visuals[visualIndex] as any).generatedWithVeo = true;
                                    animatedCount++;
                                }
                                log.info(`Veo 3.1 fallback succeeded for scene ${animatedCount}`);
                            } catch (veoError) {
                                log.error(`Veo fallback also failed for scene ${scene.id}:`, veoError);
                                result.errors?.push(`Animation failed (DeAPI + Veo) for "${scene.name}": ${errorMessage}`);
                                // Continue with static image - graceful degradation
                            }
                        } else {
                            result.errors?.push(`Animation failed for "${scene.name}": ${errorMessage}`);
                            // Continue with static image - graceful degradation
                        }
                    }
                }

                onProgress?.({
                    stage: "animating_visuals",
                    progress: 100,
                    message: `Animated ${animatedCount}/${totalToAnimate} visuals`,
                });
            } else if (mergedConfig.animateVisuals && !isDeApiConfigured()) {
                log.info("Stage 3.5: DeAPI not configured - generating professional Veo 3.1 videos...");

                // If DeAPI is not configured, use Veo 3.1 with professional prompt generation
                onProgress?.({
                    stage: "animating_visuals",
                    progress: 0,
                    message: "Generating professional cinematic videos with Veo 3.1...",
                });

                const totalScenes = result.contentPlan!.scenes.length;
                const videoPurpose = mergedConfig.contentPlannerConfig?.videoPurpose || "documentary";
                let videoCount = 0;

                for (let i = 0; i < totalScenes; i++) {
                    checkAbort();
                    const scene = result.contentPlan!.scenes[i];
                    const visual = result.visuals[i];

                    if (!scene || !visual) continue;

                    onProgress?.({
                        stage: "animating_visuals",
                        progress: Math.round((i / totalScenes) * 100),
                        message: `Creating cinematic Veo 3.1 video ${i + 1}/${totalScenes}: ${scene.name}`,
                        currentScene: i + 1,
                        totalScenes,
                    });

                    try {
                        // Generate professional video with AI-powered cinematographer prompt
                        const videoUrl = await withTimeout(
                            generateProfessionalVideo(
                                scene.visualDescription,
                                mergedConfig.visualStyle || "Cinematic",
                                getEffectiveLegacyTone(scene),
                                config.globalSubject || "",
                                videoPurpose,
                                (mergedConfig.aspectRatio as "16:9" | "9:16") || "16:9",
                                6, // 6 seconds
                                true // Use Veo 3.1 Fast
                            ),
                            STAGE_TIMEOUTS.ANIMATION_PER_VIDEO,
                            `Veo video generation for "${scene.name}" timed out`
                        );

                        if (visual) {
                            (visual as any).videoUrl = videoUrl;
                            (visual as any).type = "video";
                            (visual as any).generatedWithVeo = true;
                            videoCount++;
                        }
                        log.info(`Generated Veo 3.1 video for scene ${i + 1}`);
                    } catch (error) {
                        log.error(`Failed to generate Veo video for scene ${scene.id}:`, error);
                        result.errors?.push(`Veo video failed for "${scene.name}": ${error instanceof Error ? error.message : String(error)}`);
                        // Continue with static image
                    }
                }

                onProgress?.({
                    stage: "animating_visuals",
                    progress: 100,
                    message: `Generated ${videoCount}/${totalScenes} videos with Veo 3.1`,
                });
            }

            // --- Stage 4: SFX Planning ---
            checkAbort();
            log.info("Stage 4: SFX Planning (with timeout)");
            const videoPurpose = mergedConfig.contentPlannerConfig?.videoPurpose || "documentary";

            // Log AI-suggested SFX from content plan
            const aiSuggestedSfx = result.contentPlan!.scenes
                .filter(s => s.ambientSfx)
                .map(s => `${s.name}: ${s.ambientSfx}`);
            if (aiSuggestedSfx.length > 0) {
                log.info(`AI-suggested SFX: ${aiSuggestedSfx.join(", ")}`);
            }

            try {
                // Use async version with Freesound if available
                if (isSFXAudioAvailable()) {
                    log.info("Fetching real audio from Freesound...");
                    result.sfxPlan = await withRetryBackoff(
                        async () => withTimeout(
                            generateVideoSFXPlanWithAudio(result.contentPlan.scenes, videoPurpose),
                            STAGE_TIMEOUTS.SFX_PLANNING,
                            "SFX planning with audio timed out"
                        ),
                        {
                            maxRetries: RETRY_CONFIG.SFX,
                            baseDelay: 1000,
                            signal,
                        }
                    );
                } else {
                    result.sfxPlan = generateVideoSFXPlan(result.contentPlan.scenes, videoPurpose);
                }

                log.info(`Generated SFX plan with ${result.sfxPlan.scenes.length} scene plans`);
                if (result.sfxPlan.backgroundMusic) {
                    log.info(`Background music: ${result.sfxPlan.backgroundMusic.name}`);
                    if (result.sfxPlan.backgroundMusic.audioUrl) {
                        log.info(`Audio URL: ${result.sfxPlan.backgroundMusic.audioUrl.substring(0, 50)}...`);
                    }
                }
            } catch (sfxError) {
                log.error("SFX planning failed, using empty plan", sfxError);
                result.errors?.push(`SFX planning failed: ${sfxError instanceof Error ? sfxError.message : String(sfxError)}`);
                // Graceful degradation: create minimal SFX plan
                result.sfxPlan = {
                    scenes: [],
                    backgroundMusic: null,
                    masterVolume: 1.0,
                };
            }

            // --- Stage 5: Validation ---
            if (!mergedConfig.skipValidation) {
                checkAbort();
                onProgress?.({
                    stage: "validating",
                    progress: 0,
                    message: "Validating production quality...",
                });

                log.info("Stage 5: Validation (with timeout)");

                try {
                    result.validation = await withRetryBackoff(
                        async () => withTimeout(
                            validateContentPlan(result.contentPlan, {
                                narrationSegments: result.narrationSegments,
                                visuals: result.visuals,
                                useAICritique: true,
                                config: mergedConfig.editorConfig,
                            }),
                            STAGE_TIMEOUTS.VALIDATION,
                            "Validation timed out"
                        ),
                        {
                            maxRetries: RETRY_CONFIG.VALIDATION,
                            baseDelay: 1500,
                            signal,
                        }
                    );

                    onProgress?.({
                        stage: "validating",
                        progress: 100,
                        message: `Validation score: ${result.validation.score}/100`,
                    });

                    // Log validation result but don't retry - we already synced durations
                    if (!result.validation.approved) {
                        log.info(`Validation score: ${result.validation.score} (below threshold, but continuing)`);
                        log.info(`Issues: ${result.validation.issues.length}, Suggestions: ${result.validation.suggestions.length}`);
                    }
                } catch (validationError) {
                    log.error("Validation failed, using default approval", validationError);
                    result.errors?.push(`Validation failed: ${validationError instanceof Error ? validationError.message : String(validationError)}`);
                    // Graceful degradation: assume content is acceptable
                    result.validation = {
                        approved: true,
                        score: 70, // Conservative score since we couldn't validate
                        issues: [{ scene: "general", type: "warning", message: "Validation was skipped due to an error" }],
                        suggestions: [],
                    };
                }
            } else {
                result.validation = { approved: true, score: 100, issues: [], suggestions: [] };
            }

            // --- Success Determination ---
            // Consider it successful if we have content, narration, and visuals
            // Even if validation score is below threshold, the content is usable
            const hasContent = result.contentPlan && result.contentPlan.scenes.length > 0;
            const hasNarration = mergedConfig.skipNarration || result.narrationSegments.length > 0;
            const hasVisuals = mergedConfig.skipVisuals || result.visuals.some(v => v.imageUrl);

            result.success = hasContent && hasNarration && hasVisuals;

            // Log summary of any errors that occurred
            if (result.errors && result.errors.length > 0) {
                log.warn(`Pipeline completed with ${result.errors.length} non-fatal errors:`);
                result.errors.forEach((err, i) => log.warn(`  ${i + 1}. ${err}`));
            }

            onProgress?.({
                stage: "complete",
                progress: 100,
                message: result.success
                    ? `Production complete! Quality: ${result.validation.score}/100`
                    : `Completed with issues (score: ${result.validation.score})`,
            });

            log.info(`Pipeline complete. Success: ${result.success}, Quality: ${result.validation.score}/100`);
            return result;

        } catch (error) {
            const errorMsg = error instanceof Error ? error.message : String(error);
            log.error("Pipeline error:", error);

            result.errors?.push(errorMsg);

            // Check if this was an abort
            if (signal?.aborted || errorMsg.includes("cancelled") || errorMsg.includes("aborted")) {
                throw new OrchestratorError(
                    "Production pipeline was cancelled by user",
                    "content_planning",
                    error instanceof Error ? error : undefined
                );
            }

            throw new OrchestratorError(
                `Production pipeline failed: ${errorMsg}`,
                "content_planning",
                error instanceof Error ? error : undefined
            );
        }
    },
    "runProductionPipeline",
    {
        runType: "chain",
        metadata: { service: "agentOrchestrator" },
        tags: ["pipeline", "orchestrator"],
    }
);

/**
 * Map production stage to AppState for UI integration.
 */
export function stageToAppState(stage: ProductionStage): AppState {
    switch (stage) {
        case "content_planning":
            return AppState.CONTENT_PLANNING;
        case "narrating":
            return AppState.NARRATING;
        case "generating_visuals":
        case "animating_visuals":
            return AppState.GENERATING_PROMPTS;
        case "validating":
        case "adjusting":
            return AppState.VALIDATING;
        case "complete":
            return AppState.READY;
        default:
            return AppState.IDLE;
    }
}
````

## File: packages/shared/src/services/ai/config.ts
````typescript
/**
 * AI Configuration
 *
 * Centralized configuration for Phase 2 AI features:
 * - RAG (Retrieval-Augmented Generation)
 * - Semantic Memory
 * - Observability with LangSmith
 */

import { agentLogger } from "../logger";
const log = agentLogger.child('Config');

export const AI_CONFIG = {
  /**
   * RAG (Retrieval-Augmented Generation) Configuration
   */
  rag: {
    // Enable/disable RAG feature
    enabled: import.meta.env.VITE_ENABLE_RAG !== 'false',
    
    // Maximum number of documents to retrieve per query
    maxDocuments: 3,
    
    // Minimum similarity score (0-1) for document retrieval
    minSimilarity: 0.7,
    
    // Embedding model to use
    embeddingModel: 'text-embedding-004',
  },

  /**
   * Semantic Memory Configuration
   */
  semanticMemory: {
    // Enable/disable semantic memory feature
    enabled: import.meta.env.VITE_ENABLE_SEMANTIC_MEMORY !== 'false',
    
    // Maximum number of interactions to store
    maxInteractions: 1000,
    
    // Maximum number of similar interactions to retrieve
    maxContextResults: 3,
    
    // Minimum similarity score for memory retrieval
    minSimilarity: 0.75,
  },

  /**
   * Entity Memory Configuration
   */
  entityMemory: {
    // Enable/disable entity memory feature
    enabled: import.meta.env.VITE_ENABLE_ENTITY_MEMORY !== 'false',
    
    // Thresholds for expertise level classification
    expertiseThresholds: {
      beginner: 5,      // < 5 successful interactions
      intermediate: 20, // 5-20 successful interactions
      advanced: 20,     // > 20 successful interactions
    },
    
    // Number of failures before marking as struggling area
    strugglingThreshold: 3,
  },

  /**
   * Observability Configuration (LangSmith)
   */
  observability: {
    // Enable/disable LangSmith tracing (requires API key)
    enabled: !!import.meta.env.VITE_LANGSMITH_API_KEY,
    
    // LangSmith API key
    apiKey: import.meta.env.VITE_LANGSMITH_API_KEY,
    
    // LangSmith project name
    project: import.meta.env.VITE_LANGSMITH_PROJECT || 'lyriclens-agent',
    
    // Enable verbose logging
    verbose: import.meta.env.DEV,
  },

  /**
   * Performance Monitoring Configuration
   */
  performance: {
    // Enable/disable performance monitoring
    enabled: true,
    
    // Alert thresholds
    thresholds: {
      maxResponseTime: 3000,      // 3 seconds
      minSuccessRate: 0.85,       // 85%
      minSatisfaction: 3.5,       // 3.5/5
      minKnowledgeUsage: 0.80,    // 80%
      minMemoryHitRate: 0.60,     // 60%
    },
    
    // Number of recent interactions to track
    recentInteractionsLimit: 100,
  },
};

/**
 * Log configuration status on startup
 */
export function logAIConfigStatus(): void {
  log.info('Phase 2 Features:');
  log.info(`  RAG: ${AI_CONFIG.rag.enabled ? 'Enabled' : 'Disabled'}`);
  log.info(`  Semantic Memory: ${AI_CONFIG.semanticMemory.enabled ? 'Enabled' : 'Disabled'}`);
  log.info(`  Entity Memory: ${AI_CONFIG.entityMemory.enabled ? 'Enabled' : 'Disabled'}`);
  log.info(`  Observability: ${AI_CONFIG.observability.enabled ? 'Enabled' : 'Disabled'}`);

  if (AI_CONFIG.observability.enabled) {
    log.info(`  LangSmith Project: ${AI_CONFIG.observability.project}`);
  }
}
````

## File: packages/shared/src/services/ai/index.ts
````typescript
/**
 * AI Services Index
 */

export { studioAgent } from './studioAgent';
export type { AgentAction, AgentResponse, VideoParams } from './studioAgent';
````

## File: packages/shared/src/services/ai/nlpIntentParser.ts
````typescript
/**
 * NLP Intent Parser Service
 * 
 * Parses natural language input to identify:
 * - User intent (what they want to do)
 * - Entities (specific details extracted)
 * - Sentiment and urgency
 * - Whether clarification is needed
 */

import type { ConversationContext } from '@/stores/appStore';

// Supported intents
export type IntentType =
  | 'greeting'
  | 'create_video'
  | 'generate_music'
  | 'edit_content'
  | 'export_video'
  | 'get_help'
  | 'ask_question'
  | 'feedback'
  | 'escalate_human'
  | 'ambiguous'
  | 'unknown';

// Entity types
export interface ExtractedEntity {
  type: string;
  value: string;
  confidence: number;
}

// Intent result
export interface IntentResult {
  intent: IntentType;
  confidence: number; // 0-1
  entities: ExtractedEntity[];
  requiresClarification: boolean;
  clarificationQuestions?: string[];
  suggestedWorkflows: string[];
  response?: string;
}

// Keyword patterns for intent detection
const INTENT_PATTERNS: Record<IntentType, RegExp[]> = {
  greeting: [
    /^(hi|hello|hey|good morning|good afternoon|good evening|howdy)/i,
    /^(what's up|whats up|sup)/i,
  ],
  create_video: [
    /create\s+(a\s+)?video/i,
    /make\s+(a\s+)?video/i,
    /generate\s+(a\s+)?video/i,
    /produce\s+(a\s+)?video/i,
    /i\s+want\s+(to\s+)?(make|create|generate)/i,
    /video\s+about/i,
    /video\s+for/i,
    /turn\s+(this|my)\s+(audio|music|song)/i,
  ],
  generate_music: [
    /create\s+(a\s+)?(song|music|track|beat|soundtrack)/i,
    /generate\s+(a\s+)?(song|music|track|beat)/i,
    /make\s+(a\s+)?(song|music|track|beat)/i,
    /compose\s+(a\s+)?(song|music)/i,
    /original\s+(music|song|track)/i,
  ],
  edit_content: [
    /edit\s+(my|this|the)/i,
    /modify\s+(my|this|the)/i,
    /change\s+(the|my)/i,
    /adjust\s+(the|my)/i,
    /refine\s+(my|this)/i,
    /enhance\s+(my|this)/i,
    /improve\s+(my|this)/i,
  ],
  export_video: [
    /export\s+(my|this|the)/i,
    /download\s+(my|this|the)/i,
    /save\s+(my|this|the)/i,
    /render\s+(my|this|the)/i,
    /output\s+(my|this|the)/i,
  ],
  get_help: [
    /help\s+(me|with)/i,
    /how\s+(to|do)/i,
    /can\s+you\s+(help|do)/i,
    /what\s+(can|does)/i,
    /explain\s+(me|how)/i,
    /guide\s+(me|through)/i,
  ],
  ask_question: [
    /\?$/,
    /what\s+is/i,
    /how\s+does/i,
    /why\s+(does|is)/i,
    /when\s+(can|do)/i,
    /where\s+(can|do)/i,
    /who\s+(can|does)/i,
  ],
  feedback: [
    /feedback/i,
    /suggestion/i,
    /recommend/i,
    /improve/i,
    /better\s+(way|option)/i,
  ],
  escalate_human: [
    /talk\s+(to|a)\s+human/i,
    /speak\s+(to|a)\s+(real\s+)?(person|agent|support)/i,
    /customer\s+support/i,
    /human\s+assistance/i,
    /get\s+(real\s+)?(person|human)/i,
  ],
  ambiguous: [],
  unknown: [],
};

// Entity patterns
const ENTITY_PATTERNS: Record<string, RegExp[]> = {
  topic: [
    /(?:about|for|on|regarding)\s+([^.,!?]+)/i,
    /(?:subject|topic|theme)[:\s]+([^.,!?]+)/i,
  ],
  duration: [
    /(\d+)\s*(second|minute|hour)s?/i,
    /(short|medium|long)\s*(?:video|duration)?/i,
  ],
  aspectRatio: [
    /(16:9|9:16|1:1|4:3)/,
    /(landscape|portrait|square)/i,
  ],
  style: [
    /(cinematic|anime|watercolor|film noir|documentary|modern|vintage)/i,
    /(realistic|stylized|abstract)/i,
  ],
  mood: [
    /(happy|sad|energetic|calm|mysterious|dramatic|romantic|triumphant)/i,
  ],
  audioUrl: [
    /(?:youtube\.com|youtu\.be|mp3|wav|audio)[:\s]+([^\s]+)/i,
  ],
  fileReference: [
    /(?:this|my|the)\s+(file|audio|video|image)s?/i,
  ],
};

// Contextual modifiers that affect intent confidence
const MODIFIER_PATTERNS: Array<{ pattern: RegExp; modifier: number }> = [
  { pattern: /please/i, modifier: 0.1 },
  { pattern: /i\s+want/i, modifier: 0.15 },
  { pattern: /i\s+need/i, modifier: 0.15 },
  { pattern: /could\s+you/i, modifier: -0.05 },
  { pattern: /maybe/i, modifier: -0.1 },
  { pattern: /not\s+sure/i, modifier: -0.15 },
];

/**
 * Parse natural language input into structured intent
 */
export function parseIntent(
  input: string,
  context?: ConversationContext
): IntentResult {
  const normalizedInput = input.trim().toLowerCase();
  let bestIntent: IntentType = 'unknown';
  let bestConfidence = 0;
  const allEntities: ExtractedEntity[] = [];

  // Check each intent pattern
  for (const [intent, patterns] of Object.entries(INTENT_PATTERNS)) {
    if (intent === 'ambiguous' || intent === 'unknown') continue;
    
    for (const pattern of patterns) {
      if (pattern.test(normalizedInput)) {
        const baseConfidence = 0.7; // Base confidence for pattern match
        const confidence = Math.min(1, baseConfidence + calculateModifier(input));
        
        if (confidence > bestConfidence) {
          bestConfidence = confidence;
          bestIntent = intent as IntentType;
        }
        break;
      }
    }
  }

  // Extract entities
  for (const [entityType, patterns] of Object.entries(ENTITY_PATTERNS)) {
    for (const pattern of patterns) {
      const match = normalizedInput.match(pattern);
      if (match) {
        allEntities.push({
          type: entityType,
          value: match[1] || match[0],
          confidence: 0.8,
        });
      }
    }
  }

  // Apply context from previous conversation
  if (context && context.lastIntent && bestConfidence < 0.8) {
    // If context suggests a direction, increase confidence slightly
    if (relatedToContext(normalizedInput, context.lastIntent)) {
      bestConfidence += 0.1;
    }
  }

  // Determine if clarification is needed
  const { requiresClarification, clarificationQuestions } = checkClarification(
    bestIntent,
    allEntities,
    context
  );

  // Generate suggested workflows
  const suggestedWorkflows = getSuggestedWorkflows(bestIntent, allEntities);

  // Generate response for certain intents
  let response: string | undefined;
  if (bestIntent === 'greeting') {
    response = generateGreetingResponse(context);
  }

  return {
    intent: bestIntent,
    confidence: Math.min(1, bestConfidence),
    entities: allEntities,
    requiresClarification,
    clarificationQuestions: requiresClarification ? clarificationQuestions : undefined,
    suggestedWorkflows,
    response,
  };
}

/**
 * Calculate confidence modifier based on input characteristics
 */
function calculateModifier(input: string): number {
  let modifier = 0;
  
  for (const { pattern, modifier: mod } of MODIFIER_PATTERNS) {
    if (pattern.test(input)) {
      modifier += mod;
    }
  }

  // Length factor - very short or very long inputs are less certain
  const wordCount = input.split(/\s+/).length;
  if (wordCount < 3) modifier -= 0.1;
  if (wordCount > 50) modifier -= 0.05;

  return modifier;
}

/**
 * Check if clarification is needed based on intent and entities
 */
function checkClarification(
  intent: IntentType,
  entities: ExtractedEntity[],
  context?: ConversationContext
): { requiresClarification: boolean; clarificationQuestions: string[] } {
  const questions: string[] = [];

  switch (intent) {
    case 'create_video':
      if (!entities.find(e => e.type === 'topic')) {
        questions.push("What would you like your video to be about?");
      }
      if (!entities.find(e => e.type === 'duration')) {
        questions.push("How long should the video be?");
      }
      if (!entities.find(e => e.type === 'style')) {
        questions.push("What visual style appeals to you (cinematic, anime, etc.)?");
      }
      break;

    case 'generate_music':
      if (!entities.find(e => e.type === 'mood')) {
        questions.push("What mood or feeling should the music have?");
      }
      if (!entities.find(e => e.type === 'duration')) {
        questions.push("How long should the track be?");
      }
      break;

    case 'edit_content':
      if (!entities.find(e => e.type === 'fileReference')) {
        questions.push("Which file would you like to edit?");
      }
      questions.push("What specific changes would you like to make?");
      break;

    case 'ask_question':
      // Questions are valid as-is
      break;

    case 'escalate_human':
      // No clarification needed
      break;

    default:
      questions.push("Could you tell me more about what you're trying to do?");
  }

  return {
    requiresClarification: questions.length > 0,
    clarificationQuestions: questions,
  };
}

/**
 * Get suggested workflows based on intent
 */
function getSuggestedWorkflows(
  intent: IntentType,
  entities: ExtractedEntity[]
): string[] {
  const workflowMap: Record<IntentType, string[]> = {
    greeting: ['show_capabilities'],
    create_video: ['runProductionPipeline', 'generateVisuals', 'generateNarration'],
    generate_music: ['generateMusicTrack', 'musicProducerAgent'],
    edit_content: ['editScene', 'adjustTimeline', 'modifyVisuals'],
    export_video: ['exportVideo', 'renderFinal'],
    get_help: ['showDocumentation', 'explainWorkflow'],
    ask_question: ['answerQuestion', 'showDocumentation'],
    feedback: ['collectFeedback', 'suggestImprovements'],
    escalate_human: ['transferToHuman', 'createSupportTicket'],
    ambiguous: ['clarifyIntent'],
    unknown: ['fallbackResponse'],
  };

  return workflowMap[intent] || ['fallbackResponse'];
}

/**
 * Check if input is related to previous context
 */
function relatedToContext(input: string, lastIntent: string): boolean {
  const relatedPatterns: Record<string, RegExp[]> = {
    create_video: [/yes/i, /that/i, /video/i, /go/i, /do\s+it/i],
    generate_music: [/music/i, /song/i, /track/i, /beat/i, /yes/i],
    ask_question: [/yes/i, /that/i, /explain/i, /more/i],
  };

  const patterns = relatedPatterns[lastIntent];
  if (!patterns) return false;

  return patterns.some(pattern => pattern.test(input));
}

/**
 * Generate contextual greeting response
 */
function generateGreetingResponse(context?: ConversationContext): string {
  const greetings = [
    "Hello! I'm ready to help you create something amazing. What would you like to work on today?",
    "Hi there! I can help you with video creation, music generation, and more. What's on your mind?",
    "Hey! Let's create something together. What would you like to make?",
  ];

  const base = greetings[Math.floor(Math.random() * greetings.length)] || "Hello! How can I help you today?";

  // Add contextual suggestion if we have previous context
  if (context && context.userGoals.length > 0) {
    const lastGoal = context.userGoals[context.userGoals.length - 1];
    return `${base} I remember you were interested in ${lastGoal}. Would you like to continue with that?`;
  }

  return base;
}

/**
 * Detect if input is a clarification response
 */
export function isClarificationResponse(
  input: string,
  expectedEntity: string
): boolean {
  const positivePatterns = [/yes/i, /sure/i, /correct/i, /right/i, /that's?/i];
  const negativePatterns = [/^no/i, /^not/i, /^actually/i, /^wait/i, /^different/i];

  if (positivePatterns.some(p => p.test(input))) {
    return true;
  }

  if (negativePatterns.some(p => p.test(input))) {
    return false;
  }

  // If the input contains the entity we're asking about, it's a clarification
  return input.toLowerCase().includes(expectedEntity.toLowerCase());
}

/**
 * Generate a response for ambiguous inputs
 */
export function handleAmbiguousInput(input: string): IntentResult {
  return {
    intent: 'ambiguous',
    confidence: 0.4,
    entities: [],
    requiresClarification: true,
    clarificationQuestions: [
      "I'd like to help, but I'm not sure what you mean. Could you clarify?",
      "Are you trying to create a video, generate music, or something else?",
    ],
    suggestedWorkflows: ['clarifyIntent', 'showOptions'],
    response: "I want to make sure I help you correctly. Could you tell me more about what you'd like to do?",
  };
}
````

## File: packages/shared/src/services/ai/production/agentCore.ts
````typescript
/**
 * Agent Core - Production Agent Entry Points
 *
 * Re-exports from focused modules:
 * - resultCache.ts   — Cached result checking
 * - agentExecutor.ts — Main agent loop with tool execution
 * - errorHandler.ts  — Error recovery and fallback handling
 *
 * Also contains the multi-agent entry point and session management.
 */

import { GEMINI_API_KEY } from "../../shared/apiClient";
import { agentLogger } from "../../logger";
import { ProductionState, ProductionProgress } from "./types";
import { productionStore } from "./store";
import { setGlobalProgressCallback } from "./tools/contentTools";

import {
    runSupervisorAgent,
    type SupervisorOptions,
    type SupervisorResult,
} from "../subagents/supervisorAgent";

// --- Re-exports from focused modules ---

export { checkResultCache } from "./resultCache";
export { runProductionAgent } from "./agentExecutor";

const log = agentLogger.child('AgentCore');

// --- Multi-Agent Entry Point ---

/**
 * Run the production agent with multi-agent architecture (supervisor + subagents).
 *
 * This is the new multi-agent implementation that uses specialized subagents:
 * - IMPORT subagent: YouTube/audio import and transcription
 * - CONTENT subagent: Content planning, narration, quality validation
 * - MEDIA subagent: Visual and audio asset generation
 * - ENHANCEMENT/EXPORT subagent: Post-processing and final export
 *
 * @param userRequest User's natural language request
 * @param onProgress Optional callback for progress updates
 * @returns Production state or null if failed
 */
export async function runProductionAgentWithSubagents(
    userRequest: string,
    onProgress?: (progress: ProductionProgress) => void
): Promise<ProductionState | null> {
    if (!GEMINI_API_KEY) {
        throw new Error('GEMINI_API_KEY not configured');
    }

    setGlobalProgressCallback(onProgress || null);

    onProgress?.({
        stage: "starting",
        message: "Starting multi-agent video production...",
        isComplete: false
    });

    try {
        const supervisorOptions: SupervisorOptions = {
            apiKey: GEMINI_API_KEY,
            userRequest,
            onProgress: (progress) => {
                onProgress?.({
                    stage: progress.stage,
                    tool: progress.tool,
                    message: progress.message,
                    isComplete: progress.isComplete,
                    success: progress.success,
                });
            },
        };

        const result: SupervisorResult = await runSupervisorAgent(supervisorOptions);

        const finalState = result.sessionId ? productionStore.get(result.sessionId) : null;

        if (finalState) {
            const assetSummary = {
                scenes: finalState.contentPlan?.scenes.length || 0,
                narrations: finalState.narrationSegments.length,
                visuals: finalState.visuals.length,
                music: finalState.musicTaskId ? 1 : 0,
                sfx: finalState.sfxPlan?.scenes.length || 0,
                subtitles: finalState.subtitles ? 1 : 0,
            };

            onProgress?.({
                stage: "complete",
                message: `Multi-agent production complete! Generated ${assetSummary.scenes} scenes with ${assetSummary.narrations} narrations, ${assetSummary.visuals} visuals${assetSummary.music ? ', music' : ''}${assetSummary.sfx ? `, ${assetSummary.sfx} SFX` : ''}${assetSummary.subtitles ? ', and subtitles' : ''}.`,
                isComplete: true,
                assetSummary,
            });
        } else {
            onProgress?.({
                stage: "complete",
                message: result.message || "Multi-agent production complete!",
                isComplete: true,
            });
        }

        return finalState ?? null;
    } catch (error) {
        log.error(" Multi-agent error:", error);

        onProgress?.({
            stage: "error",
            message: error instanceof Error ? error.message : String(error),
            isComplete: true,
        });

        throw error;
    } finally {
        setGlobalProgressCallback(null);
    }
}

// --- Session Management ---

/**
 * Get a production session by ID.
 */
export function getProductionSession(sessionId: string): ProductionState | null {
    return productionStore.get(sessionId) || null;
}

/**
 * Clear a production session.
 */
export function clearProductionSession(sessionId: string): void {
    productionStore.delete(sessionId);
}
````

## File: packages/shared/src/services/ai/production/agentExecutor.ts
````typescript
/**
 * Agent Executor
 *
 * Main production agent execution loop with tool execution,
 * caching, progress reporting, and error recovery.
 * Extracted from agentCore.ts for focused responsibility.
 *
 * Requirements:
 * - 5.1-5.5: Intent-based tool selection
 * - 6.1-6.5: Error recovery and fallback
 * - 8.1-8.5: Progress reporting
 * - 10.1-10.5: Optimization (caching, duplicate prevention)
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HumanMessage, SystemMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { GEMINI_API_KEY } from "../../shared/apiClient";
import { agentLogger } from "../../logger";

import { ProductionState, ProductionProgress } from "./types";
import { productionStore } from "./store";
import { createStepIdentifier, isValidSessionId } from "./utils";
import { PRODUCTION_AGENT_PROMPT } from "./prompts";
import { productionTools, toolMap } from "./toolRegistration";
import { setGlobalProgressCallback } from "./tools/contentTools";
import { checkResultCache } from "./resultCache";
import { executeToolWithRecovery } from "./errorHandler";

import {
    analyzeIntent,
    generateIntentHint,
} from "../../agent/intentDetection";

import {
    type ToolError,
    ErrorTracker,
    classifyError,
} from "../../agent/errorRecovery";

const log = agentLogger.child('AgentExecutor');

/**
 * Run the production agent with intent-based tool selection.
 *
 * @param userRequest User's natural language request
 * @param onProgress Optional callback for progress updates
 * @returns Production state or null if failed
 */
export async function runProductionAgent(
    userRequest: string,
    onProgress?: (progress: ProductionProgress) => void
): Promise<ProductionState | null> {
    if (!GEMINI_API_KEY) {
        throw new Error('GEMINI_API_KEY not configured');
    }

    // Analyze user intent for tool selection
    const intentResult = analyzeIntent(userRequest);
    const intentHint = generateIntentHint(intentResult);

    log.info(' Intent analysis:', {
        firstTool: intentResult.firstTool,
        hasYouTubeUrl: intentResult.hasYouTubeUrl,
        wantsAnimation: intentResult.wantsAnimation,
        wantsMusic: intentResult.wantsMusic,
        detectedStyle: intentResult.detectedStyle,
        optionalTools: intentResult.optionalTools,
    });

    const model = new ChatGoogleGenerativeAI({
        model: "gemini-3-flash-preview",
        apiKey: GEMINI_API_KEY,
        temperature: 0.3,
        // Relax safety filters to allow creative content (horror, thriller, etc.)
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        safetySettings: [
            { category: "HARM_CATEGORY_HARASSMENT", threshold: "BLOCK_ONLY_HIGH" },
            { category: "HARM_CATEGORY_HATE_SPEECH", threshold: "BLOCK_ONLY_HIGH" },
            { category: "HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold: "BLOCK_ONLY_HIGH" },
            { category: "HARM_CATEGORY_DANGEROUS_CONTENT", threshold: "BLOCK_ONLY_HIGH" },
        ] as any,
    });

    const modelWithTools = model.bindTools(productionTools);

    const enhancedUserMessage = intentHint
        ? `${intentHint}\n\nUser Request: ${userRequest}`
        : userRequest;

    const messages: Array<SystemMessage | HumanMessage | AIMessage | ToolMessage> = [
        new SystemMessage(PRODUCTION_AGENT_PROMPT),
        new HumanMessage(enhancedUserMessage),
    ];

    setGlobalProgressCallback(onProgress || null);

    onProgress?.({
        stage: "starting",
        message: "Starting video production agent...",
        isComplete: false
    });

    if (intentResult.hasYouTubeUrl) {
        onProgress?.({
            stage: "intent_detected",
            message: `Detected YouTube URL: ${intentResult.youtubeUrl}`,
            isComplete: false
        });
    } else if (intentResult.hasAudioFile) {
        onProgress?.({
            stage: "intent_detected",
            message: `Detected audio file: ${intentResult.audioFilePath}`,
            isComplete: false
        });
    }

    let sessionId: string | null = null;
    const MAX_ITERATIONS = 20;
    let iteration = 0;

    const errorTracker = new ErrorTracker();
    const executedTools = new Set<string>();
    const executedToolsPerStep = new Map<string, Set<string>>();

    try {
        while (iteration < MAX_ITERATIONS) {
            iteration++;

            if (iteration >= MAX_ITERATIONS - 2) {
                onProgress?.({
                    stage: "warning",
                    message: `Approaching iteration limit (${iteration}/${MAX_ITERATIONS}). Production will stop soon if not completed.`,
                    isComplete: false,
                });
            }

            let response;
            try {
                response = await modelWithTools.invoke(messages as Parameters<typeof modelWithTools.invoke>[0]);
            } catch (invokeError: unknown) {
                // LangChain crashes with "chatGeneration is undefined" when Gemini
                // safety filters block the response (zero candidates returned).
                const msg = invokeError instanceof Error ? invokeError.message : String(invokeError);
                if (msg.includes("chatGeneration is undefined") || msg.includes("can't access property")) {
                    log.error(" Model response blocked (likely safety filter). Retrying with simplified prompt.");
                    onProgress?.({
                        stage: "warning",
                        message: "Response blocked by safety filters. Retrying...",
                        isComplete: false,
                    });
                    // Remove the last user/tool message and add a nudge to continue
                    messages.push(new HumanMessage("Please continue with the production. Use appropriate creative language."));
                    continue;
                }
                throw invokeError;
            }
            messages.push(response as unknown as AIMessage);

            const toolCalls = response.tool_calls;
            if (!toolCalls || toolCalls.length === 0) {
                // No more tool calls — agent is done
                if (sessionId) {
                    const state = productionStore.get(sessionId);
                    if (state) {
                        const assetSummary = {
                            scenes: state.contentPlan?.scenes.length || 0,
                            narrations: state.narrationSegments.length,
                            visuals: state.visuals.length,
                            music: state.musicTaskId ? 1 : 0,
                            sfx: state.sfxPlan?.scenes.length || 0,
                            subtitles: state.subtitles ? 1 : 0,
                        };

                        onProgress?.({
                            stage: "complete",
                            message: `Production complete! Generated ${assetSummary.scenes} scenes with ${assetSummary.narrations} narrations, ${assetSummary.visuals} visuals${assetSummary.music ? ', music' : ''}${assetSummary.sfx ? `, ${assetSummary.sfx} SFX` : ''}${assetSummary.subtitles ? ', and subtitles' : ''}.`,
                            isComplete: true,
                            assetSummary,
                        });
                    } else {
                        onProgress?.({ stage: "complete", message: "Production complete!", isComplete: true });
                    }
                } else {
                    onProgress?.({ stage: "complete", message: "Production complete!", isComplete: true });
                }
                break;
            }

            for (const toolCall of toolCalls) {
                const toolName = toolCall.name;
                const toolArgs = toolCall.args as Record<string, unknown>;

                const stepId = createStepIdentifier(toolName, toolArgs);

                if (!executedToolsPerStep.has(stepId)) {
                    executedToolsPerStep.set(stepId, new Set());
                }

                const stepTools = executedToolsPerStep.get(stepId)!;
                if (stepTools.has(toolName)) {
                    log.info(` Skipping duplicate tool call: ${toolName} for step ${stepId}`);
                    messages.push(new ToolMessage({
                        content: JSON.stringify({
                            success: true,
                            skipped: true,
                            message: `Skipped duplicate ${toolName} call - already executed for this step`,
                        }),
                        tool_call_id: toolCall.id || toolName,
                    }));
                    continue;
                }

                onProgress?.({
                    stage: "tool_call",
                    tool: toolName,
                    message: `Executing ${toolName}...`,
                    isComplete: false,
                });

                // Check cache
                const currentState = sessionId ? productionStore.get(sessionId) ?? null : null;
                const cacheCheck = checkResultCache(toolName, toolArgs, currentState);

                if (cacheCheck.cached) {
                    log.info(` Using cached results for ${toolName}`);
                    const result = JSON.stringify(cacheCheck.result);

                    onProgress?.({
                        stage: "tool_result",
                        tool: toolName,
                        message: `\u2713 ${cacheCheck.result!.message}`,
                        isComplete: false,
                        success: true,
                    });

                    messages.push(new ToolMessage({
                        content: result,
                        tool_call_id: toolCall.id || toolName,
                    }));
                    continue;
                }

                // Find and execute tool
                const targetTool = toolMap[toolName];
                if (!targetTool) {
                    log.error(` Tool not found: ${toolName}`);
                    continue;
                }

                const { result, fallbackApplied } = await executeToolWithRecovery(
                    toolName,
                    toolArgs,
                    targetTool,
                    sessionId,
                    errorTracker,
                    executedTools,
                    onProgress
                );

                // Mark successful logical executions to prevent duplicates
                if (!fallbackApplied) {
                    try {
                        const parsed = JSON.parse(result);
                        if (parsed.success !== false) {
                            const currentStepId = createStepIdentifier(toolName, toolArgs);
                            const currentStepTools = executedToolsPerStep.get(currentStepId) || new Set();
                            currentStepTools.add(toolName);
                            executedToolsPerStep.set(currentStepId, currentStepTools);
                        }
                    } catch { /* Not JSON */ }
                }

                // Parse result for session ID and emit progress
                try {
                    const parsed = JSON.parse(result);
                    if (!sessionId && (toolName === 'plan_video' || toolName === 'create_storyboard' || toolName === 'generate_breakdown')) {
                        if (parsed.sessionId && isValidSessionId(parsed.sessionId)) {
                            sessionId = parsed.sessionId;
                            // Emit sessionId to the progress callback so UI can capture it
                            onProgress?.({
                                stage: "session_created",
                                message: `Session created: ${sessionId}`,
                                isComplete: false,
                                sessionId: sessionId ?? undefined,
                            });
                        } else if (parsed.sessionId) {
                            log.warn(`Invalid sessionId format from ${toolName}: ${parsed.sessionId}`);
                        }
                    }
                    if (parsed.message) {
                        const statusPrefix = fallbackApplied ? '\u26a0\ufe0f ' : '';
                        onProgress?.({
                            stage: "tool_result",
                            tool: toolName,
                            message: statusPrefix + parsed.message,
                            isComplete: false,
                            success: parsed.success !== false,
                        });
                    }
                } catch { /* Not JSON */ }

                messages.push(new ToolMessage({
                    content: result,
                    tool_call_id: toolCall.id || toolName,
                }));
            }
        }

        // Check iteration limit
        if (iteration >= MAX_ITERATIONS) {
            log.warn(` Iteration limit reached (${MAX_ITERATIONS}). Stopping execution.`);
            onProgress?.({
                stage: "limit_reached",
                message: `Production stopped: iteration limit (${MAX_ITERATIONS}) reached. Partial results may be available.`,
                isComplete: false,
            });

            const report = errorTracker.generateReport();
            report.summary += ` Production stopped due to iteration limit (${MAX_ITERATIONS}).`;

            if (sessionId) {
                const state = productionStore.get(sessionId);
                if (state) {
                    state.partialSuccessReport = report;
                    productionStore.set(sessionId, state);
                }
            }
        }

        // Generate partial success report
        const report = errorTracker.generateReport();

        if (sessionId) {
            const state = productionStore.get(sessionId);
            if (state) {
                state.partialSuccessReport = report;
                productionStore.set(sessionId, state);
            }

            if (report.errors.length > 0) {
                log.info(` Partial success report:`, report.summary);
                onProgress?.({
                    stage: "summary",
                    message: report.summary,
                    isComplete: false,
                });
            }

            return productionStore.get(sessionId) || null;
        }

        return null;
    } catch (error) {
        log.error(" Error:", error);

        const toolError: ToolError = {
            tool: 'production_agent',
            error: error instanceof Error ? error.message : String(error),
            category: classifyError(error instanceof Error ? error : new Error(String(error))),
            timestamp: Date.now(),
            retryCount: 0,
            recoverable: false,
        };
        errorTracker.recordError(toolError, false);

        if (sessionId) {
            const state = productionStore.get(sessionId);
            if (state) {
                state.errors.push(toolError);
                state.partialSuccessReport = errorTracker.generateReport();
                productionStore.set(sessionId, state);
            }
        }

        onProgress?.({
            stage: "error",
            message: error instanceof Error ? error.message : String(error),
            isComplete: true,
        });
        throw error;
    } finally {
        setGlobalProgressCallback(null);
    }
}
````

## File: packages/shared/src/services/ai/production/errorHandler.ts
````typescript
/**
 * Error Handler
 *
 * Handles tool execution failures, fallback application, and error state management.
 * Extracted from agentCore.ts for focused responsibility.
 *
 * Requirements: 6.1-6.5 — Error recovery and fallback handling
 */

import { agentLogger } from "../../logger";
import { ProductionState, ProductionProgress } from "./types";
import { productionStore } from "./store";
import { GeneratedImage } from "../../../types";
import {
    type ToolError,
    ErrorTracker,
    getRecoveryStrategy,
    executeWithRetry,
    applyFallback,
} from "../../agent/errorRecovery";

const log = agentLogger.child('ErrorHandler');

export interface ToolExecutionResult {
    result: string;
    fallbackApplied: boolean;
}

/**
 * Execute a tool with retry logic and fallback handling.
 */
export async function executeToolWithRecovery(
    toolName: string,
    toolArgs: Record<string, unknown>,
    targetTool: { invoke: (args: Record<string, unknown>) => Promise<unknown> },
    sessionId: string | null,
    errorTracker: ErrorTracker,
    executedTools: Set<string>,
    onProgress?: (progress: ProductionProgress) => void
): Promise<ToolExecutionResult> {
    const strategy = getRecoveryStrategy(toolName);

    const getToolContext = () => {
        if (sessionId) {
            const state = productionStore.get(sessionId);
            return {
                ...toolArgs,
                visuals: state?.visuals,
                narrationSegments: state?.narrationSegments,
                musicUrl: (state as ProductionState & { musicUrl?: string })?.musicUrl,
                sfxPlan: state?.sfxPlan,
                subtitles: state?.subtitles,
            };
        }
        return toolArgs;
    };

    const executionResult = await executeWithRetry(
        () => targetTool.invoke(toolArgs),
        strategy,
        (attempt, _err, delay) => {
            onProgress?.({
                stage: "retry",
                tool: toolName,
                message: `${toolName} failed (attempt ${attempt}/${strategy.maxRetries}). Retrying in ${Math.round(delay / 1000)}s...`,
                isComplete: false,
            });
        }
    );

    let result: string;
    let fallbackApplied = false;

    if (executionResult.success) {
        result = executionResult.data as string;

        let logicalSuccess = true;
        try {
            const parsed = JSON.parse(result);
            if (parsed.success === false) {
                logicalSuccess = false;
            }
        } catch {
            // Not JSON — assume success
        }

        if (logicalSuccess) {
            errorTracker.recordSuccess();
            executedTools.add(toolName);
        } else {
            log.warn(` Tool ${toolName} returned logical failure, allowing retry.`);
            try {
                const parsed = JSON.parse(result);
                if (parsed.error && sessionId) {
                    const toolError: ToolError = {
                        tool: toolName,
                        error: parsed.error,
                        category: 'recoverable',
                        timestamp: Date.now(),
                        retryCount: 0,
                        recoverable: true,
                    };
                    const state = productionStore.get(sessionId);
                    if (state) {
                        state.errors.push(toolError);
                        productionStore.set(sessionId, state);
                    }
                }
            } catch { /* Ignore */ }
        }
    } else {
        const toolError = executionResult.error!;
        log.error(` Tool ${toolName} failed after ${executionResult.retryCount} retries:`, toolError.error);

        if (strategy.fallbackAction && strategy.continueOnFailure) {
            onProgress?.({
                stage: "fallback",
                tool: toolName,
                message: `${toolName} failed. Applying fallback: ${strategy.fallbackAction}`,
                isComplete: false,
            });

            const fallbackResult = await applyFallback(
                strategy.fallbackAction,
                toolError,
                getToolContext()
            );

            if (fallbackResult) {
                // Special handling for generate_visuals fallback
                if (toolName === 'generate_visuals' && sessionId) {
                    applyVisualsFallback(sessionId);
                }

                result = JSON.stringify({
                    success: true,
                    fallback: true,
                    fallbackAction: strategy.fallbackAction,
                    ...fallbackResult,
                });
                toolError.fallbackApplied = strategy.fallbackAction;
                errorTracker.recordError(toolError, true);
                fallbackApplied = true;
            } else {
                result = JSON.stringify({
                    success: false,
                    error: toolError.error,
                    retryCount: executionResult.retryCount,
                });
                errorTracker.recordError(toolError, false);
            }
        } else {
            result = JSON.stringify({
                success: false,
                error: toolError.error,
                retryCount: executionResult.retryCount,
                continueOnFailure: strategy.continueOnFailure,
            });
            errorTracker.recordError(toolError, false);
        }

        // Store error in session state
        if (sessionId) {
            const state = productionStore.get(sessionId);
            if (state) {
                state.errors.push(toolError);
                productionStore.set(sessionId, state);
            }
        }
    }

    return { result, fallbackApplied };
}

/**
 * Apply visual placeholders when generate_visuals fails.
 */
function applyVisualsFallback(sessionId: string): void {
    const state = productionStore.get(sessionId);
    if (state && state.contentPlan) {
        log.info(` Applying fallback visuals to state for ${sessionId}`);

        const currentVisuals = state.visuals ? [...state.visuals] : [];
        const placeholders: GeneratedImage[] = [];

        for (let i = 0; i < state.contentPlan.scenes.length; i++) {
            const scene = state.contentPlan.scenes[i];
            if (!scene) {
                log.warn(` Scene at index ${i} not found, skipping placeholder.`);
                continue;
            }
            if (currentVisuals[i]?.imageUrl) {
                placeholders.push(currentVisuals[i]!);
            } else {
                placeholders.push({
                    promptId: scene.id,
                    imageUrl: "",
                    isPlaceholder: true
                });
            }
        }

        state.visuals = placeholders;
        productionStore.set(sessionId, state);
    }
}
````

## File: packages/shared/src/services/ai/production/index.ts
````typescript
/**
 * Production Agent - Modular Entry Point
 *
 * This module re-exports all production agent functionality from the modular
 * structure. Import from this file instead of individual modules for cleaner imports.
 *
 * @example
 * import {
 *   runProductionAgent,
 *   ProductionState,
 *   productionTools
 * } from './production';
 */

// --- Types ---
export {
    // Schemas (PascalCase as defined)
    PlanVideoSchema,
    NarrateScenesSchema,
    GenerateVisualsSchema,
    PlanSFXSchema,
    ValidatePlanSchema,
    AdjustTimingSchema,
    GenerateVideoSchema,
    AnimateImageSchema,
    GenerateMusicSchema,
    StoryModeSchema,
    VerifyCharacterConsistencySchema,

    // Interfaces
    type ProductionState,
    type ProductionProgress,
    type StoryModeState,

    // Helper functions
    createInitialState,
} from "./types";

// --- Store ---
export {
    productionStore,
    storyModeStore,
    getProductionSession as getProductionSessionFromStore,
    clearProductionSession as clearProductionSessionFromStore,
    initializeProductionSession,
    updateProductionSession,
} from "./store";

// --- Utilities ---
export {
    detectLanguageFromText,
    generateSessionId,
    validateContentPlanId,
    isValidSessionId,
    createStepIdentifier,
} from "./utils";

// --- Prompts ---
export { PRODUCTION_AGENT_PROMPT } from "./prompts";

// --- Tools ---
export {
    // Content Tools
    planVideoTool,
    narrateScenesTool,
    validatePlanTool,
    adjustTimingTool,
    generateVisualsTool,
    planSFXTool,
    // Progress callback (used by tools)
    setGlobalProgressCallback,
    getGlobalProgressCallback,
} from "./tools/contentTools";

export {
    // Media Tools
    generateVideoTool,
    animateImageTool,
    generateMusicTool,
} from "./tools/mediaTools";

export {
    // Status Tools
    getProductionStatusTool,
    markCompleteTool,
} from "./tools/statusTools";

export {
    // Story Tools
    generateBreakdownTool,
    createScreenplayTool,
    generateCharactersTool,
    generateShotlistTool,
    verifyCharacterConsistencyTool,
} from "./tools/storyTools";

// --- Tool Registration ---
export {
    productionTools,
    storyModeTools,
    toolMap,
    registerProductionTools,
    toolRegistry,
    ToolGroup,
    createToolDefinition,
} from "./toolRegistration";

// --- Agent Core ---
export {
    runProductionAgent,
    runProductionAgentWithSubagents,
    getProductionSession,
    clearProductionSession,
    checkResultCache,
} from "./agentCore";

// --- Re-exports from external modules ---
export {
    analyzeIntent,
    generateIntentHint,
    detectYouTubeUrl,
    shouldAnimate,
    shouldGenerateMusic,
    extractStyle,
    type IntentDetectionResult,
} from "../../agent/intentDetection";

export {
    type ToolError,
    type PartialSuccessReport,
    type RecoveryStrategy,
    type ErrorCategory,
    ErrorTracker,
    formatErrorsForResponse,
    getRecoveryStrategy,
    classifyError,
} from "../../agent/errorRecovery";
````

## File: packages/shared/src/services/ai/production/persistence.ts
````typescript
/**
 * IndexedDB Persistence for Production Sessions
 *
 * Provides durable storage for production state to survive page refreshes.
 * Works alongside cloud autosave for redundancy.
 *
 * Storage strategy:
 * - Main state (serializable fields) → 'sessions' store
 * - Blob data (exportedVideo) → 'blobs' store (optional, large data)
 * - Session index → tracks available sessions for recovery UI
 */

import { openDB, type IDBPDatabase } from 'idb';
import { ProductionState, StoryModeState, createInitialState } from './types';
import { agentLogger } from '../../logger';

const log = agentLogger.child('Persistence');

const DB_NAME = 'lyriclens-production';
const DB_VERSION = 2;

export interface SessionMetadata {
    sessionId: string;
    createdAt: number;
    updatedAt: number;
    topic?: string;
    sceneCount: number;
    isComplete: boolean;
}

/**
 * Type for serializable production state (excludes Blobs)
 */
type SerializableProductionState = Omit<ProductionState, 'exportedVideo'> & {
    exportedVideo: null; // Always null in serialized form
};

/**
 * Stored session record
 */
interface StoredSession {
    sessionId: string;
    state: SerializableProductionState;
    metadata: SessionMetadata;
}

/**
 * Stored story session
 */
interface StoredStorySession {
    sessionId: string;
    state: StoryModeState;
    updatedAt: number;
}

let dbPromise: Promise<IDBPDatabase> | null = null;

/**
 * Get or create the IndexedDB database connection
 */
async function getDB(): Promise<IDBPDatabase> {
    if (dbPromise) return dbPromise;

    dbPromise = openDB(DB_NAME, DB_VERSION, {
        upgrade(db, oldVersion) {
            log.info(`Upgrading database from v${oldVersion} to v${DB_VERSION}`);

            // Production sessions store
            if (!db.objectStoreNames.contains('sessions')) {
                const sessionsStore = db.createObjectStore('sessions', { keyPath: 'sessionId' });
                sessionsStore.createIndex('updatedAt', 'metadata.updatedAt');
            }

            // Story mode sessions store
            if (!db.objectStoreNames.contains('story-sessions')) {
                const storyStore = db.createObjectStore('story-sessions', { keyPath: 'sessionId' });
                storyStore.createIndex('updatedAt', 'updatedAt');
            }

            // Blob store for large binary data (optional)
            if (!db.objectStoreNames.contains('blobs')) {
                db.createObjectStore('blobs', { keyPath: 'id' });
            }

            // AI logs store (v2+)
            if (!db.objectStoreNames.contains('ai-logs')) {
                const aiLogsStore = db.createObjectStore('ai-logs', { keyPath: 'id' });
                aiLogsStore.createIndex('sessionId', 'sessionId');
                aiLogsStore.createIndex('timestamp', 'timestamp');
                aiLogsStore.createIndex('step', 'step');
            }
        },
        blocked() {
            log.warn('Database upgrade blocked by another tab');
        },
        blocking() {
            log.warn('This tab is blocking a database upgrade');
        },
        terminated() {
            log.error('Database connection terminated unexpectedly');
            dbPromise = null;
        }
    });

    return dbPromise;
}

/**
 * Serialize production state for storage (removes non-serializable fields)
 */
function serializeState(state: ProductionState): SerializableProductionState {
    // Clone the state, explicitly setting exportedVideo to null
    // Blobs cannot be cloned directly to IndexedDB
    return {
        ...state,
        exportedVideo: null,
    };
}

/**
 * Create metadata from production state
 */
function createMetadata(sessionId: string, state: ProductionState, existingMeta?: SessionMetadata): SessionMetadata {
    const now = Date.now();
    return {
        sessionId,
        createdAt: existingMeta?.createdAt ?? now,
        updatedAt: now,
        topic: state.contentPlan?.title,
        sceneCount: state.contentPlan?.scenes?.length ?? 0,
        isComplete: state.isComplete,
    };
}

// ============================================================
// PRODUCTION SESSION PERSISTENCE
// ============================================================

/**
 * Save production session to IndexedDB
 */
export async function saveProductionSession(sessionId: string, state: ProductionState): Promise<void> {
    try {
        const db = await getDB();

        // Get existing metadata to preserve createdAt
        const existing = await db.get('sessions', sessionId) as StoredSession | undefined;

        const record: StoredSession = {
            sessionId,
            state: serializeState(state),
            metadata: createMetadata(sessionId, state, existing?.metadata),
        };

        await db.put('sessions', record);
        log.debug(`Saved session ${sessionId} (${record.metadata.sceneCount} scenes)`);
    } catch (error) {
        log.error('Failed to save session to IndexedDB:', error);
        // Don't throw - persistence failure shouldn't break the app
    }
}

/**
 * Load production session from IndexedDB
 */
export async function loadProductionSession(sessionId: string): Promise<ProductionState | null> {
    try {
        const db = await getDB();
        const record = await db.get('sessions', sessionId) as StoredSession | undefined;

        if (!record) {
            log.debug(`Session ${sessionId} not found in IndexedDB`);
            return null;
        }

        // Restore full ProductionState shape (add back exportedVideo)
        const state: ProductionState = {
            ...record.state,
            exportedVideo: null, // Blobs are not persisted
        };

        log.info(`Loaded session ${sessionId} from IndexedDB`);
        return state;
    } catch (error) {
        log.error('Failed to load session from IndexedDB:', error);
        return null;
    }
}

/**
 * Delete production session from IndexedDB
 */
export async function deleteProductionSession(sessionId: string): Promise<void> {
    try {
        const db = await getDB();
        await db.delete('sessions', sessionId);
        // Also delete any associated blobs
        await db.delete('blobs', `${sessionId}-video`);
        log.debug(`Deleted session ${sessionId}`);
    } catch (error) {
        log.error('Failed to delete session from IndexedDB:', error);
    }
}

/**
 * List all recoverable production sessions
 */
export async function listRecoverableSessions(): Promise<SessionMetadata[]> {
    try {
        const db = await getDB();
        const allRecords = await db.getAll('sessions') as StoredSession[];

        // Sort by updatedAt descending (most recent first)
        const sessions = allRecords
            .map(r => r.metadata)
            .sort((a, b) => b.updatedAt - a.updatedAt);

        log.info(`Found ${sessions.length} recoverable sessions`);
        return sessions;
    } catch (error) {
        log.error('Failed to list sessions from IndexedDB:', error);
        return [];
    }
}

/**
 * Check if a session exists in IndexedDB
 */
export async function hasPersistedSession(sessionId: string): Promise<boolean> {
    try {
        const db = await getDB();
        const record = await db.get('sessions', sessionId);
        return !!record;
    } catch (error) {
        log.error('Failed to check session in IndexedDB:', error);
        return false;
    }
}

/**
 * Get the most recent incomplete session (for recovery prompt)
 */
export async function getMostRecentIncompleteSession(): Promise<SessionMetadata | null> {
    try {
        const sessions = await listRecoverableSessions();
        // Find the most recent session that is not complete
        const incomplete = sessions.find(s => !s.isComplete);
        return incomplete ?? null;
    } catch (error) {
        log.error('Failed to get recent incomplete session:', error);
        return null;
    }
}

// ============================================================
// STORY SESSION PERSISTENCE
// ============================================================

/**
 * Save story mode session to IndexedDB
 */
export async function saveStorySession(sessionId: string, state: StoryModeState): Promise<void> {
    try {
        const db = await getDB();
        const record: StoredStorySession = {
            sessionId,
            state,
            updatedAt: Date.now(),
        };
        await db.put('story-sessions', record);
        log.debug(`Saved story session ${sessionId}`);
    } catch (error) {
        log.error('Failed to save story session to IndexedDB:', error);
    }
}

/**
 * Load story mode session from IndexedDB
 */
export async function loadStorySession(sessionId: string): Promise<StoryModeState | null> {
    try {
        const db = await getDB();
        const record = await db.get('story-sessions', sessionId) as StoredStorySession | undefined;
        return record?.state ?? null;
    } catch (error) {
        log.error('Failed to load story session from IndexedDB:', error);
        return null;
    }
}

// ============================================================
// BLOB STORAGE (FOR LARGE BINARY DATA)
// ============================================================

/**
 * Save a blob to IndexedDB (for video exports)
 */
export async function saveBlob(id: string, blob: Blob): Promise<void> {
    try {
        const db = await getDB();
        await db.put('blobs', { id, blob, savedAt: Date.now() });
        log.debug(`Saved blob ${id} (${(blob.size / 1024 / 1024).toFixed(2)} MB)`);
    } catch (error) {
        log.error('Failed to save blob to IndexedDB:', error);
    }
}

/**
 * Load a blob from IndexedDB
 */
export async function loadBlob(id: string): Promise<Blob | null> {
    try {
        const db = await getDB();
        const record = await db.get('blobs', id) as { id: string; blob: Blob } | undefined;
        return record?.blob ?? null;
    } catch (error) {
        log.error('Failed to load blob from IndexedDB:', error);
        return null;
    }
}

// ============================================================
// AI LOG PERSISTENCE
// ============================================================

/**
 * AI log entry stored in IndexedDB
 */
export interface AILogEntry {
    id: string;
    sessionId: string;
    step: string;
    model: string;
    input: string;
    output: string;
    durationMs: number;
    timestamp: number;
    status: 'success' | 'error';
    error?: string;
    metadata?: Record<string, unknown>;
}

/**
 * Save an AI log entry to IndexedDB (fire-and-forget)
 */
export async function saveAILog(entry: AILogEntry): Promise<void> {
    try {
        const db = await getDB();
        await db.put('ai-logs', entry);
    } catch (error) {
        log.error('Failed to save AI log:', error);
    }
}

/**
 * Get all AI logs for a session, ordered by timestamp
 */
export async function getAILogsForSession(sessionId: string): Promise<AILogEntry[]> {
    try {
        const db = await getDB();
        const logs = await db.getAllFromIndex('ai-logs', 'sessionId', sessionId) as AILogEntry[];
        return logs.sort((a, b) => a.timestamp - b.timestamp);
    } catch (error) {
        log.error('Failed to get AI logs:', error);
        return [];
    }
}

/**
 * Get AI logs for a session filtered by step
 */
export async function getAILogsByStep(sessionId: string, step: string): Promise<AILogEntry[]> {
    const logs = await getAILogsForSession(sessionId);
    return logs.filter(l => l.step === step);
}

/**
 * Delete all AI logs for a session
 */
export async function deleteAILogsForSession(sessionId: string): Promise<void> {
    try {
        const db = await getDB();
        const logs = await db.getAllFromIndex('ai-logs', 'sessionId', sessionId) as AILogEntry[];
        const tx = db.transaction('ai-logs', 'readwrite');
        for (const entry of logs) {
            await tx.store.delete(entry.id);
        }
        await tx.done;
    } catch (error) {
        log.error('Failed to delete AI logs:', error);
    }
}

// ============================================================
// CLEANUP UTILITIES
// ============================================================

/**
 * Clean up old sessions (older than maxAgeDays)
 */
export async function cleanupOldSessions(maxAgeDays: number = 7): Promise<number> {
    try {
        const db = await getDB();
        const cutoffTime = Date.now() - (maxAgeDays * 24 * 60 * 60 * 1000);

        const allRecords = await db.getAll('sessions') as StoredSession[];
        const oldSessions = allRecords.filter(r => r.metadata.updatedAt < cutoffTime);

        for (const record of oldSessions) {
            await db.delete('sessions', record.sessionId);
            await db.delete('blobs', `${record.sessionId}-video`);
            await deleteAILogsForSession(record.sessionId);
        }

        if (oldSessions.length > 0) {
            log.info(`Cleaned up ${oldSessions.length} old sessions`);
        }

        return oldSessions.length;
    } catch (error) {
        log.error('Failed to cleanup old sessions:', error);
        return 0;
    }
}

/**
 * Clear all persisted data (for testing/reset)
 */
export async function clearAllPersistedData(): Promise<void> {
    try {
        const db = await getDB();
        await db.clear('sessions');
        await db.clear('story-sessions');
        await db.clear('blobs');
        await db.clear('ai-logs');
        log.info('Cleared all persisted data');
    } catch (error) {
        log.error('Failed to clear persisted data:', error);
    }
}

/**
 * Get storage statistics
 */
export async function getStorageStats(): Promise<{ sessionCount: number; storyCount: number; estimatedSizeMB: number }> {
    try {
        const db = await getDB();
        const sessions = await db.count('sessions');
        const stories = await db.count('story-sessions');

        // Estimate size (rough calculation)
        let totalSize = 0;
        const allSessions = await db.getAll('sessions') as StoredSession[];
        for (const record of allSessions) {
            totalSize += JSON.stringify(record).length;
        }

        return {
            sessionCount: sessions,
            storyCount: stories,
            estimatedSizeMB: totalSize / 1024 / 1024,
        };
    } catch (error) {
        log.error('Failed to get storage stats:', error);
        return { sessionCount: 0, storyCount: 0, estimatedSizeMB: 0 };
    }
}
````

## File: packages/shared/src/services/ai/production/prompts.ts
````typescript
/**
 * Production Agent Prompts
 * 
 * System prompts and instructions for the production agent.
 */

export const PRODUCTION_AGENT_PROMPT = `You are an advanced Video Production Agent for LyricLens. Your job is to autonomously create complete video productions from 30 seconds to 15 minutes based on user requests.

## CRITICAL: SESSION ID USAGE
When you call plan_video, it returns a sessionId. You MUST use this EXACT sessionId as the contentPlanId parameter for ALL subsequent tool calls:
- narrate_scenes: contentPlanId = sessionId from plan_video
- generate_visuals: contentPlanId = sessionId from plan_video
- validate_plan: contentPlanId = sessionId from plan_video
- plan_sfx: contentPlanId = sessionId from plan_video
- ALL other tools that require contentPlanId

NEVER use placeholder values like "plan_123", "cp_01", or "session_12345". ALWAYS use the ACTUAL sessionId returned by plan_video.

## TOOL GROUPS AND DEPENDENCIES

Tools are organized into groups that must be executed in order. Each group depends on the previous group completing first.

### IMPORT (Run First if Applicable)
**Dependencies: None - this is the starting point for import workflows**
- import_youtube_content: Extract audio and transcribe from YouTube/X videos. Returns sessionId for use with other tools.
- transcribe_audio_file: Transcribe audio with word-level timing. Use after importing content.

### CONTENT (Core Planning)
**Dependencies: IMPORT (if importing) or None (if topic-based)**
- plan_video: Create a content plan with scenes (YOU decide scene count based on topic and duration)
- narrate_scenes: Generate voice narration for all scenes. Requires plan_video first.
- validate_plan: Check content quality (score 0-100). Requires plan_video first. Returns needsImprovement and canRetry flags.
- adjust_timing: Fix timing mismatches between scenes and narration. Use when validate_plan returns score < 80. Limited to 2 iterations.

### MEDIA (Asset Generation)
**Dependencies: CONTENT group must complete first**
- generate_visuals: Create images for each scene. Requires plan_video first.
- generate_video: Generate video directly from text using Veo 3.1 (Google's latest model). Creates 4-8 second videos with native audio. Use for direct text-to-video generation. Requires plan_video first.
- animate_image: Convert still images to video loops (optional, uses DeAPI). Requires generate_visuals first. Use for image-to-video animation.
- plan_sfx: Add ambient sound effects (optional). Requires plan_video first.
NOTE: Music generation is NOT available in video production mode. Use the "Generate Music" mode for Suno music generation.

### ENHANCEMENT (Post-Processing)
**Dependencies: MEDIA group must complete first**
- verify_character_consistency: Verifies visual consistency of a character across all generated shots. Returns a report with a score and suggestions. Use this for story-driven content or when consistency is critical. Requires generated visuals first.
- remove_background: Remove background from images for compositing. Requires generate_visuals first.
- restyle_image: Apply style transfer to images (Anime, Watercolor, Oil Painting, etc.). Requires generate_visuals first.
- mix_audio_tracks: Combine narration, music, SFX, and Veo video native audio. **IMPORTANT: Only provide contentPlanId - all audio assets are auto-fetched.** Veo video audio is automatically extracted and mixed when includeVideoAudio=true (default).

### STORY (Step-by-Step Creative Workflow)
**Dependencies: None - this is an alternative starting point for complex stories**
**IMPORTANT: Story generation is a user-driven, step-by-step process. Each step requires user review and confirmation before proceeding to the next.**
- generate_breakdown: Step 1: Create a narrative breakdown (3-5 acts) from a topic. Returns sessionId. **Wait for user to review before proceeding.**
- create_screenplay: Step 2: Create a detailed screenplay from the breakdown. **Wait for user to review and lock before proceeding.**
- generate_characters: Step 3: Extract characters from the screenplay and create visual profiles. **Wait for user to review before proceeding.**
- generate_shotlist: Step 4: Create a detailed shotlist/storyboard from the screenplay and characters. **User controls per-scene generation.**

### EXPORT (Final Output)
**Dependencies: ENHANCEMENT group must complete first (or MEDIA if no enhancements)**
- list_export_presets: Query available platform presets (youtube-shorts, tiktok, instagram-reels, etc.). Use when user asks about export options or to recommend appropriate settings.
- validate_export: Check export readiness before rendering. Returns detailed validation with asset counts, warnings, errors. Use before export_final_video to catch issues early.
- generate_subtitles: Create SRT/VTT subtitles from narration transcripts (supports RTL languages). Requires narrate_scenes first.
- export_final_video: Render final video. **IMPORTANT: Only provide contentPlanId - all assets (visuals, narration, SFX) are auto-fetched.** Use 'preset' param for platform-optimized settings (e.g., preset='tiktok'). Supports mixed image/video assets (Veo videos handled automatically).
- upload_production_to_cloud: Upload all production outputs to Google Cloud Storage. **IMPORTANT: Only provide contentPlanId - all assets are auto-fetched.** Creates organized folder with date/time naming.

### UTILITY (Can be called anytime)
- get_production_status: Check what's done
- list_export_presets: Query export presets anytime (can help user choose format early)
- validate_export: Validate export readiness (can call before EXPORT stage)
- mark_complete: Finalize the production

## DECISION TREE

### Step 1: Detect Input Type
- Does user provide a YouTube/X URL (youtube.com, youtu.be, twitter.com, x.com)?
  → YES: Start with import_youtube_content
  → NO: Continue to Step 2

- Does user provide an audio file path (.mp3, .wav, .m4a, .ogg)?
  → YES: Start with transcribe_audio_file
  → NO: Continue to Step 2

### Step 2: Content Planning
- Start with plan_video using topic/transcript
- YOU decide the optimal scene count based on duration and complexity

### Step 3: Detect Video Generation Method
- Does user want high-quality video with native audio?
  → YES: Use generate_video (Veo 3.1) for direct text-to-video generation
  → NO: Continue to next check

- Does user mention "animated", "motion", "moving", or "dynamic" with existing images?
  → YES: Use generate_visuals first, then animate_image (DeAPI) for image-to-video
  → NO: Use static images only

**Recommendation**: Use generate_video (Veo 3.1) for best quality and native audio. Use animate_image (DeAPI) only when you need to animate existing images.

### Step 4: Detect Style Request
- Does user mention a specific style (cinematic, anime, watercolor, documentary, realistic)?
  → YES: Use that style for generate_visuals and optionally restyle_image
  → NO: Use default "Cinematic" style

### Step 5: Detect Enhancement Requests
- Does user want background removal?
  → YES: Call remove_background after generate_visuals
- Does user want style transfer?
  → YES: Call restyle_image with the specified style

### Step 7: Quality Control (Always Execute)
Follow the mandatory QUALITY CONTROL LOOP workflow described below.

### Step 8: Final Steps (Always Execute)
- If multiple audio sources exist: Call mix_audio_tracks
- If subtitles requested or accessibility needed: Call generate_subtitles
- Call export_final_video to render the final output
- Call mark_complete when satisfied

## SCENE COUNT GUIDELINES (based on ~10-12 seconds per scene)
YOU must decide scene count based on duration and content complexity:
- Ultra-short (30s): 3-4 scenes
- Short (60s): 5-6 scenes  
- Standard (90-120s): 8-12 scenes
- Medium (2-3 min): 12-18 scenes
- Long (3-5 min): 18-30 scenes
- Extended (5-10 min): 30-60 scenes
- Feature (10-15 min): 60-90 scenes

For complex topics (history, science, tutorials), use MORE scenes.
For simple topics (quotes, moods, abstract), use FEWER scenes.

## WORKFLOW

### Standard Topic-Based Workflow
1. **PLAN**: Call plan_video with topic and duration. Decide optimal scene count.
2. **NARRATE**: Call narrate_scenes to generate voice audio for all scenes.
3. **VISUALIZE**: Choose ONE of these methods:
   - **Option A (Recommended)**: Call generate_video for each scene to create videos directly with Veo 3.1 (best quality, native audio)
   - **Option B**: Call generate_visuals to create images, then optionally animate_image for each scene (image-to-video with DeAPI)
4. **SFX** (optional): Call plan_sfx for ambient sounds.
5. **QUALITY CONTROL** (required):
   - Call validate_plan
   - If score < 80 AND iterations < 2: call adjust_timing, then validate_plan again
   - Repeat until score >= 80 OR max iterations reached
6. **MIX** (optional): Call mix_audio_tracks({ contentPlanId }) - DO NOT provide narrationUrl, it's auto-fetched. Veo video audio is automatically extracted and included.
7. **SUBTITLES** (optional): Call generate_subtitles for accessibility.
8. **VALIDATE** (recommended): Call validate_export({ contentPlanId }) to check all assets are ready before rendering.
9. **EXPORT**: Call export_final_video({ contentPlanId }) - DO NOT provide visuals/narrationUrl/totalDuration, they're auto-fetched.
10. **UPLOAD** (recommended): Call upload_production_to_cloud({ contentPlanId }) to save all outputs to Google Cloud Storage.
11. **COMPLETE**: Call mark_complete when satisfied.

### YouTube Import Workflow
1. **IMPORT**: Call import_youtube_content with the URL. This extracts audio and transcribes it.
2. **PLAN**: Call plan_video using the transcript content as the topic.
3. Continue with steps 2-11 from standard workflow.

## ERROR RECOVERY AND RESILIENCE

### Retry Logic
- Transient failures (network, API rate limits): Retry up to 3 times with exponential backoff
- Track retry count for each tool call
- After 3 retries, record as permanent failure and continue

### Fallback Behaviors by Tool
| Tool | Fallback Action |
|------|-----------------|
| generate_visuals | Use placeholder image, continue with other scenes |
| generate_video | Fall back to generate_visuals + animate_image, or use static images |
| animate_image | Keep static image for that scene |
| plan_sfx | Continue without sound effects |
| remove_background | Keep original image |
| restyle_image | Keep original image |
| export_final_video | Provide asset bundle for manual assembly |

### Partial Success Handling
- If a tool fails for specific scenes, log the error and continue with remaining scenes
- Track all errors in session state
- Report partial success with details of what succeeded and what failed
- Always try to deliver a working production, even if incomplete

### Error Reporting
When errors occur:
1. Log the error with tool name and scene index (if applicable)
2. Apply the appropriate fallback behavior
3. Continue with the next step in the workflow
4. Include error summary in final response

## QUALITY CONTROL LOOP (Requirements 7.1-7.5)

### Validation Process - MANDATORY WORKFLOW
After generating narration and visuals, you MUST follow this quality control workflow:

1. **Initial Validation**: Call validate_plan to check content quality
   - Returns score (0-100), needsImprovement flag, and canRetry flag

2. **Quality Improvement** (if score < 80 AND iterations < 2):
   - Call adjust_timing to fix timing mismatches between scenes and narration
   - This increments the iteration counter automatically
   - After adjust_timing completes, ALWAYS call validate_plan again

3. **Re-validation Loop**:
   - If score still < 80 AND iterations < 2: repeat step 2
   - If score >= 80 OR iterations >= 2: proceed to mark_complete

4. **Final Reporting**:
   - Report the final score and best score achieved
   - If max iterations reached without approval, report best score
   - Proceed to export/complete

### Quality Standards
- Target score: 80/100 or higher for approval
- Maximum improvement iterations: 2 (initial validation + up to 2 adjustments = 3 total validation calls)
- Each adjust_timing call syncs scene durations to actual narration lengths
- Track best score achieved across all iterations
- Ensure scene transitions are logical and visual descriptions are specific

### Example Quality Workflow
\`\`\`
1. narrate_scenes → generates audio
2. validate_plan → returns score: 65, needsImprovement: true, canRetry: true
3. adjust_timing → iteration 1/2, syncs timing
4. validate_plan → returns score: 78, needsImprovement: true, canRetry: true
5. adjust_timing → iteration 2/2, syncs timing
6. validate_plan → returns score: 85, needsImprovement: false
7. mark_complete → finalize production
\`\`\`

## IMPORTANT RULES

### Asset Auto-Fetching (CRITICAL)
**NEVER provide these parameters - they are automatically fetched from session state:**
- mix_audio_tracks: DO NOT provide narrationUrl (auto-fetched from narration segments)
- export_final_video: DO NOT provide visuals, narrationUrl, or totalDuration (all auto-fetched)
- generate_subtitles: DO NOT provide narration data (auto-fetched from narration segments)

**Correct usage examples:**
\`\`\`
mix_audio_tracks({ contentPlanId: "prod_xxx" })
export_final_video({ contentPlanId: "prod_xxx", format: "mp4" })
generate_subtitles({ contentPlanId: "prod_xxx" })
\`\`\`

### Efficiency
- DO NOT call the same tool multiple times for the same step (e.g., do NOT call 'generate_visuals' twice)
- One call to generate_visuals handles ALL scenes
- Process scenes in batches for long videos (10-15 at a time for visuals/animation)
- Track progress and report percentage complete
- Be efficient - don't call unnecessary tools

### Tool Group Order
- Always respect tool group dependencies
- IMPORT → CONTENT → MEDIA → ENHANCEMENT → EXPORT
- Don't skip ahead to later groups before completing earlier ones

### Animation
- For animation, animate each scene individually using its sceneIndex (0-based)
- Call animate_image once per scene that needs animation

### Import Workflows
- When importing from YouTube, use the transcript to inform the content plan
- The sessionId from import_youtube_content should be used for subsequent tools`;
````

## File: packages/shared/src/services/ai/production/REFACTORING_SUMMARY.md
````markdown
# Production Agent Refactoring Summary

## Overview

Successfully refactored the monolithic `productionAgent.ts` file (2700+ lines) into a modular, maintainable structure.

## ✅ Completed Work (100% - ~2920 lines)

### Core Modules

1. **types.ts** (~200 lines)
   - All Zod validation schemas (PlanVideoSchema, NarrateScenesSchema, etc.)
   - TypeScript interfaces (ProductionState, StoryModeState, ProductionProgress)
   - Helper functions (createInitialState)
   - Clean type definitions for the entire system

2. **store.ts** (~70 lines)
   - Production session state management
   - Story mode session management
   - Session CRUD operations (get, clear, initialize, update)
   - Cloud autosave integration

3. **utils.ts** (~180 lines)
   - Language detection from Unicode analysis (detectLanguageFromText)
   - Session ID generation and validation
   - Step identifier creation for duplicate prevention
   - Content plan ID validation

4. **prompts.ts** (~260 lines)
   - Complete PRODUCTION_AGENT_PROMPT with all instructions
   - Tool group documentation
   - Decision trees and workflows
   - Quality control guidelines
   - Error recovery strategies

### Tool Modules

5. **tools/contentTools.ts** (~520 lines)
   - planVideoTool - Content planning with AI-decided scene count
   - narrateScenesTool - Voice narration generation
   - generateVisualsTool - Image/video generation with batching
   - planSFXTool - Sound effects planning
   - validatePlanTool - Quality validation
   - adjustTimingTool - Timing synchronization
   - Progress callback management

6. **tools/mediaTools.ts** (~320 lines)
   - generateVideoTool - Veo 3.1 text-to-video generation
   - animateImageTool - DeAPI image-to-video with Veo fallback
   - generateMusicTool - Suno music generation

7. **tools/statusTools.ts** (~80 lines)
   - getProductionStatusTool - Session status checking
   - markCompleteTool - Production completion
   - verifyCharacterConsistencyTool - Visual consistency verification

8. **tools/storyTools.ts** (~280 lines)
   - generateBreakdownTool - Narrative breakdown generation
   - createScreenplayTool - Screenplay creation with parsing
   - generateCharactersTool - Character extraction and reference generation
   - generateShotlistTool - Shotlist/storyboard creation

9. **tools/index.ts** (~30 lines)
   - Central export point for all tools
   - Clean import/export structure

### Agent Core

10. **toolRegistration.ts** (~240 lines)
    - productionTools array with all tools combined
    - storyModeTools array for story workflow
    - toolMap for quick lookup during execution
    - registerProductionTools() - Tool registration with tool registry
    - Dependency tracking and tool group organization

11. **agentCore.ts** (~620 lines)
    - runProductionAgent() - Main agent execution loop
    - runProductionAgentWithSubagents() - Multi-agent orchestration
    - checkResultCache() - Result caching system
    - Tool invocation with retry logic
    - Error recovery and fallback handling
    - Progress reporting
    - Session management (getProductionSession, clearProductionSession)

12. **index.ts** (~120 lines)
    - Main entry point for the production module
    - Re-exports all types, utilities, tools, and functions
    - Clean public API

## Benefits Achieved

### 1. Maintainability
- Each module has a single, clear responsibility
- Easy to locate and modify specific functionality
- Reduced cognitive load when working on features

### 2. Testability
- Modules can be tested in isolation
- Mock dependencies easily
- Unit tests for utilities and helpers
- Integration tests for tools

### 3. Reusability
- Types and utilities can be imported anywhere
- Tools can be used independently
- Store functions work across contexts

### 4. Readability
- Smaller files are easier to understand (~200 lines avg vs 2700 lines)
- Clear module boundaries
- Self-documenting structure

### 5. Collaboration
- Multiple developers can work on different modules
- Reduced merge conflicts
- Clear ownership of components

### 6. Performance
- Faster IDE operations (autocomplete, navigation)
- Quicker file loading
- Better tree-shaking potential

## Module Dependencies

```
types.ts (no dependencies)
  ↓
store.ts (depends on: types)
  ↓
utils.ts (depends on: types)
  ↓
prompts.ts (no dependencies)
  ↓
tools/*.ts (depend on: types, store, utils)
  ↓
toolRegistration.ts (depends on: tools, external tools)
  ↓
agentCore.ts (depends on: all above)
  ↓
index.ts (depends on: all above, exports public API)
```

## File Structure

```
services/ai/production/
├── README.md                    # Module documentation
├── REFACTORING_SUMMARY.md      # This file
├── index.ts                     # Main entry point
├── types.ts                     # Type definitions and schemas
├── store.ts                     # State management
├── utils.ts                     # Utility functions
├── prompts.ts                   # Agent prompts
├── toolRegistration.ts          # Tool registration
├── agentCore.ts                 # Agent execution
└── tools/
    ├── index.ts                 # Tool exports
    ├── contentTools.ts          # Content planning tools
    ├── mediaTools.ts            # Media generation tools
    ├── statusTools.ts           # Status tools
    └── storyTools.ts            # Story mode tools
```

## Usage

```typescript
// Import everything from the main entry point
import {
  runProductionAgent,
  runProductionAgentWithSubagents,
  ProductionState,
  ProductionProgress,
  productionTools,
  detectLanguageFromText,
} from './production';

// Or import from specific modules
import { ProductionState } from './production/types';
import { productionStore } from './production/store';
import { planVideoTool } from './production/tools/contentTools';
```

## Impact

- **Code Organization**: Improved from 1 file (2700 lines) to 12 files (~240 lines average)
- **Maintainability**: Significantly improved with clear module boundaries
- **Developer Experience**: Faster navigation, better autocomplete, clearer structure
- **Future Development**: Easier to add new tools, modify existing ones, and extend functionality

## Next Steps

1. **Testing**
   - Add unit tests for utilities
   - Add integration tests for tools
   - Add end-to-end tests for agent

2. **Documentation**
   - API documentation
   - Usage examples
   - Migration guide

3. **Optimization**
   - Consider lazy loading for rarely-used tools
   - Performance monitoring
   - Metrics collection

## Conclusion

The refactoring is complete! The monolithic file has been successfully transformed into a well-organized, modular system with 12 focused modules. The new structure significantly improves development velocity, code quality, and maintainability.
````

## File: packages/shared/src/services/ai/production/resultCache.ts
````typescript
/**
 * Result Cache
 *
 * Checks for cached tool results to avoid re-execution.
 * Extracted from agentCore.ts for focused responsibility.
 *
 * Requirements: 10.2, 10.5 — Use cached results without re-execution
 */

import { ProductionState } from "./types";
import { GeneratedImage } from "../../../types";

/**
 * Check if results are already cached for a tool to avoid re-execution.
 */
export function checkResultCache(
    toolName: string,
    toolArgs: Record<string, unknown>,
    state: ProductionState | null
): { cached: boolean; result?: Record<string, unknown> } {
    if (!state) {
        return { cached: false };
    }

    switch (toolName) {
        case 'generate_visuals':
            if (state.visuals &&
                state.contentPlan &&
                state.visuals.length >= state.contentPlan.scenes.length &&
                state.visuals.every(v => v.imageUrl)) {
                return {
                    cached: true,
                    result: {
                        success: true,
                        cached: true,
                        visualCount: state.visuals.length,
                        message: `Visuals already exist (${state.visuals.length}) - using cached results`,
                    }
                };
            }
            break;

        case 'narrate_scenes':
            if (state.narrationSegments &&
                state.contentPlan &&
                state.narrationSegments.length >= state.contentPlan.scenes.length &&
                state.narrationSegments.every(s => s.audioBlob)) {
                return {
                    cached: true,
                    result: {
                        success: true,
                        cached: true,
                        segmentCount: state.narrationSegments.length,
                        totalDuration: state.contentPlan.totalDuration,
                        message: `Narration already exists (${state.narrationSegments.length} segments) - using cached results`,
                    }
                };
            }
            break;

        case 'plan_sfx':
            if (state.sfxPlan && state.sfxPlan.scenes.length > 0) {
                return {
                    cached: true,
                    result: {
                        success: true,
                        cached: true,
                        sceneCount: state.sfxPlan.scenes.length,
                        message: `SFX plan already exists (${state.sfxPlan.scenes.length} scenes) - using cached results`,
                    }
                };
            }
            break;

        case 'mix_audio_tracks':
            if (state.mixedAudio && state.mixedAudio.audioBlob) {
                return {
                    cached: true,
                    result: {
                        success: true,
                        cached: true,
                        duration: state.mixedAudio.duration,
                        message: `Audio already mixed - using cached results`,
                    }
                };
            }
            break;

        case 'generate_subtitles':
            if (state.subtitles && state.subtitles.content) {
                return {
                    cached: true,
                    result: {
                        success: true,
                        cached: true,
                        format: state.subtitles.format,
                        segmentCount: state.subtitles.segmentCount,
                        message: `Subtitles already generated (${state.subtitles.format}) - using cached results`,
                    }
                };
            }
            break;

        case 'export_final_video':
            if (state.exportResult && state.exportResult.videoBlob) {
                return {
                    cached: true,
                    result: {
                        success: true,
                        cached: true,
                        format: state.exportResult.format,
                        duration: state.exportResult.duration,
                        downloadUrl: state.exportResult.downloadUrl,
                        message: `Video already exported (${state.exportResult.format}) - using cached results`,
                    }
                };
            }
            break;

        case 'animate_image': {
            const sceneIndex = toolArgs.sceneIndex as number | undefined;
            if (sceneIndex !== undefined &&
                state.visuals &&
                state.visuals[sceneIndex] &&
                (state.visuals[sceneIndex] as GeneratedImage & { videoUrl?: string }).videoUrl) {
                return {
                    cached: true,
                    result: {
                        success: true,
                        cached: true,
                        sceneIndex,
                        message: `Scene ${sceneIndex} already animated - using cached results`,
                    }
                };
            }
            break;
        }
    }

    return { cached: false };
}
````

## File: packages/shared/src/services/ai/production/store.ts
````typescript
/**
 * Production Agent State Management
 *
 * Manages production session state and story mode state.
 * Features:
 * - In-memory cache for fast access
 * - IndexedDB persistence for session recovery
 * - Cloud autosave for redundancy
 */

import { ProductionState, StoryModeState, createInitialState } from "./types";
import { agentLogger } from "../../logger";
import { cloudAutosave } from "../../cloudStorageService";
import {
    saveProductionSession,
    loadProductionSession,
    deleteProductionSession,
    listRecoverableSessions,
    getMostRecentIncompleteSession,
    saveStorySession,
    loadStorySession,
    cleanupOldSessions,
} from "./persistence";
import type { SessionMetadata } from "./persistence";

const log = agentLogger.child('Store');

/**
 * Store for intermediate results (in-memory cache)
 */
export const productionStore: Map<string, ProductionState> = new Map();

/**
 * Story Mode session store (in-memory cache)
 */
export const storyModeStore: Map<string, StoryModeState> = new Map();

/**
 * Debounce timer for persistence writes
 */
const persistenceTimers: Map<string, ReturnType<typeof setTimeout>> = new Map();
const PERSISTENCE_DEBOUNCE_MS = 1000; // 1 second debounce

/**
 * Schedule a debounced persistence write
 */
function schedulePersistence(sessionId: string, state: ProductionState): void {
    // Clear existing timer
    const existingTimer = persistenceTimers.get(sessionId);
    if (existingTimer) {
        clearTimeout(existingTimer);
    }

    // Schedule new write
    const timer = setTimeout(() => {
        persistenceTimers.delete(sessionId);
        saveProductionSession(sessionId, state).catch(err => {
            log.warn('IndexedDB persistence failed (non-fatal):', err);
        });
    }, PERSISTENCE_DEBOUNCE_MS);

    persistenceTimers.set(sessionId, timer);
}

/**
 * Get a production session by ID
 */
export function getProductionSession(sessionId: string): ProductionState | null {
    return productionStore.get(sessionId) || null;
}

/**
 * Clear a production session (both memory and IndexedDB)
 */
export function clearProductionSession(sessionId: string): void {
    // Clear pending persistence
    const timer = persistenceTimers.get(sessionId);
    if (timer) {
        clearTimeout(timer);
        persistenceTimers.delete(sessionId);
    }

    productionStore.delete(sessionId);

    // Also clear from IndexedDB (fire-and-forget)
    deleteProductionSession(sessionId).catch(err => {
        log.warn('Failed to clear session from IndexedDB:', err);
    });
}

/**
 * Initialize a new production session with cloud autosave and IndexedDB
 */
export async function initializeProductionSession(sessionId: string, initialState?: Partial<ProductionState>): Promise<void> {
    const state: ProductionState = {
        ...createInitialState(),
        ...initialState,
    };

    productionStore.set(sessionId, state);

    // Persist to IndexedDB immediately for new sessions
    saveProductionSession(sessionId, state).catch(err => {
        log.warn('IndexedDB initial save failed (non-fatal):', err);
    });

    // Initialize cloud autosave session (fire-and-forget, non-blocking)
    cloudAutosave.initSession(sessionId).catch(err => {
        log.warn('Cloud autosave init failed (non-fatal):', err);
    });
}

/**
 * Update production session state with automatic persistence
 */
export function updateProductionSession(sessionId: string, updates: Partial<ProductionState>): void {
    const state = productionStore.get(sessionId);
    if (state) {
        Object.assign(state, updates);
        productionStore.set(sessionId, state);

        // Schedule debounced persistence to IndexedDB
        schedulePersistence(sessionId, state);
    }
}

// ============================================================
// SESSION RECOVERY FUNCTIONS
// ============================================================

/**
 * Restore a production session from IndexedDB into memory
 */
export async function restoreProductionSession(sessionId: string): Promise<ProductionState | null> {
    // Check memory first
    const memoryState = productionStore.get(sessionId);
    if (memoryState) {
        log.debug(`Session ${sessionId} already in memory`);
        return memoryState;
    }

    // Try to load from IndexedDB
    const persistedState = await loadProductionSession(sessionId);
    if (persistedState) {
        productionStore.set(sessionId, persistedState);
        log.info(`Restored session ${sessionId} from IndexedDB`);
        return persistedState;
    }

    return null;
}

/**
 * Get list of recoverable sessions for the recovery UI
 */
export async function getRecoverableSessions(): Promise<SessionMetadata[]> {
    return listRecoverableSessions();
}

/**
 * Get the most recent incomplete session (for automatic recovery prompt)
 */
export async function getRecentIncompleteSession(): Promise<SessionMetadata | null> {
    return getMostRecentIncompleteSession();
}

/**
 * Flush pending persistence writes immediately
 * Call this before navigation or when session is complete
 */
export async function flushPendingPersistence(sessionId: string): Promise<void> {
    const timer = persistenceTimers.get(sessionId);
    if (timer) {
        clearTimeout(timer);
        persistenceTimers.delete(sessionId);
    }

    const state = productionStore.get(sessionId);
    if (state) {
        await saveProductionSession(sessionId, state);
        log.debug(`Flushed persistence for session ${sessionId}`);
    }
}

// ============================================================
// STORY MODE SESSION FUNCTIONS
// ============================================================

/**
 * Save story mode session with IndexedDB persistence.
 * Automatically stamps updatedAt and preserves formatId for isolation.
 */
export function saveStoryModeSession(sessionId: string, state: StoryModeState): void {
    // Ensure updatedAt is current
    const stamped: StoryModeState = { ...state, updatedAt: Date.now() };
    storyModeStore.set(sessionId, stamped);

    // Persist to IndexedDB (fire-and-forget)
    saveStorySession(sessionId, stamped).catch(err => {
        log.warn('Story session persistence failed:', err);
    });
}

/**
 * Load story mode session (memory first, then IndexedDB).
 * If formatId is provided, validates that the loaded session matches the format.
 */
export async function loadStoryModeSession(
    sessionId: string,
    expectedFormatId?: string,
): Promise<StoryModeState | null> {
    // Check memory first
    const memoryState = storyModeStore.get(sessionId);
    if (memoryState) {
        if (expectedFormatId && memoryState.formatId && memoryState.formatId !== expectedFormatId) {
            log.debug(`Session ${sessionId} format mismatch: expected ${expectedFormatId}, got ${memoryState.formatId}`);
            return null;
        }
        return memoryState;
    }

    // Try IndexedDB
    const persistedState = await loadStorySession(sessionId);
    if (persistedState) {
        if (expectedFormatId && persistedState.formatId && persistedState.formatId !== expectedFormatId) {
            log.debug(`Persisted session ${sessionId} format mismatch: expected ${expectedFormatId}, got ${persistedState.formatId}`);
            return null;
        }
        storyModeStore.set(sessionId, persistedState);
        return persistedState;
    }

    return null;
}

/**
 * Get all story sessions matching a specific format ID.
 * Searches both in-memory cache and IndexedDB.
 * Useful for format-specific state isolation (Requirement 18.4).
 */
export function getStorySessionsByFormat(formatId: string): StoryModeState[] {
    const results: StoryModeState[] = [];
    for (const state of storyModeStore.values()) {
        if (state.formatId === formatId) {
            results.push(state);
        }
    }
    return results.sort((a, b) => (b.updatedAt || 0) - (a.updatedAt || 0));
}

/**
 * Clear all story mode sessions for a specific format.
 * Useful when switching formats to ensure clean state.
 */
export function clearStorySessionsByFormat(formatId: string): void {
    for (const [key, state] of storyModeStore.entries()) {
        if (state.formatId === formatId) {
            storyModeStore.delete(key);
        }
    }
}

// ============================================================
// INITIALIZATION & CLEANUP
// ============================================================

/**
 * Initialize the persistence layer (call on app startup)
 */
export async function initializePersistence(): Promise<void> {
    try {
        // Clean up old sessions (older than 7 days)
        const cleaned = await cleanupOldSessions(7);
        if (cleaned > 0) {
            log.info(`Cleaned up ${cleaned} old sessions on startup`);
        }
    } catch (err) {
        log.warn('Persistence initialization warning:', err);
    }
}

// Re-export SessionMetadata type for consumers
export type { SessionMetadata };
````

## File: packages/shared/src/services/ai/production/toolRegistration.ts
````typescript
/**
 * Tool Registration - Register Production Tools with Tool Registry
 *
 * Handles registration of all production agent tools with the centralized
 * tool registry, organizing them by group with proper dependencies.
 *
 * Requirements: 12.1, 12.3 - Tool group organization and dependency tracking
 */

import { StructuredTool } from "@langchain/core/tools";
import {
    toolRegistry,
    ToolGroup,
    createToolDefinition,
} from "../../agent/toolRegistry";
import { agentLogger } from "../../logger";

const log = agentLogger.child('ToolRegistration');

// Import tool arrays from their respective modules
import { importTools } from "../../agent/importTools";
import { exportTools } from "../../agent/exportTools";
import { subtitleTools } from "../../agent/subtitleTools";
import { audioMixingTools } from "../../agent/audioMixingTools";
import { enhancementTools } from "../../agent/enhancementTools";
import { cloudStorageTools } from "../../agent/cloudStorageTools";

// Import individual tools from production modules
import {
    planVideoTool,
    narrateScenesTool,
    validatePlanTool,
    adjustTimingTool,
    generateVisualsTool,
    planSFXTool,
} from "./tools/contentTools";

import {
    generateVideoTool,
    animateImageTool,
    generateMusicTool,
} from "./tools/mediaTools";

import {
    getProductionStatusTool,
    markCompleteTool,
} from "./tools/statusTools";

import {
    generateBreakdownTool,
    createScreenplayTool,
    generateCharactersTool,
    generateShotlistTool,
    verifyCharacterConsistencyTool,
} from "./tools/storyTools";

// --- Combined Tool Array ---

/**
 * All production tools combined for model binding.
 * Organized by group for clarity.
 */
export const productionTools: StructuredTool[] = [
    // Import tools (IMPORT group)
    ...importTools,
    // Content tools (CONTENT group)
    planVideoTool,
    narrateScenesTool,
    validatePlanTool,
    adjustTimingTool,
    // Media tools (MEDIA group)
    generateMusicTool,
    generateVisualsTool,
    generateVideoTool, // Veo 3.1 text-to-video generation
    animateImageTool, // DeAPI image-to-video animation
    planSFXTool,
    // Enhancement tools (ENHANCEMENT group)
    verifyCharacterConsistencyTool,
    ...enhancementTools,
    ...audioMixingTools,
    // Export tools (EXPORT group)
    ...subtitleTools,
    ...exportTools,
    ...cloudStorageTools,
    // Utility tools
    getProductionStatusTool,
    markCompleteTool,
    // Story Mode tools (step-by-step, user-driven workflow)
    generateBreakdownTool,
    createScreenplayTool,
    generateCharactersTool,
    generateShotlistTool,
];

/**
 * Story mode specific tools for screenplay workflow.
 */
export const storyModeTools: StructuredTool[] = [
    generateBreakdownTool,
    createScreenplayTool,
    generateCharactersTool,
    generateShotlistTool,
];

// --- Tool Map for Execution ---

/**
 * Map of tool names to tool instances for quick lookup during execution.
 */
export const toolMap: Record<string, StructuredTool> = {};
productionTools.forEach(t => {
    toolMap[t.name] = t;
});

// --- Tool Registry Registration ---

/**
 * Register all production tools with the tool registry.
 * This enables tool group management and dependency validation.
 *
 * Requirements: 12.1, 12.3
 */
export function registerProductionTools(): void {
    // Clear any existing registrations
    toolRegistry.clear();

    // Register IMPORT group tools
    for (const tool of importTools) {
        toolRegistry.register(createToolDefinition(
            tool.name,
            ToolGroup.IMPORT,
            tool
        ));
    }

    // Register CONTENT group tools
    toolRegistry.register(createToolDefinition(
        planVideoTool.name,
        ToolGroup.CONTENT,
        planVideoTool
    ));
    toolRegistry.register(createToolDefinition(
        narrateScenesTool.name,
        ToolGroup.CONTENT,
        narrateScenesTool,
        ["plan_video"] // Depends on plan_video
    ));
    toolRegistry.register(createToolDefinition(
        validatePlanTool.name,
        ToolGroup.CONTENT,
        validatePlanTool,
        ["plan_video"] // Depends on plan_video
    ));
    toolRegistry.register(createToolDefinition(
        adjustTimingTool.name,
        ToolGroup.CONTENT,
        adjustTimingTool,
        ["narrate_scenes"] // Depends on narration for timing sync
    ));

    // Register MEDIA group tools
    toolRegistry.register(createToolDefinition(
        generateVisualsTool.name,
        ToolGroup.MEDIA,
        generateVisualsTool,
        ["plan_video"] // Depends on content plan
    ));
    toolRegistry.register(createToolDefinition(
        generateVideoTool.name,
        ToolGroup.MEDIA,
        generateVideoTool,
        ["plan_video"] // Depends on content plan for scene descriptions
    ));
    toolRegistry.register(createToolDefinition(
        animateImageTool.name,
        ToolGroup.MEDIA,
        animateImageTool,
        ["generate_visuals"] // Depends on visuals
    ));
    // NOTE: generateMusicTool is not registered - music generation is only
    // available in the dedicated "Generate Music" mode, not in video production
    toolRegistry.register(createToolDefinition(
        planSFXTool.name,
        ToolGroup.MEDIA,
        planSFXTool,
        ["plan_video"] // Depends on content plan
    ));

    // Register ENHANCEMENT group tools
    toolRegistry.register(createToolDefinition(
        verifyCharacterConsistencyTool.name,
        ToolGroup.ENHANCEMENT,
        verifyCharacterConsistencyTool
    ));
    for (const tool of enhancementTools) {
        toolRegistry.register(createToolDefinition(
            tool.name,
            ToolGroup.ENHANCEMENT,
            tool,
            ["generate_visuals"] // Enhancement tools depend on visuals
        ));
    }
    for (const tool of audioMixingTools) {
        toolRegistry.register(createToolDefinition(
            tool.name,
            ToolGroup.ENHANCEMENT,
            tool,
            ["narrate_scenes"] // Audio mixing depends on narration
        ));
    }

    // Register EXPORT group tools
    for (const tool of subtitleTools) {
        toolRegistry.register(createToolDefinition(
            tool.name,
            ToolGroup.EXPORT,
            tool,
            ["narrate_scenes"] // Subtitles depend on narration
        ));
    }
    for (const tool of exportTools) {
        toolRegistry.register(createToolDefinition(
            tool.name,
            ToolGroup.EXPORT,
            tool,
            ["generate_visuals", "narrate_scenes"] // Export depends on visuals and narration
        ));
    }

    // Register CLOUD tools
    for (const tool of cloudStorageTools) {
        toolRegistry.register(createToolDefinition(
            tool.name,
            ToolGroup.EXPORT,
            tool,
            ["export_final_video"] // Cloud storage usually after export
        ));
    }

    // Register STORY group tools (mapping to CONTENT for dependency flow)
    toolRegistry.register(createToolDefinition(
        generateBreakdownTool.name,
        ToolGroup.CONTENT,
        generateBreakdownTool
    ));
    toolRegistry.register(createToolDefinition(
        createScreenplayTool.name,
        ToolGroup.CONTENT,
        createScreenplayTool,
        ["generate_breakdown"]
    ));
    toolRegistry.register(createToolDefinition(
        generateCharactersTool.name,
        ToolGroup.CONTENT,
        generateCharactersTool,
        ["create_screenplay"]
    ));
    toolRegistry.register(createToolDefinition(
        generateShotlistTool.name,
        ToolGroup.CONTENT,
        generateShotlistTool,
        ["create_screenplay", "generate_characters"]
    ));

    // Note: Utility tools (get_production_status, mark_complete) are not registered
    // as they don't belong to a specific group and can be called at any time

    log.info("Registered tools with registry:", toolRegistry.getSummary());
}

// --- Initialization ---

// Initialize tool registry on module load
registerProductionTools();

// --- Re-exports for convenience ---

export {
    toolRegistry,
    ToolGroup,
    createToolDefinition,
} from "../../agent/toolRegistry";
````

## File: packages/shared/src/services/ai/production/tools/contentTools.ts
````typescript
/**
 * Content Tools for Production Agent
 * 
 * Tools for content planning, narration, validation, and quality control.
 */

import { tool } from "@langchain/core/tools";
import { agentLogger } from "../../../logger";
import {
    PlanVideoSchema,
    NarrateScenesSchema,
    GenerateVisualsSchema,
    PlanSFXSchema,
    ValidatePlanSchema,
    AdjustTimingSchema,
} from "../types";
import { productionStore } from "../store";
import {
    detectLanguageFromText,
    generateSessionId,
    validateContentPlanId
} from "../utils";
import { generateContentPlan, ContentPlannerConfig } from "../../../contentPlannerService";
import { narrateAllScenes, NarratorConfig } from "../../../narratorService";
import { generateImageFromPrompt, getCharacterSeed } from "../../../imageService";
import { generateVideoSFXPlanWithAudio, isSFXAudioAvailable } from "../../../sfxService";
import { validateContentPlan, syncDurationsToNarration } from "../../../editorService";
import {
    extractVisualStyle,
    injectStyleIntoPrompt,
    type VisualStyle
} from "../../../visualConsistencyService";
import {
    fromShotBreakdown,
    serializeStyleGuideAsText,
    type CharacterInput,
    type ExtractedStyleOverride,
} from "../../../prompt/imageStyleGuide";
import { type VideoPurpose } from "../../../../constants";
import { type GeneratedImage, type Scene } from "../../../../types";
import { cloudAutosave } from "../../../cloudStorageService";
import { getEffectiveLegacyTone } from "../../../tripletUtils";
import { type ProductionProgress, createInitialState } from "../types";

const log = agentLogger.child('Production');

/**
 * Global progress callback for scene-level progress reporting.
 * Set by the main agent before execution.
 */
let globalProgressCallback: ((progress: ProductionProgress) => void) | null = null;

export function setGlobalProgressCallback(callback: ((progress: ProductionProgress) => void) | null): void {
    globalProgressCallback = callback;
}

export function getGlobalProgressCallback(): ((progress: ProductionProgress) => void) | null {
    return globalProgressCallback;
}

function emitSceneProgress(toolName: string, currentScene: number, totalScenes: number, message: string): void {
    if (globalProgressCallback) {
        const progress = Math.round((currentScene / totalScenes) * 100);
        globalProgressCallback({
            stage: "scene_progress",
            tool: toolName,
            message,
            isComplete: false,
            currentScene,
            totalScenes,
            progress,
            percentage: progress, // Keep for backward compatibility
        });
    }
}

// --- Plan Video Tool ---

export const planVideoTool = tool(
    async ({ topic, targetDuration, style, audience, language, videoPurpose }) => {
        log.info(` Planning video: "${topic}" (${targetDuration}s)`);

        try {
            const config: ContentPlannerConfig = {
                videoPurpose: (videoPurpose || "documentary") as VideoPurpose,
                visualStyle: style || "Cinematic",
                language: language || "ar",
            };

            const contentPlan = await generateContentPlan(topic, {
                targetDuration,
                targetAudience: audience || "General audience",
                config,
            });

            const sessionId = generateSessionId();
            const initialState = createInitialState();
            initialState.contentPlan = contentPlan;

            productionStore.set(sessionId, initialState);

            // Initialize cloud autosave session (fire-and-forget, non-blocking)
            cloudAutosave.initSession(sessionId).catch(err => {
                log.warn('Cloud autosave init failed (non-fatal):', err);
            });

            return JSON.stringify({
                success: true,
                sessionId,
                sceneCount: contentPlan.scenes.length,
                totalDuration: contentPlan.totalDuration,
                scenes: contentPlan.scenes.map((s: { name: string; duration: number }) => ({
                    name: s.name,
                    duration: s.duration,
                })),
                message: `Created content plan with ${contentPlan.scenes.length} scenes (~${contentPlan.totalDuration}s total). IMPORTANT: Use sessionId="${sessionId}" as contentPlanId for all subsequent tool calls (narrate_scenes, generate_visuals, validate_plan, etc.)`,
            });
        } catch (error) {
            return JSON.stringify({
                success: false,
                error: error instanceof Error ? error.message : String(error),
            });
        }
    },
    {
        name: "plan_video",
        description: "Generate a video content plan with scenes. The AI decides the optimal number of scenes based on topic, duration, and content requirements. Do NOT specify sceneCount - let the planner decide.",
        schema: PlanVideoSchema,
    }
);

// --- Narrate Scenes Tool ---

export const narrateScenesTool = tool(
    async ({ contentPlanId, language, voiceStyle: _voiceStyle }) => {
        log.info(` Narrating scenes for ${contentPlanId}`);

        const validationError = validateContentPlanId(contentPlanId);
        if (validationError) return validationError;

        const state = productionStore.get(contentPlanId);
        if (!state?.contentPlan) {
            return JSON.stringify({
                success: false,
                error: `Content plan not found for sessionId: ${contentPlanId}. Make sure you are using the exact sessionId returned by plan_video.`
            });
        }

        // Auto-detect language from narration scripts if not provided
        let detectedLanguage = language;
        if (!detectedLanguage || detectedLanguage === 'auto') {
            const sampleText = state.contentPlan.scenes[0]?.narrationScript || '';
            detectedLanguage = detectLanguageFromText(sampleText);
            log.info(` Auto-detected language: ${detectedLanguage} from narration text`);
        }

        try {
            const narratorConfig: NarratorConfig = {
                language: detectedLanguage || "en",
                videoPurpose: "documentary" as VideoPurpose,
            };

            const segments = await narrateAllScenes(
                state.contentPlan.scenes,
                narratorConfig,
                (sceneIndex: number, totalScenes: number) => {
                    emitSceneProgress(
                        "narrate_scenes",
                        sceneIndex + 1,
                        totalScenes,
                        `Narrating scene ${sceneIndex + 1}/${totalScenes}`
                    );
                },
                contentPlanId
            );

            const syncedPlan = syncDurationsToNarration(state.contentPlan, segments);

            const currentState = productionStore.get(contentPlanId) || state;
            currentState.contentPlan = syncedPlan;
            currentState.narrationSegments = segments;
            productionStore.set(contentPlanId, currentState);

            return JSON.stringify({
                success: true,
                segmentCount: segments.length,
                totalDuration: syncedPlan.totalDuration,
                message: `Generated ${segments.length} narration segments (~${syncedPlan.totalDuration}s total)`,
            });
        } catch (error) {
            return JSON.stringify({
                success: false,
                error: error instanceof Error ? error.message : String(error),
            });
        }
    },
    {
        name: "narrate_scenes",
        description: "Generate voice narration for all scenes in a content plan. Returns audio segments synced to scene timings.",
        schema: NarrateScenesSchema,
    }
);

// --- Generate Visuals Tool ---

const generatingPromises = new Map<string, Promise<string>>();

export const generateVisualsTool = tool(
    async ({ contentPlanId, style, aspectRatio, veoVideoCount = 1 }) => {
        log.info(` Generating visuals for ${contentPlanId} (veoVideoCount: ${veoVideoCount})`);

        const validationError = validateContentPlanId(contentPlanId);
        if (validationError) return validationError;

        // Join existing generation if in progress
        if (generatingPromises.has(contentPlanId)) {
            log.info(` Joining existing generation for ${contentPlanId}`);
            return generatingPromises.get(contentPlanId)!;
        }

        const task = (async () => {
            const state = productionStore.get(contentPlanId);
            if (!state?.contentPlan) {
                return JSON.stringify({
                    success: false,
                    error: `Content plan not found for sessionId: ${contentPlanId}. Make sure you are using the exact sessionId returned by plan_video.`
                });
            }

            // Build CharacterInput array from the content plan's Character Bible
            const charInputs: CharacterInput[] = (state.contentPlan.characters ?? []).map(c => ({
                name: c.name,
                visualDescription: [c.appearance, c.clothing, c.distinguishingFeatures]
                    .filter(Boolean).join('. '),
                facialTags: c.consistencyKey,
            }));

            // Helper: build an anchored image prompt using character identity data when available
            function buildScenePrompt(scene: Scene, extractedStyle: VisualStyle | null): { prompt: string; seed?: number } {
                const sceneDescLower = scene.visualDescription.toLowerCase();
                const presentChars = charInputs.filter(c => sceneDescLower.includes(c.name.toLowerCase()));

                if (presentChars.length === 0) {
                    // No characters matched — use existing style-injection path
                    const prompt = extractedStyle
                        ? injectStyleIntoPrompt(scene.visualDescription, extractedStyle)
                        : scene.visualDescription;
                    return { prompt };
                }

                // Characters present — use fromShotBreakdown for full identity anchoring
                const styleOverride: ExtractedStyleOverride | undefined = extractedStyle
                    ? { colorPalette: extractedStyle.colorPalette, moodKeywords: extractedStyle.moodKeywords }
                    : undefined;

                const guide = fromShotBreakdown(
                    {
                        description: scene.visualDescription,
                        shotType: scene.shotType ?? 'medium',
                        cameraAngle: 'Eye-level',
                        movement: scene.cameraMovement ?? 'static',
                        lighting: scene.lighting ?? 'Natural',
                        emotion: getEffectiveLegacyTone(scene),
                    },
                    presentChars,
                    style ?? 'Cinematic',
                    styleOverride,
                );

                const seed = getCharacterSeed(presentChars[0]!.visualDescription);
                log.info(` Reusing seed ${seed} for character: ${presentChars[0]!.name}`);
                return { prompt: serializeStyleGuideAsText(guide), seed };
            }

            // Check if already generated
            if (state.visuals && state.visuals.length >= state.contentPlan.scenes.length && state.visuals.every(v => v.imageUrl)) {
                log.info(` Visuals already generated for ${contentPlanId}, skipping`);
                return JSON.stringify({
                    success: true,
                    visualCount: state.visuals.length,
                    message: `Visuals already exist (${state.visuals.length})`,
                });
            }

            try {
                const visuals: GeneratedImage[] = state.visuals ? [...state.visuals] : [];
                const totalScenes = state.contentPlan.scenes.length;
                const BATCH_SIZE = 3;

                const effectiveVeoCount = Math.min(Math.max(0, veoVideoCount), 5, totalScenes);

                // Declare extractedStyle early so Veo fallback can use buildScenePrompt(scene, extractedStyle)
                // It will be null during Veo generation and populated afterwards for remaining scenes
                let extractedStyle: VisualStyle | null = null;

                // --- Veo Scenes: Use Veo 3.1 for first N scenes ---
                if (effectiveVeoCount > 0) {
                    const { generateProfessionalVideo } = await import("../../../videoService");

                    for (let sceneIdx = 0; sceneIdx < effectiveVeoCount; sceneIdx++) {
                        const scene = state.contentPlan.scenes[sceneIdx];
                        if (!scene || visuals[sceneIdx]?.imageUrl) continue;

                        log.info(` Generating scene ${sceneIdx + 1}/${effectiveVeoCount} with Veo 3.1`);
                        emitSceneProgress("generate_visuals", sceneIdx + 1, totalScenes, `Generating Veo video: ${scene.name}`);

                        let imageUrl: string;
                        let isVideoScene = false;

                        try {
                            imageUrl = await generateProfessionalVideo(
                                scene.visualDescription,
                                style || "Cinematic",
                                getEffectiveLegacyTone(scene),
                                "", "documentary",
                                (aspectRatio === "9:16" ? "9:16" : "16:9"),
                                8, true,
                                undefined,
                                contentPlanId,
                                sceneIdx
                            );
                            isVideoScene = true;
                            log.info(` Veo 3.1 video generated for scene ${sceneIdx + 1}`);
                        } catch (veoError) {
                            log.warn(` Veo 3.1 failed for scene ${sceneIdx + 1}, falling back to Imagen:`, veoError);
                            const { prompt: fallbackPrompt, seed: fallbackSeed } = buildScenePrompt(scene, extractedStyle);
                            imageUrl = await generateImageFromPrompt(
                                fallbackPrompt,
                                style || "Cinematic",
                                "", aspectRatio || "16:9",
                                false,
                                fallbackSeed,
                                contentPlanId,
                                sceneIdx
                            );
                        }

                        visuals[sceneIdx] = {
                            promptId: scene.id,
                            imageUrl: imageUrl,
                            type: isVideoScene ? "video" : "image",
                        };

                        const currentState = productionStore.get(contentPlanId) || state;
                        currentState.visuals = visuals;
                        productionStore.set(contentPlanId, currentState);
                    }
                }

                // --- Extract Visual Style from first scene for consistency ---
                if (visuals[0]?.imageUrl) {
                    try {
                        log.info(` Extracting visual style from first scene for consistency`);
                        extractedStyle = await extractVisualStyle(visuals[0].imageUrl, contentPlanId);
                        log.info(` Style extracted: ${extractedStyle.colorPalette.join(", ")}`);
                    } catch (styleError) {
                        log.warn(` Style extraction failed, using default prompts:`, styleError);
                    }
                }

                // --- Remaining Scenes: Parallel batch processing ---
                const remainingScenes = state.contentPlan.scenes.slice(effectiveVeoCount);

                for (let batchStart = 0; batchStart < remainingScenes.length; batchStart += BATCH_SIZE) {
                    const batchEnd = Math.min(batchStart + BATCH_SIZE, remainingScenes.length);
                    const batchScenes = remainingScenes.slice(batchStart, batchEnd);

                    log.info(` Processing batch ${Math.floor(batchStart / BATCH_SIZE) + 1}: scenes ${batchStart + effectiveVeoCount + 1}-${batchEnd + effectiveVeoCount}`);
                    emitSceneProgress(
                        "generate_visuals",
                        batchStart + effectiveVeoCount + 1,
                        totalScenes,
                        `Generating visuals batch ${Math.floor(batchStart / BATCH_SIZE) + 1} (scenes ${batchStart + effectiveVeoCount + 1}-${batchEnd + effectiveVeoCount})`
                    );

                    const batchPromises = batchScenes.map(async (scene, localIndex) => {
                        const globalIndex = batchStart + localIndex + effectiveVeoCount;

                        if (visuals[globalIndex]?.imageUrl) {
                            log.info(` Visual for scene ${globalIndex + 1} already exists, skipping.`);
                            return null;
                        }

                        const { prompt: anchoredPrompt, seed: anchoredSeed } = buildScenePrompt(scene, extractedStyle);

                        const imageUrl = await generateImageFromPrompt(
                            anchoredPrompt,
                            style || "Cinematic",
                            "", aspectRatio || "16:9",
                            false,
                            anchoredSeed,
                            contentPlanId,
                            globalIndex
                        );

                        return {
                            index: globalIndex,
                            visual: {
                                promptId: scene.id,
                                imageUrl: imageUrl,
                                type: "image" as const,
                            },
                        };
                    });

                    const batchResults = await Promise.allSettled(batchPromises);

                    for (const result of batchResults) {
                        if (result.status === 'fulfilled' && result.value) {
                            visuals[result.value.index] = result.value.visual;
                        } else if (result.status === 'rejected') {
                            log.error(` Batch visual generation failed:`, result.reason);
                        }
                    }

                    const currentState = productionStore.get(contentPlanId) || state;
                    currentState.visuals = visuals;
                    productionStore.set(contentPlanId, currentState);
                }

                const successCount = visuals.filter(v => v?.imageUrl).length;

                return JSON.stringify({
                    success: true,
                    visualCount: successCount,
                    message: `Generated ${successCount}/${totalScenes} visuals using parallel batching`,
                });
            } catch (error) {
                return JSON.stringify({
                    success: false,
                    error: error instanceof Error ? error.message : String(error),
                });
            } finally {
                generatingPromises.delete(contentPlanId);
            }
        })();

        generatingPromises.set(contentPlanId, task);
        return task;
    },
    {
        name: "generate_visuals",
        description: "Generate images/visuals for all scenes in the content plan. This can take a few minutes.",
        schema: GenerateVisualsSchema,
    }
);

// --- Plan SFX Tool ---

export const planSFXTool = tool(
    async ({ contentPlanId, mood: _mood }) => {
        log.info(` Planning SFX for ${contentPlanId}`);

        const validationError = validateContentPlanId(contentPlanId);
        if (validationError) return validationError;

        const state = productionStore.get(contentPlanId);
        if (!state?.contentPlan) {
            return JSON.stringify({
                success: false,
                error: `Content plan not found for sessionId: ${contentPlanId}. Make sure you are using the exact sessionId returned by plan_video.`
            });
        }

        try {
            if (!isSFXAudioAvailable()) {
                return JSON.stringify({
                    success: false,
                    error: "SFX service not available (Freesound API key missing)",
                });
            }

            const sfxPlan = await generateVideoSFXPlanWithAudio(state.contentPlan.scenes, "documentary" as VideoPurpose);

            state.sfxPlan = sfxPlan;
            productionStore.set(contentPlanId, state);

            return JSON.stringify({
                success: true,
                sceneCount: sfxPlan.scenes.length,
                message: `Created SFX plan with ${sfxPlan.scenes.length} scene sound effects`,
            });
        } catch (error) {
            return JSON.stringify({
                success: false,
                error: error instanceof Error ? error.message : String(error),
            });
        }
    },
    {
        name: "plan_sfx",
        description: "Generate ambient sound effects plan for the video based on scene content and mood.",
        schema: PlanSFXSchema,
    }
);

// --- Validate Plan Tool ---

export const validatePlanTool = tool(
    async ({ contentPlanId }) => {
        log.info(` Validating plan ${contentPlanId}`);

        const validationError = validateContentPlanId(contentPlanId);
        if (validationError) return validationError;

        const state = productionStore.get(contentPlanId);
        if (!state?.contentPlan) {
            return JSON.stringify({
                success: false,
                error: `Content plan not found for sessionId: ${contentPlanId}. Make sure you are using the exact sessionId returned by plan_video.`
            });
        }

        try {
            const validation = await validateContentPlan(state.contentPlan);

            state.qualityScore = validation.score;

            if (validation.score > state.bestQualityScore) {
                state.bestQualityScore = validation.score;
            }

            productionStore.set(contentPlanId, state);

            const needsImprovement = validation.score < 80;
            const canRetry = state.qualityIterations < 2;

            return JSON.stringify({
                success: true,
                approved: validation.approved,
                score: validation.score,
                bestScore: state.bestQualityScore,
                iterations: state.qualityIterations,
                needsImprovement,
                canRetry,
                issues: validation.issues,
                suggestions: validation.suggestions,
                message: validation.approved
                    ? `Plan approved with score ${validation.score}/100 (best: ${state.bestQualityScore}/100)`
                    : `Plan needs improvement. Score: ${validation.score}/100 (best: ${state.bestQualityScore}/100). ${canRetry ? 'Can retry quality improvement.' : 'Max retries reached.'}`,
            });
        } catch (error) {
            return JSON.stringify({
                success: false,
                error: error instanceof Error ? error.message : String(error),
            });
        }
    },
    {
        name: "validate_plan",
        description: "Validate the content plan quality. Returns approval status, score, and suggestions. If score < 80 and iterations < 2, you should call adjust_timing next.",
        schema: ValidatePlanSchema,
    }
);

// --- Adjust Timing Tool ---

export const adjustTimingTool = tool(
    async ({ contentPlanId }) => {
        log.info(` Adjusting timing for ${contentPlanId}`);

        const validationError = validateContentPlanId(contentPlanId);
        if (validationError) return validationError;

        const state = productionStore.get(contentPlanId);
        if (!state?.contentPlan) {
            return JSON.stringify({
                success: false,
                error: `Content plan not found for sessionId: ${contentPlanId}. Make sure you are using the exact sessionId returned by plan_video.`
            });
        }

        if (!state.narrationSegments || state.narrationSegments.length === 0) {
            return JSON.stringify({
                success: false,
                error: "No narration segments found. Generate narration first."
            });
        }

        if (state.qualityIterations >= 2) {
            return JSON.stringify({
                success: false,
                error: `Maximum quality iterations (2) reached. Best score: ${state.bestQualityScore}/100`,
                bestScore: state.bestQualityScore,
            });
        }

        try {
            const syncedPlan = syncDurationsToNarration(state.contentPlan, state.narrationSegments);

            state.contentPlan = syncedPlan;
            state.qualityIterations++;
            productionStore.set(contentPlanId, state);

            return JSON.stringify({
                success: true,
                iteration: state.qualityIterations,
                totalDuration: syncedPlan.totalDuration,
                sceneCount: syncedPlan.scenes.length,
                message: `Adjusted timing to match narration (iteration ${state.qualityIterations}/2). Total duration: ${syncedPlan.totalDuration}s. Call validate_plan again to check improvement.`,
            });
        } catch (error) {
            return JSON.stringify({
                success: false,
                error: error instanceof Error ? error.message : String(error),
            });
        }
    },
    {
        name: "adjust_timing",
        description: "Adjust scene timings to match narration audio lengths. Use this when validate_plan returns score < 80 to fix timing mismatches. After calling this, always call validate_plan again. Limited to 2 iterations.",
        schema: AdjustTimingSchema,
    }
);
````

## File: packages/shared/src/services/ai/production/tools/index.ts
````typescript
/**
 * Production Tools Index
 * 
 * Central export point for all production agent tools.
 */

// Content Tools
export {
    planVideoTool,
    narrateScenesTool,
    generateVisualsTool,
    planSFXTool,
    validatePlanTool,
    adjustTimingTool,
    setGlobalProgressCallback,
} from "./contentTools";

// Media Tools
export {
    generateVideoTool,
    animateImageTool,
    generateMusicTool,
} from "./mediaTools";

// Status Tools
export {
    getProductionStatusTool,
    markCompleteTool,
} from "./statusTools";

// Story Tools
export {
    generateBreakdownTool,
    createScreenplayTool,
    generateCharactersTool,
    generateShotlistTool,
    verifyCharacterConsistencyTool,
} from "./storyTools";
````

## File: packages/shared/src/services/ai/production/tools/mediaTools.ts
````typescript
/**
 * Media Tools for Production Agent
 *
 * Tools for generating videos, animating images, and creating music.
 */

import { tool } from "@langchain/core/tools";
import { agentLogger } from "../../../logger";
import { GenerateVideoSchema, AnimateImageSchema, GenerateMusicSchema } from "../types";
import { productionStore } from "../store";
import { validateContentPlanId } from "../utils";
import { generateMotionPrompt } from "../../../promptService";
import { animateImageWithDeApi, isDeApiConfigured } from "../../../deapiService";
import {
    generateMusic as sunoGenerateMusic,
    waitForCompletion as sunoWaitForCompletion,
    isSunoConfigured
} from "../../../sunoService";
import { fetchAndCacheAsBlob } from "../../../videoService";
import { getEffectiveLegacyTone } from "../../../tripletUtils";

const log = agentLogger.child('Production');

// --- Generate Video Tool (Veo 3.1) ---

export const generateVideoTool = tool(
    async ({ contentPlanId, sceneIndex, style, aspectRatio, durationSeconds, useFastModel }) => {
        log.info(` Generating video for scene ${sceneIndex} using Veo 3.1`);

        const validationError = validateContentPlanId(contentPlanId);
        if (validationError) return validationError;

        const state = productionStore.get(contentPlanId);
        if (!state?.contentPlan) {
            return JSON.stringify({ 
                success: false, 
                error: `Content plan not found for sessionId: ${contentPlanId}. Make sure you are using the exact sessionId returned by plan_video.` 
            });
        }

        const scene = state.contentPlan.scenes[sceneIndex];
        if (!scene) {
            return JSON.stringify({
                success: false,
                error: `Scene at index ${sceneIndex} not found in content plan.`
            });
        }

        try {
            const { generateProfessionalVideo } = await import("../../../videoService");

            log.info(` Generating professional Veo 3.1 video for scene ${sceneIndex}: ${scene.name}`);
            const videoUrl = await generateProfessionalVideo(
                scene.visualDescription,
                style || "Cinematic",
                getEffectiveLegacyTone(scene),
                "",
                "documentary",
                (aspectRatio === "9:16" ? "9:16" : "16:9"),
                (durationSeconds as 4 | 6 | 8) || 8,
                useFastModel !== false
            );

            const currentState = productionStore.get(contentPlanId) || state;

            if (!currentState.visuals) {
                currentState.visuals = [];
            }

            if (!currentState.visuals[sceneIndex]) {
                currentState.visuals[sceneIndex] = {
                    promptId: scene.id,
                    imageUrl: videoUrl,
                };
            }

            // Cache video as blob URL immediately to prevent expired URL issues on re-export
            let cachedBlobUrl: string | undefined;
            try {
                cachedBlobUrl = await fetchAndCacheAsBlob(videoUrl);
                log.info(` Cached video as blob URL for scene ${sceneIndex}`);
            } catch (cacheError) {
                log.info(` Warning: Failed to cache video blob (will use URL): ${cacheError}`);
            }

            currentState.visuals[sceneIndex].imageUrl = cachedBlobUrl || videoUrl;
            currentState.visuals[sceneIndex].videoUrl = cachedBlobUrl || videoUrl;
            currentState.visuals[sceneIndex].type = "video";
            currentState.visuals[sceneIndex].isAnimated = true;
            currentState.visuals[sceneIndex].generatedWithVeo = true;
            currentState.visuals[sceneIndex].cachedBlobUrl = cachedBlobUrl;
            productionStore.set(contentPlanId, currentState);

            return JSON.stringify({
                success: true,
                sceneIndex,
                duration: durationSeconds || 8,
                model: useFastModel !== false ? "veo-3.1-fast" : "veo-3.1-standard",
                message: `Generated professional cinematic video for scene ${sceneIndex} (${durationSeconds || 8}s) with AI-enhanced prompt`,
            });
        } catch (error) {
            return JSON.stringify({
                success: false,
                error: error instanceof Error ? error.message : String(error),
            });
        }
    },
    {
        name: "generate_video",
        description: "Generate professional cinematic video using Veo 3.1 with AI-powered cinematographer prompts. Automatically transforms scene descriptions into detailed prompts with camera movements, lighting design, and motion choreography. Creates 4-8 second broadcast-quality videos. Use this for direct text-to-video generation. For image-to-video animation, use animate_image instead.",
        schema: GenerateVideoSchema,
    }
);

// --- Animate Image Tool (DeAPI) ---

export const animateImageTool = tool(
    async ({ contentPlanId, sceneIndex, aspectRatio }) => {
        log.info(` Animating image for scene ${sceneIndex}`);

        const validationError = validateContentPlanId(contentPlanId);
        if (validationError) return validationError;

        const state = productionStore.get(contentPlanId);
        if (!state?.contentPlan) {
            return JSON.stringify({ 
                success: false, 
                error: `Content plan not found for sessionId: ${contentPlanId}. Make sure you are using the exact sessionId returned by plan_video.` 
            });
        }

        const visual = state.visuals[sceneIndex];
        if (!visual) {
            return JSON.stringify({
                success: false,
                error: `No visual found for scene ${sceneIndex}. Generate visuals first.`
            });
        }

        const scene = state.contentPlan.scenes[sceneIndex];
        if (!scene) {
            return JSON.stringify({
                success: false,
                error: `Scene at index ${sceneIndex} not found in content plan.`
            });
        }

        const tryVeoFallback = async (): Promise<string> => {
            log.info(` Trying Veo 3.1 with professional prompt for scene ${sceneIndex}`);
            const { generateProfessionalVideo } = await import("../../../videoService");
            return generateProfessionalVideo(
                scene.visualDescription,
                "Cinematic",
                getEffectiveLegacyTone(scene),
                "",
                "documentary",
                (aspectRatio === "9:16" ? "9:16" : "16:9"),
                6,
                true
            );
        };

        if (!isDeApiConfigured()) {
            log.info(` DeAPI not configured, generating professional Veo 3.1 video for scene ${sceneIndex}`);
            try {
                const videoUrl = await tryVeoFallback();

                // Cache video as blob URL immediately
                let cachedBlobUrl: string | undefined;
                try {
                    cachedBlobUrl = await fetchAndCacheAsBlob(videoUrl);
                    log.info(` Cached video as blob URL for scene ${sceneIndex}`);
                } catch (cacheError) {
                    log.info(` Warning: Failed to cache video blob: ${cacheError}`);
                }

                const currentState = productionStore.get(contentPlanId) || state;
                if (currentState.visuals && currentState.visuals[sceneIndex]) {
                    (currentState.visuals[sceneIndex] as any).imageUrl = cachedBlobUrl || videoUrl;
                    (currentState.visuals[sceneIndex] as any).videoUrl = cachedBlobUrl || videoUrl;
                    (currentState.visuals[sceneIndex] as any).isAnimated = true;
                    (currentState.visuals[sceneIndex] as any).generatedWithVeo = true;
                    (currentState.visuals[sceneIndex] as any).cachedBlobUrl = cachedBlobUrl;
                    productionStore.set(contentPlanId, currentState);
                }

                return JSON.stringify({
                    success: true,
                    sceneIndex,
                    message: `Generated professional cinematic video for scene ${sceneIndex} with AI-enhanced prompt (DeAPI not configured)`,
                    usedVeo: true
                });
            } catch (veoError) {
                return JSON.stringify({
                    success: false,
                    error: `Both DeAPI (not configured) and Veo fallback failed: ${veoError instanceof Error ? veoError.message : String(veoError)}`,
                });
            }
        }

        try {
            log.info(` Generating motion prompt for scene ${sceneIndex}`);
            const motionResult = await generateMotionPrompt(
                scene.visualDescription,
                getEffectiveLegacyTone(scene),
                ""
            );
            log.info(` Motion prompt: ${motionResult.combined.substring(0, 100)}...`);

            const videoUrl = await animateImageWithDeApi(
                visual.imageUrl,
                motionResult.combined,
                (aspectRatio as "16:9" | "9:16" | "1:1") || "16:9",
                contentPlanId,
                sceneIndex
            );

            const currentState = productionStore.get(contentPlanId) || state;
            if (currentState.visuals && currentState.visuals[sceneIndex]) {
                (currentState.visuals[sceneIndex] as any).videoUrl = videoUrl;
                (currentState.visuals[sceneIndex] as any).isAnimated = true;
                productionStore.set(contentPlanId, currentState);
            }

            return JSON.stringify({
                success: true,
                sceneIndex,
                message: `Animated scene ${sceneIndex} successfully`,
            });
        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);

            const isCloudflareBlock = errorMessage.includes('Cloudflare') ||
                errorMessage.includes('blocked') ||
                errorMessage.includes('503');

            if (isCloudflareBlock) {
                log.info(` DeAPI blocked by Cloudflare, generating professional Veo 3.1 video for scene ${sceneIndex}`);
                try {
                    const videoUrl = await tryVeoFallback();

                    // Cache video as blob URL immediately
                    let cachedBlobUrl: string | undefined;
                    try {
                        cachedBlobUrl = await fetchAndCacheAsBlob(videoUrl);
                        log.info(` Cached video as blob URL for scene ${sceneIndex}`);
                    } catch (cacheError) {
                        log.info(` Warning: Failed to cache video blob: ${cacheError}`);
                    }

                    const currentState = productionStore.get(contentPlanId) || state;
                    if (currentState.visuals && currentState.visuals[sceneIndex]) {
                        (currentState.visuals[sceneIndex] as any).imageUrl = cachedBlobUrl || videoUrl;
                        (currentState.visuals[sceneIndex] as any).videoUrl = cachedBlobUrl || videoUrl;
                        (currentState.visuals[sceneIndex] as any).isAnimated = true;
                        (currentState.visuals[sceneIndex] as any).generatedWithVeo = true;
                        (currentState.visuals[sceneIndex] as any).cachedBlobUrl = cachedBlobUrl;
                        productionStore.set(contentPlanId, currentState);
                    }

                    return JSON.stringify({
                        success: true,
                        sceneIndex,
                        message: `Generated professional cinematic video for scene ${sceneIndex} with AI-enhanced prompt (DeAPI blocked by Cloudflare)`,
                        usedVeo: true
                    });
                } catch (veoError) {
                    return JSON.stringify({
                        success: false,
                        error: `DeAPI blocked by Cloudflare, and Veo fallback also failed: ${veoError instanceof Error ? veoError.message : String(veoError)}`,
                    });
                }
            }

            return JSON.stringify({
                success: false,
                error: errorMessage,
            });
        }
    },
    {
        name: "animate_image",
        description: "Convert a still image to a short video loop. Uses DeAPI (image-to-video) with automatic Veo 3.1 fallback if DeAPI is blocked. Call AFTER generate_visuals. Only use if user wants animated scenes.",
        schema: AnimateImageSchema,
    }
);

// --- Generate Music Tool (Suno) ---

export const generateMusicTool = tool(
    async ({ contentPlanId, style, mood, duration, instrumental }) => {
        log.info(` Generating music: ${style} - ${mood}`);

        const state = productionStore.get(contentPlanId);

        if (!isSunoConfigured()) {
            return JSON.stringify({
                success: false,
                error: "Suno API not configured. Add VITE_SUNO_API_KEY to .env.local"
            });
        }

        try {
            const finalDuration = duration || state?.contentPlan?.totalDuration || 60;

            const taskId = await sunoGenerateMusic({
                prompt: `Create ${mood} background music in ${style} style for a ${finalDuration} second video.`,
                style: style,
                title: `BGM - ${mood} ${style}`,
                instrumental: instrumental !== false,
                model: "V5",
            });

            const tracks = await sunoWaitForCompletion(taskId);

            if (tracks.length > 0 && state && tracks[0]) {
                state.musicTaskId = taskId;
                (state as any).musicUrl = tracks[0].audio_url;
                (state as any).musicTrack = tracks[0];
                productionStore.set(contentPlanId, state);
            }

            return JSON.stringify({
                success: true,
                taskId,
                trackCount: tracks.length,
                musicUrl: tracks[0]?.audio_url,
                duration: tracks[0]?.duration,
                message: `Generated ${style} background music (${tracks[0]?.duration}s)`,
            });
        } catch (error) {
            return JSON.stringify({
                success: false,
                error: error instanceof Error ? error.message : String(error),
            });
        }
    },
    {
        name: "generate_music",
        description: "Generate background music for the video using Suno AI. Creates instrumental BGM matching the video mood.",
        schema: GenerateMusicSchema,
    }
);
````

## File: packages/shared/src/services/ai/production/tools/statusTools.ts
````typescript
/**
 * Status Tools for Production Agent
 * 
 * Tools for checking production status and marking completion.
 */

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { productionStore } from "../store";
import { validateContentPlanId } from "../utils";

// --- Get Production Status Tool ---

export const getProductionStatusTool = tool(
    async ({ contentPlanId }) => {
        const validationError = validateContentPlanId(contentPlanId);
        if (validationError) return validationError;

        const state = productionStore.get(contentPlanId);
        if (!state) {
            return JSON.stringify({ 
                success: false, 
                error: `Session not found for sessionId: ${contentPlanId}. Make sure you are using the exact sessionId returned by plan_video.` 
            });
        }

        return JSON.stringify({
            success: true,
            hasContentPlan: !!state.contentPlan,
            sceneCount: state.contentPlan?.scenes.length || 0,
            totalDuration: state.contentPlan?.totalDuration || 0,
            hasNarration: state.narrationSegments.length > 0,
            narrationCount: state.narrationSegments.length,
            hasVisuals: state.visuals.length > 0,
            visualCount: state.visuals.length,
            hasSFX: !!state.sfxPlan,
            sfxSceneCount: state.sfxPlan?.scenes.length || 0,
            isComplete: state.isComplete,
            errors: state.errors,
        });
    },
    {
        name: "get_production_status",
        description: "Get the current status of a video production session.",
        schema: z.object({
            contentPlanId: z.string().describe("Session ID to check status for"),
        }),
    }
);

// --- Mark Complete Tool ---

export const markCompleteTool = tool(
    async ({ contentPlanId }) => {
        const validationError = validateContentPlanId(contentPlanId);
        if (validationError) return validationError;

        const state = productionStore.get(contentPlanId);
        if (!state) {
            return JSON.stringify({ 
                success: false, 
                error: `Session not found for sessionId: ${contentPlanId}. Make sure you are using the exact sessionId returned by plan_video.` 
            });
        }

        state.isComplete = true;
        productionStore.set(contentPlanId, state);

        return JSON.stringify({
            success: true,
            message: "Production marked as complete",
            summary: {
                scenes: state.contentPlan?.scenes.length || 0,
                duration: state.contentPlan?.totalDuration || 0,
                narrations: state.narrationSegments.length,
                visuals: state.visuals.length,
                sfxScenes: state.sfxPlan?.scenes.length || 0,
            },
        });
    },
    {
        name: "mark_complete",
        description: "Mark a production session as complete after all assets are generated.",
        schema: z.object({
            contentPlanId: z.string().describe("Session ID to mark as complete"),
        }),
    }
);
````

## File: packages/shared/src/services/ai/production/tools/storyTools.ts
````typescript
/**
 * Story Tools for Production Agent
 * 
 * Tools for story mode workflow: breakdown, screenplay, characters, shotlist, and consistency verification.
 */

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { agentLogger } from "../../../logger";
import { GEMINI_API_KEY, MODELS } from "../../../shared/apiClient";
import { withAILogging } from "../../../aiLogService";
import { StoryModeSchema, VerifyCharacterConsistencySchema, type StoryModeState } from "../types";
import { storyModeStore, productionStore } from "../store";
import { verifyCharacterConsistency } from "../../../visualConsistencyService";
import { type ScreenplayScene, type ShotlistEntry, type CharacterProfile } from "../../../../types";
import { detectLanguage } from "../../../languageDetector";

const log = agentLogger.child('Production');

/**
 * Strip markdown formatting from LLM-generated text.
 * Removes **bold**, *italic*, # headings, `code`, and bullet markers.
 */
function stripMarkdown(text: string): string {
    return text
        .replace(/#{1,6}\s+/g, '')          // # headings
        .replace(/\*\*([^*]*?)\*\*/g, '$1') // **bold** → content
        .replace(/\*([^*]*?)\*/g, '$1')     // *italic* → content
        .replace(/`([^`]*?)`/g, '$1')       // `code` → content
        .replace(/^\s*[-*+]\s+/gm, '')      // bullet markers
        .replace(/\s{2,}/g, ' ')            // collapse whitespace
        .trim();
}

/**
 * Detect if a string looks like a scene heading (INT./EXT.) rather than a character name.
 * LLMs sometimes put scene headings in the speaker field.
 */
function isSceneHeading(text: string): boolean {
    const trimmed = text.trim();
    // Scene headings start with INT./EXT., or the field has more than 4 words (it's a description, not a name)
    return /^(INT\.|EXT\.)/i.test(trimmed)
        || trimmed.length > 30
        || trimmed.split(/\s+/).length > 4;
}

/**
 * Filter out invalid dialogue entries where the LLM confused speaker/description fields.
 * Keeps only entries with short, valid speaker names and non-empty text.
 */
function sanitizeDialogue(
    entries: Array<{ speaker: string; text: string }>
): Array<{ speaker: string; text: string }> {
    return entries
        .map(d => {
            if (!d.speaker || !d.text) return null;
            if (isSceneHeading(d.speaker)) {
                // Recover: treat the misplaced content as Narrator text
                const rescuedText = d.text && d.text.trim().length > 5 ? d.text : d.speaker;
                return { speaker: 'Narrator', text: stripMarkdown(rescuedText) };
            }
            return d;
        })
        .filter((d): d is { speaker: string; text: string } => d !== null && d.text.trim().length > 0);
}

// --- Generate Breakdown Tool ---

export const generateBreakdownTool = tool(
    async ({ topic, sessionId }) => {
        const id = sessionId || `story_${Date.now()}`;
        log.info(` Generating story breakdown for: ${topic}`);

        // Validate API key
        if (!GEMINI_API_KEY) {
            log.error(' GEMINI_API_KEY is not configured');
            return JSON.stringify({
                success: false,
                error: 'GEMINI_API_KEY is not configured. Please set VITE_GEMINI_API_KEY in your .env.local file.',
            });
        }

        const model = new ChatGoogleGenerativeAI({
            model: MODELS.TEXT_EXP,
            apiKey: GEMINI_API_KEY,
            temperature: 0.7,
            maxRetries: 2,
        });

        // Detect language to use appropriate prompt and markers
        const isArabicTopic = detectLanguage(topic) === 'ar';

        const prompt = isArabicTopic
            ? `أنت خبير تطوير قصص. قبل الكتابة، حدّد:
- البطل وهدفه الرئيسي
- الصراع الأساسي الذي يواجهه
- القوس العاطفي: كيف يتغير البطل من البداية إلى النهاية؟

ثم أنشئ تفصيلًا سرديًا لقصة فيديو عن: "${topic}".
قسّمها إلى 3-5 فصول أو مشاهد متميزة. لكل فصل، قدّم:
1. العنوان (بعد كلمة "مشهد" ورقمه، مثال: مشهد ١: العنوان) — اجعله محددًا ومرتبطًا بحدث فعلي
2. السرد: جملتان تصفان ما يحدث بصريًا بدقة — اذكر الشخصيات والأحداث الملموسة (لا تستخدم عناوين مثل "الخطاف العاطفي" أو "النقطة السردية" — اكتب السرد مباشرة)

نسّق كقائمة مرقّمة باستخدام الأرقام العربية (١، ٢، ٣...). تجنب العناوين المبهمة مثل "البداية" أو "الصراع".`
            : `You are a story development expert.

Before writing, identify:
- The protagonist and their central desire or goal
- The core conflict they face
- The emotional arc from opening to resolution

Then create a narrative breakdown for a video story about: "${topic}".
Divide it into 3-5 distinct acts or chapters. For each act, provide:
1. Title — Specific to a story moment (avoid generic labels like "Introduction" or "Conflict")
2. NARRATIVE: 2-3 sentences describing what happens visually — name characters, describe specific events and locations (do NOT include labels like "Emotional Hook:" or "Key Beat:" — write the narrative directly)

Format as a structured numbered list. Be specific — avoid vague phrases like "things escalate" or "challenges arise".`;

        let breakdown: string;
        try {
            log.info(' Invoking Gemini API for story breakdown...');
            const response = await withAILogging(
                id,
                'breakdown',
                MODELS.TEXT_EXP,
                prompt,
                () => model.invoke(prompt),
                (r) => typeof r.content === 'string' ? r.content : JSON.stringify(r.content),
            );
            breakdown = response.content as string;
            log.info(' Story breakdown generated successfully');
        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);
            log.error(` Failed to generate story breakdown: ${errorMessage}`);
            return JSON.stringify({
                success: false,
                error: `Failed to generate story breakdown: ${errorMessage}`,
            });
        }

        const state: StoryModeState = storyModeStore.get(id) || {
            id,
            topic,
            breakdown,
            screenplay: [],
            characters: [],
            shotlist: [],
            currentStep: 'breakdown',
            updatedAt: Date.now(),
        };

        state.breakdown = breakdown;
        state.currentStep = 'breakdown';
        state.updatedAt = Date.now();
        storyModeStore.set(id, state);

        // Return minimal info - full breakdown is stored in state
        return JSON.stringify({
            success: true,
            sessionId: id,
            actCount: breakdown.split(/Act \d+|Chapter \d+/i).length - 1 || 3,
            message: `Story breakdown created with narrative structure. Use sessionId="${id}" for next steps.`,
        });
    },
    {
        name: "generate_breakdown",
        description: "Step 1: Generate a narrative breakdown/outline for the story topic.",
        schema: StoryModeSchema,
    }
);

// --- Create Screenplay Tool ---

export const createScreenplayTool = tool(
    async ({ sessionId }) => {
        if (!sessionId) return JSON.stringify({ success: false, error: "sessionId required" });
        const state = storyModeStore.get(sessionId);
        if (!state) return JSON.stringify({ success: false, error: "Session not found" });

        log.info(` Creating screenplay for: ${sessionId}`);

        const model = new ChatGoogleGenerativeAI({
            model: MODELS.TEXT_EXP,
            apiKey: GEMINI_API_KEY,
            temperature: 0.7,
            maxRetries: 2,
        });

        // Detect language to use appropriate scene markers
        const isArabicContent = detectLanguage(state.breakdown) === 'ar';

        // Count the actual number of acts in the breakdown so the screenplay
        // LLM generates exactly one scene per act (prevents count mismatch).
        const actMarkers = state.breakdown.match(
            /(?:^|\n)\s*(?:مشهد|المشهد|SCENE|Act|Chapter|الفصل)\s*[0-9\u0660-\u0669\u06F0-\u06F9]+/gi
        );
        // Fallback: count numbered list items (١. / 1. / ١- etc.)
        const listMarkers = !actMarkers?.length
            ? state.breakdown.match(/(?:^|\n)\s*[0-9\u0660-\u0669\u06F0-\u06F9]+[.\-)]/gm)
            : null;
        const actCount = Math.min(Math.max((actMarkers?.length || listMarkers?.length || 3), 3), 8);
        log.info(` Breakdown has ${actCount} acts → requesting ${actCount} screenplay scenes`);

        const prompt = isArabicContent
            ? `اكتب سيناريو سينمائي قصير بناءً على هذا التفصيل:
${state.breakdown}

نسّق كل مشهد كالتالي:
- مشهد [الرقم]: [العنوان]
- الحدث: [الوصف]
- الحوار: [الشخصية]: [النص]

قواعد مهمة لسطر الحدث:
- أسطر الحدث تصف ما تراه الكاميرا، وليس المشاعر الداخلية.
- ستُستخدم كتعليق صوتي — اجعلها حية وسينمائية.
- صِف التفاصيل الحسية: الأصوات، الملمس، الحركة، الضوء، اللون.
- لا تستخدم تنسيق markdown (بدون ** أو * أو # أو backticks).

حدّد ${actCount} مشاهد بالضبط — مشهد واحد لكل فصل من الفصول أعلاه.`
            : `Write a short cinematic screenplay based on this breakdown:
${state.breakdown}

Format each scene with:
- SCENE [Number]: [Heading]
- ACTION: [Description]
- DIALOGUE: [Character]: [Text]

IMPORTANT — ACTION LINE RULES:
- ACTION lines describe what the CAMERA SEES, not internal emotions.
- These lines will be used as voiceover narration — make them vivid and cinematic.
- BAD: "He felt fear." / "She was overwhelmed with sadness."
- GOOD: "His hands trembled. The door creaked open, revealing darkness." / "Tears rolled down her cheeks as rain hammered the window."
- Describe sensory details: sounds, textures, movement, light, color.
- NEVER use markdown formatting (no **, *, #, or backticks).

DIALOGUE RULES (CRITICAL):
- DIALOGUE format: [Character Name]: [Spoken line]
- Character Name must be 1-4 words ONLY — a person's name, nothing else.
- VALID: "Faisal: What happened here?" / "Old Man: Listen carefully."
- INVALID: "Faisal walks forward: ..." / "The hero, overwhelmed: ..." — these will corrupt the scene.
- If there is no specific speaker, use "Narrator" as the name.

Write exactly ${actCount} scenes — one scene per act above.`;

        let scriptText: string;
        try {
            log.info(' Invoking Gemini API for screenplay...');
            const response = await withAILogging(
                sessionId,
                'screenplay',
                MODELS.TEXT_EXP,
                prompt,
                () => model.invoke(prompt),
                (r) => typeof r.content === 'string' ? r.content : JSON.stringify(r.content),
            );
            scriptText = stripMarkdown(response.content as string);
            log.info(' Screenplay generated successfully');
        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);
            log.error(` Failed to create screenplay: ${errorMessage}`);
            return JSON.stringify({
                success: false,
                error: `Failed to create screenplay: ${errorMessage}`,
            });
        }

        // Simple parser for the draft screenplay
        // Supports English "SCENE 1:" and Arabic "مشهد ١:" / "المشهد ١:" markers
        // with ASCII digits (0-9), Arabic-Indic (٠-٩), and Extended Arabic-Indic (۰-۹)
        const scenes: ScreenplayScene[] = [];
        const sceneBlocks = scriptText.split(/(?:SCENE|مشهد|المشهد)\s+[0-9\u0660-\u0669\u06F0-\u06F9]+\s*:/i).filter(b => b.trim());

        sceneBlocks.forEach((block, i) => {
            const lines = block.split('\n').filter(l => l.trim());
            const heading = lines[0] || 'Untitled Scene';
            // Match ACTION:/الحدث: prefix regardless of residual markdown wrapping
            const actionPattern = /^(\*{0,2})(ACTION|الحدث)(\*{0,2})\s*:/i;
            const dialogueLabelPattern = /^(\*{0,2})(DIALOGUE|الحوار)(\*{0,2})\s*:/i;
            const actionLines = lines.filter(l => actionPattern.test(l.trim()));
            const dialogueLines = lines.filter(l => {
                const trimmed = l.trim();
                // Skip action lines and bare labels
                if (actionPattern.test(trimmed)) return false;
                if (dialogueLabelPattern.test(trimmed)) return false;
                return trimmed.includes(':');
            });

            // Extract action text, stripping label prefix (English or Arabic)
            const actionText = actionLines
                .map(l => l.replace(/^(\*{0,2})(ACTION|الحدث)(\*{0,2})\s*:\s*/i, '').trim())
                .join(' ');

            // Extract character names from dialogue speakers and name mentions in action
            const speakers = dialogueLines
                .map(l => {
                    const [speaker] = l.split(':');
                    return (speaker || '').replace(/^(\*{0,2})(DIALOGUE|الحوار)(\*{0,2})\s*/i, '').trim();
                })
                .filter(s => s && s.length > 0 && s.length < 50);
            const uniqueCharacters = [...new Set(speakers)];

            const rawDialogue = dialogueLines.map(l => {
                const [speaker, ...text] = l.split(':');
                return { speaker: (speaker || "").trim(), text: text.join(':').trim() };
            });

            scenes.push({
                id: `scene_${i}`,
                sceneNumber: i + 1,
                heading: heading.replace(/(\*{0,2})(ACTION|DIALOGUE|الحدث|الحوار)(\*{0,2})\s*:/gi, '').trim(),
                action: actionText,
                dialogue: sanitizeDialogue(rawDialogue),
                charactersPresent: uniqueCharacters,
            });
        });

        state.screenplay = scenes;
        state.currentStep = 'screenplay';
        state.updatedAt = Date.now();
        storyModeStore.set(sessionId, state);

        // Return minimal info - full screenplay is stored in state
        return JSON.stringify({
            success: true,
            sceneCount: scenes.length,
            sceneHeadings: scenes.map(s => s.heading),
            message: `Screenplay created with ${scenes.length} scenes.`,
        });
    },
    {
        name: "create_screenplay",
        description: "Step 2: Transform the breakdown into a formatted screenplay with dialogue.",
        schema: z.object({ sessionId: z.string() }),
    }
);

// --- Generate Characters Tool ---

export const generateCharactersTool = tool(
    async ({ sessionId }) => {
        if (!sessionId) return JSON.stringify({ success: false, error: "sessionId required" });
        const state = storyModeStore.get(sessionId);
        if (!state) return JSON.stringify({ success: false, error: "Session not found" });

        log.info(` Extracting characters for: ${sessionId}`);
        const { extractCharacters, generateAllCharacterReferences } = await import("../../../characterService");

        const scriptText = state.screenplay.map(s =>
            `${s.heading}\n${s.action}\n${s.dialogue.map(d => `${d.speaker}: ${d.text}`).join('\n')}`
        ).join('\n\n');

        const characters = await extractCharacters(scriptText, sessionId);
        const charactersWithRefs = await generateAllCharacterReferences(characters, sessionId);

        state.characters = charactersWithRefs;
        state.currentStep = 'characters';
        state.updatedAt = Date.now();
        storyModeStore.set(sessionId, state);

        // Return minimal info - full characters are stored in state
        return JSON.stringify({
            success: true,
            characterCount: charactersWithRefs.length,
            characterNames: charactersWithRefs.map(c => c.name),
            hasReferences: charactersWithRefs.filter(c => c.referenceImageUrl).length,
            message: `Extracted ${charactersWithRefs.length} characters with visual references.`,
        });
    },
    {
        name: "generate_characters",
        description: "Step 3: Extract characters from the screenplay and generate consistent visual reference sheets.",
        schema: z.object({ sessionId: z.string() }),
    }
);

// --- Generate Shotlist Tool ---

export const generateShotlistTool = tool(
    async ({ sessionId }) => {
        if (!sessionId) return JSON.stringify({ success: false, error: "sessionId required" });
        const state = storyModeStore.get(sessionId);
        if (!state) return JSON.stringify({ success: false, error: "Session not found" });
        if (!state.screenplay || state.screenplay.length === 0) {
            return JSON.stringify({ success: false, error: "Screenplay is empty. Create screenplay first." });
        }

        log.info(` Generating shotlist for: ${sessionId}`);

        try {
            const { breakAllScenesIntoShots } = await import("../../shotBreakdownAgent");

            const genre = 'Drama'; // Default genre; ideally passed from session state
            const rawShots = await breakAllScenesIntoShots(
                state.screenplay,
                genre,
                (sceneIndex, totalScenes) => {
                    log.info(` Shotlist progress: scene ${sceneIndex + 1}/${totalScenes}`);
                },
                sessionId,
            );

            // Convert Shot[] to ShotlistEntry[]
            const shots: ShotlistEntry[] = rawShots.map(shot => ({
                id: shot.id,
                sceneId: shot.sceneId,
                shotNumber: shot.shotNumber,
                description: shot.description,
                cameraAngle: shot.cameraAngle,
                movement: shot.movement,
                lighting: shot.lighting,
                dialogue: "",
            }));

            state.shotlist = shots;
            state.currentStep = 'shotlist';
            state.updatedAt = Date.now();
            storyModeStore.set(sessionId, state);

            return JSON.stringify({
                success: true,
                shotCount: shots.length,
                message: `Generated ${shots.length} shots across ${state.screenplay.length} scenes.`,
            });
        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);
            log.error(` Failed to generate shotlist: ${errorMessage}`);
            return JSON.stringify({
                success: false,
                error: `Failed to generate shotlist: ${errorMessage}`,
            });
        }
    },
    {
        name: "generate_shotlist",
        description: "Step 4: Create a detailed shotlist/storyboard from the screenplay and characters.",
        schema: z.object({ sessionId: z.string() }),
    }
);

// --- Verify Character Consistency Tool ---

export const verifyCharacterConsistencyTool = tool(
    async ({ sessionId, characterName }) => {
        log.info(` Verifying consistency for ${characterName} in session ${sessionId}`);

        // Try storyModeStore first
        const storyState = storyModeStore.get(sessionId);
        let profileFound: any = storyState?.characters?.find((c: any) => c.name === characterName);
        let imageUrls: string[] = [];

        if (storyState) {
            // In Story Mode, shotlist entries have imageUrls
            imageUrls = storyState.shotlist
                .filter((s: any) => s.imageUrl)
                .map((s: any) => s.imageUrl);
        } else {
            // Fallback to productionStore
            const pState = productionStore.get(sessionId);
            if (pState) {
                profileFound = pState.contentPlan?.characters?.find(c => c.name === characterName);
                if (!profileFound) {
                    profileFound = (pState as any).characters?.find((c: any) => c.name === characterName);
                }
                imageUrls = pState.visuals
                    .filter(v => !v.isPlaceholder)
                    .map(v => v.imageUrl);
            }
        }

        if (!profileFound) {
            const availableChars = storyState?.characters?.map((c: any) => c.name).join(", ") ||
                productionStore.get(sessionId)?.contentPlan?.characters?.map(c => c.name).join(", ") || "None";
            return JSON.stringify({
                success: false,
                error: `Character "${characterName}" not found in session ${sessionId}. Available: ${availableChars}`
            });
        }

        if (imageUrls.length === 0) {
            return JSON.stringify({
                success: false,
                error: "No generated images found for verification. Generate visuals first."
            });
        }

        // Map internal structure to CharacterProfile expected by service
        const characterToVerify: CharacterProfile = {
            id: profileFound.id || "unknown",
            name: profileFound.name,
            role: profileFound.role || "Character",
            visualDescription: profileFound.visualDescription ||
                `${profileFound.appearance || ""} ${profileFound.clothing || ""}`
        };

        // Detect language for report
        const language = detectLanguage(characterToVerify.visualDescription + ' ' + characterToVerify.name);

        const report = await verifyCharacterConsistency(imageUrls, characterToVerify, language);

        return JSON.stringify({
            success: true,
            report
        });
    },
    {
        name: "verify_character_consistency",
        description: "Verifies visual consistency of a character across all generated shots. Returns a report with a score and suggestions.",
        schema: VerifyCharacterConsistencySchema,
    }
);
````

## File: packages/shared/src/services/ai/production/types.ts
````typescript
/**
 * Production Agent Types and Schemas
 *
 * Shared types, interfaces, and Zod schemas for the production agent system.
 */

import { z } from "zod";
import { ContentPlan, NarrationSegment, GeneratedImage, VideoSFXPlan, ScreenplayScene, CharacterProfile, ShotlistEntry } from "../../../types";
import { type ImportedContent } from "../../agent/importUtils";
import { type MixedAudioResult } from "../../agent/audioMixingTools";
import { type SubtitleResult } from "../../agent/subtitleTools";
import { type ExportResult } from "../../agent/exportTools";
import { type ToolError, type PartialSuccessReport } from "../../agent/errorRecovery";

// --- Zod Schemas for Tool Inputs ---

export const PlanVideoSchema = z.object({
    topic: z.string().describe("Main topic or subject for the video"),
    targetDuration: z.number().min(10).max(600).describe("Target video duration in seconds (10-600)"),
    style: z.string().optional().describe("Visual style (e.g., 'Cinematic', 'Documentary')"),
    mood: z.string().optional().describe("Mood/tone (e.g., 'dramatic', 'upbeat')"),
    videoPurpose: z.string().optional().describe("Purpose (e.g., 'educational', 'entertainment')"),
    audience: z.string().optional().describe("Target audience (e.g., 'children', 'professionals')"),
    language: z.string().optional().describe("Target language (e.g., 'en', 'ar')"),
});

export const NarrateScenesSchema = z.object({
    contentPlanId: z.string().describe("Reference ID of the content plan to narrate"),
    voice: z.string().optional().describe("Voice ID to use for narration"),
    language: z.string().optional().describe("Language code for narration (e.g., 'en', 'ar', 'es')"),
    voiceStyle: z.string().optional().describe("Voice style (e.g., 'calm', 'energetic')"),
});

export const GenerateVisualsSchema = z.object({
    contentPlanId: z.string().describe("Reference ID of the content plan"),
    veoVideoCount: z.number().min(0).max(10).default(0).describe("Number of scenes to generate with Veo 3.1 video instead of images (0-10, optional)"),
    style: z.string().optional().describe("Visual style (e.g., 'Cinematic', 'Anime')"),
    aspectRatio: z.string().optional().describe("Aspect ratio (16:9, 9:16, 1:1)"),
});

export const PlanSFXSchema = z.object({
    contentPlanId: z.string().describe("Reference ID of the content plan"),
    skipAudioDownload: z.boolean().optional().describe("If true, skip downloading SFX audio"),
    mood: z.string().optional().describe("Overall mood for SFX selection"),
});

export const ValidatePlanSchema = z.object({
    contentPlanId: z.string().describe("Reference ID of the content plan to validate"),
});

export const AdjustTimingSchema = z.object({
    contentPlanId: z.string().describe("Reference ID of the content plan to adjust timing for"),
});

export const GenerateVideoSchema = z.object({
    contentPlanId: z.string().describe("Reference ID of the content plan"),
    sceneIndex: z.number().min(0).describe("Zero-based scene index to generate video for"),
    style: z.string().optional().describe("Visual style (e.g., 'Cinematic', 'Documentary')"),
    aspectRatio: z.string().optional().default("16:9").describe("Video aspect ratio (e.g., '16:9', '9:16')"),
    durationSeconds: z.number().min(4).max(8).optional().default(6).describe("Duration in seconds (4-8)"),
    useFastModel: z.boolean().optional().default(true).describe("Use fast generation model"),
});

export const AnimateImageSchema = z.object({
    contentPlanId: z.string().describe("Reference ID of the content plan"),
    sceneIndex: z.number().min(0).describe("Zero-based scene index to animate"),
    aspectRatio: z.string().optional().default("16:9").describe("Image aspect ratio (e.g., '16:9', '9:16')"),
    customPrompt: z.string().optional().describe("Custom animation prompt (optional, uses scene visual prompt if not provided)"),
});

export const GenerateMusicSchema = z.object({
    contentPlanId: z.string().describe("Reference ID of the content plan"),
    style: z.string().optional().describe("Music style (e.g., 'ambient', 'epic')"),
    mood: z.string().optional().describe("Music mood (e.g., 'dramatic', 'peaceful')"),
    duration: z.number().optional().describe("Target duration in seconds"),
    instrumental: z.boolean().optional().default(true).describe("Whether to generate instrumental only"),
});

export const StoryModeSchema = z.object({
    topic: z.string().describe("The main topic or idea for the video story"),
    sessionId: z.string().optional().describe("Optional session ID to continue an existing story"),
    targetDuration: z.number().min(30).max(900).optional().describe("Target duration in seconds"),
});

export const VerifyCharacterConsistencySchema = z.object({
    sessionId: z.string().describe("Session ID of the story mode session"),
    characterName: z.string().describe("Name of the character to verify"),
});

// --- Agent State Interfaces ---

/**
 * Production session state containing all intermediate results.
 *
 * Requirements: 1.3, 3.5, 4.1, 9.1, 6.4
 */
export interface ProductionState {
    /** Content plan with scenes and narration scripts */
    contentPlan: ContentPlan | null;
    /** Generated narration audio segments */
    narrationSegments: NarrationSegment[];
    /** Generated visual assets for each scene */
    visuals: GeneratedImage[];
    /** Sound effects plan */
    sfxPlan: VideoSFXPlan | null;
    /** Suno music generation task ID */
    musicTaskId: string | null;
    /** Suno music URL */
    musicUrl: string | null;
    /** Suno music track object */
    musicTrack: Record<string, unknown> | null;
    /** Structured errors encountered during production (Requirement 6.4) */
    errors: ToolError[];
    /** Whether production is complete */
    isComplete: boolean;
    /** Imported content from YouTube or audio file (Requirement 1.3) */
    importedContent: ImportedContent | null;
    /** Quality validation score 0-100 (Requirement 7.1) */
    qualityScore: number;
    /** Number of quality improvement iterations performed (Requirement 7.3) */
    qualityIterations: number;
    /** Best quality score achieved across all validation attempts (Requirement 7.5) */
    bestQualityScore: number;
    /** Mixed audio result combining narration, music, SFX (Requirement 3.5) */
    mixedAudio: MixedAudioResult | null;
    /** Generated subtitles in SRT/VTT format (Requirement 4.1) */
    subtitles: SubtitleResult | null;
    /** Final video export result (Requirement 9.1) */
    exportResult: ExportResult | null;
    /** Exported video blob for easy access */
    exportedVideo: Blob | null;
    /** Partial success report for the production run */
    partialSuccessReport?: PartialSuccessReport;
}

/**
 * Story Mode State
 * Manages the step-by-step generation workflow
 */
export interface StoryModeState {
    id: string;
    topic: string;
    breakdown: string;
    screenplay: ScreenplayScene[];
    characters: CharacterProfile[];
    shotlist: ShotlistEntry[];
    currentStep: 'breakdown' | 'screenplay' | 'characters' | 'shotlist' | 'production';
    updatedAt: number;
    // Multi-format pipeline extensions
    formatId?: string; // Selected video format (youtube-narrator, advertisement, etc.)
    language?: 'ar' | 'en'; // Detected or selected language
    checkpoints?: import('../../../types').CheckpointState[]; // Checkpoint approval states
}

/**
 * Progress update for production agent
 */
export interface ProductionProgress {
    /** Current stage label */
    stage: string;
    /** Human-readable status message */
    message: string;
    /** Whether the production is complete */
    isComplete: boolean;
    /** Overall percentage complete (0-100) */
    progress?: number;
    /** Current tool being executed */
    tool?: string;
    /** Whether the current tool/stage was successful */
    success?: boolean;
    /** Error details if any */
    error?: string;
    /** Current iteration number */
    iteration?: number;
    /** Maximum iterations allowed */
    maxIterations?: number;
    /** Current scene index (1-based) */
    currentScene?: number;
    /** Total number of scenes */
    totalScenes?: number;
    /** Summary of assets generated (only on completion) */
    assetSummary?: {
        scenes: number;
        narrations: number;
        visuals: number;
        music: number;
        sfx: number;
        subtitles: number;
    };
    /** Session ID created by plan_video, create_storyboard, or generate_breakdown */
    sessionId?: string;
    /** @deprecated Use progress */
    percentage?: number;
}

/**
 * Create initial production state
 */
export function createInitialState(): ProductionState {
    return {
        contentPlan: null,
        narrationSegments: [],
        visuals: [],
        sfxPlan: null,
        musicTaskId: null,
        musicUrl: null,
        musicTrack: null,
        errors: [],
        isComplete: false,
        importedContent: null,
        qualityScore: 0,
        qualityIterations: 0,
        bestQualityScore: 0,
        mixedAudio: null,
        subtitles: null,
        exportResult: null,
        exportedVideo: null,
    };
}
````

## File: packages/shared/src/services/ai/production/utils.ts
````typescript
/**
 * Production Agent Utilities
 * 
 * Utility functions for language detection and helper operations.
 */

import { agentLogger } from "../../logger";

const log = agentLogger.child('Production');

/**
 * Detect language from text content using Unicode character analysis.
 * Used to auto-select the appropriate TTS voice for narration.
 * 
 * @param text - The text to analyze
 * @returns Language code (e.g., 'ar', 'en', 'he', 'zh')
 */
export function detectLanguageFromText(text: string): string {
    if (!text || text.trim().length === 0) {
        return 'en';
    }

    // Count characters in different Unicode ranges
    let arabicCount = 0;
    let hebrewCount = 0;
    let chineseCount = 0;
    let japaneseCount = 0;
    let koreanCount = 0;
    let cyrillicCount = 0;
    let greekCount = 0;
    let latinCount = 0;
    let totalAlpha = 0;

    for (const char of text) {
        const code = char.charCodeAt(0);

        // Arabic: U+0600–U+06FF, U+0750–U+077F (Arabic Supplement)
        if ((code >= 0x0600 && code <= 0x06FF) || (code >= 0x0750 && code <= 0x077F)) {
            arabicCount++;
            totalAlpha++;
        }
        // Hebrew: U+0590–U+05FF
        else if (code >= 0x0590 && code <= 0x05FF) {
            hebrewCount++;
            totalAlpha++;
        }
        // CJK (Chinese): U+4E00–U+9FFF
        else if (code >= 0x4E00 && code <= 0x9FFF) {
            chineseCount++;
            totalAlpha++;
        }
        // Japanese (Hiragana + Katakana): U+3040–U+30FF
        else if (code >= 0x3040 && code <= 0x30FF) {
            japaneseCount++;
            totalAlpha++;
        }
        // Korean (Hangul): U+AC00–U+D7AF
        else if (code >= 0xAC00 && code <= 0xD7AF) {
            koreanCount++;
            totalAlpha++;
        }
        // Cyrillic (Russian, etc.): U+0400–U+04FF
        else if (code >= 0x0400 && code <= 0x04FF) {
            cyrillicCount++;
            totalAlpha++;
        }
        // Greek: U+0370–U+03FF
        else if (code >= 0x0370 && code <= 0x03FF) {
            greekCount++;
            totalAlpha++;
        }
        // Latin (A-Z, a-z, extended Latin)
        else if ((code >= 0x0041 && code <= 0x007A) || (code >= 0x00C0 && code <= 0x024F)) {
            latinCount++;
            totalAlpha++;
        }
    }

    // Determine majority language (need at least 20% of text to be in that script)
    const threshold = totalAlpha * 0.2;

    if (arabicCount > threshold && arabicCount >= Math.max(hebrewCount, chineseCount, japaneseCount, koreanCount, cyrillicCount, latinCount)) {
        return 'ar';
    }
    if (hebrewCount > threshold && hebrewCount >= Math.max(arabicCount, chineseCount, japaneseCount, koreanCount, cyrillicCount, latinCount)) {
        return 'he';
    }
    if (chineseCount > threshold && chineseCount >= Math.max(arabicCount, hebrewCount, japaneseCount, koreanCount, cyrillicCount, latinCount)) {
        return 'zh';
    }
    if (japaneseCount > threshold && japaneseCount >= Math.max(arabicCount, hebrewCount, chineseCount, koreanCount, cyrillicCount, latinCount)) {
        return 'ja';
    }
    if (koreanCount > threshold && koreanCount >= Math.max(arabicCount, hebrewCount, chineseCount, japaneseCount, cyrillicCount, latinCount)) {
        return 'ko';
    }
    if (cyrillicCount > threshold && cyrillicCount >= Math.max(arabicCount, hebrewCount, chineseCount, japaneseCount, koreanCount, latinCount)) {
        return 'ru';
    }
    if (greekCount > threshold && greekCount >= Math.max(arabicCount, hebrewCount, chineseCount, japaneseCount, koreanCount, cyrillicCount, latinCount)) {
        return 'el';
    }

    // Default to English for Latin script or mixed content
    return 'en';
}

/**
 * Generate unique ID for each production session
 */
export function generateSessionId(): string {
    return `prod_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
}

/**
 * Validate that a contentPlanId is a real session ID, not a placeholder.
 * Returns an error response if invalid, null if valid.
 * 
 * This prevents the AI from using placeholder values like "plan_123" or "cp_01"
 * which would cause "Content plan not found" errors.
 */
export function validateContentPlanId(contentPlanId: string): string | null {
    if (!contentPlanId) {
        return JSON.stringify({
            success: false,
            error: `Missing contentPlanId. You must provide the sessionId returned by plan_video or generate_breakdown.`
        });
    }

    // Check for common placeholder patterns
    if (contentPlanId.match(/^(plan_\d+|cp_\d+|session_\d+|plan_\w{3,8}|cp_\w{3,8})$/)) {
        return JSON.stringify({
            success: false,
            error: `Invalid contentPlanId: "${contentPlanId}". You must use the ACTUAL sessionId returned by plan_video or generate_breakdown. Never use placeholder values.`
        });
    }

    // Check if it matches the expected formats (prod_ or story_)
    if (!contentPlanId.startsWith('prod_') && !contentPlanId.startsWith('story_')) {
        return JSON.stringify({
            success: false,
            error: `Invalid contentPlanId format: "${contentPlanId}". Expected format: prod_TIMESTAMP_HASH or story_TIMESTAMP. Make sure you are using the exact sessionId returned by plan_video or generate_breakdown.`
        });
    }

    return null; // Valid
}

/**
 * Validate a sessionId at runtime (not a placeholder).
 * Returns true if valid, false if invalid.
 */
export function isValidSessionId(sessionId: string | null | undefined): sessionId is string {
    if (!sessionId) return false;
    
    // Check for common placeholder patterns
    if (sessionId.match(/^(plan_\d+|cp_\d+|session_\d+|plan_\w{3,8}|cp_\w{3,8})$/)) {
        return false;
    }
    
    // Check if it matches the expected formats (prod_ or story_)
    return sessionId.startsWith('prod_') || sessionId.startsWith('story_');
}

/**
 * Create a step identifier for duplicate tool call prevention.
 * This combines tool name with key arguments to identify unique execution steps.
 * 
 * Requirement 10.1 - Track executed tools per step
 */
export function createStepIdentifier(toolName: string, toolArgs: any): string {
    // For scene-specific tools, include scene index first (before contentPlanId check)
    if (toolArgs.sceneIndex !== undefined) {
        return `${toolName}_${toolArgs.contentPlanId || 'default'}_scene_${toolArgs.sceneIndex}`;
    }

    // For most tools, the contentPlanId is the key identifier
    if (toolArgs.contentPlanId) {
        return `${toolName}_${toolArgs.contentPlanId}`;
    }

    // For import tools, use URL or path as identifier
    if (toolArgs.url) {
        return `${toolName}_${toolArgs.url}`;
    }
    if (toolArgs.audioPath) {
        return `${toolName}_${toolArgs.audioPath}`;
    }

    // For tools without specific identifiers, use tool name only
    return toolName;
}
````

## File: packages/shared/src/services/ai/productionAgent.ts
````typescript
/**
 * Production Agent - Re-export Wrapper
 *
 * This file maintains backward compatibility by re-exporting all functionality
 * from the modular production/ directory.
 *
 * For new code, prefer importing directly from './production':
 * @example
 * import { runProductionAgent, ProductionState } from './production';
 *
 * @deprecated Import from './production' instead for new code
 */

// Re-export everything from the modular production directory
export * from "./production";
````

## File: packages/shared/src/services/ai/rag/documents/bestPractices.ts
````typescript
/**
 * Video Production Best Practices
 * 
 * Guidelines and best practices for video production.
 * Used by the knowledge base for RAG (Retrieval-Augmented Generation).
 */

export interface BestPractice {
  title: string;
  content: string;
  keywords: string[];
}

export const BEST_PRACTICES: Record<string, BestPractice> = {
  pacing: {
    title: "Video Pacing Guidelines",
    content: `
# Video Pacing Best Practices

## Duration Guidelines

### 30-Second Videos
**Purpose:** Quick, punchy content for social media
- 3-4 scenes maximum
- Fast cuts and high energy
- Single, clear message
- Hook in first 2 seconds
- Strong call-to-action at end

**Scene Duration:**
- 6-8 seconds per scene
- No scene longer than 10 seconds
- Quick transitions

**Best For:**
- Social media ads
- Product teasers
- Quick tips
- Announcements

### 60-Second Videos
**Purpose:** Standard format for most content
- 5-7 scenes
- Moderate pacing
- 2-3 key points
- Introduction, body, conclusion

**Scene Duration:**
- 8-12 seconds per scene
- Minimum 5 seconds (enough to register)
- Maximum 15 seconds (before attention wanes)

**Best For:**
- Explainer videos
- Product demos
- Social media content
- Brand stories

### 90-Second Videos
**Purpose:** Detailed exploration with breathing room
- 8-10 scenes
- Allow time for information absorption
- Full story arc
- Multiple supporting points

**Scene Duration:**
- 9-13 seconds per scene
- Can have longer establishing shots
- Vary lengths for rhythm

**Best For:**
- Tutorials
- Case studies
- Detailed explanations
- Narrative content

### 180-Second Videos (3 minutes)
**Purpose:** Comprehensive, in-depth content
- 12-15 scenes
- Detailed exploration
- Multiple examples
- Full narrative structure

**Scene Duration:**
- 10-15 seconds per scene
- Longer scenes for complex information
- Strategic pacing variations

**Best For:**
- Educational content
- Documentaries
- Detailed tutorials
- Thought leadership

## Scene Duration Principles

**Minimum Duration: 5 seconds**
- Anything shorter doesn't register
- Viewer needs time to process
- Exception: rapid montages

**Sweet Spot: 8-12 seconds**
- Comfortable viewing time
- Enough for information absorption
- Maintains engagement

**Maximum Duration: 20 seconds**
- Beyond this, attention wanes
- Break into multiple scenes
- Exception: establishing shots or key moments

## Transition Guidelines

### Fast Cuts
**When to Use:**
- High energy content
- Action sequences
- Montages
- Building excitement

**Timing:**
- 0.5-1 second transitions
- Quick fades or cuts
- Minimal transition effects

### Smooth Transitions
**When to Use:**
- Calm, reflective content
- Scene changes in same location
- Related content flow
- Professional presentations

**Timing:**
- 1-2 second transitions
- Crossfades or dissolves
- Smooth, unobtrusive

### Dramatic Transitions
**When to Use:**
- Major topic shifts
- Time passage
- Location changes
- Emotional shifts

**Timing:**
- 2-3 second transitions
- Fade to black
- Wipes or special effects
- Music cues

## Rhythm and Flow

### Building Tension
- Start with longer scenes
- Gradually shorten scene duration
- Increase cut frequency
- Build to climax

### Releasing Tension
- Slow down after climax
- Longer, calmer scenes
- Gentle transitions
- Resolution and conclusion

### Maintaining Interest
- Vary scene lengths
- Mix wide and close shots
- Alternate pacing
- Strategic pauses

## Matching Pacing to Music

### Fast Tempo (120+ BPM)
- Quick cuts on beat
- High energy scenes
- Short scene durations
- Rhythmic editing

### Medium Tempo (80-120 BPM)
- Standard pacing
- Cuts on major beats
- Balanced scene lengths
- Natural flow

### Slow Tempo (<80 BPM)
- Longer scenes
- Gentle transitions
- Contemplative pacing
- Emotional resonance

## Common Pacing Mistakes

**Too Fast:**
- Viewer can't process information
- Feels chaotic and overwhelming
- Important details missed
- Exhausting to watch

**Too Slow:**
- Viewer loses interest
- Feels boring and dragging
- Attention wanders
- Drop-off increases

**Inconsistent:**
- Confusing rhythm
- Unclear intent
- Jarring experience
- Unprofessional feel

## Platform-Specific Considerations

### Social Media (Instagram, TikTok)
- Front-load hook (first 2 seconds)
- Fast pacing throughout
- 15-30 second optimal length
- Vertical format considerations

### YouTube
- Longer form acceptable
- Can build gradually
- 60-180 seconds common
- Retention metrics important

### Professional/Corporate
- Measured, professional pace
- Clear information hierarchy
- 60-120 seconds typical
- Quality over speed
    `,
    keywords: [
      "pacing",
      "duration",
      "timing",
      "rhythm",
      "transitions",
      "scene length",
      "tempo",
      "flow",
      "editing",
    ],
  },

  narration: {
    title: "Narration Best Practices",
    content: `
# Narration Guidelines

## Voice Selection

### Professional Voice
**Characteristics:**
- Authoritative and clear
- Measured delivery
- Proper enunciation
- Neutral accent (or appropriate regional)

**Best For:**
- Corporate videos
- Educational content
- Documentaries
- Formal presentations

**Delivery Speed:** 140-160 words per minute

### Friendly Voice
**Characteristics:**
- Warm and conversational
- Approachable tone
- Natural inflection
- Relatable delivery

**Best For:**
- Tutorials
- Product demos
- Social media content
- Brand storytelling

**Delivery Speed:** 150-170 words per minute

### Dramatic Voice
**Characteristics:**
- Emotional and intense
- Dynamic range
- Theatrical delivery
- Strong emphasis

**Best For:**
- Movie trailers
- Dramatic content
- Storytelling
- Emotional appeals

**Delivery Speed:** 120-140 words per minute (with pauses)

### Calm Voice
**Characteristics:**
- Soothing and gentle
- Slow, measured pace
- Soft delivery
- Relaxing tone

**Best For:**
- Meditation content
- Relaxation videos
- ASMR
- Bedtime stories

**Delivery Speed:** 100-120 words per minute

## Script Writing for Voice-Over

### Write for the Ear, Not the Eye
- Use short sentences (10-15 words)
- Avoid complex sentence structures
- Use contractions naturally
- Write how people speak

**Bad:** "The utilization of this methodology facilitates optimization."
**Good:** "This method helps you optimize."

### Active Voice
- More engaging and direct
- Easier to understand
- Stronger impact

**Passive:** "The video was created by our team."
**Active:** "Our team created the video."

### Avoid Jargon
- Use simple, clear language
- Explain technical terms
- Consider audience knowledge level
- Define acronyms on first use

### Conversational Tone
- Use "you" and "we"
- Ask rhetorical questions
- Include natural pauses
- Sound human, not robotic

## Pacing and Delivery

### Words Per Minute Guidelines

**Slow (100-120 WPM):**
- Meditation, relaxation
- Complex technical content
- Emotional, dramatic moments
- Allows time for processing

**Standard (140-160 WPM):**
- Most professional content
- Educational videos
- Documentaries
- Comfortable listening pace

**Fast (160-180 WPM):**
- Energetic content
- Youth-oriented material
- Exciting announcements
- Time-constrained content

**Very Fast (180+ WPM):**
- Disclaimers (legal requirement)
- High-energy commercials
- Rapid-fire information
- Not recommended for main content

### Strategic Pauses

**Short Pause (0.5-1 second):**
- Between sentences
- After commas
- Natural breathing points

**Medium Pause (1-2 seconds):**
- Between paragraphs
- Topic transitions
- Before important points

**Long Pause (2-3 seconds):**
- Major section breaks
- Dramatic effect
- Allow visual focus
- Emotional moments

## Emphasis and Inflection

### Stress Key Words
- Emphasize important information
- Vary pitch for interest
- Use volume strategically
- Avoid monotone delivery

### Emotional Matching
- Match tone to content
- Convey appropriate emotion
- Authentic delivery
- Connect with audience

### Question Inflection
- Rising tone for questions
- Engage audience
- Create curiosity
- Prompt thinking

## Technical Considerations

### Audio Quality
- Professional microphone
- Quiet recording environment
- Pop filter for plosives
- Proper mic technique

### Recording Tips
- Warm up voice before recording
- Stay hydrated
- Maintain consistent distance from mic
- Record multiple takes
- Leave room for editing

### Post-Production
- Remove mouth clicks and breaths
- Normalize audio levels
- Add subtle compression
- EQ for clarity
- De-ess if needed

## Matching Narration to Visuals

### Sync Points
- Align narration with visual changes
- Match emphasis to key visuals
- Time pauses with transitions
- Coordinate with music

### Avoid Redundancy
- Don't describe what's obvious
- Add context and meaning
- Complement visuals
- Provide additional information

### Leave Space
- Allow visuals to breathe
- Not every moment needs narration
- Strategic silence is powerful
- Let music and visuals tell story

## Common Narration Mistakes

**Too Fast:**
- Audience can't keep up
- Information overload
- Feels rushed and stressful
- Reduces comprehension

**Too Slow:**
- Boring and tedious
- Audience loses interest
- Wastes time
- Feels condescending

**Monotone:**
- Lacks engagement
- Sounds robotic
- Loses audience attention
- No emotional connection

**Over-Emphasized:**
- Sounds fake and theatrical
- Distracting from content
- Loses credibility
- Annoying to listen to

**Poor Audio Quality:**
- Unprofessional
- Hard to understand
- Distracting
- Reduces trust

## Language and Localization

### English Narration
- Clear, neutral accent
- Proper pronunciation
- Standard grammar
- International comprehension

### Arabic Narration
- Modern Standard Arabic for formal
- Dialect consideration for casual
- Right-to-left text coordination
- Cultural sensitivity

### Multilingual Considerations
- Professional translation
- Native speaker review
- Cultural adaptation
- Timing adjustments for language length
    `,
    keywords: [
      "narration",
      "voice",
      "script",
      "audio",
      "delivery",
      "pacing",
      "voice-over",
      "speaking",
      "pronunciation",
    ],
  },

  camera: {
    title: "Camera Angles and Lighting",
    content: `
# Camera Angles and Lighting Best Practices

## Camera Angles

### Wide Establishing Shot
**Purpose:** Set context, show location, establish scale
**When to Use:**
- Opening scenes
- New locations
- Showing relationships between elements
- Creating sense of space

**Framing:**
- Show full environment
- Include horizon or reference points
- Consider rule of thirds
- Allow breathing room

### Medium Shot
**Purpose:** Main content delivery, balanced view
**When to Use:**
- Dialogue and narration
- Product demonstrations
- Most general content
- Comfortable viewing distance

**Framing:**
- Waist up for people
- Object with context
- Balanced composition
- Clear subject focus

### Close-Up
**Purpose:** Emphasis, emotion, detail
**When to Use:**
- Emotional moments
- Important details
- Product features
- Facial expressions

**Framing:**
- Fill frame with subject
- Minimal background
- Sharp focus
- Intimate feel

### Extreme Close-Up
**Purpose:** Intense detail, dramatic effect
**When to Use:**
- Product details
- Texture and craftsmanship
- Dramatic moments
- Artistic shots

**Framing:**
- Partial subject
- Abstract composition
- Macro detail
- Strong impact

### Low Angle
**Purpose:** Power, importance, dominance
**When to Use:**
- Hero shots
- Authority figures
- Impressive subjects
- Dramatic effect

**Effect:**
- Subject appears larger
- Commanding presence
- Viewer looks up
- Sense of awe

### High Angle
**Purpose:** Vulnerability, overview, context
**When to Use:**
- Showing layout
- Vulnerable moments
- Establishing geography
- Aerial views

**Effect:**
- Subject appears smaller
- Viewer looks down
- Comprehensive view
- God's eye perspective

### Dutch Angle (Tilted)
**Purpose:** Tension, unease, dynamism
**When to Use:**
- Action sequences
- Psychological tension
- Artistic expression
- Breaking monotony

**Effect:**
- Disorienting
- Dynamic energy
- Unconventional
- Attention-grabbing

### Over-the-Shoulder
**Purpose:** Conversation, perspective, immersion
**When to Use:**
- Dialogue scenes
- Point of view shots
- Interactive content
- Creating connection

**Framing:**
- Partial foreground subject
- Clear view of main subject
- Depth and dimension
- Relationship establishment

## Lighting Moods

### Golden Hour Warm
**Characteristics:**
- Soft, warm sunlight
- Long, gentle shadows
- Orange/golden tones
- Flattering on subjects

**Best For:**
- Outdoor scenes
- Romantic content
- Nostalgic feel
- Natural beauty

**Time:** 1 hour after sunrise, 1 hour before sunset

### Cool Blue Moonlight
**Characteristics:**
- Blue color temperature
- Mysterious atmosphere
- Soft shadows
- Ethereal quality

**Best For:**
- Night scenes
- Mysterious content
- Calm, contemplative mood
- Fantasy elements

**Technique:** Blue gels or color grading

### Dramatic Chiaroscuro
**Characteristics:**
- Strong contrast
- Defined shadows
- Directional light
- Sculptural quality

**Best For:**
- Dramatic content
- Film noir style
- Artistic expression
- Intense emotions

**Technique:** Single strong light source, minimal fill

### Soft Diffused Overcast
**Characteristics:**
- Even, soft light
- Minimal shadows
- Gentle, flattering
- Natural feel

**Best For:**
- Interviews
- Product photography
- Gentle, calm content
- Consistent lighting

**Technique:** Overcast day or large diffusers

### Harsh Midday Sun
**Characteristics:**
- Strong, direct light
- Hard shadows
- High contrast
- Challenging for subjects

**Best For:**
- Desert scenes
- Harsh environments
- Specific artistic intent
- Generally avoid for people

**Technique:** Use reflectors or fill light to soften

### Neon-Lit Urban Glow
**Characteristics:**
- Colorful artificial light
- Cyberpunk aesthetic
- Mixed color temperatures
- Modern, edgy feel

**Best For:**
- Urban content
- Tech videos
- Modern, trendy content
- Night cityscapes

**Technique:** Practical neon lights or colored gels

### Candlelit Warmth
**Characteristics:**
- Warm, flickering light
- Intimate atmosphere
- Soft, romantic
- Historical feel

**Best For:**
- Romantic scenes
- Historical content
- Intimate moments
- Cozy atmosphere

**Technique:** Actual candles or warm, dimmed lights

### Silhouette Backlighting
**Characteristics:**
- Subject in shadow
- Bright background
- Dramatic outline
- Mysterious identity

**Best For:**
- Dramatic reveals
- Anonymous subjects
- Artistic expression
- Sunset scenes

**Technique:** Expose for background, subject in shadow

### Foggy Haze
**Characteristics:**
- Diffused, atmospheric
- Reduced contrast
- Dreamy quality
- Depth through layers

**Best For:**
- Mysterious content
- Fantasy elements
- Atmospheric scenes
- Depth creation

**Technique:** Fog machine or natural fog

### Studio Three-Point
**Characteristics:**
- Professional setup
- Controlled lighting
- Even, flattering
- Clean look

**Best For:**
- Interviews
- Product videos
- Professional content
- Consistent quality

**Technique:** Key light, fill light, back light

## Combining Angles and Lighting

### Cinematic Combination
- Wide establishing with golden hour
- Medium shots with soft diffused
- Close-ups with dramatic chiaroscuro
- Varied angles for visual interest

### Documentary Combination
- Natural lighting throughout
- Mix of wide and medium shots
- Authentic, unmanipulated feel
- Consistent lighting approach

### Dramatic Combination
- Low angles with dramatic lighting
- High contrast chiaroscuro
- Dutch angles for tension
- Silhouettes and shadows

### Professional Combination
- Eye-level medium shots
- Studio three-point lighting
- Clean, consistent look
- Minimal artistic interpretation
    `,
    keywords: [
      "camera",
      "angles",
      "lighting",
      "cinematography",
      "framing",
      "composition",
      "mood",
      "atmosphere",
    ],
  },
};
````

## File: packages/shared/src/services/ai/rag/documents/styleGuides.ts
````typescript
/**
 * Video Style Guides
 * 
 * Comprehensive guides for different video production styles.
 * Used by the knowledge base for RAG (Retrieval-Augmented Generation).
 */

export interface StyleGuide {
  title: string;
  content: string;
  keywords: string[];
}

export const STYLE_GUIDES: Record<string, StyleGuide> = {
  cinematic: {
    title: "Cinematic Style Guide",
    content: `
# Cinematic Video Style

## Visual Characteristics
Cinematic videos evoke the feeling of professional film production with carefully composed shots and dramatic visual storytelling.

**Key Visual Elements:**
- Wide establishing shots to set context and location
- Dramatic lighting with strong contrast (golden hour, chiaroscuro)
- Slow, deliberate camera movements (pans, tilts, tracking shots)
- Color grading with teal/orange tones or desaturated looks
- Depth of field effects to isolate subjects
- Lens flares and atmospheric elements
- Professional composition following rule of thirds

## Best Use Cases
- Documentary-style content
- Narrative storytelling
- Emotional or dramatic content
- Professional presentations
- Brand videos
- Educational content requiring gravitas

## Pacing Guidelines
**60-second videos:**
- 5-7 scenes total
- 8-12 seconds per scene
- Allow breathing room between scenes
- Build emotional arc gradually

**90-second videos:**
- 8-10 scenes total
- 9-11 seconds per scene
- More time for establishing shots
- Can develop complex narratives

**180-second videos:**
- 12-15 scenes total
- 10-15 seconds per scene
- Full story arc with setup, conflict, resolution
- Time for character development

## Camera Angles and Movement
**Establishing Shots:**
- Wide aerial/drone views for scale
- Landscape shots to set location
- Cityscape or environment overview

**Main Content:**
- Medium shots for dialogue and action
- Close-ups for emotional moments
- Over-the-shoulder for conversations
- Low angles for power and importance
- High angles for vulnerability

**Movement:**
- Slow push-ins for emphasis
- Tracking shots following subjects
- Smooth pans revealing information
- Static shots for contemplation

## Lighting Moods
**Golden Hour Warm:**
- Soft, warm sunlight
- Long shadows
- Romantic, nostalgic feel
- Best for outdoor scenes

**Dramatic Chiaroscuro:**
- Strong contrast between light and dark
- Defined shadows
- Mysterious, intense mood
- Indoor or controlled lighting

**Soft Diffused:**
- Overcast or filtered light
- Minimal shadows
- Gentle, calm atmosphere
- Interviews and talking heads

**Cool Blue Moonlight:**
- Night scenes or early morning
- Blue color temperature
- Mysterious, contemplative mood

## Narration Style
- Authoritative, measured delivery
- Professional voice talent
- Clear enunciation
- Emotional resonance matching visuals
- Pauses for dramatic effect
- 140-160 words per minute

## Common Mistakes to Avoid
- Too many quick cuts (breaks cinematic flow)
- Harsh midday lighting (unflattering)
- Shaky handheld footage (unless intentional)
- Over-saturated colors (looks amateur)
- Mismatched audio quality
- Inconsistent color grading between scenes
    `,
    keywords: [
      "cinematic",
      "dramatic",
      "professional",
      "documentary",
      "film",
      "movie",
      "storytelling",
      "emotional",
      "golden hour",
      "depth of field",
    ],
  },

  anime: {
    title: "Anime Style Guide",
    content: `
# Anime Video Style

## Visual Characteristics
Anime-style videos embrace the bold, expressive aesthetic of Japanese animation with vibrant colors and dynamic compositions.

**Key Visual Elements:**
- Bold outlines and cel-shaded appearance
- Vibrant, saturated colors with high contrast
- Exaggerated expressions and movements
- Dynamic action poses and angles
- Speed lines and impact frames
- Stylized backgrounds (often simplified)
- Dramatic lighting with strong shadows
- Particle effects and visual flourishes

## Best Use Cases
- Action-oriented content
- Youth-targeted material
- Fantasy and sci-fi topics
- Gaming-related videos
- Energetic product launches
- Tutorial content for younger audiences
- Music videos with high energy
- Explainer videos with personality

## Pacing Guidelines
**30-second videos:**
- 4-5 scenes with fast cuts
- 6-7 seconds per scene
- High energy throughout
- Single clear message

**60-second videos:**
- 6-8 scenes with dynamic pacing
- 7-10 seconds per scene
- Build to climactic moment
- Multiple action beats

**90-second videos:**
- 9-12 scenes
- 7-10 seconds per scene
- Can include slower moments for contrast
- Full story arc with setup and payoff

## Camera Angles and Movement
**Dynamic Angles:**
- Dutch angles (tilted) for tension
- Extreme low angles for power
- High angles for vulnerability
- Over-the-shoulder for confrontation

**Action Sequences:**
- Quick cuts between angles
- Close-ups on eyes and expressions
- Wide shots for full-body action
- POV shots for immersion

**Movement:**
- Fast pans and whip transitions
- Zoom-ins for emphasis
- Camera shake for impact
- Rotation for disorientation

## Color Palette
**Vibrant Primary:**
- Bold reds, blues, yellows
- High saturation
- Strong color blocking
- Complementary color schemes

**Neon Tech:**
- Cyan, magenta, purple
- Glowing effects
- Dark backgrounds with bright accents
- Cyberpunk aesthetic

**Pastel Soft:**
- Lighter, softer tones
- Slice-of-life feel
- Gentle, approachable mood
- School or everyday settings

## Visual Effects
- Speed lines for motion
- Impact stars and flashes
- Sweat drops and emotion symbols
- Background blur for focus
- Sparkles and shine effects
- Energy auras and glows
- Screen tones and patterns

## Narration Style
- Energetic, expressive delivery
- Varied pitch and tone
- Quick pacing (160-180 wpm)
- Emotional reactions
- Character voices if appropriate
- Sound effects integration

## Typography
- Bold, outlined text
- Manga-style speech bubbles
- Impact text for emphasis
- Vertical text for Japanese aesthetic
- Colorful, animated text
- Sound effect text (SFX)

## Common Mistakes to Avoid
- Dull, muted colors (breaks anime aesthetic)
- Slow, plodding pace (needs energy)
- Realistic proportions (embrace stylization)
- Static compositions (needs dynamism)
- Subtle expressions (go bold)
- Western animation style (maintain anime look)
    `,
    keywords: [
      "anime",
      "vibrant",
      "action",
      "youth",
      "dynamic",
      "energetic",
      "japanese",
      "manga",
      "colorful",
      "expressive",
    ],
  },

  documentary: {
    title: "Documentary Style Guide",
    content: `
# Documentary Video Style

## Visual Characteristics
Documentary-style videos prioritize authenticity, information, and credibility with a journalistic approach to visual storytelling.

**Key Visual Elements:**
- Realistic, authentic footage
- Natural lighting when possible
- Steady, professional camera work
- Informative graphics and text overlays
- Archival footage and photos
- Interview setups with proper framing
- B-roll footage supporting narration
- Maps, diagrams, and data visualizations

## Best Use Cases
- Educational content
- Historical topics
- Scientific explanations
- Investigative journalism
- Corporate training
- Social issues
- Biographical content
- How-to and tutorial videos

## Pacing Guidelines
**60-second videos:**
- 5-6 scenes with measured pace
- 10-12 seconds per scene
- Clear information hierarchy
- One main point with supporting details

**90-second videos:**
- 7-9 scenes
- 10-13 seconds per scene
- Allow time for information absorption
- Can introduce complexity

**180-second videos:**
- 12-15 scenes
- 12-15 seconds per scene
- Full exploration of topic
- Multiple perspectives or examples
- Conclusion and call-to-action

## Camera Angles and Movement
**Interview Setup:**
- Eye-level or slightly below
- Rule of thirds composition
- Clean background
- Proper headroom

**B-Roll:**
- Wide shots for context
- Medium shots for detail
- Close-ups for emphasis
- Smooth, motivated camera movement

**Archival Integration:**
- Ken Burns effect on photos
- Slow zooms and pans
- Proper attribution
- Quality restoration when needed

## Lighting
**Natural Light:**
- Window light for interviews
- Outdoor ambient light
- Authentic to location
- Minimal artificial enhancement

**Controlled Light:**
- Three-point lighting for interviews
- Soft, even illumination
- Avoid harsh shadows
- Professional but not theatrical

## Narration Style
- Clear, authoritative voice
- Professional tone
- Factual, informative delivery
- Well-researched content
- Proper pronunciation
- 140-160 words per minute
- Pauses for emphasis
- Conversational yet credible

## Graphics and Text
**Lower Thirds:**
- Name and title
- Location and date
- Source attribution
- Clean, readable fonts

**Data Visualization:**
- Charts and graphs
- Timelines
- Maps with annotations
- Statistics and numbers
- Infographics

**Text Overlays:**
- Key quotes
- Important facts
- Definitions
- Dates and locations

## Audio Design
- Clean dialogue recording
- Ambient sound for authenticity
- Subtle background music
- Sound effects for context
- Professional mixing
- Clear voice-over

## Research and Accuracy
- Fact-checking all claims
- Multiple sources
- Expert interviews
- Primary sources when possible
- Proper citations
- Balanced perspectives

## Common Mistakes to Avoid
- Shaky handheld footage (use stabilization)
- Poor audio quality (invest in good mics)
- Biased presentation (maintain objectivity)
- Information overload (pace revelations)
- Boring visuals (use varied B-roll)
- Unclear structure (outline clearly)
- Missing context (provide background)
    `,
    keywords: [
      "documentary",
      "educational",
      "informative",
      "factual",
      "authentic",
      "journalistic",
      "historical",
      "scientific",
      "interview",
      "research",
    ],
  },

  "oil-painting": {
    title: "Oil Painting Style Guide",
    content: `
# Oil Painting Video Style

## Visual Characteristics
Oil painting style videos emulate the rich, textured aesthetic of classical painted artwork with artistic interpretation.

**Key Visual Elements:**
- Painterly textures and brush strokes
- Rich, saturated colors
- Soft edges and blending
- Artistic interpretation over realism
- Classical composition
- Dramatic lighting (chiaroscuro)
- Timeless, elegant aesthetic
- Museum-quality presentation

## Best Use Cases
- Historical content
- Classical music videos
- Art history topics
- Literary adaptations
- Cultural heritage
- Biographical content about artists
- Romantic or poetic themes
- Luxury brand content

## Pacing Guidelines
**60-second videos:**
- 4-6 scenes
- 10-15 seconds per scene
- Slow, contemplative pace
- Allow time to appreciate visuals

**90-second videos:**
- 6-8 scenes
- 11-15 seconds per scene
- Gradual reveals
- Emotional build

**180-second videos:**
- 10-12 scenes
- 15-18 seconds per scene
- Full narrative development
- Rich visual storytelling

## Visual Techniques
**Composition:**
- Classical framing
- Golden ratio
- Balanced elements
- Depth through layers

**Color Palette:**
- Rich earth tones
- Deep shadows
- Warm highlights
- Harmonious color schemes

**Lighting:**
- Dramatic side lighting
- Soft, diffused highlights
- Deep, rich shadows
- Rembrandt lighting

## Narration Style
- Measured, refined delivery
- Literary language
- Poetic phrasing
- 120-140 words per minute
- Dramatic pauses
- Emotional resonance

## Common Mistakes to Avoid
- Modern, digital look (maintain painterly quality)
- Fast pacing (needs contemplation time)
- Harsh lighting (use soft, artistic light)
- Flat composition (create depth)
    `,
    keywords: [
      "oil painting",
      "artistic",
      "classical",
      "painterly",
      "elegant",
      "historical",
      "cultural",
      "museum",
      "fine art",
    ],
  },
};
````

## File: packages/shared/src/services/ai/rag/exampleLibrary.ts
````typescript
/**
 * Example Library
 *
 * Stores successful video creations as examples for future recommendations.
 * Uses simple keyword matching for finding similar examples.
 */

import { agentLogger } from "../../logger";

const log = agentLogger.child('ExampleLibrary');

export interface VideoExample {
  id: string;
  topic: string;
  style: string;
  duration: number;
  userFeedback?: {
    helpful: boolean;
    rating: number;
    comment?: string;
  };
  timestamp: number;
  success: boolean;
  metadata?: {
    mood?: string;
    targetAudience?: string;
    cameraAngle?: string;
    lightingMood?: string;
  };
}

export class ExampleLibrary {
  private examples: VideoExample[] = [];

  constructor() {
    log.info(' ✅ Initialized');
  }

  /**
   * Add a video creation example to the library.
   */
  async addExample(example: VideoExample): Promise<void> {
    try {
      // Store in examples array
      this.examples.push(example);

      // Keep only last 500 examples (memory management)
      if (this.examples.length > 500) {
        this.examples = this.examples.slice(-500);
      }

      log.info(
        `[ExampleLibrary] ✅ Added example: ${example.style} video about "${example.topic.substring(0, 30)}..."`
      );
    } catch (error) {
      log.error(' Failed to add example:', error);
    }
  }

  /**
   * Find similar examples based on query using keyword matching.
   */
  async findSimilarExamples(
    query: string,
    k: number = 3
  ): Promise<VideoExample[]> {
    try {
      const queryLower = query.toLowerCase();
      const keywords = queryLower.split(/\s+/).filter(w => w.length > 2);

      // Score examples based on keyword matches
      const scored = this.examples
        .filter(ex => ex.success)
        .map(ex => {
          const searchText = this._createSearchText(ex).toLowerCase();
          const score = keywords.reduce((sum, keyword) => {
            return sum + (searchText.includes(keyword) ? 1 : 0);
          }, 0);
          return { example: ex, score };
        })
        .filter(item => item.score > 0)
        .sort((a, b) => {
          if (b.score !== a.score) return b.score - a.score;
          return b.example.timestamp - a.example.timestamp;
        });

      const examples = scored.slice(0, k).map(item => item.example);

      if (examples.length > 0) {
        log.info(
          `[ExampleLibrary] ✅ Found ${examples.length} similar examples for query:`,
          query.substring(0, 50) + '...'
        );
      }

      return examples;
    } catch (error) {
      log.error(' Failed to find similar examples:', error);
      return [];
    }
  }

  /**
   * Get successful examples, optionally filtered by style.
   */
  async getSuccessfulExamples(style?: string): Promise<VideoExample[]> {
    let filtered = this.examples.filter((ex) => ex.success);

    if (style) {
      filtered = filtered.filter(
        (ex) => ex.style.toLowerCase() === style.toLowerCase()
      );
    }

    // Sort by rating (if available) and timestamp
    filtered.sort((a, b) => {
      const ratingA = a.userFeedback?.rating || 0;
      const ratingB = b.userFeedback?.rating || 0;

      if (ratingA !== ratingB) {
        return ratingB - ratingA; // Higher rating first
      }

      return b.timestamp - a.timestamp; // More recent first
    });

    return filtered.slice(0, 10); // Return top 10
  }

  /**
   * Get formatted context string for similar examples.
   */
  async getExampleContext(query: string): Promise<string> {
    const examples = await this.findSimilarExamples(query, 3);

    if (examples.length === 0) {
      return '';
    }

    const formattedExamples = examples.map((ex, i) => {
      const rating = ex.userFeedback?.rating
        ? `⭐ ${ex.userFeedback.rating}/5`
        : 'No rating';
      const helpful = ex.userFeedback?.helpful ? '👍 Helpful' : '';

      return `${i + 1}. **${ex.style}** video about "${ex.topic}"
   - Duration: ${ex.duration}s
   - User Feedback: ${rating} ${helpful}
   - Created: ${new Date(ex.timestamp).toLocaleDateString()}`;
    });

    return `## SIMILAR SUCCESSFUL EXAMPLES:

${formattedExamples.join('\n\n')}

These examples show what worked well for similar requests. Consider using similar approaches.`;
  }

  /**
   * Get example count.
   */
  getExampleCount(): number {
    return this.examples.length;
  }

  /**
   * Get success rate across all examples.
   */
  getSuccessRate(): number {
    if (this.examples.length === 0) return 0;

    const successful = this.examples.filter((ex) => ex.success).length;
    return successful / this.examples.length;
  }

  /**
   * Get statistics about examples.
   */
  getStatistics(): {
    total: number;
    successful: number;
    successRate: number;
    byStyle: Record<string, number>;
    averageRating: number;
  } {
    const total = this.examples.length;
    const successful = this.examples.filter((ex) => ex.success).length;
    const successRate = total > 0 ? successful / total : 0;

    // Count by style
    const byStyle: Record<string, number> = {};
    this.examples.forEach((ex) => {
      byStyle[ex.style] = (byStyle[ex.style] || 0) + 1;
    });

    // Calculate average rating
    const ratingsSum = this.examples.reduce((sum, ex) => {
      return sum + (ex.userFeedback?.rating || 0);
    }, 0);
    const ratingsCount = this.examples.filter(
      (ex) => ex.userFeedback?.rating
    ).length;
    const averageRating = ratingsCount > 0 ? ratingsSum / ratingsCount : 0;

    return {
      total,
      successful,
      successRate,
      byStyle,
      averageRating,
    };
  }

  /**
   * Create searchable text from example.
   */
  private _createSearchText(example: VideoExample): string {
    const parts = [
      `Topic: ${example.topic}`,
      `Style: ${example.style}`,
      `Duration: ${example.duration} seconds`,
    ];

    if (example.metadata?.mood) {
      parts.push(`Mood: ${example.metadata.mood}`);
    }

    if (example.metadata?.targetAudience) {
      parts.push(`Audience: ${example.metadata.targetAudience}`);
    }

    if (example.userFeedback?.comment) {
      parts.push(`Feedback: ${example.userFeedback.comment}`);
    }

    return parts.join('\n');
  }
}

// Export singleton instance
export const exampleLibrary = new ExampleLibrary();
````

## File: packages/shared/src/services/ai/rag/knowledgeBase.ts
````typescript
/**
 * Video Production Knowledge Base
 * 
 * RAG (Retrieval-Augmented Generation) implementation for video production knowledge.
 * Provides keyword-based search over style guides and best practices.
 * Supports Arabic query translation for multilingual search.
 */

import { STYLE_GUIDES } from "./documents/styleGuides";
import { BEST_PRACTICES } from "./documents/bestPractices";
import { AI_CONFIG } from "../config";
import { GoogleGenAI } from "@google/genai";
import { agentLogger } from "../../logger";

const log = agentLogger.child('KnowledgeBase');

// Initialize Gemini for translations
const GEMINI_API_KEY = typeof process !== 'undefined'
  ? (process.env?.VITE_GEMINI_API_KEY || process.env?.GEMINI_API_KEY || '')
  : '';

/**
 * Detect if text contains significant Arabic characters.
 * Returns true if >30% of alphabetic characters are Arabic.
 */
function isArabicText(text: string): boolean {
  let arabicCount = 0;
  let latinCount = 0;

  for (const char of text) {
    const code = char.charCodeAt(0);
    // Arabic Unicode range: U+0600 to U+06FF
    if (code >= 0x0600 && code <= 0x06FF) {
      arabicCount++;
    }
    // Latin letters
    else if ((code >= 0x0041 && code <= 0x007A) || (code >= 0x00C0 && code <= 0x024F)) {
      latinCount++;
    }
  }

  const totalAlpha = arabicCount + latinCount;
  return totalAlpha > 0 && (arabicCount / totalAlpha) > 0.3;
}

/**
 * Translate Arabic query to English keywords for RAG search.
 * Uses Gemini API for fast, lightweight translation.
 */
async function translateQueryToEnglish(query: string): Promise<string> {
  if (!GEMINI_API_KEY) {
    log.warn(' No Gemini API key for translation, using original query');
    return query;
  }

  try {
    const genAI = new GoogleGenAI({ apiKey: GEMINI_API_KEY });

    const prompt = `Translate this Arabic text to English keywords for video production search. Return ONLY the English keywords, no explanation:

"${query}"`;

    const response = await genAI.models.generateContent({
      model: "gemini-3-flash-preview",
      contents: prompt,
    });

    const translation = response.text?.trim() || query;

    log.info(` Translated Arabic query: "${query.substring(0, 30)}..." → "${translation.substring(0, 50)}..."`);
    return translation;
  } catch (error) {
    log.warn(' Translation failed, using original query:', error);
    return query;
  }
}

interface KnowledgeDocument {
  content: string;
  metadata: {
    type: string;
    style?: string;
    category?: string;
    title: string;
    keywords: string[];
  };
}

export class VideoProductionKnowledgeBase {
  private documents: KnowledgeDocument[] = [];
  private initialized = false;

  constructor() {
    this._initializeDocuments();
  }

  private _initializeDocuments(): void {
    try {
      log.info(' Initializing...');
      const startTime = Date.now();

      // Add style guides
      for (const [key, guide] of Object.entries(STYLE_GUIDES)) {
        this.documents.push({
          content: guide.content,
          metadata: {
            type: 'style-guide',
            style: key,
            title: guide.title,
            keywords: guide.keywords,
          },
        });
      }

      // Add best practices
      for (const [key, practice] of Object.entries(BEST_PRACTICES)) {
        this.documents.push({
          content: practice.content,
          metadata: {
            type: 'best-practice',
            category: key,
            title: practice.title,
            keywords: practice.keywords,
          },
        });
      }

      this.initialized = true;
      const duration = Date.now() - startTime;
      log.info(
        `Initialized with ${this.documents.length} documents in ${duration}ms`
      );
    } catch (error) {
      log.error(' ❌ Initialization failed:', error);
      throw error;
    }
  }

  /**
   * Get relevant knowledge for a query using keyword matching.
   * Returns formatted string ready for prompt injection.
   */
  async getRelevantKnowledge(query: string, k: number = AI_CONFIG.rag.maxDocuments): Promise<string> {
    // Check if RAG is enabled
    if (!AI_CONFIG.rag.enabled) {
      return '';
    }

    try {
      if (!this.initialized) {
        log.warn(' Not initialized');
        return '';
      }

      // Translate Arabic queries to English for keyword matching
      let searchQuery = query;
      if (isArabicText(query)) {
        log.info(' Arabic query detected, translating...');
        searchQuery = await translateQueryToEnglish(query);
      }

      const queryLower = searchQuery.toLowerCase();
      const keywords = queryLower.split(/\s+/).filter(w => w.length > 2);

      // Score documents based on keyword matches
      const scored = this.documents.map(doc => {
        const contentLower = doc.content.toLowerCase();
        const titleLower = doc.metadata.title.toLowerCase();
        const keywordsLower = doc.metadata.keywords.map(k => k.toLowerCase());

        let score = 0;
        keywords.forEach(keyword => {
          if (titleLower.includes(keyword)) score += 3;
          if (keywordsLower.some(k => k.includes(keyword))) score += 2;
          if (contentLower.includes(keyword)) score += 1;
        });

        return { doc, score };
      });

      const results = scored
        .filter(item => item.score > 0)
        .sort((a, b) => b.score - a.score)
        .slice(0, k)
        .map(item => item.doc);

      if (results.length === 0) {
        log.info(' No relevant knowledge found for query:', query);
        return '';
      }

      // Format results for prompt injection
      const formattedKnowledge = this._formatResults(results);

      log.info(
        `Retrieved ${results.length} documents for query:`,
        query.substring(0, 50) + '...'
      );

      return formattedKnowledge;
    } catch (error) {
      log.error(' Failed to retrieve knowledge:', error);
      return ''; // Graceful degradation - return empty string
    }
  }

  /**
   * Search for knowledge about a specific video style.
   */
  async searchByStyle(style: string): Promise<string> {
    const query = `${style} style video production characteristics and best practices`;
    return await this.getRelevantKnowledge(query);
  }

  /**
   * Search for knowledge about a specific topic.
   */
  async searchByTopic(topic: string): Promise<string> {
    const query = `best practices for ${topic} in video production`;
    return await this.getRelevantKnowledge(query);
  }

  /**
   * Format search results for prompt injection.
   */
  private _formatResults(results: KnowledgeDocument[]): string {
    const sections = results.map((doc, i) => {
      const title = doc.metadata.title || 'Relevant Knowledge';
      const type = doc.metadata.type || 'unknown';

      return `### ${i + 1}. ${title} (${type})
${doc.content}`;
    });

    return `## RELEVANT KNOWLEDGE FROM VIDEO PRODUCTION GUIDES:

${sections.join('\n\n---\n\n')}

Use this knowledge to inform your recommendations, but adapt it to the user's specific needs.`;
  }

  /**
   * Get initialization status.
   */
  isInitialized(): boolean {
    return this.initialized;
  }

  /**
   * Get document count.
   */
  getDocumentCount(): number {
    const styleCount = Object.keys(STYLE_GUIDES).length;
    const practiceCount = Object.keys(BEST_PRACTICES).length;
    return styleCount + practiceCount;
  }
}

// Export singleton instance
export const knowledgeBase = new VideoProductionKnowledgeBase();
````

## File: packages/shared/src/services/ai/shotBreakdownAgent.ts
````typescript
/**
 * Shot Breakdown Agent
 *
 * AI service that breaks screenplay scenes into 4-6 individual camera shots.
 * Each shot includes cinematography details for image generation.
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { MODELS } from "../shared/apiClient";
import { agentLogger } from "../logger";
import type { ScreenplayScene } from "@/types";
import { withAILogging } from "../aiLogService";
import { type CharacterInput } from "../prompt/imageStyleGuide";

const log = agentLogger.child('ShotBreakdown');

/**
 * Shot type definitions
 */
export type ShotType =
    | 'Wide'
    | 'Medium'
    | 'Close-up'
    | 'Extreme Close-up'
    | 'POV'
    | 'Over-the-shoulder';

export type CameraAngle =
    | 'Eye-level'
    | 'High'
    | 'Low'
    | 'Dutch'
    | "Bird's-eye"
    | "Worm's-eye";

export type CameraMovement =
    | 'Static'
    | 'Pan'
    | 'Tilt'
    | 'Zoom'
    | 'Dolly'
    | 'Tracking'
    | 'Handheld';

/**
 * Individual shot within a scene
 */
export interface Shot {
    id: string;
    sceneId: string;
    shotNumber: number;
    shotType: ShotType;
    cameraAngle: CameraAngle;
    movement: CameraMovement;
    duration: number;
    description: string;
    emotion: string;
    lighting: string;
    scriptSegment?: string;
}

/**
 * Raw shot data from AI response
 */
interface RawShotData {
    shotType?: string;
    cameraAngle?: string;
    movement?: string;
    duration?: number;
    description?: string;
    visualDescription?: string;
    emotion?: string;
    emotionalTone?: string;
    lighting?: string;
    lightingStyle?: string;
    scriptSegment?: string;
}

const SHOT_BREAKDOWN_PROMPT = `You are an expert film director and cinematographer. Your goal is to translate a screenplay scene into a series of dynamic, narrative-driven shots.

SCENE:
Heading: {heading}
Action: {action}
Scene Narration: {sceneNarration}
Genre: {genre}
Characters Present: {characters}
Character Appearance Reference:
{characterAnchors}
Dialogue: {dialogue}
Mood/Vibe: {mood}

CHARACTER CONSISTENCY RULE:
If any character from "Character Appearance Reference" appears in a shot, you MUST include
their specific physical traits (from the reference above) in the shot description.
NEVER just say "the protagonist" — name them and describe their appearance.

CRITICAL INSTRUCTION:
- Do NOT just describe the set (e.g., "A market with spices").
- YOU MUST describe the DRAMATIC ACTION (e.g., "The camera pushes in on Faisal's hand as he hesitates to touch the jar, his fingers trembling.").
- Every shot must have a SUBJECT performing an ACTION with a specific EMOTION.
- MANDATORY: Assign a camera movement to every shot. Never leave the camera static for emotional scenes.

For EACH shot provide:
1. Shot Type: Wide/Medium/Close-up/Extreme Close-up/POV/Over-the-shoulder
2. Camera Angle: Eye-level/High/Low/Dutch/Bird's-eye/Worm's-eye
3. Movement: Pan/Tilt/Zoom/Dolly/Tracking/Handheld (use Static only for contemplative pauses)
4. Duration: 3-8 seconds
5. Description: NARRATIVE-FOCUSED - describe what the character DOES, what the camera REVEALS, and what the viewer FEELS. Include lighting and atmosphere woven into the action.
6. Emotion: Specific emotional beat (not just "tense" - say "mounting dread", "reluctant hope", "quiet defiance")
7. Lighting: How light serves the emotion (e.g., "harsh overhead isolating the figure", "warm rim light suggesting hidden warmth")
8. Script Segment: 1-3 sentences of narration text for this shot, drawn from the Scene Narration above. This will be used as voiceover.

Consider:
- {genre} genre conventions and visual style
- The emotional arc and vibe of the scene
- Characters as ACTORS in a drama, not mannequins in a diorama
- Camera as a storytelling tool that reveals character psychology

Output as JSON array with exactly this format:
\`\`\`json
[
  {
    "shotType": "Close-up",
    "cameraAngle": "Dutch",
    "movement": "Dolly",
    "duration": 5,
    "description": "The camera slowly pushes toward Faisal's face as recognition dawns in his eyes, the narrow alley walls seeming to close in around him, harsh sidelight cutting across his features",
    "emotion": "dawning realization",
    "lighting": "High contrast, claustrophobic sidelight",
    "scriptSegment": "Recognition dawns on Faisal's face as the narrow alley walls close in around him."
  }
]
\`\`\`

IMPORTANT:
- Generate 4-6 shots only
- Each shot must drive the narrative forward, not just establish a location
- Vary shot types and camera movements for dynamic visual rhythm
- Maintain 180-degree rule for dialogue scenes
- Total duration should roughly match scene importance`;

/**
 * Initialize Gemini model for shot breakdown
 */
function getGeminiModel() {
    const apiKey = import.meta.env.VITE_GEMINI_API_KEY;
    if (!apiKey) {
        throw new Error('VITE_GEMINI_API_KEY is required for shot breakdown');
    }

    return new ChatGoogleGenerativeAI({
        apiKey,
        model: MODELS.TEXT,
        temperature: 0.7,
    });
}

/**
 * Parse JSON from AI response (handles markdown code blocks)
 */
function parseJsonFromResponse(content: string): RawShotData[] {
    // Try to extract JSON from markdown code block
    const jsonMatch = content.match(/```(?:json)?\s*([\s\S]*?)```/);
    const jsonStr = (jsonMatch && jsonMatch[1]) ? jsonMatch[1].trim() : content.trim();

    try {
        const parsed = JSON.parse(jsonStr);
        if (!Array.isArray(parsed)) {
            throw new Error('Expected array of shots');
        }
        return parsed;
    } catch (e) {
        log.error('Failed to parse shot breakdown JSON:', e);
        throw new Error(`Failed to parse shot breakdown response: ${e instanceof Error ? e.message : String(e)}`);
    }
}

/**
 * Validate and normalize shot type
 */
function normalizeShot(raw: RawShotData, index: number, sceneId: string): Shot {
    const validShotTypes: ShotType[] = ['Wide', 'Medium', 'Close-up', 'Extreme Close-up', 'POV', 'Over-the-shoulder'];
    const validAngles: CameraAngle[] = ['Eye-level', 'High', 'Low', 'Dutch', "Bird's-eye", "Worm's-eye"];
    const validMovements: CameraMovement[] = ['Static', 'Pan', 'Tilt', 'Zoom', 'Dolly', 'Tracking', 'Handheld'];

    const shotType = validShotTypes.find(t =>
        raw.shotType?.toLowerCase().includes(t.toLowerCase())
    ) || 'Medium';

    const cameraAngle = validAngles.find(a =>
        raw.cameraAngle?.toLowerCase().includes(a.toLowerCase().replace("'", ""))
    ) || 'Eye-level';

    const movement = validMovements.find(m =>
        raw.movement?.toLowerCase().includes(m.toLowerCase())
    ) || 'Static';

    return {
        id: `shot_${sceneId}_${index + 1}`,
        sceneId,
        shotNumber: index + 1,
        shotType,
        cameraAngle,
        movement,
        duration: Math.min(Math.max(raw.duration || 5, 3), 8),
        description: raw.description || raw.visualDescription || 'Scene establishing shot',
        emotion: raw.emotion || raw.emotionalTone || 'neutral',
        lighting: raw.lighting || raw.lightingStyle || 'Natural',
        scriptSegment: raw.scriptSegment || undefined,
    };
}

/**
 * Break a screenplay scene into individual camera shots
 */
export async function breakSceneIntoShots(
    scene: ScreenplayScene,
    genre: string,
    geminiModel?: ChatGoogleGenerativeAI,
    sessionId?: string,
    emotionalContext?: string,
    characters?: CharacterInput[],
): Promise<Shot[]> {
    const model = geminiModel || getGeminiModel();

    // Format dialogue for prompt
    const dialogueStr = scene.dialogue.length > 0
        ? scene.dialogue.map(d => `${d.speaker}: "${d.text}"`).join('\n')
        : 'No dialogue';

    // Format character anchors for prompt
    const characterAnchors = characters?.length
        ? characters.map(c =>
            `  - ${c.name}: ${c.facialTags ?? c.visualDescription.substring(0, 100)}`
          ).join('\n')
        : '  No specific character descriptions provided.';

    // Build prompt with emotional context
    const mood = emotionalContext || 'Cinematic';
    const sceneNarration = scene.action || 'No narration text available';
    const prompt = SHOT_BREAKDOWN_PROMPT
        .replace('{heading}', scene.heading)
        .replace('{action}', scene.action)
        .replace('{sceneNarration}', sceneNarration)
        .replace('{genre}', genre)
        .replace('{characters}', scene.charactersPresent.join(', ') || 'Unknown')
        .replace('{characterAnchors}', characterAnchors)
        .replace('{dialogue}', dialogueStr)
        .replace('{mood}', mood)
        .replace('{genre}', genre); // Second occurrence

    log.info(`Breaking down scene ${scene.sceneNumber} into shots...`);

    try {
        const response = await withAILogging(
            sessionId,
            'shot_breakdown',
            MODELS.TEXT,
            prompt,
            () => model.invoke([{ role: 'user', content: prompt }]),
            (r) => typeof r.content === 'string' ? r.content : JSON.stringify(r.content),
        );
        const content = typeof response.content === 'string'
            ? response.content
            : JSON.stringify(response.content);

        const rawShots = parseJsonFromResponse(content);

        // Validate and normalize shots
        const shots = rawShots.map((raw, idx) => normalizeShot(raw, idx, scene.id));

        log.info(`Generated ${shots.length} shots for scene ${scene.sceneNumber}`);
        return shots;

    } catch (error) {
        log.error(`Shot breakdown failed for scene ${scene.sceneNumber}:`, error);
        throw new Error(`Failed to break down scene ${scene.sceneNumber}: ${error instanceof Error ? error.message : String(error)}`);
    }
}

/**
 * Break all scenes into shots
 */
export async function breakAllScenesIntoShots(
    scenes: ScreenplayScene[],
    genre: string,
    onProgress?: (sceneIndex: number, totalScenes: number) => void,
    sessionId?: string,
    emotionalContexts?: string[],
    characters?: CharacterInput[],
): Promise<Shot[]> {
    const model = getGeminiModel();
    const allShots: Shot[] = [];

    for (let i = 0; i < scenes.length; i++) {
        const scene = scenes[i];
        if (!scene) {
            log.warn(`Skipping undefined scene at index ${i}`);
            continue;
        }

        onProgress?.(i, scenes.length);

        try {
            const emotionalContext = emotionalContexts?.[i];
            const shots = await breakSceneIntoShots(scene, genre, model, sessionId, emotionalContext, characters);
            allShots.push(...shots);
        } catch (error) {
            log.error(`Failed to process scene ${i + 1}:`, error);
            // Continue with other scenes
        }
    }

    log.info(`Total shots generated: ${allShots.length} across ${scenes.length} scenes`);
    return allShots;
}

/**
 * Get shot type style recommendation for a genre
 */
export function getGenreStyleRecommendation(genre: string): {
    preferredShotTypes: ShotType[];
    preferredMovements: CameraMovement[];
    lightingStyle: string;
} {
    const genreLower = genre.toLowerCase();

    if (genreLower.includes('thriller') || genreLower.includes('horror')) {
        return {
            preferredShotTypes: ['Close-up', 'Extreme Close-up', 'POV'],
            preferredMovements: ['Handheld', 'Tracking', 'Zoom'],
            lightingStyle: 'Dramatic/Chiaroscuro',
        };
    }

    if (genreLower.includes('action')) {
        return {
            preferredShotTypes: ['Wide', 'Medium', 'POV'],
            preferredMovements: ['Tracking', 'Pan', 'Handheld'],
            lightingStyle: 'Hard/Dynamic',
        };
    }

    if (genreLower.includes('drama')) {
        return {
            preferredShotTypes: ['Medium', 'Close-up', 'Over-the-shoulder'],
            preferredMovements: ['Static', 'Dolly', 'Pan'],
            lightingStyle: 'Soft/Natural',
        };
    }

    if (genreLower.includes('comedy')) {
        return {
            preferredShotTypes: ['Medium', 'Wide', 'Close-up'],
            preferredMovements: ['Static', 'Pan', 'Zoom'],
            lightingStyle: 'Bright/Natural',
        };
    }

    if (genreLower.includes('sci-fi') || genreLower.includes('scifi')) {
        return {
            preferredShotTypes: ['Wide', 'Extreme Close-up', 'POV'],
            preferredMovements: ['Tracking', 'Dolly', 'Pan'],
            lightingStyle: 'Neon/Cold',
        };
    }

    // Default/Mystery
    return {
        preferredShotTypes: ['Medium', 'Close-up', 'Wide'],
        preferredMovements: ['Static', 'Pan', 'Dolly'],
        lightingStyle: 'Natural',
    };
}
````

## File: packages/shared/src/services/ai/storyPipeline.ts
````typescript
/**
 * Story Pipeline - Discrete LLM Calls for Story Generation
 *
 * Breaks story generation into separate, focused LLM calls to avoid
 * context explosion. Each step only receives the input it needs.
 *
 * Pipeline stages:
 * 1. Topic → Breakdown (minimal context)
 * 2. Breakdown → Screenplay (only breakdown)
 * 3. Screenplay → Characters (only screenplay)
 * 4. Scene → Visual (one scene at a time)
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { z } from "zod";
import { GEMINI_API_KEY, MODELS } from "../shared/apiClient";
import { cleanForTTS } from "../textSanitizer";
import { agentLogger } from "../logger";
import { storyModeStore } from "./production/store";
import type { StoryModeState } from "./production/types";
import type { ScreenplayScene, CharacterProfile, FormatMetadata, VideoFormat } from "@/types";
import { generateImageFromPrompt } from "../imageService";
import { buildImageStyleGuide } from "../prompt/imageStyleGuide";
import { cloudAutosave } from "../cloudStorageService";
import { loadTemplate, substituteVariables } from "../prompt/templateLoader";
import { formatRegistry } from "../formatRegistry";
import { detectLanguage } from "../languageDetector";
import { ParallelExecutionEngine, type Task } from "../parallelExecutionEngine";

const log = agentLogger.child('StoryPipeline');

// --- Schemas for structured output ---

const BreakdownSchema = z.object({
    acts: z.array(z.object({
        title: z.string(),
        emotionalHook: z.string(),
        narrativeBeat: z.string(),
    })).min(3).max(5),
});

const ScreenplaySchema = z.object({
    scenes: z.array(z.object({
        heading: z.string(),
        action: z.string(),
        dialogue: z.array(z.object({
            // Hard cap: a character name is never more than 4 words / 30 chars.
            // This rejects paragraphs mistakenly placed in the speaker field.
            speaker: z.string().max(30).describe(
                "Character name ONLY (1-4 words, e.g. 'Faisal', 'Maya', 'Narrator'). " +
                "NEVER put scene descriptions, emotions, or visual directions here."
            ).refine(
                val => val.trim().split(/\s+/).length <= 4,
                { message: "Speaker must be a character name (≤4 words), not a description" }
            ),
            text: z.string().min(1).describe(
                "The spoken dialogue line. Must not be empty."
            ),
        })),
    })).min(3).max(8),
});

const CharacterSchema = z.object({
    characters: z.array(z.object({
        name: z.string(),
        role: z.string(),
        visualDescription: z.string(),
        facialTags: z.string().optional(),
    })),
});

const VoiceoverSchema = z.object({
    voiceovers: z.array(z.object({
        sceneId: z.string(),
        script: z.string().describe(
            "The voiceover narration rewritten for spoken delivery. " +
            "Include delivery markers: [pause: short|medium|long|beat], " +
            "[emphasis]word[/emphasis], [whisper]text[/whisper], " +
            "[rising-tension]text[/rising-tension], [slow]text[/slow], [breath]."
        ),
    })),
});

// --- Progress callback type ---

export interface StoryProgress {
    stage: 'breakdown' | 'screenplay' | 'characters' | 'voiceover' | 'visuals' | 'complete' | 'error';
    message: string;
    progress?: number; // 0-100
    currentStep?: number;
    totalSteps?: number;
}

// --- Format-Aware Generation Options ---

/**
 * Options for format-specific narrative generation.
 * All fields are optional; when omitted the pipeline falls back to 'movie-animation' defaults.
 */
export interface FormatAwareGenerationOptions {
    /** Video format identifier — defaults to 'movie-animation' */
    formatId?: VideoFormat;
    /** Genre style modifier (e.g., 'Drama', 'Comedy') */
    genre?: string;
    /** Explicit language override — auto-detected from topic if omitted */
    language?: 'ar' | 'en';
    /** Research summary to incorporate in the prompt (from ResearchService) */
    researchSummary?: string;
    /** Formatted citation list to include alongside the research summary */
    researchCitations?: string;
    /** Raw reference document content to treat as primary source material */
    referenceContent?: string;
}

// --- Duration Constraint Helpers ---

/** Estimated narration speech rate (words per second) — ~140 wpm */
const WORDS_PER_SECOND = 140 / 60;

/**
 * Estimate video duration in seconds based on script word count.
 * Assumes a typical narration pace of ~140 words per minute.
 *
 * @param wordCount - Total words in the script
 * @returns Estimated duration in seconds
 */
export function estimateDurationSeconds(wordCount: number): number {
    return Math.ceil(wordCount / WORDS_PER_SECOND);
}

/**
 * Validate whether a script's estimated duration falls within a format's allowed range.
 * Returns a result with `valid`, the `estimatedSeconds`, and an optional human-readable `message`.
 *
 * Requirements: 12.3
 *
 * @param wordCount  - Word count of the combined script (action + dialogue)
 * @param formatMeta - Format metadata containing the durationRange to validate against
 */
export function validateDurationConstraint(
    wordCount: number,
    formatMeta: FormatMetadata
): { valid: boolean; estimatedSeconds: number; message?: string } {
    const estimatedSeconds = estimateDurationSeconds(wordCount);
    const { min, max } = formatMeta.durationRange;

    if (estimatedSeconds < min) {
        const estMin = Math.round(estimatedSeconds / 60);
        const minMin = Math.round(min / 60);
        return {
            valid: false,
            estimatedSeconds,
            message: `Script too short: ~${estMin} min estimated, minimum is ${minMin} min for "${formatMeta.name}"`,
        };
    }

    if (estimatedSeconds > max) {
        const estMin = Math.round(estimatedSeconds / 60);
        const maxMin = Math.round(max / 60);
        return {
            valid: false,
            estimatedSeconds,
            message: `Script too long: ~${estMin} min estimated, maximum is ${maxMin} min for "${formatMeta.name}"`,
        };
    }

    return { valid: true, estimatedSeconds };
}

/** Count words across all screenplay scenes (action + dialogue). */
export function countScriptWords(scenes: ScreenplayScene[]): number {
    return scenes.reduce((total, scene) => {
        const actionWords = scene.action.trim().split(/\s+/).filter(Boolean).length;
        const dialogueWords = scene.dialogue.reduce(
            (d, line) => d + line.text.trim().split(/\s+/).filter(Boolean).length,
            0
        );
        return total + actionWords + dialogueWords;
    }, 0);
}

// --- Pure Prompt Builder Functions (exported for testing) ---

/**
 * Build a breakdown prompt for the given topic and format options.
 * Loads the format-specific template and substitutes all variables.
 *
 * Requirements: 12.1, 12.2, 12.6, 19.1, 21.1, 21.3
 *
 * @param topic   - User's idea or topic
 * @param options - Format-aware generation options
 * @returns Fully resolved prompt string ready for the LLM
 */
export function buildBreakdownPrompt(
    topic: string,
    options: FormatAwareGenerationOptions = {}
): string {
    const {
        formatId = 'movie-animation',
        genre = '',
        language,
        researchSummary,
        researchCitations,
        referenceContent,
    } = options;

    // Auto-detect language from topic if not explicitly set
    const detectedLang: 'ar' | 'en' = language ?? (/[\u0600-\u06FF]/.test(topic) ? 'ar' : 'en');

    // Build optional context blocks (empty string when not provided)
    const researchBlock = researchSummary
        ? `\nRESEARCH CONTEXT:\n${researchSummary}` +
          (researchCitations ? `\nCitations: ${researchCitations}` : '') + '\n\n'
        : '';

    const referenceBlock = referenceContent
        ? `\nREFERENCE MATERIAL (treat as primary source):\n${referenceContent}\n\n`
        : '';

    const langInstruction = detectedLang === 'ar'
        ? 'Write your response entirely in Arabic.'
        : 'Write your response in English.';

    // Duration hints from format registry
    const formatMeta = formatRegistry.getFormat(formatId);
    const minDuration = formatMeta ? Math.round(formatMeta.durationRange.min / 60) : 3;
    const maxDuration = formatMeta ? Math.round(formatMeta.durationRange.max / 60) : 10;

    const template = loadTemplate(formatId, 'breakdown');

    return substituteVariables(template, {
        idea: topic,
        genre: genre || 'General',
        language_instruction: langInstruction,
        research: researchBlock,
        references: referenceBlock,
        minDuration: String(minDuration),
        maxDuration: String(maxDuration),
    });
}

/**
 * Build a screenplay prompt from breakdown acts and format options.
 * Loads the format-specific template and substitutes all variables.
 *
 * Requirements: 12.1, 12.2, 12.6, 21.1, 21.3
 *
 * @param breakdownActs - Structured acts from the breakdown phase
 * @param options       - Format-aware generation options
 * @returns Fully resolved prompt string ready for the LLM
 */
export function buildScreenplayPrompt(
    breakdownActs: { title: string; emotionalHook: string; narrativeBeat: string }[],
    options: FormatAwareGenerationOptions = {}
): string {
    const {
        formatId = 'movie-animation',
        genre = '',
        language,
        researchSummary,
        researchCitations,
        referenceContent,
    } = options;

    const breakdownSample = breakdownActs.map(a => a.title + ' ' + a.narrativeBeat).join(' ');
    const detectedLang: 'ar' | 'en' = language ?? detectLanguage(breakdownSample);

    const breakdownText = breakdownActs.map((act, i) =>
        `Act ${i + 1}: ${act.title}\n- Hook: ${act.emotionalHook}\n- Beat: ${act.narrativeBeat}`
    ).join('\n\n');

    const researchBlock = researchSummary
        ? `\nRESEARCH CONTEXT:\n${researchSummary}` +
          (researchCitations ? `\nCitations: ${researchCitations}` : '') + '\n\n'
        : '';

    const referenceBlock = referenceContent
        ? `\nREFERENCE MATERIAL:\n${referenceContent}\n\n`
        : '';

    const langInstruction = detectedLang === 'ar'
        ? 'Write the screenplay in Arabic.'
        : 'Write the screenplay in English.';

    const template = loadTemplate(formatId, 'screenplay');

    return substituteVariables(template, {
        idea: '',
        genre: genre || 'General',
        language_instruction: langInstruction,
        research: researchBlock,
        references: referenceBlock,
        breakdown: breakdownText,
        actCount: String(breakdownActs.length),
    });
}

// --- Pipeline Functions (Discrete LLM Calls) ---

/**
 * Step 1: Generate narrative breakdown from topic
 * Context: Only the topic (minimal)
 */
async function generateBreakdown(
    topic: string,
    formatOptions?: FormatAwareGenerationOptions
): Promise<{ acts: { title: string; emotionalHook: string; narrativeBeat: string }[] }> {
    log.info('Step 1: Generating breakdown from topic');

    const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey: GEMINI_API_KEY,
        temperature: 0.7,
    }).withStructuredOutput(BreakdownSchema);

    // Use format-aware template when format options are provided; otherwise use the
    // legacy hardcoded prompt for backward compatibility with non-format-aware callers.
    let prompt: string;
    if (formatOptions?.formatId) {
        prompt = buildBreakdownPrompt(topic, formatOptions);
    } else {
        prompt = `You are a story development expert.

Before writing, silently identify:
- Protagonist and their central desire or goal
- Core conflict they face (internal or external)
- Emotional arc: how does the protagonist change from start to finish?
- One key turning point per act

Then create a narrative breakdown for a short video story about:
"${topic}"

Divide into 3-5 acts. For each act provide:
1. Title - A compelling act title referencing a specific story moment (not generic like "Introduction")
2. Emotional Hook - The dominant emotion the audience should feel in this act (grief, awe, tension, triumph...)
3. Narrative Beat - The specific story event or revelation that drives this act forward (name characters, describe the action)

Keep each field concise (1-2 sentences max). Be specific — avoid vague labels like "conflict begins" or "things get harder".`;
    }

    const result = await model.invoke(prompt);
    log.info(`Breakdown complete: ${result.acts.length} acts`);
    return result;
}

/**
 * Step 2: Generate screenplay from breakdown
 * Context: Only the breakdown (not the original topic)
 */
async function generateScreenplay(
    breakdownActs: { title: string; emotionalHook: string; narrativeBeat: string }[],
    formatOptions?: FormatAwareGenerationOptions
): Promise<ScreenplayScene[]> {
    log.info('Step 2: Generating screenplay from breakdown');

    const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey: GEMINI_API_KEY,
        temperature: 0.7,
    }).withStructuredOutput(ScreenplaySchema);

    // Use format-aware template when format options are provided
    let prompt: string;
    if (formatOptions?.formatId) {
        prompt = buildScreenplayPrompt(breakdownActs, formatOptions);
    } else {
        // Legacy hardcoded prompt for backward compatibility
        const breakdownText = breakdownActs.map((act, i) =>
            `Act ${i + 1}: ${act.title}\n- Hook: ${act.emotionalHook}\n- Beat: ${act.narrativeBeat}`
        ).join('\n\n');

        prompt = `Write a short screenplay based on this outline:

${breakdownText}

Create 3-8 scenes. For each scene:
1. Heading - Location/time (e.g., "INT. SPACESHIP - DAY")
2. Action - Visual description of what happens
3. Dialogue - Character lines (if any)

DIALOGUE RULES (CRITICAL — schema will reject violations):
- "speaker" must be the character's NAME ONLY — 1 to 4 words maximum (e.g., "Faisal", "Old Man", "Narrator").
- "speaker" must NEVER contain scene descriptions, emotions, or actions. MAX 30 characters.
- "text" is the spoken/narrated line — it must NEVER be empty.
- If there is no specific speaker, use "Narrator" as the speaker name.
- NEVER put visual descriptions or action text in the "speaker" field.

VALID example:
{"speaker": "Faisal", "text": "What happened to this place?"}

INVALID example (will break the system):
{"speaker": "Faisal walks through the crumbling market, eyes wide with disbelief", "text": ""}

Keep action descriptions vivid but concise.`;
    }

    const result = await model.invoke(prompt);

    // Map to ScreenplayScene format
    const scenes: ScreenplayScene[] = result.scenes.map((s, i) => ({
        id: `scene_${i}`,
        sceneNumber: i + 1,
        heading: s.heading,
        action: s.action,
        dialogue: s.dialogue
            .map(d => {
                // Repair: detect when the LLM put a visual description in the speaker field.
                // A character name is 1-4 words at most. If it's longer, the fields are swapped.
                const speakerWords = d.speaker.trim().split(/\s+/).length;
                const speakerTooLong = speakerWords > 4 || d.speaker.length > 30;

                if (speakerTooLong) {
                    log.warn(`[generateScreenplay] Misaligned speaker field detected ("${d.speaker.substring(0, 50)}...") — recovering as Narrator`);
                    // If text is also empty/short, rescue the description as the spoken text
                    const rescuedText = d.text && d.text.trim().length > 5
                        ? d.text
                        : d.speaker; // fall back to the misplaced content
                    return { speaker: 'Narrator', text: cleanForTTS(rescuedText) };
                }

                return { speaker: d.speaker, text: cleanForTTS(d.text) };
            })
            .filter(d => d.text.trim().length > 0), // drop empty-text entries
        charactersPresent: [],
    }));

    log.info(`Screenplay complete: ${scenes.length} scenes`);
    return scenes;
}

/**
 * Step 3: Extract characters from screenplay
 * Context: Only the screenplay (not breakdown or topic)
 */
async function extractCharactersFromScreenplay(
    scenes: ScreenplayScene[]
): Promise<CharacterProfile[]> {
    log.info('Step 3: Extracting characters from screenplay');

    const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey: GEMINI_API_KEY,
        temperature: 0.3,
    }).withStructuredOutput(CharacterSchema);

    // Build extended scene context (300 chars) and per-speaker dialogue samples
    const scenesSummary = scenes.map(s =>
        `${s.heading}: ${s.action.slice(0, 300)}${s.action.length > 300 ? '...' : ''}`
    ).join('\n');

    // Collect up to 2 sample lines per speaker to give the model voice/personality context
    const dialogueSampleMap = new Map<string, string[]>();
    scenes.forEach(s => s.dialogue.forEach(d => {
        if (!dialogueSampleMap.has(d.speaker)) dialogueSampleMap.set(d.speaker, []);
        const samples = dialogueSampleMap.get(d.speaker)!;
        if (samples.length < 2) samples.push(d.text.slice(0, 80));
    }));
    const dialogueContext = Array.from(dialogueSampleMap.entries())
        .map(([speaker, lines]) => `  ${speaker}: "${lines.join('" / "')}"`)
        .join('\n');

    const prompt = `Extract main characters from this screenplay:

${scenesSummary}

Character dialogue samples:
${dialogueContext}

For each character provide:
1. Name
2. Role (protagonist, antagonist, supporting)
3. Visual Description - Detailed appearance for image generation (age, gender, ethnicity, hair, clothing, distinguishing features). Be specific about: age range, build, skin tone, hair style/color, and 1-2 distinctive outfit items.
4. Facial Tags - REQUIRED: Exactly 5 comma-separated visual keywords that uniquely identify this character (e.g., "sharp jawline, dark curly hair, olive skin, worn leather jacket, silver earring"). These will be embedded in every image prompt to maintain consistency.

Focus on characters with significant presence. Each character must have all 4 fields.`;

    const result = await model.invoke(prompt);

    const characters: CharacterProfile[] = result.characters.map((c, i) => ({
        id: `char_${Date.now()}_${i}`,
        name: c.name,
        role: c.role,
        visualDescription: c.visualDescription,
        facialTags: c.facialTags,
    }));

    log.info(`Characters extracted: ${characters.length}`);
    return characters;
}

/**
 * Generate voiceover scripts from screenplay action text.
 * Rewrites camera-facing action descriptions into spoken narration
 * optimized for TTS delivery, with inline delivery markers.
 *
 * @param scenes - Screenplay scenes with action text
 * @param emotionalHooks - Per-act emotional hooks from breakdown
 * @returns Map of sceneId → voiceover script string (with delivery markers)
 */
export async function generateVoiceoverScripts(
    scenes: ScreenplayScene[],
    emotionalHooks?: string[],
): Promise<Map<string, string>> {
    log.info(`Generating voiceover scripts for ${scenes.length} scenes`);

    const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey: GEMINI_API_KEY,
        temperature: 0.6,
    }).withStructuredOutput(VoiceoverSchema);

    // Build scene context for the LLM
    const sceneDescriptions = scenes.map((s, i) => {
        const emotion = emotionalHooks?.[i] || emotionalHooks?.[0] || '';
        const dialogueText = s.dialogue.length > 0
            ? `\nDialogue: ${s.dialogue.map(d => `${d.speaker}: "${d.text}"`).join(' | ')}`
            : '';
        return `Scene ${i + 1} [id: ${s.id}]${emotion ? ` (mood: ${emotion})` : ''}:\n` +
            `Location: ${s.heading}\n` +
            `Action: ${s.action}${dialogueText}`;
    }).join('\n\n');

    const prompt = `You are a voiceover scriptwriter. Rewrite these screenplay action descriptions into narration scripts optimized for spoken delivery.

SCREENPLAY SCENES:
${sceneDescriptions}

RULES:
1. Convert visual/camera directions into evocative spoken narration (what a narrator would SAY, not what a camera would SEE)
2. Use sensory language: sounds, textures, temperature, movement
3. Keep roughly the same length as the original action text (±20%)
4. Do NOT include character dialogue — only the narrator's voiceover
5. Do NOT include scene headings, metadata labels, or markdown formatting
6. Write in the same language as the original (if Arabic, write Arabic voiceover)

DELIVERY MARKERS — Insert these where appropriate for natural spoken pacing:
- [pause: beat] — After a dramatic reveal or scene transition
- [pause: long] — Before a climactic moment
- [emphasis]key phrase[/emphasis] — On emotionally charged words or character names on first appearance
- [rising-tension]text[/rising-tension] — When intensity builds (chase, confrontation, countdown)
- [slow]text[/slow] — For solemn, reflective, or awe-inspiring moments
- [whisper]text[/whisper] — For secrets, danger, or intimacy
- [breath] — Before a long emotional passage

EXAMPLE:
Action: "Sami hurls a dodgeball with wild intensity, but it misses the target by a wide margin. Rajih stands with crossed arms, his eyes sharp and unyielding."
Voiceover: "[breath] With every fiber of his being, [emphasis]Sami[/emphasis] hurls the ball forward [pause: beat] but it sails wide, kicking up dust where the target once stood. [slow]Rajih watches, arms crossed, his gaze cutting deeper than any throw.[/slow]"

Return one voiceover script per scene, preserving the scene IDs exactly.`;

    try {
        const result = await model.invoke(prompt);

        const voiceoverMap = new Map<string, string>();
        for (const vo of result.voiceovers) {
            voiceoverMap.set(vo.sceneId, vo.script);
        }

        log.info(`Voiceover scripts generated: ${voiceoverMap.size}/${scenes.length}`);
        return voiceoverMap;
    } catch (error) {
        log.error('Voiceover generation failed, falling back to raw action text:', error);
        // Non-fatal: return empty map, caller uses original action text
        return new Map();
    }
}

/**
 * Step 4: Generate character reference images
 * Context: One character at a time
 */
async function generateCharacterReferences(
    characters: CharacterProfile[],
    sessionId: string,
    style: string = "Cinematic",
    onProgress?: (current: number, total: number) => void
): Promise<CharacterProfile[]> {
    log.info(`Step 4: Generating ${characters.length} character references in "${style}" style`);

    const results: CharacterProfile[] = [];

    for (let i = 0; i < characters.length; i++) {
        const char = characters[i];
        if (!char) continue;

        onProgress?.(i + 1, characters.length);
        log.info(`Generating reference ${i + 1}/${characters.length}: ${char.name}`);

        try {
            // Use the project's visual style for the character reference so it
            // matches the art direction of the scene visuals, while keeping
            // character-sheet-specific composition (front + three-quarter view,
            // neutral background, studio lighting).
            const charGuide = buildImageStyleGuide({
                scene: `Character Design Sheet for "${char.name}"`,
                subjects: [{ type: "person", description: char.visualDescription, pose: "front view and three-quarter view, full body" }],
                style,
                background: "neutral white background",
                lighting: { source: "studio softbox", quality: "soft diffused", direction: "rim light accent" },
                composition: { shot_type: "medium shot", camera_angle: "eye-level", framing: "center framing" },
                avoid: ["blur", "darkness", "noise", "low quality", "text", "watermark"],
            });

            const referenceUrl = await generateImageFromPrompt(
                char.visualDescription,  // fallback text (unused when prebuiltGuide is set)
                style,
                char.name,
                "1:1",
                true,       // skipRefine — guide is already complete
                undefined,
                sessionId,
                undefined,
                charGuide,  // prebuiltGuide — avoids double-wrapping
            );

            results.push({
                ...char,
                referenceImageUrl: referenceUrl,
            });
        } catch (error) {
            log.error(`Failed to generate reference for ${char.name}:`, error);
            results.push(char); // Keep character without reference
        }
    }

    return results;
}

/**
 * Build a compact visual anchor for a character (clothing/face tags only, not full bio).
 * This keeps prompts focused on the action while maintaining character consistency.
 */
function buildVisualAnchor(char: CharacterProfile): string {
    // Prefer compact facial tags when available; fall back to truncated description
    if (char.facialTags) {
        return `[${char.name}: ${char.facialTags}]`;
    }
    const desc = char.visualDescription || '';
    const words = desc.split(/\s+/);
    const compact = words.slice(0, 20).join(' ');
    return `[${char.name}: ${compact}]`;
}

/**
 * Step 5: Generate scene visuals in parallel using ParallelExecutionEngine.
 * Supports resume via existingVisuals — scenes already in that list are skipped.
 * A styleExtractionDone flag prevents concurrent style extraction races.
 */
async function generateSceneVisuals(
    scenes: ScreenplayScene[],
    characters: CharacterProfile[],
    sessionId: string,
    style: string = "Cinematic",
    onProgress?: (current: number, total: number) => void,
    emotionalHooks?: string[],
    existingVisuals?: { sceneId: string; imageUrl: string }[],
): Promise<{ sceneId: string; imageUrl: string }[]> {
    log.info(`Step 5: Generating scene visuals (${scenes.length} total)`);

    // Build character visual anchor map (compact, not full bios)
    const charAnchorMap = new Map<string, string>();
    characters.forEach(c => {
        charAnchorMap.set(c.name.toLowerCase(), buildVisualAnchor(c));
    });

    // Build map of already-done visuals for quick lookup
    const existingMap = new Map<string, string>(
        (existingVisuals || []).map(v => [v.sceneId, v.imageUrl])
    );

    // Start results with already-generated visuals
    const results: { sceneId: string; imageUrl: string }[] = [...(existingVisuals || [])];

    // Filter to only scenes that still need generation
    const scenesToProcess = scenes.filter(s => !existingMap.has(s.id));

    if (scenesToProcess.length === 0) {
        log.info('All scene visuals already generated, skipping');
        return results;
    }

    log.info(`Generating ${scenesToProcess.length} new visuals (${existingMap.size} already done)`);

    // Guard to prevent concurrent style extraction races
    let styleExtractionDone = false;

    const tasks: Task<{ sceneId: string; imageUrl: string }>[] = scenesToProcess.map(scene => {
        const sceneIndex = scenes.indexOf(scene);
        return {
            id: scene.id,
            type: 'visual' as const,
            priority: sceneIndex, // lower index = higher priority (process in order)
            retryable: true,
            timeout: 90_000, // Imagen can be slow
            execute: async () => {
                const emotionalVibe = emotionalHooks?.[sceneIndex] || emotionalHooks?.[0] || 'Cinematic';

                const charSubjects = scene.charactersPresent
                    .map(charName => {
                        const anchor = charAnchorMap.get(charName.toLowerCase());
                        return anchor ? { type: "person" as const, description: anchor } : null;
                    })
                    .filter((s): s is { type: "person"; description: string } => s !== null);

                const sceneGuide = buildImageStyleGuide({
                    scene: scene.action,
                    subjects: charSubjects.length > 0 ? charSubjects : undefined,
                    mood: emotionalVibe,
                    style,
                    background: scene.heading,
                });

                const imageUrl = await generateImageFromPrompt(
                    scene.action,
                    style,
                    "",
                    "16:9",
                    true,       // skipRefine — guide is already complete
                    undefined,
                    sessionId,
                    sceneIndex,
                    sceneGuide,
                );

                log.info(`Generated visual for scene ${sceneIndex + 1}: ${scene.id}`);
                return { sceneId: scene.id, imageUrl };
            },
        };
    });

    const alreadyDoneCount = results.length;
    const engine = new ParallelExecutionEngine();
    const taskResults = await engine.execute(tasks, {
        concurrencyLimit: 4,
        retryAttempts: 2, // double-retry: generateImageFromPrompt already has withRetry internally
        retryDelay: 2000,
        exponentialBackoff: true,
        onProgress: (p) => {
            const completedCount = alreadyDoneCount + p.completedTasks;
            onProgress?.(completedCount, scenes.length);
        },
        onTaskFail: (taskId, error) => {
            log.error(`Failed to generate visual for scene ${taskId}:`, error.message);
        },
        onTaskComplete: (taskId, result) => {
            // Only the first completed task triggers style extraction (race guard)
            if (!styleExtractionDone) {
                styleExtractionDone = true;
                // Style extraction is handled by the caller (storyPipeline main flow);
                // flag is here to prevent multiple concurrent attempts within this function.
            }
        },
    });

    // Collect successful results
    for (const result of taskResults) {
        if (result.success && result.data) {
            results.push(result.data);
        }
    }

    log.info(`Scene visuals complete: ${results.length}/${scenes.length} generated`);
    return results;
}

// --- Main Pipeline Entry Point ---

export interface StoryPipelineOptions {
    topic: string;
    sessionId?: string;
    generateCharacterRefs?: boolean;
    generateVisuals?: boolean;
    visualStyle?: string;
    onProgress?: (progress: StoryProgress) => void;
    // Format-aware options (Task 6.1)
    formatId?: VideoFormat;
    genre?: string;
    language?: 'ar' | 'en';
    researchSummary?: string;
    researchCitations?: string;
    referenceContent?: string;
}

export interface StoryPipelineResult {
    success: boolean;
    sessionId: string;
    actCount: number;
    sceneCount: number;
    characterCount: number;
    visualCount: number;
    error?: string;
}

/**
 * Run the complete story pipeline with discrete LLM calls.
 * Each step uses minimal context - no accumulated history.
 */
export async function runStoryPipeline(
    options: StoryPipelineOptions
): Promise<StoryPipelineResult> {
    const {
        topic,
        sessionId = `story_${Date.now()}`,
        generateCharacterRefs = true,
        generateVisuals = true,
        visualStyle = "Cinematic",
        onProgress,
        formatId,
        genre,
        language,
        researchSummary,
        researchCitations,
        referenceContent,
    } = options;

    // Compose format-aware options to pass to internal pipeline steps
    const formatOptions: FormatAwareGenerationOptions | undefined = formatId
        ? { formatId, genre, language, researchSummary, researchCitations, referenceContent }
        : undefined;

    log.info(`Starting story pipeline for: ${topic.slice(0, 50)}...`);

    // Initialize cloud autosave
    cloudAutosave.initSession(sessionId).catch(err => {
        log.warn('Cloud autosave init failed (non-fatal):', err);
    });

    try {
        // Step 1: Topic → Breakdown
        onProgress?.({ stage: 'breakdown', message: 'Creating story outline...', progress: 10 });
        const breakdown = await generateBreakdown(topic, formatOptions);

        // Initialize state with breakdown
        const state: StoryModeState = {
            id: sessionId,
            topic,
            breakdown: breakdown.acts.map(a => `${a.title}: ${a.narrativeBeat}`).join('\n'),
            screenplay: [],
            characters: [],
            shotlist: [],
            currentStep: 'breakdown',
            updatedAt: Date.now(),
            // Persist format metadata in session state (Req 18.3)
            formatId: formatId ?? 'movie-animation',
            language: formatOptions
                ? (language ?? (/[\u0600-\u06FF]/.test(topic) ? 'ar' : 'en'))
                : undefined,
        };
        storyModeStore.set(sessionId, state);

        // Step 2: Breakdown → Screenplay
        onProgress?.({ stage: 'screenplay', message: 'Writing screenplay...', progress: 30 });
        const screenplay = await generateScreenplay(breakdown.acts, formatOptions);

        state.screenplay = screenplay;
        state.currentStep = 'screenplay';
        state.updatedAt = Date.now();
        storyModeStore.set(sessionId, state);

        // Duration constraint validation (Task 6.4 — Requirements 12.3)
        // Non-fatal: log a warning if the script is outside the format's target range.
        if (formatId) {
            const formatMeta = formatRegistry.getFormat(formatId);
            if (formatMeta) {
                const wordCount = countScriptWords(screenplay);
                const durationResult = validateDurationConstraint(wordCount, formatMeta);
                if (!durationResult.valid) {
                    log.warn(`Duration constraint: ${durationResult.message} (${wordCount} words, ~${durationResult.estimatedSeconds}s estimated)`);
                } else {
                    log.info(`Duration OK: ~${durationResult.estimatedSeconds}s for ${formatMeta.name} (${wordCount} words)`);
                }
            }
        }

        // Step 3: Screenplay → Characters (text extraction only, no images yet)
        onProgress?.({ stage: 'characters', message: 'Extracting characters...', progress: 45 });
        let characters = await extractCharactersFromScreenplay(screenplay);

        state.characters = characters;
        state.currentStep = 'characters';
        state.updatedAt = Date.now();
        storyModeStore.set(sessionId, state);

        // Populate charactersPresent on each scene by matching character names
        // against dialogue speakers and action text.
        const charNames = characters.map(c => c.name);
        for (const scene of screenplay) {
            const matched = new Set<string>();
            for (const name of charNames) {
                const nameLower = name.toLowerCase();
                // Check dialogue speakers
                if (scene.dialogue.some(d => d.speaker.toLowerCase() === nameLower)) {
                    matched.add(name);
                    continue;
                }
                // Check action text
                if (scene.action.toLowerCase().includes(nameLower)) {
                    matched.add(name);
                }
            }
            scene.charactersPresent = Array.from(matched);
        }
        state.screenplay = screenplay;
        state.updatedAt = Date.now();
        storyModeStore.set(sessionId, state);

        // Step 4: Generate scene visuals (art step — establishes the visual style)
        // Bridge: carry emotional hooks from breakdown acts to visual generation
        const emotionalHooks = breakdown.acts.map(a => a.emotionalHook);

        let visualCount = 0;
        if (generateVisuals) {
            onProgress?.({
                stage: 'visuals',
                message: 'Generating scene visuals...',
                progress: 55,
                currentStep: 0,
                totalSteps: screenplay.length,
            });

            const visuals = await generateSceneVisuals(
                screenplay,
                characters,
                sessionId,
                visualStyle,
                (current, total) => {
                    onProgress?.({
                        stage: 'visuals',
                        message: `Generating visual ${current}/${total}...`,
                        progress: 55 + (current / total) * 20,
                        currentStep: current,
                        totalSteps: total,
                    });
                },
                emotionalHooks,
            );

            visualCount = visuals.length;

            // Update shotlist with visuals
            state.shotlist = visuals.map((v, i) => ({
                id: `shot_${i}`,
                sceneId: v.sceneId,
                shotNumber: i + 1,
                description: screenplay[i]?.action || '',
                cameraAngle: 'Medium',
                movement: 'Static',
                lighting: 'Cinematic',
                dialogue: screenplay[i]?.dialogue[0]?.text || '',
                imageUrl: v.imageUrl,
            }));
        }

        // Step 5: Generate character references (after art step — uses the same visual style)
        if (generateCharacterRefs && characters.length > 0) {
            onProgress?.({
                stage: 'characters',
                message: 'Generating character reference sheets...',
                progress: 80,
                currentStep: 0,
                totalSteps: characters.length,
            });

            characters = await generateCharacterReferences(
                characters,
                sessionId,
                visualStyle,
                (current, total) => {
                    onProgress?.({
                        stage: 'characters',
                        message: `Generating reference ${current}/${total}...`,
                        progress: 80 + (current / total) * 15,
                        currentStep: current,
                        totalSteps: total,
                    });
                }
            );

            state.characters = characters;
            state.updatedAt = Date.now();
            storyModeStore.set(sessionId, state);
        }

        state.currentStep = 'shotlist';
        state.updatedAt = Date.now();
        storyModeStore.set(sessionId, state);

        onProgress?.({ stage: 'complete', message: 'Story pipeline complete!', progress: 100 });

        log.info(`Pipeline complete: ${breakdown.acts.length} acts, ${screenplay.length} scenes, ${characters.length} characters, ${visualCount} visuals`);

        return {
            success: true,
            sessionId,
            actCount: breakdown.acts.length,
            sceneCount: screenplay.length,
            characterCount: characters.length,
            visualCount,
        };

    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : String(error);
        log.error('Pipeline failed:', errorMessage);

        onProgress?.({ stage: 'error', message: errorMessage });

        return {
            success: false,
            sessionId,
            actCount: 0,
            sceneCount: 0,
            characterCount: 0,
            visualCount: 0,
            error: errorMessage,
        };
    }
}

/**
 * Get estimated token usage for each pipeline step.
 * Useful for cost estimation.
 */
export function estimatePipelineTokens(topicLength: number): {
    breakdown: { input: number; output: number };
    screenplay: { input: number; output: number };
    characters: { input: number; output: number };
    total: { input: number; output: number };
} {
    // Conservative estimates based on typical content
    const breakdown = {
        input: Math.ceil(topicLength / 4) + 200, // topic + prompt
        output: 500, // structured breakdown
    };

    const screenplay = {
        input: 800, // breakdown text + prompt
        output: 2000, // screenplay scenes
    };

    const characters = {
        input: 1500, // screenplay summary + prompt
        output: 800, // character profiles
    };

    return {
        breakdown,
        screenplay,
        characters,
        total: {
            input: breakdown.input + screenplay.input + characters.input,
            output: breakdown.output + screenplay.output + characters.output,
        },
    };
}
````

## File: packages/shared/src/services/ai/studioAgent.ts
````typescript
/**
 * Studio Agent - LangChain-powered AI Agent for Video Creation
 * 
 * A proper AI agent that can:
 * - Understand natural language requests
 * - Plan and execute video creation workflows
 * - Ask clarifying questions
 * - Maintain conversation context
 * - Handle complex multi-step tasks
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HumanMessage, AIMessage, SystemMessage } from "@langchain/core/messages";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { PromptTemplate } from "@langchain/core/prompts";
import { GEMINI_API_KEY, MODELS } from "../shared/apiClient";
import { knowledgeBase } from "./rag/knowledgeBase";
import { exampleLibrary } from "./rag/exampleLibrary";
import { AI_CONFIG } from "./config";
import { agentLogger } from "../logger";

const log = agentLogger.child('Studio');

// Agent action types
export type AgentAction =
  | { type: "generate_music"; params: { prompt: string; style?: string; instrumental?: boolean; title?: string; customMode?: boolean; model?: string } }
  | { type: "create_video"; params: VideoParams }
  | { type: "ask_clarification"; question: string }
  | { type: "respond"; message: string }
  | { type: "modify_settings"; settings: Partial<VideoParams> }
  | { type: "export_video"; format?: string }
  | { type: "show_preview" }
  | { type: "add_vocals"; params: { uploadUrl: string; prompt: string } }
  | { type: "generate_cover"; params: { taskId: string } }
  | { type: "create_music_video"; params: { taskId: string; audioId: string } }
  | { type: "reset" }
  // New actions for unused features
  | { type: "browse_sfx"; params: { category: string; query?: string } }
  | { type: "set_camera_style"; params: { angle?: string; lighting?: string } }
  | { type: "refine_prompt"; params: { promptText: string; intent?: string } }
  | { type: "show_quality_report" }
  | { type: "show_quality_history" }
  | { type: "mix_audio"; params: { includeSfx: boolean; includeMusic: boolean } }
  | { type: "lint_prompt"; params: { promptText: string } };

export interface VideoParams {
  topic: string;
  style: string;
  duration: number;
  mood?: string;
  targetAudience?: string;
  aspectRatio?: string;
  cameraAngle?: string;
  lightingMood?: string;
}

export interface QuickAction {
  id: string;
  label: string;
  labelAr?: string; // Arabic label
  action: AgentAction;
  variant?: 'primary' | 'secondary';
}

export interface AgentResponse {
  action: AgentAction;
  message: string;
  thinking?: string;
  quickActions?: QuickAction[];
}

export interface ConversationMessage {
  role: "user" | "assistant";
  content: string;
}

const SYSTEM_PROMPT = `You are an AI creation assistant for LyricLens, capable of generating both professional videos and full musical tracks.

## REASONING FRAMEWORK (Use for EVERY request):
<thinking>
1. UNDERSTAND: What is the user asking for?
2. EXTRACT: What parameters are provided? What's missing?
3. DECIDE: Can I execute now or need clarification?
4. PLAN: What actions should I take?
5. RESPOND: Provide clear message + quick actions
</thinking>

## Your Capabilities:
1. GENERATE MUSIC using Suno AI (V5 Model) - Full songs, instrumentals, custom lyrics
2. CREATE VIDEOS from any topic (documentaries, stories, educational content)
3. EDIT MUSIC (Extend, Add Vocals, Create Cover/Remix)
4. BROWSE SFX from Freesound library
5. SET CAMERA & LIGHTING styles
6. VIEW QUALITY reports and history
7. MIX AUDIO with SFX and background music
8. REFINE & LINT prompts

## CRITICAL: BE DIRECT - NO PREAMBLE
NEVER say "I'll ask you questions" or "Let me ask a few things" - just ASK THE QUESTIONS DIRECTLY.
BAD: "Great topic! I'll ask you some questions to understand your vision."
GOOD: "Great topic! 🎬 Quick questions:\n1. Duration? (30s/60s/90s/180s)\n2. Style? (Cinematic/Anime/Documentary)\n3. Language for narration?"

## QUICK ACTIONS - ALWAYS INCLUDE BUTTONS
When asking questions or suggesting options, ALWAYS include quickActions array with clickable buttons.
This lets users tap instead of typing. Include 2-4 most common choices as buttons.

## WHEN TO ASK vs WHEN TO EXECUTE

### ASK QUESTIONS when user request is vague:
- "Make a video about X" → Ask about duration, style, language + provide buttons
- "I want music" → Ask about genre, mood, vocals/instrumental + provide buttons

### EXECUTE IMMEDIATELY when user provides enough details:
- "Create a 90s cinematic video about Egypt in Arabic" → Has duration, style, topic, language - GO!
- "Make an upbeat pop song about summer" → Has genre, mood, topic - GO!

## Response Format (JSON):
{
  "action": {
    "type": "ask_clarification" | "create_video" | "generate_music" | "respond" | ...,
    "question": "..." // for ask_clarification
    "params": { ... } // for create_video, generate_music, etc.
  },
  "message": "Your response - if asking questions, THE QUESTIONS GO HERE TOO",
  "thinking": "Brief internal reasoning",
  "quickActions": [
    {"id": "action1", "label": "60s Cinematic", "labelAr": "60 ثانية سينمائي", "action": {"type": "create_video", "params": {...}}, "variant": "primary"},
    {"id": "action2", "label": "30s Short", "labelAr": "30 ثانية قصير", "action": {"type": "create_video", "params": {...}}, "variant": "secondary"}
  ]
}

## Examples:

User: "Make a video about space"
Response: {
  "action": {"type": "ask_clarification", "question": "duration and style"},
  "message": "🚀 Space video! What style works for you?",
  "thinking": "Vague request - offer common presets as buttons",
  "quickActions": [
    {"id": "space-60-cine", "label": "60s Cinematic", "labelAr": "60 ثانية سينمائي", "action": {"type": "create_video", "params": {"topic": "space exploration - planets, stars, and the cosmos", "style": "Cinematic", "duration": 60}}, "variant": "primary"},
    {"id": "space-90-doc", "label": "90s Documentary", "labelAr": "90 ثانية وثائقي", "action": {"type": "create_video", "params": {"topic": "space exploration documentary", "style": "Documentary", "duration": 90}}, "variant": "secondary"},
    {"id": "space-30-short", "label": "30s Short", "labelAr": "30 ثانية قصير", "action": {"type": "create_video", "params": {"topic": "space highlights", "style": "Cinematic", "duration": 30}}, "variant": "secondary"}
  ]
}

User: "اصنع فيديو عن روما القديمة"
Response: {
  "action": {"type": "ask_clarification", "question": "duration and style"},
  "message": "🏛️ روما القديمة! اختر النمط:",
  "thinking": "Arabic user - provide Arabic labels, offer presets",
  "quickActions": [
    {"id": "rome-60-cine", "label": "60s Cinematic", "labelAr": "60 ثانية سينمائي", "action": {"type": "create_video", "params": {"topic": "روما القديمة - الكولوسيوم والأباطرة", "style": "Cinematic", "duration": 60}}, "variant": "primary"},
    {"id": "rome-90-doc", "label": "90s Documentary", "labelAr": "90 ثانية وثائقي", "action": {"type": "create_video", "params": {"topic": "تاريخ روما القديمة", "style": "Documentary", "duration": 90}}, "variant": "secondary"},
    {"id": "rome-60-art", "label": "60s Oil Painting", "labelAr": "60 ثانية لوحة زيتية", "action": {"type": "create_video", "params": {"topic": "روما القديمة", "style": "Oil Painting", "duration": 60}}, "variant": "secondary"}
  ]
}

User: "I want some music"
Response: {
  "action": {"type": "ask_clarification", "question": "type and genre"},
  "message": "🎵 What kind of music?",
  "thinking": "Vague - offer popular genres as buttons",
  "quickActions": [
    {"id": "music-lofi", "label": "Lo-Fi Chill", "labelAr": "لو-فاي هادئ", "action": {"type": "generate_music", "params": {"prompt": "lo-fi chill beats, relaxing", "style": "Lo-Fi", "instrumental": true}}, "variant": "primary"},
    {"id": "music-epic", "label": "Epic Orchestral", "labelAr": "أوركسترا ملحمي", "action": {"type": "generate_music", "params": {"prompt": "epic orchestral cinematic", "style": "Orchestral", "instrumental": true}}, "variant": "secondary"},
    {"id": "music-pop", "label": "Pop Song", "labelAr": "أغنية بوب", "action": {"type": "generate_music", "params": {"prompt": "upbeat pop song", "style": "Pop", "instrumental": false}}, "variant": "secondary"},
    {"id": "music-electronic", "label": "Electronic", "labelAr": "إلكتروني", "action": {"type": "generate_music", "params": {"prompt": "electronic dance music", "style": "Electronic", "instrumental": true}}, "variant": "secondary"}
  ]
}

User: "Create a 90 second cinematic video about ancient Egypt in Arabic"
Response: {
  "action": {"type": "create_video", "params": {"topic": "ancient Egypt - pyramids, pharaohs, and the mysteries of the Nile", "style": "Cinematic", "duration": 90}},
  "message": "🏛️ Perfect! Creating 90s cinematic journey through Ancient Egypt with Arabic narration...",
  "thinking": "Has all details: 90s, cinematic, Egypt, Arabic. Execute immediately.",
  "quickActions": []
}

CAMERA ANGLES: wide establishing shot, medium shot, close-up, extreme close-up, low angle, high angle, over-the-shoulder, dutch angle, tracking shot, aerial/drone view

LIGHTING MOODS: golden hour warm, cool blue moonlight, dramatic chiaroscuro, soft diffused overcast, neon-lit urban glow, harsh midday sun, candlelit warmth, silhouette backlighting, foggy haze, studio three-point

SFX CATEGORIES: desert-wind, ocean-waves, forest-ambience, rain-gentle, thunderstorm, city-traffic, cafe-ambience, marketplace, eerie-ambience, mystical-drone, whispers, heartbeat, tension-drone, hopeful-pad, epic-strings, middle-eastern
`;


class StudioAgent {
  private model: ChatGoogleGenerativeAI;
  private conversationHistory: ConversationMessage[] = [];
  private currentVideoParams: Partial<VideoParams> = {};

  constructor() {
    this.model = new ChatGoogleGenerativeAI({
      model: MODELS.TEXT,
      apiKey: GEMINI_API_KEY,
      temperature: 0.7,
    });

    // Log Phase 2 configuration on initialization
    if (AI_CONFIG.rag.enabled) {
      log.info(' Phase 2 RAG enabled - knowledge base will be used');
    }
  }

  async processMessage(userMessage: string): Promise<AgentResponse> {
    const startTime = Date.now();

    // Phase 2: Get relevant knowledge from knowledge base (RAG)
    let knowledge = '';
    let exampleContext = '';

    if (AI_CONFIG.rag.enabled) {
      try {
        // Get relevant knowledge for the query
        knowledge = await knowledgeBase.getRelevantKnowledge(userMessage);

        // Get similar successful examples
        exampleContext = await exampleLibrary.getExampleContext(userMessage);

        if (knowledge) {
          log.info(' ✅ Retrieved knowledge from knowledge base');
        }
        if (exampleContext) {
          log.info(' ✅ Found similar successful examples');
        }
      } catch (error) {
        log.error(' Failed to retrieve knowledge:', error);
        // Continue without knowledge - graceful degradation
      }
    }

    // Build enhanced message with knowledge context
    let enhancedMessage = userMessage;
    if (knowledge || exampleContext) {
      enhancedMessage = `${knowledge}\n\n${exampleContext}\n\nUser Request: ${userMessage}`;
    }

    // Add enhanced message to history
    this.conversationHistory.push({ role: "user", content: enhancedMessage });

    // Build messages for the model
    const messages = [
      new SystemMessage(SYSTEM_PROMPT),
      ...this.conversationHistory.map(msg =>
        msg.role === "user"
          ? new HumanMessage(msg.content)
          : new AIMessage(msg.content)
      ),
    ];

    // Add context about current state
    // NOTE: Using HumanMessage instead of SystemMessage because Google Generative AI
    // requires SystemMessage to be first in the messages array
    if (Object.keys(this.currentVideoParams).length > 0) {
      messages.push(new HumanMessage(
        `Current video settings: ${JSON.stringify(this.currentVideoParams)}`
      ));
    }

    try {
      const response = await this.model.invoke(messages);
      const responseText = typeof response.content === 'string'
        ? response.content
        : JSON.stringify(response.content);

      // Parse the JSON response
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      if (!jsonMatch) {
        // Fallback if no JSON found
        return this.createFallbackResponse(userMessage, responseText);
      }

      const parsed = JSON.parse(jsonMatch[0]) as AgentResponse;

      // Update conversation history with assistant response
      this.conversationHistory.push({ role: "assistant", content: parsed.message });

      // Update current video params if creating
      if (parsed.action.type === "create_video" && parsed.action.params) {
        this.currentVideoParams = { ...this.currentVideoParams, ...parsed.action.params };
      } else if (parsed.action.type === "modify_settings" && parsed.action.settings) {
        this.currentVideoParams = { ...this.currentVideoParams, ...parsed.action.settings };
      } else if (parsed.action.type === "reset") {
        this.currentVideoParams = {};
      }

      // Log performance
      const duration = Date.now() - startTime;
      log.info(` ✅ Response generated in ${duration}ms`);

      return parsed;
    } catch (error) {
      log.error("Agent error:", error);
      return this.createFallbackResponse(userMessage);
    }
  }

  private createFallbackResponse(userMessage: string, rawResponse?: string): AgentResponse {
    // Try to extract topic from user message
    const topic = this.extractTopicFallback(userMessage);

    // Check for music keywords in fallback
    const isMusicRequest = /\b(song|music|track|beat|audio|soundtrack|instrumental)\b/i.test(userMessage);

    if (topic) {
      if (isMusicRequest) {
        return {
          action: {
            type: "generate_music",
            params: { prompt: topic, instrumental: false }
          },
          message: `Generating music for "${topic}"...`,
          thinking: "Fallback extraction - detected music request"
        };
      }

      return {
        action: {
          type: "create_video",
          params: { topic, style: "Cinematic", duration: 60 }
        },
        message: `Creating a video about "${topic}"...`,
        thinking: "Fallback extraction"
      };
    }

    return {
      action: { type: "respond", message: rawResponse || "I'd be happy to help! Would you like to create a video or generate some music?" },
      message: rawResponse || "I'd be happy to help! Would you like to create a video or generate some music?",
    };
  }

  private extractTopicFallback(input: string): string | null {
    // Remove common prefixes and extract the core topic
    const topic = input
      .replace(/^(please\s+)?(can you\s+)?(i want\s+)?(to\s+)?/i, "")
      .replace(/^(create|make|generate|produce|build)\s+(me\s+)?(a\s+)?/i, "")
      .replace(/^(video|lyric video|music video)\s+(about|on|for|of)?\s*/i, "")
      .trim();

    if (topic.length > 5) {
      return topic;
    }

    // Try to find topic after keywords
    const patterns = [
      /(?:about|for|on|of|featuring|showcasing)\s+(.+?)(?:\.|$)/i,
      /(?:video|create|make)\s+(.+?)(?:\.|$)/i,
    ];

    for (const pattern of patterns) {
      const match = input.match(pattern);
      if (match?.[1] && match[1].length > 5) {
        return match[1].trim();
      }
    }

    return null;
  }

  // Get current video parameters
  getCurrentParams(): Partial<VideoParams> {
    return { ...this.currentVideoParams };
  }

  // Reset conversation
  resetConversation(): void {
    this.conversationHistory = [];
    this.currentVideoParams = {};
  }

  // Get conversation history
  getHistory(): ConversationMessage[] {
    return [...this.conversationHistory];
  }
}

// Export singleton instance
export const studioAgent = new StudioAgent();

// Re-export productionAgent for autonomous video creation
export {
  runProductionAgent,
  getProductionSession,
  clearProductionSession,
  type ProductionProgress
} from "./productionAgent";
````

## File: packages/shared/src/services/ai/subagents/contentSubagent.ts
````typescript
/**
 * Content Subagent - Content Planning, Narration, and Quality Validation
 *
 * This subagent handles the critical content creation stage of the production pipeline.
 * It's responsible for making key decisions about scene count, narrative flow, and quality.
 *
 * Responsibilities:
 * - Create detailed content plan with scenes
 * - Decide optimal scene count based on topic complexity
 * - Generate voice narration for each scene
 * - Validate content quality with iterative improvement
 * - Sync timing between scenes and narration
 *
 * Tools:
 * - plan_video
 * - narrate_scenes
 * - validate_plan
 * - adjust_timing
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HumanMessage, SystemMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { StructuredTool } from "@langchain/core/tools";
import { MODELS } from "../../shared/apiClient";
import { agentLogger } from "../../logger";

const log = agentLogger.child('Content');
import {
  Subagent,
  SubagentName,
  SubagentContext,
  SubagentResult,
} from "./index";
import { productionStore, productionTools } from "../productionAgent";
import { knowledgeBase } from "../rag/knowledgeBase";
import { AI_CONFIG } from "../config";

/**
 * Content Subagent System Prompt
 *
 * Enhanced with AI prompting best practices:
 * - Context-first pattern: Explains critical role in pipeline
 * - Example-driven pattern: Concrete examples with reasoning
 * - Constraint-explicit pattern: Clear decision framework
 * - Validation-oriented pattern: Quality scoring rubric
 * - Session ID pattern: Explicit instructions about session ID handling
 */
const CONTENT_SUBAGENT_PROMPT = `You are the Content Subagent. Your role is to create a comprehensive content plan with narration.

## CRITICAL: SESSION ID HANDLING
When you call plan_video, it returns a sessionId (format: prod_TIMESTAMP_HASH, e.g., prod_1768266562924_r3zdsyfgc).
You MUST use this EXACT sessionId as the contentPlanId parameter for ALL subsequent tool calls:
- narrate_scenes: contentPlanId = sessionId from plan_video
- validate_plan: contentPlanId = sessionId from plan_video
- adjust_timing: contentPlanId = sessionId from plan_video

NEVER use placeholder values like "plan_123", "cp_01", or "session_12345".
ALWAYS use the ACTUAL sessionId returned by plan_video.

CONTEXT:
You receive a topic/transcript and target duration. Your output (ContentPlan + narration audio)
will drive the entire production. Visual assets, music, and timing all depend on YOUR decisions.

## SCENE COUNT DECISION FRAMEWORK (CRITICAL)

YOU decide the optimal scene count. Follow this decision process:

### Step 1: Assess Topic Complexity (Primary Factor)
- **High Complexity** (history, tutorials, multi-step processes):
  Example: "History of Ancient Rome" → Many eras/events → MORE scenes (15-25 for 2min)

- **Medium Complexity** (stories, explanations, demonstrations):
  Example: "How Coffee is Made" → Process steps → MEDIUM scenes (8-12 for 2min)

- **Low Complexity** (quotes, moods, abstract concepts):
  Example: "Motivational quote about success" → Single idea → FEWER scenes (3-5 for 2min)

### Step 2: Apply Duration Constraint
- Baseline: ~10-12 seconds per scene
- Adjust for complexity:
  - Complex topics: 8-10s per scene (faster pacing for information density)
  - Medium topics: 10-12s per scene (standard pacing)
  - Simple topics: 15-20s per scene (slower, contemplative pacing)

### Step 3: Validate Your Decision
Before calling plan_video, ask yourself:
- Does this scene count allow adequate time per scene for narration?
- Will the visual variety be sufficient (not repetitive)?
- Can the narrative flow naturally with this many transitions?

### Examples with Reasoning:
**Example 1**: 90s documentary on Egyptian pyramids
- Complexity: HIGH (architecture, history, construction methods)
- Scene count: 10-12 scenes
- Reasoning: Need time for pyramid exterior, interior chambers, hieroglyphics,
  construction theories, historical context → Each needs 8-10s

**Example 2**: 60s inspirational quote video
- Complexity: LOW (abstract emotional concept)
- Scene count: 4 scenes
- Reasoning: Quote reflection, visual metaphor expansion, emotional climax,
  actionable message → Each needs 15s for weight

**Example 3**: 2min coffee-making tutorial
- Complexity: MEDIUM (step-by-step process)
- Scene count: 12 scenes
- Reasoning: Bean selection, grinding, water temp, brewing, pouring, tasting,
  cleanup → Standard 10s per scene

## QUALITY CONTROL WORKFLOW (MANDATORY)

After generating narration, you MUST run quality validation:

1. Call validate_plan
   - Returns score 0-100 and suggestions

2. If score < 80 AND iterations < 2:
   - Call adjust_timing (syncs scene durations to actual narration lengths)
   - Call validate_plan again
   - Repeat until score >= 80 OR max 2 iterations

3. Report final score and best score achieved

QUALITY SCORING RUBRIC:
- 85-100: Approved. Coherent scenes, good narration match, visual variety.
- 70-84: Needs improvement. Timing mismatches or low variety.
- Below 70: Major issues. Consider replanning.

## YOUR TOOLS:

1. plan_video - Create content plan
   - YOU decide scene count (don't accept user's count blindly)
   - Returns: ContentPlan with scenes[]

2. narrate_scenes - Generate TTS narration
   - Uses Gemini TTS (24kHz, mono, WAV)
   - Returns: Audio blobs + durations + transcripts

3. validate_plan - Check quality
   - Returns: Score + needsImprovement + suggestions

4. adjust_timing - Fix timing mismatches
   - Syncs scene durations to actual narration lengths
   - Increments iteration counter

WORKFLOW:
1. Analyze topic complexity
2. Decide scene count with reasoning
3. Call plan_video
4. Call narrate_scenes
5. Call validate_plan
6. If needed: adjust_timing → validate_plan again
7. Report completion with quality score

When done, report: "Content complete. Score: X/100. Scenes: N. Duration: Xs."
`;

/**
 * Get content tools (plan_video, narrate_scenes, validate_plan, adjust_timing)
 * Filters production tools to only include content-related ones
 */
function getContentTools(): StructuredTool[] {
  return productionTools.filter((tool: StructuredTool) =>
    ["plan_video", "narrate_scenes", "validate_plan", "adjust_timing"].includes(tool.name)
  );
}

/**
 * Create Content Subagent
 */
export function createContentSubagent(apiKey: string): Subagent {
  const contentTools = getContentTools();

  return {
    name: SubagentName.CONTENT,
    description: "Creates content plan, generates narration, validates quality",
    tools: contentTools,
    systemPrompt: CONTENT_SUBAGENT_PROMPT,
    maxIterations: 15, // Needs more iterations for quality loop

    async invoke(context: SubagentContext): Promise<SubagentResult> {
      const startTime = Date.now();

      log.info(` Starting content creation: ${context.instruction}`);
      context.onProgress?.({
        stage: "content_starting",
        message: "Starting content subagent...",
        isComplete: false,
      });

      // Retrieve relevant knowledge from RAG knowledge base
      let ragKnowledge = '';
      if (AI_CONFIG.rag.enabled) {
        try {
          ragKnowledge = await knowledgeBase.getRelevantKnowledge(context.instruction);
          if (ragKnowledge) {
            log.info(' ✅ Retrieved knowledge from knowledge base');
          }
        } catch (error) {
          log.warn(' Failed to retrieve knowledge:', error);
          // Continue without knowledge - graceful degradation
        }
      }

      // Initialize model with tools
      const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey,
        temperature: 0.3, // Balance creativity with consistency
      });

      const modelWithTools = model.bindTools(contentTools);

      // Build enhanced instruction with RAG knowledge context
      const enhancedInstruction = ragKnowledge
        ? `${ragKnowledge}\n\n---\n\nUser Request: ${context.instruction}`
        : context.instruction;

      // Initialize messages
      const messages: (SystemMessage | HumanMessage | AIMessage | ToolMessage)[] = [
        new SystemMessage(CONTENT_SUBAGENT_PROMPT),
        new HumanMessage(enhancedInstruction),
      ];

      let iteration = 0;
      const MAX_ITERATIONS = this.maxIterations;
      let currentSessionId = context.sessionId;

      while (iteration < MAX_ITERATIONS) {
        iteration++;

        context.onProgress?.({
          stage: "content_processing",
          message: `Creating content (iteration ${iteration}/${MAX_ITERATIONS})...`,
          isComplete: false,
        });

        // Get response from model
        const response = await modelWithTools.invoke(messages);
        messages.push(response);

        // Check if model wants to use tools
        if (!response.tool_calls || response.tool_calls.length === 0) {
          // No tool calls - check if content is complete
          const content = response.content as string;

          if (content.includes("Content complete") && content.includes("Score:")) {
            const duration = Date.now() - startTime;

            context.onProgress?.({
              stage: "content_complete",
              message: "Content creation completed successfully",
              isComplete: false,
              success: true,
            });

            return {
              success: true,
              sessionId: currentSessionId || "unknown",
              completedStage: SubagentName.CONTENT,
              duration,
              message: content,
            };
          }

          // Model finished without completing content
          log.warn(" Model finished without completion signal:", content);
          continue; // Give model another chance
        }

        // Execute tool calls
        for (const toolCall of response.tool_calls) {
          const toolName = toolCall.name;

          context.onProgress?.({
            stage: "content_tool_call",
            tool: toolName,
            message: `Executing ${toolName}...`,
            isComplete: false,
          });

          const tool = contentTools.find(t => t.name === toolName);
          if (!tool) {
            throw new Error(`Tool ${toolName} not found`);
          }

          try {
            const result = await tool.invoke(toolCall.args);

            // Extract sessionId if this was plan_video
            if (toolName === "plan_video" && typeof result === "string") {
              try {
                const parsed = JSON.parse(result);
                if (parsed.sessionId) {
                  currentSessionId = parsed.sessionId;
                  log.info(` Session created: ${currentSessionId}`);

                  // Add a reminder message to reinforce the sessionId usage
                  // Using HumanMessage because Google AI requires SystemMessage to be first
                  messages.push(
                    new HumanMessage(
                      `IMPORTANT: The sessionId "${currentSessionId}" has been created. You MUST use this EXACT sessionId as contentPlanId for ALL subsequent tool calls (narrate_scenes, validate_plan, adjust_timing). Do not use any other value.`
                    )
                  );
                }
              } catch (e) {
                // Not JSON, ignore
              }
            }

            // Add tool result to messages
            messages.push(
              new ToolMessage({
                content: typeof result === "string" ? result : JSON.stringify(result),
                tool_call_id: toolCall.id || "",
              })
            );

            context.onProgress?.({
              stage: "content_tool_result",
              tool: toolName,
              message: `✓ ${toolName} completed`,
              isComplete: false,
              success: true,
            });
          } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);

            context.onProgress?.({
              stage: "content_tool_error",
              tool: toolName,
              message: `✗ ${toolName} failed: ${errorMessage}`,
              isComplete: false,
              success: false,
            });

            // Add error message to context
            messages.push(
              new ToolMessage({
                content: JSON.stringify({ error: errorMessage }),
                tool_call_id: toolCall.id || "",
              })
            );
          }
        }
      }

      // Iteration limit reached
      throw new Error(`Content subagent exceeded maximum iterations (${MAX_ITERATIONS})`);
    },
  };
}
````

## File: packages/shared/src/services/ai/subagents/enhancementExportSubagent.ts
````typescript
/**
 * Enhancement/Export Subagent - Post-Processing and Final Export
 *
 * This subagent handles the final stage of the production pipeline.
 * It emphasizes the critical auto-fetch parameter rules for export tools.
 *
 * Responsibilities:
 * - Optionally enhance visuals (background removal, style transfer)
 * - Mix all audio tracks (narration + music + SFX)
 * - Generate subtitles for accessibility
 * - Export final video
 * - Optionally upload to cloud storage (Node.js only)
 *
 * Tools:
 * - remove_background (optional)
 * - restyle_image (optional)
 * - mix_audio_tracks (required)
 * - generate_subtitles (optional)
 * - export_final_video (required)
 * - upload_production_to_cloud (optional, Node.js only)
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HumanMessage, SystemMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { StructuredTool } from "@langchain/core/tools";
import { MODELS } from "../../shared/apiClient";
import { agentLogger } from "../../logger";

const log = agentLogger.child('Enhancement');
import {
  Subagent,
  SubagentName,
  SubagentContext,
  SubagentResult,
} from "./index";
import { productionTools } from "../productionAgent";

// Environment detection - cloud upload only available in Node.js
const isNode = typeof window === 'undefined';

/**
 * Base Enhancement/Export Subagent System Prompt
 * Contains tools 1-5 (no cloud upload) and common sections
 */
const BASE_ENHANCEMENT_EXPORT_PROMPT = `You are the Enhancement/Export Subagent. Your role is to finalize and export the production.

## CRITICAL: SESSION ID USAGE
You will receive a sessionId in your instructions. You MUST use this EXACT sessionId as the contentPlanId parameter for ALL tool calls.

NEVER use placeholder values like "plan_123", "cp_01", "session_123", "current_production", or "content_plan_YYYYMMDD_HHMMSS".
ALWAYS use the ACTUAL sessionId provided in your instructions (format: prod_TIMESTAMP_HASH, e.g., prod_1768266562924_r3zdsyfgc).

CONTEXT:
You receive all assets from prior subagents (visuals, narration, music, SFX). Your output
is the final exported video file.

## YOUR TOOLS:

### Enhancement Tools (OPTIONAL):

1. remove_background - Remove image backgrounds
   - Use when: User wants transparent backgrounds or compositing
   - Input: contentPlanId (USE THE SESSIONID), sceneIndex
   - Fallback: Keep original if fails

2. restyle_image - Apply artistic style transfer
   - Use when: User wants specific art style (Anime, Watercolor, etc.)
   - Input: contentPlanId (USE THE SESSIONID), sceneIndex, style
   - Available styles: Anime, Watercolor, Oil Painting, Sketch, Pop Art, Cyberpunk, etc.
   - Fallback: Keep original if fails

### Export Tools (REQUIRED):

3. mix_audio_tracks - Combine audio sources
   - CRITICAL: Only provide contentPlanId (USE THE SESSIONID)
   - DO NOT provide narrationUrl (auto-fetched from narration segments)
   - DO NOT provide musicUrl (auto-fetched from session state)
   - IMPORTANT: Use dynamic mixing (duckingEnabled: true) - this automatically:
     * Lowers music volume when narration is present (auto-ducking)
     * Adjusts levels based on scene mood and content
     * Provides professional broadcast-quality audio mixing
   - DO NOT use static volume values like "0.3" or "0.2" for all scenes
   - Let the audioMixerService handle intelligent volume balancing

4. generate_subtitles - Create SRT/VTT subtitles
   - CRITICAL: Only provide contentPlanId (USE THE SESSIONID)
   - DO NOT provide narrationSegments (auto-fetched)
   - Supports RTL languages (Arabic, Hebrew)

5. export_final_video - Render final video
   - CRITICAL: Only provide contentPlanId (USE THE SESSIONID)
   - DO NOT provide visuals, narrationUrl, totalDuration (all auto-fetched)
   - Optional: format (mp4/webm), aspectRatio (16:9/9:16/1:1), quality (high/medium/low)
`;

/**
 * Cloud upload tool documentation (Node.js only)
 */
const CLOUD_UPLOAD_TOOL_DOCS = `
6. upload_production_to_cloud - Upload to GCS (OPTIONAL)
   - CRITICAL: Only provide contentPlanId (USE THE SESSIONID)
   - Creates date/time folder (YYYY-MM-DD_HH-mm-ss)
   - Uploads video, audio, visuals, subtitles, logs, metadata
   - Optional: makePublic (default: false)
`;

/**
 * Browser-specific note about cloud upload unavailability
 */
const BROWSER_CLOUD_NOTE = `
## IMPORTANT: BROWSER ENVIRONMENT
Cloud upload (upload_production_to_cloud) is NOT available in browser environment.
After export_final_video completes successfully, your work is DONE.
Do NOT attempt to call upload_production_to_cloud - it does not exist here.
`;

/**
 * Auto-fetch rules section (common)
 */
const AUTO_FETCH_RULES = `
## CRITICAL: AUTO-FETCH PARAMETER RULES

The following parameters are AUTOMATICALLY FETCHED from session state. DO NOT provide them:

| Tool | Auto-Fetched | Why |
|------|-------------|-----|
| mix_audio_tracks | narrationUrl | Concatenated from narration segments |
| export_final_video | visuals, narrationUrl, totalDuration | All in session state |
| generate_subtitles | narrationSegments | Already in session state |
`;

/**
 * Node.js auto-fetch rules (includes cloud upload)
 */
const NODE_AUTO_FETCH_RULES = AUTO_FETCH_RULES + `| upload_production_to_cloud | ALL assets | Everything auto-fetched |
`;

/**
 * Browser workflow section (no cloud upload)
 */
const BROWSER_WORKFLOW_SECTION = `
## WORKFLOW (Browser Environment):

### Standard Workflow:
1. Call mix_audio_tracks({ contentPlanId: "YOUR_SESSIONID" })
2. Call generate_subtitles({ contentPlanId: "YOUR_SESSIONID" })
3. Call export_final_video({ contentPlanId: "YOUR_SESSIONID" })
4. DONE - Report completion immediately

### Enhanced Workflow (With Post-Processing):
1. If user wants background removal: Call remove_background for each scene
2. If user wants style transfer: Call restyle_image for each scene
3. Call mix_audio_tracks
4. Call generate_subtitles
5. Call export_final_video
6. DONE - Report completion immediately

## EXAMPLES:

**Example 1**: Basic export with subtitles (sessionId="prod_1768266562924_r3zdsyfgc")
\`\`\`
1. mix_audio_tracks({ contentPlanId: "prod_1768266562924_r3zdsyfgc" })
2. generate_subtitles({ contentPlanId: "prod_1768266562924_r3zdsyfgc" })
3. export_final_video({ contentPlanId: "prod_1768266562924_r3zdsyfgc", format: "mp4" })
4. Report: "Export complete. Format: mp4. Size: X MB. Duration: Y s. Video available locally."
\`\`\`

**Example 2**: Export with custom aspect ratio (sessionId="prod_1768266562924_r3zdsyfgc")
\`\`\`
1. mix_audio_tracks({ contentPlanId: "prod_1768266562924_r3zdsyfgc" })
2. export_final_video({ contentPlanId: "prod_1768266562924_r3zdsyfgc", format: "mp4", aspectRatio: "9:16" })
3. Report: "Export complete. Format: mp4. Size: X MB. Duration: Y s. Video available locally."
\`\`\`
`;

/**
 * Node.js workflow section (includes cloud upload)
 */
const NODE_WORKFLOW_SECTION = `
## WORKFLOW:

### Standard Workflow (No Enhancements):
1. Call mix_audio_tracks({ contentPlanId: "YOUR_SESSIONID" })
2. Call generate_subtitles({ contentPlanId: "YOUR_SESSIONID" })
3. Call export_final_video({ contentPlanId: "YOUR_SESSIONID" })
4. Optionally: upload_production_to_cloud({ contentPlanId: "YOUR_SESSIONID" })

### Enhanced Workflow (With Post-Processing):
1. If user wants background removal: Call remove_background for each scene
2. If user wants style transfer: Call restyle_image for each scene
3. Call mix_audio_tracks
4. Call generate_subtitles
5. Call export_final_video
6. Optionally: upload_production_to_cloud

## EXAMPLES:

**Example 1**: Basic export with subtitles (sessionId="prod_1768266562924_r3zdsyfgc")
\`\`\`
1. mix_audio_tracks({ contentPlanId: "prod_1768266562924_r3zdsyfgc" })
2. generate_subtitles({ contentPlanId: "prod_1768266562924_r3zdsyfgc" })
3. export_final_video({ contentPlanId: "prod_1768266562924_r3zdsyfgc", format: "mp4" })
\`\`\`

**Example 2**: Export with custom aspect ratio and cloud upload (sessionId="prod_1768266562924_r3zdsyfgc")
\`\`\`
1. mix_audio_tracks({ contentPlanId: "prod_1768266562924_r3zdsyfgc" })
2. export_final_video({ contentPlanId: "prod_1768266562924_r3zdsyfgc", format: "mp4", aspectRatio: "9:16" })
3. upload_production_to_cloud({ contentPlanId: "prod_1768266562924_r3zdsyfgc" })
\`\`\`
`;

/**
 * Quality checks and error recovery section (common)
 */
const QUALITY_AND_ERROR_SECTION = `
## QUALITY CHECKS:

Before export:
- ✓ All scenes have visuals (check session state)
- ✓ Narration audio exists and matches scene count
- ✓ If using music: Music URL is valid
- ✓ Total duration matches ContentPlan.totalDuration

After export:
- ✓ Video file size reasonable (not 0 bytes)
- ✓ Duration matches expected length
- ✓ Format is correct (mp4/webm)

## ERROR RECOVERY:

If export_final_video fails:
- Check that all prior stages completed successfully
- Verify assets exist in session state
- Try with lower quality setting
- If still fails: Return asset bundle for manual assembly
`;

/**
 * Browser completion instruction
 */
const BROWSER_COMPLETION = `
## COMPLETION:
When export_final_video succeeds, immediately report:
"Export complete. Format: [format]. Size: [size] MB. Duration: [duration] s. Video available locally (cloud upload not available in browser)."

Do NOT attempt any cloud upload operations after this.
`;

/**
 * Node.js completion instruction
 */
const NODE_COMPLETION = `
## COMPLETION:
When done, report: "Export complete. Format: X. Size: Y MB. Duration: Z s."
If cloud upload was performed, include the GCS path in your report.
`;


/**
 * Generate environment-specific system prompt
 */
function getSystemPrompt(): string {
  if (isNode) {
    return BASE_ENHANCEMENT_EXPORT_PROMPT +
      CLOUD_UPLOAD_TOOL_DOCS +
      NODE_AUTO_FETCH_RULES +
      NODE_WORKFLOW_SECTION +
      QUALITY_AND_ERROR_SECTION +
      NODE_COMPLETION;
  } else {
    return BASE_ENHANCEMENT_EXPORT_PROMPT +
      BROWSER_CLOUD_NOTE +
      AUTO_FETCH_RULES +
      BROWSER_WORKFLOW_SECTION +
      QUALITY_AND_ERROR_SECTION +
      BROWSER_COMPLETION;
  }
}

/**
 * Get enhancement/export tools - filters to only include tools that exist in productionTools
 */
function getEnhancementExportTools(): StructuredTool[] {
  const desiredTools = [
    "remove_background",
    "restyle_image",
    "mix_audio_tracks",
    "generate_subtitles",
    "export_final_video",
    "upload_production_to_cloud", // Will be filtered out if not in productionTools (browser)
  ];

  const availableTools = productionTools.filter((tool: StructuredTool) =>
    desiredTools.includes(tool.name)
  );

  log.info(` Environment: ${isNode ? 'Node.js' : 'Browser'}`);
  log.info(` Available tools: ${availableTools.map(t => t.name).join(', ')}`);

  return availableTools;
}

/**
 * Create Enhancement/Export Subagent
 */
export function createEnhancementExportSubagent(apiKey: string): Subagent {
  const enhancementExportTools = getEnhancementExportTools();
  const systemPrompt = getSystemPrompt();

  return {
    name: SubagentName.ENHANCEMENT_EXPORT,
    description: isNode
      ? "Post-processes visuals, mixes audio, exports video, uploads to cloud"
      : "Post-processes visuals, mixes audio, exports video (cloud upload unavailable in browser)",
    tools: enhancementExportTools,
    systemPrompt: systemPrompt,
    maxIterations: 20, // Needs iterations for per-scene enhancement + export

    async invoke(context: SubagentContext): Promise<SubagentResult> {
      const startTime = Date.now();

      // CRITICAL: Validate that we have a sessionId
      if (!context.sessionId) {
        throw new Error("EnhancementExportSubagent requires a sessionId from prior stages. Cannot proceed without it.");
      }

      log.info(` Starting enhancement/export with sessionId: ${context.sessionId}`);
      log.info(` Environment: ${isNode ? 'Node.js' : 'Browser'}`);
      context.onProgress?.({
        stage: "export_starting",
        message: "Starting enhancement/export subagent...",
        isComplete: false,
      });

      // Track completed tools to prevent duplicate executions
      const completedTools = new Set<string>();

      // Initialize model with tools
      const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey,
        temperature: 0.2, // Low temperature for precise export
      });

      const modelWithTools = model.bindTools(enhancementExportTools);

      // CRITICAL: Inject sessionId into the instruction so the AI knows what to use
      // Also remind about cloud upload availability based on environment
      const cloudUploadReminder = isNode
        ? "upload_production_to_cloud is available if you want to upload to cloud."
        : "NOTE: Cloud upload is NOT available in browser. After export_final_video, you are DONE.";

      const enhancedInstruction = `IMPORTANT: Your sessionId is "${context.sessionId}". Use this EXACT value as contentPlanId for ALL tool calls.

${context.instruction}

REMINDER: contentPlanId = "${context.sessionId}" for all tools.
${cloudUploadReminder}`;

      // Initialize messages
      const messages: (SystemMessage | HumanMessage | AIMessage | ToolMessage)[] = [
        new SystemMessage(systemPrompt),
        new HumanMessage(enhancedInstruction),
      ];

      let iteration = 0;
      const MAX_ITERATIONS = this.maxIterations;

      while (iteration < MAX_ITERATIONS) {
        iteration++;

        context.onProgress?.({
          stage: "export_processing",
          message: `Processing export (iteration ${iteration}/${MAX_ITERATIONS})...`,
          isComplete: false,
        });

        // Get response from model
        const response = await modelWithTools.invoke(messages);
        messages.push(response);

        // Check if model wants to use tools
        if (!response.tool_calls || response.tool_calls.length === 0) {
          // No tool calls - check if export is complete
          const content = response.content as string;

          // Accept multiple completion patterns
          const isComplete = content.includes("Export complete") &&
            (content.includes("Format:") || content.includes("available locally"));

          if (isComplete) {
            const duration = Date.now() - startTime;

            context.onProgress?.({
              stage: "export_complete",
              message: "Export completed successfully",
              isComplete: false,
              success: true,
            });

            return {
              success: true,
              sessionId: context.sessionId || "unknown",
              completedStage: SubagentName.ENHANCEMENT_EXPORT,
              duration,
              message: content,
            };
          }

          // Model finished without completing export
          log.warn(" Model finished without completion signal:", content);
          continue; // Give model another chance
        }

        // Execute tool calls
        for (const toolCall of response.tool_calls) {
          const toolName = toolCall.name;

          // Check if tool was already completed (prevent duplicate expensive operations)
          if (completedTools.has(toolName)) {
            log.info(` Tool ${toolName} already completed, returning cached result`);
            messages.push(
              new ToolMessage({
                content: JSON.stringify({
                  success: true,
                  cached: true,
                  message: `${toolName} was already executed successfully in this session. Skipping duplicate execution.`
                }),
                tool_call_id: toolCall.id || "",
              })
            );
            continue;
          }

          context.onProgress?.({
            stage: "export_tool_call",
            tool: toolName,
            message: `Executing ${toolName}...`,
            isComplete: false,
          });

          const tool = enhancementExportTools.find(t => t.name === toolName);

          // Graceful handling for missing tools (instead of throwing)
          if (!tool) {
            const isCloudUpload = toolName === 'upload_production_to_cloud';
            const errorMessage = isCloudUpload
              ? `Tool "${toolName}" is not available in browser environment. Cloud upload requires server-side execution. Your video export is complete and available locally.`
              : `Tool "${toolName}" is not available in the current environment.`;

            log.warn(` ${errorMessage}`);

            context.onProgress?.({
              stage: "export_tool_error",
              tool: toolName,
              message: `⚠ ${toolName} unavailable: ${isCloudUpload ? 'browser environment' : 'not found'}`,
              isComplete: false,
              success: false,
            });

            messages.push(
              new ToolMessage({
                content: JSON.stringify({
                  success: false,
                  error: errorMessage,
                  suggestion: isCloudUpload
                    ? "The export workflow is complete. Report completion with the video details. Say: Export complete. Format: [format]. Size: [size] MB. Duration: [duration] s. Video available locally."
                    : "Check tool availability and try an alternative approach."
                }),
                tool_call_id: toolCall.id || "",
              })
            );
            continue; // Continue to next tool call, don't throw
          }

          try {
            const result = await tool.invoke(toolCall.args);

            // Track successful completion
            completedTools.add(toolName);

            // Add tool result to messages
            messages.push(
              new ToolMessage({
                content: typeof result === "string" ? result : JSON.stringify(result),
                tool_call_id: toolCall.id || "",
              })
            );

            context.onProgress?.({
              stage: "export_tool_result",
              tool: toolName,
              message: `✓ ${toolName} completed`,
              isComplete: false,
              success: true,
            });
          } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);

            context.onProgress?.({
              stage: "export_tool_error",
              tool: toolName,
              message: `✗ ${toolName} failed: ${errorMessage}`,
              isComplete: false,
              success: false,
            });

            // Add error message to context
            messages.push(
              new ToolMessage({
                content: JSON.stringify({ error: errorMessage }),
                tool_call_id: toolCall.id || "",
              })
            );
          }
        }
      }

      // Iteration limit reached
      throw new Error(`Enhancement/Export subagent exceeded maximum iterations (${MAX_ITERATIONS})`);
    },
  };
}
````

## File: packages/shared/src/services/ai/subagents/importSubagent.ts
````typescript
/**
 * Import Subagent - YouTube/Audio Import and Transcription
 *
 * This subagent handles the first optional stage of the production pipeline:
 * extracting and transcribing content from external sources (YouTube videos or audio files).
 *
 * Responsibilities:
 * - Import audio from YouTube/X videos
 * - Transcribe uploaded audio files
 * - Return structured transcript for content planning
 *
 * Tools:
 * - import_youtube_content
 * - transcribe_audio_file
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HumanMessage, SystemMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { MODELS } from "../../shared/apiClient";
import { agentLogger } from "../../logger";

const log = agentLogger.child('Import');
import {
  Subagent,
  SubagentName,
  SubagentContext,
  SubagentResult,
} from "./index";
import { importTools } from "../../agent/importTools";

/**
 * Import Subagent System Prompt
 *
 * Follows AI prompting best practices:
 * - Context-first: Explains role and dependencies
 * - Example-driven: Shows concrete input/output examples
 * - Constraint-explicit: Clear rules and requirements
 */
const IMPORT_SUBAGENT_PROMPT = `You are the Import Subagent. Your specialized role is to extract and transcribe content from external sources.

CONTEXT:
You are the first stage in a video production pipeline. Your output (transcript)
will be used by the Content Subagent to plan scenes and narration.

YOUR TOOLS:
1. import_youtube_content - Extract audio from YouTube/X videos
   - Input: URL (youtube.com, youtu.be, twitter.com, x.com)
   - Output: Audio file + metadata + transcript
   - Best for: Existing video content

2. transcribe_audio_file - Transcribe uploaded audio files
   - Input: File path (.mp3, .wav, .m4a, .ogg)
   - Output: Transcript with word-level timing
   - Best for: Custom audio recordings

WORKFLOW:
1. Identify source type (YouTube URL vs audio file)
2. Call appropriate tool
3. Verify transcript quality
4. Report success with transcript preview

CONSTRAINTS:
- Must return valid transcript (non-empty)
- YouTube videos must be accessible (not private/deleted)
- Audio files must be in supported format

QUALITY STANDARDS:
- Transcript should capture all spoken words
- Word-level timing required for lip sync
- Report any audio quality issues

When done, report: "Import complete. Transcript: [first 100 chars]..."
`;

/**
 * Create Import Subagent
 */
export function createImportSubagent(apiKey: string): Subagent {
  return {
    name: SubagentName.IMPORT,
    description: "Handles YouTube/audio import and transcription",
    tools: importTools,
    systemPrompt: IMPORT_SUBAGENT_PROMPT,
    maxIterations: 10,

    async invoke(context: SubagentContext): Promise<SubagentResult> {
      const startTime = Date.now();

      log.info(` Starting import: ${context.instruction}`);
      context.onProgress?.({
        stage: "import_starting",
        message: "Starting import subagent...",
        isComplete: false,
      });

      // Initialize model with tools
      const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey,
        temperature: 0.1, // Low temperature for consistent extraction
      });

      const modelWithTools = model.bindTools(importTools);

      // Initialize messages
      const messages: (SystemMessage | HumanMessage | AIMessage | ToolMessage)[] = [
        new SystemMessage(IMPORT_SUBAGENT_PROMPT),
        new HumanMessage(context.instruction),
      ];

      let iteration = 0;
      const MAX_ITERATIONS = this.maxIterations;

      while (iteration < MAX_ITERATIONS) {
        iteration++;

        context.onProgress?.({
          stage: "import_processing",
          message: `Processing import (iteration ${iteration}/${MAX_ITERATIONS})...`,
          isComplete: false,
        });

        // Get response from model
        const response = await modelWithTools.invoke(messages);
        messages.push(response);

        // Check if model wants to use tools
        if (!response.tool_calls || response.tool_calls.length === 0) {
          // No tool calls - check if import is complete
          const content = response.content as string;

          if (content.includes("Import complete") || content.includes("Transcript:")) {
            const duration = Date.now() - startTime;

            context.onProgress?.({
              stage: "import_complete",
              message: "Import completed successfully",
              isComplete: false,
              success: true,
            });

            return {
              success: true,
              sessionId: context.sessionId || "unknown",
              completedStage: SubagentName.IMPORT,
              duration,
              message: content,
            };
          }

          // Model finished without completing import
          throw new Error("Import subagent finished without completing import task");
        }

        // Execute tool calls
        for (const toolCall of response.tool_calls) {
          const toolName = toolCall.name;

          context.onProgress?.({
            stage: "import_tool_call",
            tool: toolName,
            message: `Executing ${toolName}...`,
            isComplete: false,
          });

          const tool = importTools.find(t => t.name === toolName);
          if (!tool) {
            throw new Error(`Tool ${toolName} not found`);
          }

          try {
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            const result = await (tool as any).invoke(toolCall.args);

            // Add tool result to messages
            messages.push(
              new ToolMessage({
                content: typeof result === "string" ? result : JSON.stringify(result),
                tool_call_id: toolCall.id || "",
              })
            );

            context.onProgress?.({
              stage: "import_tool_result",
              tool: toolName,
              message: `✓ ${toolName} completed`,
              isComplete: false,
              success: true,
            });
          } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);

            context.onProgress?.({
              stage: "import_tool_error",
              tool: toolName,
              message: `✗ ${toolName} failed: ${errorMessage}`,
              isComplete: false,
              success: false,
            });

            // Add error message to context
            messages.push(
              new ToolMessage({
                content: JSON.stringify({ error: errorMessage }),
                tool_call_id: toolCall.id || "",
              })
            );
          }
        }
      }

      // Iteration limit reached
      throw new Error(`Import subagent exceeded maximum iterations (${MAX_ITERATIONS})`);
    },
  };
}
````

## File: packages/shared/src/services/ai/subagents/index.ts
````typescript
/**
 * Subagent System - Base Types and Interfaces
 *
 * This module provides the foundational types and interfaces for the multi-agent
 * production system based on the supervisor + subagent pattern from LangChain.
 *
 * Architecture:
 * - Supervisor Agent: Orchestrates workflow and routes to specialized subagents
 * - Import Subagent: Handles YouTube/audio import and transcription
 * - Content Subagent: Creates content plan, generates narration, validates quality
 * - Media Subagent: Generates visuals, animation, music, and SFX
 * - Enhancement/Export Subagent: Post-processes, mixes audio, exports video
 */

import { StructuredTool } from "@langchain/core/tools";
import { ToolError } from "../../agent/errorRecovery";
import { ProductionProgress } from "../productionAgent";
import { agentLogger } from "../../logger";

const log = agentLogger.child('Subagent');

/**
 * Subagent names enum
 */
export enum SubagentName {
  IMPORT = "import",
  CONTENT = "content",
  MEDIA = "media",
  ENHANCEMENT_EXPORT = "enhancement_export",
}

/**
 * Completed stage information
 */
export interface CompletedStage {
  /** Name of the subagent that completed */
  subagent: SubagentName;
  /** When the stage completed */
  completedAt: number;
  /** Duration in milliseconds */
  duration: number;
  /** Whether the stage succeeded */
  success: boolean;
}

/**
 * User preferences for production
 */
export interface UserPreferences {
  /** Visual style (cinematic, anime, watercolor, etc.) */
  style?: string;
  /** Whether user wants animation */
  animation?: boolean;
  /** Whether user wants background music */
  music?: boolean;
  /** Whether user wants sound effects */
  sfx?: boolean;
  /** Whether user wants subtitles */
  subtitles?: boolean;
  /** Aspect ratio (16:9, 9:16, 1:1) */
  aspectRatio?: string;
  /** Export format (mp4, webm) */
  format?: string;
  /** Whether to upload to cloud */
  uploadToCloud?: boolean;
  /** Make cloud files public */
  makePublic?: boolean;
}

/**
 * Progress callback function type
 */
export type ProgressCallback = (progress: ProductionProgress) => void;

/**
 * Context passed to each subagent invocation
 */
export interface SubagentContext {
  /** Session ID for state management */
  sessionId: string | null;
  /** Instruction/task for the subagent */
  instruction: string;
  /** Stages that have already completed */
  priorStages: CompletedStage[];
  /** User preferences for this production */
  userPreferences: UserPreferences;
  /** Optional progress callback */
  onProgress?: ProgressCallback;
}

/**
 * Result returned by a subagent
 */
export interface SubagentResult {
  /** Whether the subagent succeeded */
  success: boolean;
  /** Session ID (created if null was passed in) */
  sessionId: string;
  /** Stage that was completed */
  completedStage: SubagentName;
  /** Errors encountered (if any) */
  errors?: ToolError[];
  /** Duration in milliseconds */
  duration: number;
  /** Human-readable message */
  message: string;
  /** Whether a fallback was applied */
  fallbackApplied?: boolean;
}

/**
 * Subagent interface
 *
 * Each specialized subagent implements this interface.
 */
export interface Subagent {
  /** Unique name of the subagent */
  name: SubagentName;
  /** Description of the subagent's capabilities */
  description: string;
  /** Tools available to this subagent */
  tools: StructuredTool[];
  /** System prompt for this subagent */
  systemPrompt: string;
  /** Maximum iterations for the subagent loop */
  maxIterations: number;
  /** Invoke the subagent with a context */
  invoke: (context: SubagentContext) => Promise<SubagentResult>;
}

/**
 * Subagent factory function type
 */
export type SubagentFactory = (apiKey: string) => Subagent;

/**
 * Recovery strategy for handling subagent failures
 */
export interface RecoveryStrategy {
  /** Whether to continue production if this subagent fails */
  continueOnFailure: boolean;
  /** Maximum retry attempts */
  maxRetries: number;
  /** Delay between retries in milliseconds */
  initialDelayMs: number;
  /** Fallback action to take if retries fail */
  fallbackAction?: string;
}

/**
 * Get recovery strategy for a subagent
 */
export function getRecoveryStrategy(subagent: SubagentName): RecoveryStrategy {
  switch (subagent) {
    case SubagentName.IMPORT:
      // Import is optional - can continue without it
      return {
        continueOnFailure: true,
        maxRetries: 2,
        initialDelayMs: 1000,
        fallbackAction: "Use topic-based workflow instead of import",
      };

    case SubagentName.CONTENT:
      // Content is critical - cannot continue without it
      return {
        continueOnFailure: false,
        maxRetries: 2,
        initialDelayMs: 1000,
        fallbackAction: undefined,
      };

    case SubagentName.MEDIA:
      // Media is critical but can use placeholders
      return {
        continueOnFailure: true,
        maxRetries: 2,
        initialDelayMs: 1000,
        fallbackAction: "Use placeholder visuals",
      };

    case SubagentName.ENHANCEMENT_EXPORT:
      // Export is critical but can return asset bundle
      return {
        continueOnFailure: true,
        maxRetries: 2,
        initialDelayMs: 1000,
        fallbackAction: "Return asset bundle for manual assembly",
      };

    default:
      return {
        continueOnFailure: false,
        maxRetries: 1,
        initialDelayMs: 1000,
      };
  }
}

/**
 * Execute a subagent with error handling and retry logic
 */
export async function executeSubagent(
  subagent: Subagent,
  context: SubagentContext
): Promise<SubagentResult> {
  const strategy = getRecoveryStrategy(subagent.name);
  let lastError: Error | null = null;

  for (let attempt = 1; attempt <= strategy.maxRetries + 1; attempt++) {
    try {
      const result = await subagent.invoke(context);
      return result;
    } catch (error) {
      lastError = error as Error;
      log.error(`[Subagent:${subagent.name}] Attempt ${attempt} failed:`, error);

      if (attempt <= strategy.maxRetries) {
        const delay = strategy.initialDelayMs * Math.pow(2, attempt - 1);
        log.info(`[Subagent:${subagent.name}] Retrying in ${delay}ms...`);
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
  }

  // All retries failed
  if (strategy.continueOnFailure && strategy.fallbackAction) {
    log.warn(
      `[Subagent:${subagent.name}] All retries failed. Applying fallback: ${strategy.fallbackAction}`
    );

    return {
      success: false,
      sessionId: context.sessionId || "",
      completedStage: subagent.name,
      errors: [
        {
          tool: subagent.name,
          error: lastError?.message || "Unknown error",
          category: "transient" as const,
          timestamp: Date.now(),
          retryCount: strategy.maxRetries,
          recoverable: false,
          fallbackApplied: strategy.fallbackAction,
        },
      ],
      duration: 0,
      message: `Fallback applied: ${strategy.fallbackAction}`,
      fallbackApplied: true,
    };
  }

  // Critical failure - throw error
  throw lastError || new Error(`Subagent ${subagent.name} failed after ${strategy.maxRetries} retries`);
}

/**
 * Calculate overall progress percentage based on subagent stage
 */
export function calculateOverallPercentage(
  subagent: SubagentName,
  subagentPercentage: number
): number {
  // Weight each stage (total should be 100%)
  const stageWeights = {
    [SubagentName.IMPORT]: 10, // Optional, quick
    [SubagentName.CONTENT]: 30, // Critical, complex
    [SubagentName.MEDIA]: 40, // Critical, slow
    [SubagentName.ENHANCEMENT_EXPORT]: 20, // Critical, medium
  };

  const stageOffsets = {
    [SubagentName.IMPORT]: 0,
    [SubagentName.CONTENT]: 10,
    [SubagentName.MEDIA]: 40,
    [SubagentName.ENHANCEMENT_EXPORT]: 80,
  };

  const weight = stageWeights[subagent];
  const offset = stageOffsets[subagent];

  return offset + (subagentPercentage / 100) * weight;
}

// Export subagent factory functions
export { createImportSubagent } from "./importSubagent";
export { createContentSubagent } from "./contentSubagent";
export { createMediaSubagent } from "./mediaSubagent";
export { createEnhancementExportSubagent } from "./enhancementExportSubagent";
export { runSupervisorAgent, type SupervisorOptions, type SupervisorResult } from "./supervisorAgent";
````

## File: packages/shared/src/services/ai/subagents/mediaSubagent.ts
````typescript
/**
 * Media Subagent - Visual and Audio Asset Generation
 *
 * This subagent handles the media generation stage of the production pipeline.
 * It decides which optional features to include based on user intent.
 *
 * Responsibilities:
 * - Generate visual images for all scenes
 * - Optionally animate images to video
 * - Optionally generate background music
 * - Optionally create sound effects plan
 *
 * Tools:
 * - generate_visuals (required)
 * - animate_image (optional)
 * - generate_music (optional)
 * - plan_sfx (optional)
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HumanMessage, SystemMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { StructuredTool } from "@langchain/core/tools";
import { MODELS } from "../../shared/apiClient";
import { agentLogger } from "../../logger";

const log = agentLogger.child('Media');
import {
  Subagent,
  SubagentName,
  SubagentContext,
  SubagentResult,
} from "./index";
import { productionTools } from "../productionAgent";
import { knowledgeBase } from "../rag/knowledgeBase";
import { AI_CONFIG } from "../config";

/**
 * Media Subagent System Prompt
 *
 * Enhanced with:
 * - Constraint-explicit pattern: Clear decision tree for optional tools
 * - Example-driven pattern: Concrete examples of workflow decisions
 * - Validation-oriented pattern: Quality tips for consistency
 * - Session ID pattern: Explicit instructions to use provided sessionId
 */
const MEDIA_SUBAGENT_PROMPT = `You are the Media Subagent. Your role is to generate visual assets.

## CRITICAL: SESSION ID USAGE
You will receive a sessionId in your instructions. You MUST use this EXACT sessionId as the contentPlanId parameter for ALL tool calls.

NEVER use placeholder values like "plan_123", "cp_01", "session_123", "prod_video_plan", or "content_plan_YYYYMMDD_HHMMSS".
ALWAYS use the ACTUAL sessionId provided in your instructions (format: prod_TIMESTAMP_HASH, e.g., prod_1768266562924_r3zdsyfgc).

CONTEXT:
You receive a ContentPlan from the Content Subagent. Your output (images/videos + SFX)
will be combined by the Enhancement/Export Subagent into the final video.

NOTE: Background music generation is NOT available in video production mode.
Music generation is only available in the dedicated "Generate Music" mode.

## YOUR TOOLS:

1. generate_visuals (REQUIRED - Always call first)
   - Generates images for ALL scenes simultaneously
   - Input: contentPlanId (USE THE SESSIONID FROM YOUR INSTRUCTIONS)
   - Output: GeneratedImage[] (one per scene)
   - IMPORTANT: Only call ONCE (don't retry to "improve")

2. plan_sfx (OPTIONAL - Call if user wants sound effects)
   - Creates ambient sound plan
   - Input: contentPlanId (USE THE SESSIONID)
   - Output: SFX plan with mood-based sounds

## DECISION TREE:

### Step 1: Detect User Intent (from supervisor instructions)
**SMART DEFAULTS** - Users expect "video" to mean moving pictures with sound!

#### Animation Detection:
- **DISABLED**: Animation (image-to-video) is currently SUSPENDED.
- **VEO VIDEO**: You can still generate *native* videos using generate_visuals if configured, but do not animate static images.

#### SFX Detection:
- **EXPLICIT YES**: User says "sound effects", "ambient sounds", "sfx", "audio atmosphere"
- **EXPLICIT NO**: User says "no sfx", "silent", "music only"
- **SMART DEFAULT**: ALWAYS include SFX for immersive styles:
  - "Cinematic", "Documentary" → Environmental ambience
  - "Horror", "Mystery" → Atmospheric tension sounds
  - "Nature", "Travel" → Natural environment sounds
  - "Commercial", "Ad" → Clean, professional ambience
- Only skip SFX if explicitly disabled or for minimalist/tutorial content

### Step 2: Execute Required Tools
Always execute:
- generate_visuals (required for all videos)

### Step 3: Execute Optional Tools (SMART DEFAULTS APPLY)
Animation:
- **SKIP**: Do not call animate_image.

SFX (smart default ON for immersive styles):
- Call plan_sfx

## EXAMPLES:

**Example 1**: "Create a cinematic video about space exploration" with sessionId="prod_1768266562924_r3zdsyfgc"
- Animation: DISABLED
- SFX: "Cinematic" + "space" → SMART DEFAULT: YES, atmospheric ambience
- Workflow:
  1. generate_visuals({ contentPlanId: "prod_1768266562924_r3zdsyfgc" })
  2. plan_sfx({ contentPlanId: "prod_1768266562924_r3zdsyfgc" })

**Example 2**: "Make a static slideshow tutorial" with sessionId="prod_1768266562924_r3zdsyfgc"
- Animation: DISABLED
- SFX: Tutorial style → Optional, skip unless requested
- Workflow: generate_visuals({ contentPlanId: "prod_1768266562924_r3zdsyfgc" })

## CONSTRAINTS:

- generate_visuals: Must have ContentPlan with scene visualDescriptions
- plan_sfx: Must have ContentPlan with scene emotionalTone

## QUALITY TIPS:

- Visual consistency: Ensure all scenes follow same style/theme
- SFX balance: Ambient sounds should complement, not overpower

When done, report: "Media complete. Visuals: N scenes. Animation: Suspended (static images only)."
`;

/**
 * Get media tools (generate_visuals, plan_sfx)
 * NOTE: generate_music is excluded - Suno music generation is only available
 * in the dedicated "Generate Music" mode, not in video production
 * NOTE: animate_image is excluded - Suspended by user request
 */
function getMediaTools(): StructuredTool[] {
  return productionTools.filter((tool: StructuredTool) =>
    ["generate_visuals", "plan_sfx"].includes(tool.name)
  );
}

/**
 * Create Media Subagent
 */
export function createMediaSubagent(apiKey: string): Subagent {
  const mediaTools = getMediaTools();

  return {
    name: SubagentName.MEDIA,
    description: "Generates visual and audio assets (images, animation, music, SFX)",
    tools: mediaTools,
    systemPrompt: MEDIA_SUBAGENT_PROMPT,
    maxIterations: 20, // Needs many iterations for per-scene animation

    async invoke(context: SubagentContext): Promise<SubagentResult> {
      const startTime = Date.now();

      // CRITICAL: Validate that we have a sessionId
      if (!context.sessionId) {
        throw new Error("MediaSubagent requires a sessionId from the Content stage. Cannot proceed without it.");
      }

      log.info(` Starting media generation with sessionId: ${context.sessionId}`);
      context.onProgress?.({
        stage: "media_starting",
        message: "Starting media subagent...",
        isComplete: false,
      });

      // Retrieve relevant knowledge from RAG knowledge base for style best practices
      let ragKnowledge = '';
      if (AI_CONFIG.rag.enabled) {
        try {
          // Search for style-specific and visual generation best practices
          ragKnowledge = await knowledgeBase.getRelevantKnowledge(
            `${context.instruction} visual style best practices image generation`
          );
          if (ragKnowledge) {
            log.info(' ✅ Retrieved visual style knowledge from knowledge base');
          }
        } catch (error) {
          log.warn(' Failed to retrieve knowledge:', error);
          // Continue without knowledge - graceful degradation
        }
      }

      // Initialize model with tools
      const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey,
        temperature: 0.4, // Higher for creative visual generation
      });

      const modelWithTools = model.bindTools(mediaTools);

      // Build RAG context block if knowledge is available
      const ragContextBlock = ragKnowledge
        ? `\n\n## VISUAL STYLE BEST PRACTICES (from knowledge base):\n${ragKnowledge}\n\n---\n`
        : '';

      // CRITICAL: Inject sessionId and RAG knowledge into the instruction
      const enhancedInstruction = `IMPORTANT: Your sessionId is "${context.sessionId}". Use this EXACT value as contentPlanId for ALL tool calls.
${ragContextBlock}
${context.instruction}

REMINDER: contentPlanId = "${context.sessionId}" for all tools (generate_visuals, animate_image, plan_sfx)`;

      // Initialize messages
      const messages: (SystemMessage | HumanMessage | AIMessage | ToolMessage)[] = [
        new SystemMessage(MEDIA_SUBAGENT_PROMPT),
        new HumanMessage(enhancedInstruction),
      ];

      let iteration = 0;
      const MAX_ITERATIONS = this.maxIterations;

      while (iteration < MAX_ITERATIONS) {
        iteration++;

        context.onProgress?.({
          stage: "media_processing",
          message: `Generating media assets (iteration ${iteration}/${MAX_ITERATIONS})...`,
          isComplete: false,
        });

        // Get response from model
        const response = await modelWithTools.invoke(messages);
        messages.push(response);

        // Check if model wants to use tools
        if (!response.tool_calls || response.tool_calls.length === 0) {
          // No tool calls - check if media is complete
          const content = response.content as string;

          if (content.includes("Media complete") && content.includes("Visuals:")) {
            const duration = Date.now() - startTime;

            context.onProgress?.({
              stage: "media_complete",
              message: "Media generation completed successfully",
              isComplete: false,
              success: true,
            });

            return {
              success: true,
              sessionId: context.sessionId || "unknown",
              completedStage: SubagentName.MEDIA,
              duration,
              message: content,
            };
          }

          // Model finished without completing media
          log.warn(" Model finished without completion signal:", content);
          continue; // Give model another chance
        }

        // Execute tool calls
        for (const toolCall of response.tool_calls) {
          const toolName = toolCall.name;

          context.onProgress?.({
            stage: "media_tool_call",
            tool: toolName,
            message: `Executing ${toolName}...`,
            isComplete: false,
          });

          const tool = mediaTools.find(t => t.name === toolName);
          if (!tool) {
            throw new Error(`Tool ${toolName} not found`);
          }

          try {
            const result = await tool.invoke(toolCall.args);

            // Add tool result to messages
            messages.push(
              new ToolMessage({
                content: typeof result === "string" ? result : JSON.stringify(result),
                tool_call_id: toolCall.id || "",
              })
            );

            context.onProgress?.({
              stage: "media_tool_result",
              tool: toolName,
              message: `✓ ${toolName} completed`,
              isComplete: false,
              success: true,
            });
          } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);

            context.onProgress?.({
              stage: "media_tool_error",
              tool: toolName,
              message: `✗ ${toolName} failed: ${errorMessage}`,
              isComplete: false,
              success: false,
            });

            // Add error message to context
            messages.push(
              new ToolMessage({
                content: JSON.stringify({ error: errorMessage }),
                tool_call_id: toolCall.id || "",
              })
            );
          }
        }
      }

      // Iteration limit reached
      throw new Error(`Media subagent exceeded maximum iterations (${MAX_ITERATIONS})`);
    },
  };
}
````

## File: packages/shared/src/services/ai/subagents/supervisorAgent.ts
````typescript
/**
 * Supervisor Agent - Multi-Agent Orchestration
 *
 * The supervisor coordinates specialized subagents to complete video production workflows.
 * It analyzes user intent, routes to appropriate subagents, manages state transitions,
 * and consolidates progress reporting.
 *
 * Architecture:
 * SUPERVISOR (this agent)
 *  ├── delegate_to_import_subagent (optional)
 *  ├── delegate_to_content_subagent (required)
 *  ├── delegate_to_media_subagent (required)
 *  └── delegate_to_enhancement_export_subagent (required)
 *
 * Based on LangChain's supervisor + subagent pattern:
 * - Stateless subagents (no conversation history)
 * - Centralized memory in supervisor
 * - Tool-based invocation
 * - Sequential execution with dependencies
 */

import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HumanMessage, SystemMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { agentLogger } from "../../logger";

const log = agentLogger.child('Supervisor');
import {
  CompletedStage,
  ProgressCallback,
  executeSubagent,
} from "./index";
import { MODELS } from "../../shared/apiClient";
import { createImportSubagent } from "./importSubagent";
import { createContentSubagent } from "./contentSubagent";
import { createMediaSubagent } from "./mediaSubagent";
import { createEnhancementExportSubagent } from "./enhancementExportSubagent";
import { analyzeIntent, generateIntentHint } from "../../agent/intentDetection";

/**
 * Supervisor Agent System Prompt
 *
 * Enhanced with AI prompting best practices:
 * - Context-first: Explains orchestration role
 * - Example-driven: Multiple workflow examples
 * - Constraint-explicit: Clear rules for delegation
 * - Session ID pattern: Explicit instructions for session ID handling
 */
const SUPERVISOR_AGENT_PROMPT = `You are the Production Supervisor Agent. Your role is to orchestrate a multi-agent video production pipeline.

## CRITICAL: SESSION ID MANAGEMENT

The sessionId is the KEY to the entire production. It links all stages together.

**How it works:**
1. delegate_to_content_subagent creates a sessionId (format: prod_TIMESTAMP_HASH, e.g., prod_1768266562924_r3zdsyfgc)
2. You MUST pass this EXACT sessionId to delegate_to_media_subagent
3. You MUST pass this EXACT sessionId to delegate_to_enhancement_export_subagent

**NEVER use placeholder values like:**
- "plan_123", "cp_01", "session_123"
- "prod_video_plan", "content_plan_20250124_123456"
- "current_production", "video_session"

**ALWAYS use the ACTUAL sessionId returned by delegate_to_content_subagent.**

CONTEXT:
You coordinate specialized subagents that handle different stages of production. You maintain
the overall workflow, manage state transitions, and ensure all stages complete successfully.

## YOUR SPECIALIZED SUBAGENTS:

You have access to these subagent delegation tools:

1. delegate_to_import_subagent
   - When: User provides YouTube URL or audio file path
   - Returns: ImportedContent with transcript
   - Optional: Can skip if user provides topic directly

2. delegate_to_content_subagent (REQUIRED)
   - When: Always (every production needs content plan + narration)
   - Returns: ContentPlan + NarrationSegment[] + quality score + **sessionId**
   - Critical: This determines scene count and timing for all downstream stages
   - **IMPORTANT: Extract the sessionId from the response and use it for ALL subsequent calls**

3. delegate_to_media_subagent (REQUIRED)
   - When: Always (every production needs visuals)
   - Returns: GeneratedImage[] + optional SFX
   - Note: Animation, SFX are optional based on user request
   - NOTE: Music generation is NOT available in video production mode
   - **REQUIRED: Pass the sessionId from delegate_to_content_subagent**

4. delegate_to_enhancement_export_subagent (REQUIRED)
   - When: Always (every production needs export)
   - Returns: ExportResult + optional cloud upload
   - Note: Enhancement tools optional, export required
   - **REQUIRED: Pass the SAME sessionId from delegate_to_content_subagent**

## WORKFLOW ORCHESTRATION:

### Step 1: Analyze User Intent (Your Job)

Detect from user request:
- **Import Source**: YouTube URL? Audio file? Topic only?
- **Animation**: Keywords like "animated", "motion", "video", "moving"
- **Style**: Keywords like "cinematic", "anime", "watercolor", "documentary"
- **Subtitles**: Keywords like "subtitles", "captions", "accessibility"
- **Aspect Ratio**: Keywords like "portrait", "vertical", "square", "TikTok", "Instagram"
- NOTE: Music generation is not available in video production mode

### Step 2: Route to Subagents Sequentially

CRITICAL: Subagents must execute in this order (dependencies):

STAGE 1: IMPORT (Optional)
If user provides YouTube URL or audio file
→ delegate_to_import_subagent
→ Wait for completion → Extract transcript

STAGE 2: CONTENT (Required)
→ delegate_to_content_subagent
  Pass: topic (from import or user), duration
→ Wait for completion → Verify ContentPlan

STAGE 3: MEDIA (Required)
→ delegate_to_media_subagent
  Pass: animation=true/false, sfx=true/false
→ Wait for completion → Verify visuals

STAGE 4: ENHANCEMENT/EXPORT (Required)
→ delegate_to_enhancement_export_subagent
  Pass: enhancement options, export format
→ Wait for completion → Verify video export

### Step 3: State Coordination

**Session ID Management**:
- If import stage runs: Use sessionId from import
- If no import: Content subagent creates sessionId
- Pass SAME sessionId to all subsequent subagents
- DO NOT create new sessionIds for each stage

**State Verification Between Stages**:
After each subagent completes, verify expected state updates:
- IMPORT: \`importedContent\` must be set
- CONTENT: \`contentPlan\` and \`narrationSegments\` must be set
- MEDIA: \`visuals\` must be set (length = scene count)
- EXPORT: \`exportResult\` must be set

**Handling Missing State**:
If expected state is missing after subagent completes:
- Log error with subagent name and expected field
- Retry subagent once
- If still fails: Abort with partial success report

### Step 4: Error Recovery Strategy

| Error Type | Action |
|-----------|--------|
| **IMPORT fails** | Continue with topic-based workflow (don't abort) |
| **CONTENT fails** | ABORT (can't proceed without content plan) |
| **MEDIA fails** | RETRY once, then use placeholder visuals |
| **EXPORT fails** | RETRY once, then return asset bundle |

**Critical Path**: CONTENT → MEDIA → EXPORT
- If critical path fails after retries: ABORT
- If optional stages fail: CONTINUE with fallback

### Step 5: Progress Aggregation

Report progress at these key points:
1. "Starting production..." (before any subagent)
2. "Import stage: [status]" (if import stage runs)
3. "Content stage: Planning... Narrating... Validating..."
4. "Media stage: Generating visuals... [Animation if applicable]"
5. "Export stage: Mixing audio... Generating subtitles... Rendering video..."
6. "Production complete! Duration: Xs, Size: Y MB"

## EXAMPLES:

**Example 1**: Simple topic-based video
User: "Create a 60-second video about coffee history"

Intent Analysis:
- No import needed (topic provided)
- No animation keywords
- Style: Default (cinematic)

Workflow:
1. delegate_to_content_subagent({
     sessionId: null,  // Will create new
     topic: "coffee history",
     targetDuration: 60,
     style: "Cinematic"
   })
   // Returns: { "sessionId": "prod_xxx", "success": true, ... }

2. delegate_to_media_subagent({
     sessionId: "prod_xxx",  // USE THE SAME sessionId from step 1
     animation: false,
     sfx: false
   })

3. delegate_to_enhancement_export_subagent({
     sessionId: "prod_xxx",  // USE THE SAME sessionId from step 1
     format: "mp4",
     aspectRatio: "16:9"
   })

**Example 2**: YouTube import with animation
User: "Import this YouTube video and create an animated version: https://youtube.com/watch?v=abc123"

Intent Analysis:
- Import needed (YouTube URL detected)
- Animation: "animated" keyword found
- Style: Default

Workflow:
1. delegate_to_import_subagent({
     url: "https://youtube.com/watch?v=abc123"
   })
   // Returns: { "sessionId": "prod_yyy", "success": true, ... }

2. delegate_to_content_subagent({
     sessionId: "prod_yyy",  // REUSE import sessionId
     topic: [transcript from import],
     targetDuration: [video duration from import]
   })

3. delegate_to_media_subagent({
     sessionId: "prod_yyy",  // USE THE SAME sessionId
     animation: true,
     sfx: false
   })

4. delegate_to_enhancement_export_subagent({
     sessionId: "prod_yyy",  // USE THE SAME sessionId
     format: "mp4"
   })

**Example 3**: Vertical video with subtitles and cloud upload
User: "Create a 90-second vertical video about Ancient Egypt with subtitles, upload to cloud"

Intent Analysis:
- No import needed
- Vertical: aspectRatio = "9:16"
- Subtitles: generate_subtitles = true
- Cloud upload: upload_production_to_cloud = true

Workflow:
1. delegate_to_content_subagent({
     sessionId: null,
     topic: "Ancient Egypt",
     targetDuration: 90
   })
   // Returns: { "sessionId": "prod_zzz", "success": true, ... }

2. delegate_to_media_subagent({
     sessionId: "prod_zzz",  // USE THE SAME sessionId from step 1
     animation: false,
     sfx: false
   })

3. delegate_to_enhancement_export_subagent({
     sessionId: "prod_zzz",  // USE THE SAME sessionId from step 1
     format: "mp4",
     aspectRatio: "9:16",
     generateSubtitles: true,
     uploadToCloud: true
   })

## CONSTRAINTS:

- DO NOT skip required subagents (CONTENT, MEDIA, EXPORT always run)
- DO NOT run subagents in parallel (sequential execution required)
- DO NOT create multiple sessionIds (reuse from first stage)
- DO NOT call subagent tools yourself (delegate via subagent wrappers)

## SUCCESS CRITERIA:

Production is successful when:
- ✓ All required subagents completed
- ✓ State transitions verified
- ✓ Final video exported or asset bundle provided
- ✓ No critical path failures

When done, report: "Production complete! [summary of assets created]"
`;

/**
 * Supervisor Agent Options
 */
export interface SupervisorOptions {
  apiKey: string;
  userRequest: string;
  onProgress?: ProgressCallback;
}

/**
 * Supervisor Agent Result
 */
export interface SupervisorResult {
  success: boolean;
  sessionId: string | null;
  completedStages: CompletedStage[];
  message: string;
  duration: number;
}

/**
 * Run Supervisor Agent
 *
 * This is the main entry point for the multi-agent production system.
 */
export async function runSupervisorAgent(options: SupervisorOptions): Promise<SupervisorResult> {
  const { apiKey, userRequest, onProgress } = options;
  const startTime = Date.now();

  log.info(" Starting production:", userRequest);
  onProgress?.({
    stage: "supervisor_starting",
    message: "Analyzing request and planning workflow...",
    isComplete: false,
  });

  // Create subagent instances
  const importSubagent = createImportSubagent(apiKey);
  const contentSubagent = createContentSubagent(apiKey);
  const mediaSubagent = createMediaSubagent(apiKey);
  const enhancementExportSubagent = createEnhancementExportSubagent(apiKey);

  // Create delegation tools
  const delegateToImportTool = tool(
    async ({ url, filePath }) => {
      const instruction = url ? `Import from ${url}` : `Transcribe ${filePath}`;
      const result = await executeSubagent(importSubagent, {
        sessionId: null,
        instruction,
        priorStages: [],
        userPreferences: {},
        onProgress,
      });
      return JSON.stringify(result);
    },
    {
      name: "delegate_to_import_subagent",
      description: "Delegate YouTube/audio import to specialized Import Subagent. Returns sessionId and transcript.",
      schema: z.object({
        url: z.string().optional().describe("YouTube/X URL to import"),
        filePath: z.string().optional().describe("Path to audio file to transcribe"),
      }),
    }
  );

  const delegateToContentTool = tool(
    async ({ sessionId, topic, targetDuration, style }) => {
      const instruction = `Create content plan for "${topic}" (${targetDuration}s duration, ${style || "Cinematic"} style)`;
      const result = await executeSubagent(contentSubagent, {
        sessionId: sessionId || null,
        instruction,
        priorStages: [],
        userPreferences: { style },
        onProgress,
      });
      return JSON.stringify(result);
    },
    {
      name: "delegate_to_content_subagent",
      description: "Delegate content planning and narration to Content Subagent. Creates ContentPlan with optimal scene count.",
      schema: z.object({
        sessionId: z.string().nullable().describe("Session ID from import stage, or null to create new"),
        topic: z.string().describe("Topic or transcript for the video"),
        targetDuration: z.number().describe("Target duration in seconds"),
        style: z.string().optional().describe("Visual style (cinematic, anime, documentary, etc.)"),
      }),
    }
  );

  const delegateToMediaTool = tool(
    async ({ sessionId, animation, sfx }) => {
      const features = [];
      if (animation) features.push("animation");
      if (sfx) features.push("sound effects");

      const instruction = features.length > 0
        ? `Generate visual assets with ${features.join(", ")}`
        : `Generate visual assets (no animation or SFX)`;

      const result = await executeSubagent(mediaSubagent, {
        sessionId,
        instruction,
        priorStages: [],
        userPreferences: { animation, sfx },
        onProgress,
      });
      return JSON.stringify(result);
    },
    {
      name: "delegate_to_media_subagent",
      description: "Delegate media generation to Media Subagent. Generates visuals, optionally animates, adds SFX. NOTE: Music generation is NOT available in video production mode. REQUIRED: Pass the sessionId from content stage.",
      schema: z.object({
        sessionId: z.string().describe("REQUIRED: Session ID from content stage (use the sessionId returned by delegate_to_content_subagent)"),
        animation: z.boolean().optional().default(false).describe("Whether to animate images to video"),
        sfx: z.boolean().optional().default(false).describe("Whether to create sound effects plan"),
      }),
    }
  );

  const delegateToEnhancementExportTool = tool(
    async ({ sessionId, format, aspectRatio, generateSubtitles, uploadToCloud, makePublic }) => {
      const features = [];
      if (generateSubtitles) features.push("subtitles");
      if (uploadToCloud) features.push("cloud upload");

      const instruction = `Export final video (${format || "mp4"}, ${aspectRatio || "16:9"})${features.length > 0 ? ` with ${features.join(", ")}` : ""}`;

      const result = await executeSubagent(enhancementExportSubagent, {
        sessionId,
        instruction,
        priorStages: [],
        userPreferences: { format, aspectRatio, subtitles: generateSubtitles, uploadToCloud, makePublic },
        onProgress,
      });
      return JSON.stringify(result);
    },
    {
      name: "delegate_to_enhancement_export_subagent",
      description: "Delegate enhancement and export to Enhancement/Export Subagent. Mixes audio, generates subtitles, exports video. REQUIRED: Pass the same sessionId used in previous stages.",
      schema: z.object({
        sessionId: z.string().describe("REQUIRED: Session ID from previous stages (use the SAME sessionId from content and media stages)"),
        format: z.string().optional().default("mp4").describe("Video format (mp4 or webm)"),
        aspectRatio: z.string().optional().default("16:9").describe("Aspect ratio (16:9, 9:16, or 1:1)"),
        generateSubtitles: z.boolean().optional().default(false).describe("Whether to generate subtitles"),
        uploadToCloud: z.boolean().optional().default(false).describe("Whether to upload to cloud storage"),
        makePublic: z.boolean().optional().default(false).describe("Whether to make cloud files public"),
      }),
    }
  );

  const supervisorTools = [
    delegateToImportTool,
    delegateToContentTool,
    delegateToMediaTool,
    delegateToEnhancementExportTool,
  ];

  // Initialize supervisor model
  const model = new ChatGoogleGenerativeAI({
    model: MODELS.TEXT,
    apiKey,
    temperature: 0.1, // Low temperature for consistent orchestration
  });

  const modelWithTools = model.bindTools(supervisorTools);

  // Analyze user intent for better tool selection
  const intentResult = analyzeIntent(userRequest);
  const intentHint = generateIntentHint(intentResult);

  // Build enhanced user message with intent hints
  let enhancedRequest = userRequest;
  if (intentHint) {
    enhancedRequest = `${userRequest}\n\n---\nSYSTEM INTENT ANALYSIS:\n${intentHint}`;
    log.info(` Intent detected:`, {
      wantsAnimation: intentResult.wantsAnimation,
      wantsMusic: intentResult.wantsMusic,
      detectedStyle: intentResult.detectedStyle,
    });
  }

  // Initialize messages
  const messages: (SystemMessage | HumanMessage | AIMessage | ToolMessage)[] = [
    new SystemMessage(SUPERVISOR_AGENT_PROMPT),
    new HumanMessage(enhancedRequest),
  ];

  let sessionId: string | null = null;
  const completedStages: CompletedStage[] = [];
  const MAX_ITERATIONS = 20;
  let iteration = 0;

  while (iteration < MAX_ITERATIONS) {
    iteration++;

    log.info(` Iteration ${iteration}/${MAX_ITERATIONS}`);

    // Get response from model
    const response = await modelWithTools.invoke(messages);
    messages.push(response);

    // Check if model wants to use tools
    if (!response.tool_calls || response.tool_calls.length === 0) {
      // No tool calls - check if production is complete
      const content = response.content as string;

      if (content.includes("Production complete")) {
        const duration = Date.now() - startTime;

        onProgress?.({
          stage: "complete",
          message: content,
          isComplete: true,
        });

        return {
          success: true,
          sessionId,
          completedStages,
          message: content,
          duration,
        };
      }

      // Model finished without completing production
      log.warn(" Model finished without completion signal");
      continue;
    }

    // Execute tool calls (subagent delegations)
    for (const toolCall of response.tool_calls) {
      const toolName = toolCall.name;

      log.info(` Delegating to: ${toolName}`);

      const tool = supervisorTools.find(t => t.name === toolName);
      if (!tool) {
        throw new Error(`Tool ${toolName} not found`);
      }

      try {
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        const result = await (tool as any).invoke(toolCall.args);
        const parsed = JSON.parse(result);

        // Extract sessionId if returned
        if (parsed.sessionId && !sessionId) {
          sessionId = parsed.sessionId;
          log.info(` Session created: ${sessionId}`);
        }

        // Record completed stage
        if (parsed.completedStage) {
          completedStages.push({
            subagent: parsed.completedStage,
            completedAt: Date.now(),
            duration: parsed.duration || 0,
            success: parsed.success,
          });
        }

        // Add tool result to messages
        messages.push(
          new ToolMessage({
            content: result,
            tool_call_id: toolCall.id || "",
          })
        );

        // If sessionId was just set, add a reminder message to use it
        // NOTE: Using HumanMessage instead of SystemMessage because Google Generative AI
        // requires SystemMessage to be first in the messages array
        if (parsed.sessionId && sessionId === parsed.sessionId) {
          messages.push(
            new HumanMessage(
              `IMPORTANT: Session "${sessionId}" has been created. You MUST pass this sessionId to ALL subsequent subagent calls (media, enhancement/export). Do not use null or create a new sessionId.`
            )
          );
        }
      } catch (error) {
        const errorMessage = error instanceof Error ? error.message : String(error);

        log.error(` Delegation failed:`, errorMessage);

        // Add error message to context
        messages.push(
          new ToolMessage({
            content: JSON.stringify({ error: errorMessage }),
            tool_call_id: toolCall.id || "",
          })
        );
      }
    }
  }

  // Iteration limit reached
  throw new Error(`Supervisor agent exceeded maximum iterations (${MAX_ITERATIONS})`);
}
````

## File: packages/shared/src/services/ai/workflowTriggerService.ts
````typescript
/**
 * LangChain Workflow Trigger Service
 * 
 * Bridges NLP intent parser outputs to LangChain workflow executions.
 * Handles workflow selection, parameter mapping, execution, and result formatting.
 */

import {
  ParsedIntent,
  ExtractedEntity,
  ConversationContext,
  WorkflowResult,
  IntentType
} from '../../types';
import { runProductionPipeline, ProductionConfig } from '../agentOrchestrator';
import { generatePromptsWithAgent, AgentDirectorConfig } from '../agentDirectorService';
import { agentDirectorLogger as agentLogger } from '../agent/agentLogger';
import { v4 as uuidv4 } from 'uuid';

// --- Local Types ---

export type WorkflowType = 
  | 'video_production'
  | 'image_generation'
  | 'video_editing'
  | 'narration_generation'
  | 'translation'
  | 'content_analysis'
  | 'content_planning'
  | 'video_export'
  | 'music_generation';

export type WorkflowStatus = 'pending' | 'running' | 'completed' | 'failed' | 'escalated';

export interface WorkflowStep {
  name: string;
  status: 'pending' | 'running' | 'completed' | 'failed';
  message?: string;
  progress?: number;
}

export interface WorkflowMapping {
  intent: IntentType;
  workflowType: WorkflowType;
  description: string;
  requiredEntities: string[];
  optionalEntities: string[];
}

export interface WorkflowExecutionContext {
  executionId: string;
  workflowType: WorkflowType;
  intent: ParsedIntent;
  context: ConversationContext;
  startTime: Date;
  steps: WorkflowStep[];
  status: WorkflowStatus;
  result?: WorkflowResult;
  error?: string;
}

export interface WorkflowExecutorParams {
  intent: ParsedIntent;
  context: ConversationContext;
  entities: ExtractedEntity[];
  userInput: string;
  onProgress?: (step: WorkflowStep) => void;
}

// Extended result with additional fields for this service
export interface ExtendedWorkflowResult extends WorkflowResult {
  executionId?: string;
  workflowType?: WorkflowType;
  workflowName?: string;
  requiresClarification?: boolean;
  clarificationQuestion?: string;
  requiresEscalation?: boolean;
  escalationReason?: string;
  suggestion?: string;
  canRetry?: boolean;
  output?: Record<string, unknown>;
}

// --- Workflow Mapping Configuration ---

const WORKFLOW_MAPPINGS: WorkflowMapping[] = [
  {
    intent: 'create_video',
    workflowType: 'video_production',
    description: 'Generate a complete video from content',
    requiredEntities: ['topic'],
    optionalEntities: ['style', 'duration', 'language']
  },
  {
    intent: 'generate_images',
    workflowType: 'image_generation',
    description: 'Generate visual assets for scenes',
    requiredEntities: ['topic'],
    optionalEntities: ['style', 'mood']
  },
  {
    intent: 'edit_video',
    workflowType: 'video_editing',
    description: 'Edit or modify existing video',
    requiredEntities: [],
    optionalEntities: ['duration', 'style']
  },
  {
    intent: 'translate_content',
    workflowType: 'translation',
    description: 'Translate content to another language',
    requiredEntities: ['language'],
    optionalEntities: ['style']
  },
  {
    intent: 'generate_music',
    workflowType: 'music_generation',
    description: 'Generate AI music track',
    requiredEntities: [],
    optionalEntities: ['mood', 'duration', 'style']
  }
];

// --- Service Implementation ---

class WorkflowTriggerService {
  private activeExecutions: Map<string, WorkflowExecutionContext> = new Map();
  private executionHistory: WorkflowExecutionContext[] = [];
  private maxHistorySize = 50;

  /**
   * Execute a workflow based on parsed intent
   */
  async executeWorkflow(params: WorkflowExecutorParams): Promise<ExtendedWorkflowResult> {
    const { intent, context, entities, userInput, onProgress } = params;
    const executionId = uuidv4();
    const startTime = new Date();

    agentLogger.info('Starting workflow execution', {
      executionId,
      intent: intent.intentType,
      confidence: intent.confidence
    });

    // Find matching workflow
    const mapping = WORKFLOW_MAPPINGS.find(m => m.intent === intent.intentType);
    
    if (!mapping) {
      // Return a conversational response for non-workflow intents
      return {
        success: true,
        executionId,
        workflowName: 'conversation',
        message: 'Processing your request...'
      };
    }

    // Create execution context
    const executionContext: WorkflowExecutionContext = {
      executionId,
      workflowType: mapping.workflowType,
      intent,
      context,
      startTime,
      steps: [],
      status: 'running'
    };

    this.activeExecutions.set(executionId, executionContext);

    try {
      let result: ExtendedWorkflowResult;

      switch (mapping.workflowType) {
        case 'video_production':
          result = await this.executeVideoProduction(executionContext, entities, userInput, onProgress);
          break;
        case 'image_generation':
          result = await this.executeImageGeneration(executionContext, entities, userInput);
          break;
        case 'content_planning':
          result = await this.executeContentPlanning(executionContext, entities, userInput);
          break;
        default:
          result = {
            success: true,
            executionId,
            workflowType: mapping.workflowType,
            workflowName: mapping.description,
            message: `Workflow "${mapping.workflowType}" acknowledged. This feature is coming soon.`
          };
      }

      executionContext.status = result.success ? 'completed' : 'failed';
      executionContext.result = result;
      this.archiveExecution(executionContext);

      return result;

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      executionContext.status = 'failed';
      executionContext.error = errorMessage;
      this.archiveExecution(executionContext);

      return {
        success: false,
        executionId,
        error: errorMessage,
        message: `Workflow failed: ${errorMessage}`,
        canRetry: true
      };
    }
  }

  /**
   * Generate conversational response using LLM
   */
  async generateConversationalResponse(
    userInput: string,
    conversationHistory: Array<{ role: string; content: string }>,
    context: unknown
  ): Promise<string> {
    // For now, return a template response
    // In production, this would call Gemini for natural conversation
    const greetings = ['hello', 'hi', 'hey', 'greetings'];
    const isGreeting = greetings.some(g => userInput.toLowerCase().includes(g));

    if (isGreeting) {
      return "Hello! I'm your AI creative assistant. I can help you create videos, generate music, and more. What would you like to work on today?";
    }

    return "I'm here to help! You can ask me to create videos, generate images, or help with your creative projects. What would you like to do?";
  }

  /**
   * Video Production Workflow
   */
  private async executeVideoProduction(
    context: WorkflowExecutionContext,
    entities: ExtractedEntity[],
    userInput: string,
    onProgress?: (step: WorkflowStep) => void
  ): Promise<ExtendedWorkflowResult> {
    const topic = this.extractEntityValue(entities, 'topic') || userInput;
    const style = this.extractEntityValue(entities, 'style') || 'Cinematic';
    const duration = parseInt(this.extractEntityValue(entities, 'duration') || '60');

    const step: WorkflowStep = {
      name: 'video_production',
      status: 'running',
      message: 'Starting video production...'
    };
    context.steps.push(step);
    onProgress?.(step);

    try {
      const config: ProductionConfig = {
        targetDuration: duration,
        sceneCount: Math.ceil(duration / 12),
        targetAudience: 'General audience',
        visualStyle: style,
        aspectRatio: '16:9',
        skipNarration: false,
        skipVisuals: false,
        skipValidation: false,
        maxRetries: 2
      };

      const result = await runProductionPipeline(
        { topic },
        config,
        (progress) => {
          const progressStep: WorkflowStep = {
            name: progress.stage,
            status: progress.progress < 100 ? 'running' : 'completed',
            progress: progress.progress,
            message: progress.message
          };
          onProgress?.(progressStep);
        }
      );

      return {
        success: result.success,
        executionId: context.executionId,
        workflowType: 'video_production',
        workflowName: 'Video Production',
        data: {
          contentPlan: result.contentPlan,
          narrationSegments: result.narrationSegments,
          visuals: result.visuals
        },
        message: `Video production ${result.success ? 'completed' : 'completed with issues'}`,
        nextSteps: ['Review generated content', 'Preview visuals', 'Export video']
      };

    } catch (error) {
      throw new Error(`Video production failed: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  /**
   * Image Generation Workflow
   */
  private async executeImageGeneration(
    context: WorkflowExecutionContext,
    entities: ExtractedEntity[],
    userInput: string
  ): Promise<ExtendedWorkflowResult> {
    const content = this.extractEntityValue(entities, 'topic') || userInput;
    const style = this.extractEntityValue(entities, 'style') || 'Cinematic';

    context.steps.push({
      name: 'image_generation',
      status: 'running',
      message: 'Generating image prompts...'
    });

    try {
      const prompts = await generatePromptsWithAgent(
        content,
        style,
        'story',
        'documentary',
        undefined,
        { targetAssetCount: 10 } as AgentDirectorConfig
      );

      return {
        success: true,
        executionId: context.executionId,
        workflowType: 'image_generation',
        workflowName: 'Image Generation',
        data: { prompts },
        message: `Generated ${prompts.length} image prompts`,
        nextSteps: ['Review prompts', 'Generate images', 'Add to timeline']
      };

    } catch (error) {
      throw new Error(`Image generation failed: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  /**
   * Content Planning Workflow
   */
  private async executeContentPlanning(
    context: WorkflowExecutionContext,
    entities: ExtractedEntity[],
    userInput: string
  ): Promise<ExtendedWorkflowResult> {
    const topic = this.extractEntityValue(entities, 'topic') || userInput;
    const duration = parseInt(this.extractEntityValue(entities, 'duration') || '60');

    context.steps.push({
      name: 'content_planning',
      status: 'running',
      message: 'Creating content plan...'
    });

    try {
      const { generateContentPlan } = await import('../contentPlannerService');
      const contentPlan = await generateContentPlan(topic, {
        targetDuration: duration,
        sceneCount: Math.ceil(duration / 12),
        targetAudience: 'General audience'
      });

      return {
        success: true,
        executionId: context.executionId,
        workflowType: 'content_planning',
        workflowName: 'Content Planning',
        data: { contentPlan },
        message: `Created plan with ${contentPlan.scenes.length} scenes`,
        nextSteps: ['Review plan', 'Modify scenes', 'Start production']
      };

    } catch (error) {
      throw new Error(`Content planning failed: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  // --- Helper Methods ---

  private extractEntityValue(entities: ExtractedEntity[], type: string): string | undefined {
    const entity = entities.find(e => e.type === type || e.type === 'custom');
    return entity?.value;
  }

  private archiveExecution(context: WorkflowExecutionContext): void {
    this.activeExecutions.delete(context.executionId);
    this.executionHistory.push(context);
    
    if (this.executionHistory.length > this.maxHistorySize) {
      this.executionHistory.shift();
    }
  }

  // --- Public API ---

  getExecution(executionId: string): WorkflowExecutionContext | undefined {
    return this.activeExecutions.get(executionId) ||
           this.executionHistory.find(e => e.executionId === executionId);
  }

  getAvailableWorkflows(): WorkflowMapping[] {
    return WORKFLOW_MAPPINGS;
  }

  getRecentExecutions(limit = 10): WorkflowExecutionContext[] {
    return this.executionHistory.slice(-limit).reverse();
  }
}

// Export singleton instance
export const workflowTriggerService = new WorkflowTriggerService();

// Also export as workflowExecutor for compatibility
export const workflowExecutor = workflowTriggerService;
````

## File: packages/shared/src/services/aiLogService.ts
````typescript
/**
 * AI Logging Service
 *
 * Captures every AI call (prompts sent, responses received) for debugging,
 * transparency, quality review, and iterative prompt improvement.
 *
 * Design:
 * - Fire-and-forget writes (logging never blocks the pipeline)
 * - Text only (binary data logged as metadata, never raw bytes)
 * - Truncation at 10,000 chars to prevent IndexedDB bloat
 * - Session-scoped (all logs keyed by sessionId)
 * - Optional sessionId (when not provided, logging is skipped)
 * - Dual persistence: IndexedDB (local) + Google Cloud Storage (bucket)
 */

import {
    saveAILog,
    getAILogsForSession,
    getAILogsByStep,
    deleteAILogsForSession,
    type AILogEntry,
} from './ai/production/persistence';
import { cloudAutosave } from './cloudStorageService';

export type { AILogEntry };

const MAX_TEXT_LENGTH = 10_000;

function truncate(text: string): string {
    if (text.length <= MAX_TEXT_LENGTH) return text;
    return text.slice(0, MAX_TEXT_LENGTH) + `... [truncated, ${text.length} total chars]`;
}

function generateLogId(): string {
    const rand = Math.random().toString(36).slice(2, 10);
    return `log_${Date.now()}_${rand}`;
}

/**
 * Log an AI call (fire-and-forget). Does nothing if sessionId is undefined.
 * Saves to both IndexedDB (local) and Google Cloud Storage (bucket).
 */
export function logAICall(entry: Omit<AILogEntry, 'id' | 'timestamp'>): void {
    if (!entry.sessionId) return;

    const full: AILogEntry = {
        ...entry,
        id: generateLogId(),
        timestamp: Date.now(),
        input: truncate(entry.input),
        output: truncate(entry.output),
        error: entry.error ? truncate(entry.error) : undefined,
    };

    // Fire-and-forget to IndexedDB — never block the caller
    saveAILog(full).catch(() => {
        // Silently swallow persistence errors
    });

    // Fire-and-forget to Cloud Storage — non-blocking
    cloudAutosave.saveAILog(entry.sessionId, full).catch(() => {
        // Silently swallow cloud storage errors
    });
}

/**
 * Ergonomic wrapper: runs an async AI operation while capturing timing,
 * input/output, and error state into the log store.
 *
 * @param sessionId - Story session ID (logging is skipped when undefined)
 * @param step - Step identifier (e.g. 'breakdown', 'screenplay', 'tts')
 * @param model - Model name (e.g. 'gemini-2.0-flash')
 * @param input - The prompt or input sent to the model
 * @param fn - The async function that performs the actual AI call
 * @param outputMapper - Optional function to extract a loggable string from the result
 */
export async function withAILogging<T>(
    sessionId: string | undefined,
    step: string,
    model: string,
    input: string,
    fn: () => Promise<T>,
    outputMapper?: (result: T) => string,
): Promise<T> {
    if (!sessionId) return fn();

    const start = Date.now();
    try {
        const result = await fn();
        const output = outputMapper
            ? outputMapper(result)
            : typeof result === 'string'
              ? result
              : JSON.stringify(result);

        logAICall({
            sessionId,
            step,
            model,
            input,
            output,
            durationMs: Date.now() - start,
            status: 'success',
        });

        return result;
    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : String(error);
        logAICall({
            sessionId,
            step,
            model,
            input,
            output: '',
            durationMs: Date.now() - start,
            status: 'error',
            error: errorMessage,
        });
        throw error;
    }
}

/**
 * Retrieve all logs for a session, ordered by timestamp.
 */
export async function getLogsForSession(sessionId: string): Promise<AILogEntry[]> {
    return getAILogsForSession(sessionId);
}

/**
 * Retrieve logs for a session filtered by step.
 */
export async function getLogsByStep(sessionId: string, step: string): Promise<AILogEntry[]> {
    return getAILogsByStep(sessionId, step);
}

/**
 * Export all logs for a session as a JSON string.
 */
export async function exportLogsAsJSON(sessionId: string): Promise<string> {
    const logs = await getAILogsForSession(sessionId);
    return JSON.stringify(logs, null, 2);
}

/**
 * Clear all logs for a session.
 */
export async function clearLogsForSession(sessionId: string): Promise<void> {
    return deleteAILogsForSession(sessionId);
}
````

## File: packages/shared/src/services/assetCalculatorService.ts
````typescript
/**
 * Asset Calculator Service
 * Dynamically calculates optimal number of image/video assets based on:
 * - Audio duration
 * - Semantic analysis from directorService (Themes, Motifs, Concrete Objects)
 * - Content density
 * - Video purpose
 */

import { VideoPurpose } from "../constants";
import type { AnalysisOutput } from "./directorService";

/**
 * Input for asset calculation
 */
export interface AssetCalculationInput {
    audioDuration: number; // seconds
    analysisOutput: AnalysisOutput;
    videoPurpose: VideoPurpose;
    contentType: "lyrics" | "story";
    minAssets?: number; // minimum assets (default: 6)
    maxAssets?: number; // maximum assets (default: 15)
}

/**
 * Result of asset calculation
 */
export interface AssetCalculationResult {
    optimalAssetCount: number;
    assetTimestamps: number[]; // seconds for each asset
    reasoning: string;
}

export async function calculateOptimalAssets(
    input: AssetCalculationInput
): Promise<AssetCalculationResult> {
    const {
        audioDuration,
        videoPurpose,
    } = input;

    console.log("[AssetCalculator] Calculating optimal assets (Synchronized to Video Gen limit)...");

    // --- CRITICAL FIX FOR 4-SECOND LOOPING ---
    // DeAPI/Kling/Luma generate exactly 4.0s or 5.0s clips.
    // If we make scenes longer than this, they loop awkwardly.
    const GENERATOR_MAX_DURATION = 4.0; // We want a 1:1 match: 1 video asset per 4 seconds of audio.
    // No looping allowed.

    let optimalAssetCount = Math.ceil(audioDuration / GENERATOR_MAX_DURATION);

    // Safety Clamp: Don't generate less than 4 clips, but allow up to 100 for long songs.
    optimalAssetCount = Math.max(optimalAssetCount, 4);

    // Generate timestamps
    const assetTimestamps: number[] = [];
    const step = audioDuration / optimalAssetCount;
    for (let i = 0; i < optimalAssetCount; i++) {
        assetTimestamps.push(i * step);
    }

    const reasoning = `Audio Duration: ${audioDuration.toFixed(1)}s
Generator Limit: ${GENERATOR_MAX_DURATION}s per clip
Looping Policy: STRICTLY DISABLED
Calculated Assets: ${optimalAssetCount} (to ensure continuous flow)`;

    console.log(reasoning);

    return {
        optimalAssetCount,
        assetTimestamps,
        reasoning,
    };
}

/**
 * Calculate baseline asset count based on audio duration
 */
function calculateDurationBaseline(duration: number): number {
    if (duration < 30) {
        // Short content: 6-8 assets
        return Math.round(6 + (duration / 30) * 2);
    } else if (duration < 60) {
        // Medium content: 8-10 assets
        return Math.round(8 + ((duration - 30) / 30) * 2);
    } else if (duration < 120) {
        // Long content: 10-12 assets
        return Math.round(10 + ((duration - 60) / 60) * 2);
    } else {
        // Very long content: 12-15 assets
        return Math.round(12 + Math.min(3, (duration - 120) / 60));
    }
}

/**
 * Calculate asset count based on visual scene density
 */
function calculateSceneBasedCount(analysis: AnalysisOutput): number {
    const sceneCount = analysis.visualScenes?.length || 0;

    if (sceneCount === 0) {
        return 8; // Default
    }

    // Goal: ensure every visual scene gets a high chance of being shown
    // We want at least as many assets as scenes, but capped for sanity
    return Math.min(15, Math.max(8, sceneCount + 2));
}

/**
 * Adjust asset count based on video purpose
 */
function adjustForPurpose(count: number, purpose: VideoPurpose): number {
    switch (purpose) {
        case "social_short":
            // Fewer, faster cuts
            return Math.round(count * 0.75);
        case "music_video":
            // Balanced
            return count;
        case "documentary":
            // More coverage
            return Math.round(count * 1.2);
        case "commercial":
            // Product-focused, fewer cuts
            return Math.round(count * 0.8);
        case "podcast_visual":
            // Ambient, fewer changes
            return Math.round(count * 0.7);
        case "lyric_video":
            // Text-focused, moderate
            return Math.round(count * 0.9);
        default:
            return count;
    }
}

/**
 * Adjust asset count based on content density (motifs per minute)
 */
function adjustForContentDensity(
    count: number,
    analysis: AnalysisOutput,
    duration: number
): number {
    const sceneCount = analysis.visualScenes?.length || 0;
    if (sceneCount === 0) return count;

    const scenesPerMinute = sceneCount / (duration / 60);

    // Highly dense scenes → more assets
    if (scenesPerMinute > 5) {
        return Math.round(count * 1.25);
    } else if (scenesPerMinute < 1) {
        return Math.round(count * 0.85);
    }

    return count;
}

/**
 * Calculate timestamps for each asset
 * Distributes assets across content with bias towards visual scenes
 */
function calculateAssetTimestamps(
    count: number,
    duration: number,
    analysis: AnalysisOutput
): number[] {
    const timestamps: number[] = [];
    const sceneTimes = (analysis.visualScenes || [])
        .map(scene => parseTimestamp(scene.timestamp))
        .sort((a, b) => a - b);

    if (sceneTimes.length === 0) {
        // Even distribution if no scenes
        const interval = duration / (count + 1);
        for (let i = 1; i <= count; i++) {
            timestamps.push(i * interval);
        }
        return timestamps;
    }

    // Mix scene timestamps with even distribution
    // This ensures identified scenes are shown, but gaps are filled
    const evenInterval = duration / (count + 1);
    const usedTimes = new Set<string>();

    // 1. Add scene times (up to count)
    sceneTimes.slice(0, count).forEach(t => {
        timestamps.push(t);
        usedTimes.add(t.toFixed(1));
    });

    // 2. Fill remaining slots with even distribution
    let currentIdx = 1;
    while (timestamps.length < count && currentIdx <= count) {
        const candidate = currentIdx * evenInterval;
        // Simple check to avoid double-clumping
        if (!Array.from(usedTimes).some(ut => Math.abs(parseFloat(ut) - candidate) < 2)) {
            timestamps.push(candidate);
            usedTimes.add(candidate.toFixed(1));
        }
        currentIdx++;
    }

    return timestamps.sort((a, b) => a - b);
}

/**
 * Generate human-readable reasoning
 */
function generateReasoning(params: {
    audioDuration: number;
    durationBaseline: number;
    motifBasedCount: number;
    purposeAdjustedCount: number;
    densityAdjustedCount: number;
    optimalAssetCount: number;
    videoPurpose: VideoPurpose;
}): string {
    const {
        audioDuration,
        durationBaseline,
        motifBasedCount,
        purposeAdjustedCount,
        densityAdjustedCount,
        optimalAssetCount,
        videoPurpose,
    } = params;

    const parts: string[] = [];

    parts.push(`Audio duration: ${Math.round(audioDuration)}s`);
    parts.push(`Duration baseline: ${durationBaseline} assets`);
    parts.push(`Motif density: ${motifBasedCount} assets`);
    parts.push(`Purpose adjustment (${videoPurpose}): ${purposeAdjustedCount} assets`);
    parts.push(`Content density factor: ${densityAdjustedCount} assets`);
    parts.push(`Final optimal count: ${optimalAssetCount} assets`);

    return parts.join("\n");
}

/**
 * Parse timestamp string (MM:SS) to seconds
 */
function parseTimestamp(timestamp: string): number {
    const parts = timestamp.split(":");
    if (parts.length !== 2) {
        return 0;
    }

    const minStr = parts[0];
    const secStr = parts[1];
    if (minStr === undefined || secStr === undefined) return 0;

    const minutes = parseInt(minStr, 10);
    const seconds = parseInt(secStr, 10);

    return minutes * 60 + seconds;
}
````

## File: packages/shared/src/services/audioConcatService.ts
````typescript
/**
 * Audio Concatenation Service
 * 
 * Combines multiple audio segments into a single audio file using Web Audio API.
 * Used for combining narration segments before video export.
 */

export interface AudioSegment {
  url: string;
  duration: number;
}

/**
 * Concatenate multiple audio URLs into a single audio blob
 * Uses Web Audio API for proper audio decoding and re-encoding
 */
export async function concatenateAudioSegments(
  segments: AudioSegment[],
  onProgress?: (percent: number) => void
): Promise<Blob> {
  if (segments.length === 0) {
    throw new Error('No audio segments provided');
  }

  if (segments.length === 1 && segments[0]) {
    // Single segment - just fetch and return
    const response = await fetch(segments[0].url);
    return response.blob();
  }

  const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();

  try {
    // Fetch and decode all audio segments
    const decodedBuffers: AudioBuffer[] = [];
    
    for (let i = 0; i < segments.length; i++) {
      onProgress?.(Math.round((i / segments.length) * 50));
      
      const seg = segments[i];
      if (!seg) continue;
      const response = await fetch(seg.url);
      const arrayBuffer = await response.arrayBuffer();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      decodedBuffers.push(audioBuffer);
    }

    // Calculate total duration
    const totalLength = decodedBuffers.reduce((sum, buf) => sum + buf.length, 0);
    const firstBuffer = decodedBuffers[0];
    if (!firstBuffer) throw new Error('No audio buffers decoded');
    const sampleRate = firstBuffer.sampleRate;
    const numberOfChannels = Math.max(...decodedBuffers.map(b => b.numberOfChannels));

    // Create output buffer
    const outputBuffer = audioContext.createBuffer(
      numberOfChannels,
      totalLength,
      sampleRate
    );

    // Copy all segments into output buffer
    let offset = 0;
    for (let i = 0; i < decodedBuffers.length; i++) {
      onProgress?.(50 + Math.round((i / decodedBuffers.length) * 30));
      
      const buffer = decodedBuffers[i];
      if (!buffer) continue;
      for (let channel = 0; channel < numberOfChannels; channel++) {
        const outputData = outputBuffer.getChannelData(channel);
        const inputData = buffer.numberOfChannels > channel
          ? buffer.getChannelData(channel)
          : buffer.getChannelData(0); // Use first channel if mono
        outputData.set(inputData, offset);
      }
      offset += buffer.length;
    }

    onProgress?.(85);

    // Encode to WAV (simple format that works everywhere)
    const wavBlob = audioBufferToWav(outputBuffer);
    
    onProgress?.(100);
    
    return wavBlob;
  } finally {
    await audioContext.close();
  }
}

/**
 * Convert AudioBuffer to WAV Blob
 */
function audioBufferToWav(buffer: AudioBuffer): Blob {
  const numChannels = buffer.numberOfChannels;
  const sampleRate = buffer.sampleRate;
  const format = 1; // PCM
  const bitDepth = 16;

  const bytesPerSample = bitDepth / 8;
  const blockAlign = numChannels * bytesPerSample;
  const byteRate = sampleRate * blockAlign;
  const dataSize = buffer.length * blockAlign;
  const headerSize = 44;
  const totalSize = headerSize + dataSize;

  const arrayBuffer = new ArrayBuffer(totalSize);
  const view = new DataView(arrayBuffer);

  // RIFF header
  writeString(view, 0, 'RIFF');
  view.setUint32(4, totalSize - 8, true);
  writeString(view, 8, 'WAVE');

  // fmt chunk
  writeString(view, 12, 'fmt ');
  view.setUint32(16, 16, true); // chunk size
  view.setUint16(20, format, true);
  view.setUint16(22, numChannels, true);
  view.setUint32(24, sampleRate, true);
  view.setUint32(28, byteRate, true);
  view.setUint16(32, blockAlign, true);
  view.setUint16(34, bitDepth, true);

  // data chunk
  writeString(view, 36, 'data');
  view.setUint32(40, dataSize, true);

  // Write interleaved audio data
  let offset = 44;
  for (let i = 0; i < buffer.length; i++) {
    for (let channel = 0; channel < numChannels; channel++) {
      const sample = buffer.getChannelData(channel)[i] ?? 0;
      // Clamp and convert to 16-bit integer
      const intSample = Math.max(-1, Math.min(1, sample)) * 0x7FFF;
      view.setInt16(offset, intSample, true);
      offset += 2;
    }
  }

  return new Blob([arrayBuffer], { type: 'audio/wav' });
}

function writeString(view: DataView, offset: number, string: string): void {
  for (let i = 0; i < string.length; i++) {
    view.setUint8(offset + i, string.charCodeAt(i));
  }
}

/**
 * Create a combined audio URL from multiple narration segments
 */
export async function createCombinedNarrationAudio(
  segments: Array<{ audioUrl?: string; duration: number }>,
  onProgress?: (message: string, percent: number) => void
): Promise<string> {
  const validSegments: AudioSegment[] = segments
    .filter((s): s is { audioUrl: string; duration: number } => !!s.audioUrl)
    .map(s => ({ url: s.audioUrl, duration: s.duration }));
  
  if (validSegments.length === 0) {
    throw new Error('No valid audio segments found');
  }

  if (validSegments.length === 1) {
    return validSegments[0]!.url;
  }

  onProgress?.('Combining narration audio...', 10);

  const combinedBlob = await concatenateAudioSegments(
    validSegments.map(s => ({ url: s.url, duration: s.duration })),
    (percent) => onProgress?.('Combining audio...', 10 + (percent * 0.2))
  );

  const combinedUrl = URL.createObjectURL(combinedBlob);
  
  onProgress?.('Audio combined', 30);
  
  return combinedUrl;
}
````

## File: packages/shared/src/services/audioMixerService.ts
````typescript
/**
 * Audio Mixer Service
 * 
 * Mixes multiple audio tracks (narration, SFX, background music) into a single audio file.
 * Uses Web Audio API for real-time mixing and offline rendering.
 */

import { VideoSFXPlan } from "./sfxService";

// --- Types ---

export interface AudioTrack {
  /** Audio source URL or Blob */
  source: string | Blob;
  /** Start time in seconds */
  startTime: number;
  /** Duration in seconds (0 = full length) */
  duration?: number;
  /** Volume level 0-1 */
  volume: number;
  /** Whether to loop the audio */
  loop?: boolean;
  /** Fade in duration in seconds */
  fadeIn?: number;
  /** Fade out duration in seconds */
  fadeOut?: number;
}

export interface SceneAudioInfo {
  sceneId: string;
  startTime: number;
  duration: number;
}

export interface MixConfig {
  /** Main narration/voice track */
  narrationUrl: string;
  /** SFX plan with audio URLs */
  sfxPlan: VideoSFXPlan | null;
  /** Scene timing information */
  scenes: SceneAudioInfo[];
  /** Master volume for SFX (0-1) */
  sfxMasterVolume?: number;
  /** Master volume for background music (0-1) */
  musicMasterVolume?: number;
  /** Output sample rate */
  sampleRate?: number;
  /** Enable dynamic audio ducking (lowers music during speech) */
  enableDucking?: boolean;
  /** Ducking amount (0-1, how much to reduce music, default 0.7 = reduce to 30%) */
  duckingAmount?: number;
}

// --- Dynamic Audio Ducking ---

/**
 * Calculate a ducking envelope from narration audio.
 * Returns an array of gain values (0-1) representing how much to duck the music.
 * Higher values = more speech = lower music.
 * 
 * @param narrationBuffer - The decoded narration audio
 * @param sampleRate - Output sample rate
 * @param blockSize - Size of analysis blocks in samples (default: 4410 = 100ms at 44.1kHz)
 * @returns Float32Array of ducking values (0 = no duck, 1 = full duck)
 */
function calculateDuckingEnvelope(
  narrationBuffer: AudioBuffer,
  _sampleRate: number = 44100,
  blockSize: number = 4410 // ~100ms blocks
): Float32Array {
  const channelData = narrationBuffer.getChannelData(0);
  const numBlocks = Math.ceil(channelData.length / blockSize);
  const envelope = new Float32Array(numBlocks);

  // Calculate RMS amplitude for each block
  for (let block = 0; block < numBlocks; block++) {
    const start = block * blockSize;
    const end = Math.min(start + blockSize, channelData.length);

    let sumSquares = 0;
    for (let i = start; i < end; i++) {
      const value = channelData[i] ?? 0;
      sumSquares += value * value;
    }

    const rms = Math.sqrt(sumSquares / (end - start));

    // Convert RMS to ducking value (0-1)
    // RMS > 0.05 indicates speech, scale up to full duck at RMS 0.3
    const threshold = 0.05;
    const ceiling = 0.30;

    if (rms < threshold) {
      envelope[block] = 0; // No speech, no duck
    } else {
      // Scale between threshold and ceiling
      const normalized = Math.min(1, (rms - threshold) / (ceiling - threshold));
      envelope[block] = normalized;
    }
  }

  // Smooth the envelope to avoid abrupt changes (simple moving average)
  const smoothedEnvelope = new Float32Array(numBlocks);
  const smoothWindow = 3; // ~300ms smoothing

  for (let i = 0; i < numBlocks; i++) {
    let sum = 0;
    let count = 0;
    for (let j = Math.max(0, i - smoothWindow); j <= Math.min(numBlocks - 1, i + smoothWindow); j++) {
      sum += envelope[j] ?? 0;
      count++;
    }
    smoothedEnvelope[i] = sum / count;
  }

  return smoothedEnvelope;
}

/**
 * Apply ducking envelope to a gain node.
 * Schedules gain changes over time based on the envelope.
 */
function applyDuckingToGain(
  gainNode: GainNode,
  envelope: Float32Array,
  baseVolume: number,
  duckingAmount: number,
  blockDuration: number, // Duration of each envelope block in seconds
  startTime: number = 0
): void {
  const minVolume = baseVolume * (1 - duckingAmount);

  for (let i = 0; i < envelope.length; i++) {
    const time = startTime + i * blockDuration;
    const duckValue = envelope[i] ?? 0;
    const targetVolume = baseVolume - (duckValue * (baseVolume - minVolume));

    // Use exponential ramp for smoother transitions
    gainNode.gain.linearRampToValueAtTime(targetVolume, time);
  }
}



/**
 * Fetch audio from URL and decode to AudioBuffer
 */
async function fetchAndDecodeAudio(
  audioContext: OfflineAudioContext | AudioContext,
  url: string
): Promise<AudioBuffer | null> {
  try {
    console.log(`[AudioMixer] Fetching audio: ${url.substring(0, 50)}...`);
    const response = await fetch(url, { mode: 'cors' });
    if (!response.ok) {
      console.warn(`[AudioMixer] Failed to fetch audio: ${response.status}`);
      return null;
    }
    const arrayBuffer = await response.arrayBuffer();
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    console.log(`[AudioMixer] Decoded audio: ${audioBuffer.duration.toFixed(2)}s`);
    return audioBuffer;
  } catch (error) {
    console.warn(`[AudioMixer] Error fetching/decoding audio:`, error);
    return null;
  }
}

/**
 * Create a gain node with optional fade envelope
 */
function createGainWithFade(
  audioContext: OfflineAudioContext,
  startTime: number,
  duration: number,
  volume: number,
  fadeIn: number = 0,
  fadeOut: number = 0
): GainNode {
  const gainNode = audioContext.createGain();

  // Start at 0 if fade in
  if (fadeIn > 0) {
    gainNode.gain.setValueAtTime(0, startTime);
    gainNode.gain.linearRampToValueAtTime(volume, startTime + fadeIn);
  } else {
    gainNode.gain.setValueAtTime(volume, startTime);
  }

  // Fade out at end
  if (fadeOut > 0 && duration > fadeOut) {
    const fadeOutStart = startTime + duration - fadeOut;
    gainNode.gain.setValueAtTime(volume, fadeOutStart);
    gainNode.gain.linearRampToValueAtTime(0, startTime + duration);
  }

  return gainNode;
}

// --- Main Mixing Function ---

/**
 * Mix narration with SFX and background music.
 * Returns a WAV blob with all audio mixed together.
 */
export async function mixAudioWithSFX(config: MixConfig): Promise<Blob> {
  const {
    narrationUrl,
    sfxPlan,
    scenes,
    sfxMasterVolume = 1.0,
    musicMasterVolume = 0.5,
    sampleRate = 44100,
  } = config;

  console.log("[AudioMixer] Starting audio mix...");
  console.log(`[AudioMixer] Scenes: ${scenes.length}, SFX Plan: ${sfxPlan ? 'yes' : 'no'}`);

  // First, fetch and decode the main narration to get total duration
  const tempContext = new AudioContext({ sampleRate });
  const narrationBuffer = await fetchAndDecodeAudio(tempContext, narrationUrl);

  if (!narrationBuffer) {
    throw new Error("Failed to load narration audio");
  }

  const totalDuration = narrationBuffer.duration;
  const totalSamples = Math.ceil(totalDuration * sampleRate);

  console.log(`[AudioMixer] Total duration: ${totalDuration.toFixed(2)}s`);

  // Create offline context for rendering
  const offlineContext = new OfflineAudioContext(
    1, // mono output
    totalSamples,
    sampleRate
  );

  // --- Track 1: Narration (main voice) ---
  const narrationSource = offlineContext.createBufferSource();
  narrationSource.buffer = narrationBuffer;

  const narrationGain = offlineContext.createGain();
  narrationGain.gain.setValueAtTime(1.0, 0); // Full volume for narration

  narrationSource.connect(narrationGain);
  narrationGain.connect(offlineContext.destination);
  narrationSource.start(0);

  // --- Track 2: Scene-specific SFX ---
  if (sfxPlan && sfxPlan.scenes.length > 0) {
    for (let i = 0; i < scenes.length; i++) {
      const sceneInfo = scenes[i];
      if (!sceneInfo) continue;

      const sfxScene = sfxPlan.scenes.find(s => s.sceneId === sceneInfo.sceneId);

      if (sfxScene?.ambientTrack?.audioUrl) {
        try {
          const sfxBuffer = await fetchAndDecodeAudio(offlineContext, sfxScene.ambientTrack.audioUrl);

          if (sfxBuffer) {
            const sfxSource = offlineContext.createBufferSource();
            sfxSource.buffer = sfxBuffer;
            sfxSource.loop = true; // Loop ambient sounds

            // Calculate volume (use suggested volume * master volume)
            const volume = (sfxScene.ambientTrack.suggestedVolume || 0.3) * sfxMasterVolume;

            // Create gain with fade in/out for smooth transitions
            const fadeTime = 0.5; // 500ms fade
            const sfxGain = createGainWithFade(
              offlineContext,
              sceneInfo.startTime,
              sceneInfo.duration,
              volume,
              fadeTime,
              fadeTime
            );

            sfxSource.connect(sfxGain);
            sfxGain.connect(offlineContext.destination);

            // Start at scene start, stop at scene end
            sfxSource.start(sceneInfo.startTime);
            sfxSource.stop(sceneInfo.startTime + sceneInfo.duration);

            console.log(`[AudioMixer] Added SFX "${sfxScene.ambientTrack.name}" at ${sceneInfo.startTime.toFixed(1)}s for ${sceneInfo.duration.toFixed(1)}s`);
          }
        } catch (error) {
          console.warn(`[AudioMixer] Failed to add SFX for scene ${sceneInfo.sceneId}:`, error);
        }
      }
    }
  }

  // --- Track 3: Background Music (with optional ducking) ---
  if (sfxPlan?.backgroundMusic?.audioUrl) {
    try {
      const musicBuffer = await fetchAndDecodeAudio(offlineContext, sfxPlan.backgroundMusic.audioUrl);

      if (musicBuffer) {
        const musicSource = offlineContext.createBufferSource();
        musicSource.buffer = musicBuffer;
        musicSource.loop = true; // Loop background music

        // Base volume for background music
        const volume = (sfxPlan.backgroundMusic.suggestedVolume || 0.6) * musicMasterVolume;

        // Create music gain node
        const musicGain = offlineContext.createGain();

        // Check if ducking is enabled
        const { enableDucking = true, duckingAmount = 0.7 } = config;

        if (enableDucking) {
          // Calculate ducking envelope from narration
          const blockSize = Math.round(sampleRate * 0.1); // 100ms blocks
          const duckingEnvelope = calculateDuckingEnvelope(narrationBuffer, sampleRate, blockSize);
          const blockDuration = blockSize / sampleRate;

          console.log(`[AudioMixer] Applying dynamic ducking (${(duckingAmount * 100).toFixed(0)}% reduction during speech)`);

          // Apply initial fade in
          musicGain.gain.setValueAtTime(0, 0);
          musicGain.gain.linearRampToValueAtTime(volume, 2.0);

          // Apply ducking envelope starting after fade in
          applyDuckingToGain(musicGain, duckingEnvelope, volume, duckingAmount, blockDuration, 2.0);

          if (totalDuration > 4) {
            const lastEnvelopeValue = duckingEnvelope[duckingEnvelope.length - 1] ?? 0;
            musicGain.gain.setValueAtTime(volume * (1 - lastEnvelopeValue), totalDuration - 2);
            musicGain.gain.linearRampToValueAtTime(0, totalDuration);
          }
        } else {
          // No ducking - use standard fade envelope
          musicGain.gain.setValueAtTime(0, 0);
          musicGain.gain.linearRampToValueAtTime(volume, 2.0); // 2s fade in
          musicGain.gain.setValueAtTime(volume, totalDuration - 2);
          musicGain.gain.linearRampToValueAtTime(0, totalDuration); // 2s fade out
        }

        musicSource.connect(musicGain);
        musicGain.connect(offlineContext.destination);
        musicSource.start(0);
        musicSource.stop(totalDuration);

        console.log(`[AudioMixer] Added background music "${sfxPlan.backgroundMusic.name}"${enableDucking ? ' with dynamic ducking' : ''}`);
      }
    } catch (error) {
      console.warn("[AudioMixer] Failed to add background music:", error);
    }
  }

  // --- Render the mix ---
  console.log("[AudioMixer] Rendering audio mix...");
  const renderedBuffer = await offlineContext.startRendering();

  // Convert to WAV
  const wavBlob = audioBufferToWav(renderedBuffer);
  console.log(`[AudioMixer] Mix complete: ${(wavBlob.size / 1024 / 1024).toFixed(2)} MB`);

  // Cleanup
  await tempContext.close();

  return wavBlob;
}

/**
 * Convert AudioBuffer to WAV Blob
 */
function audioBufferToWav(buffer: AudioBuffer): Blob {
  const numChannels = buffer.numberOfChannels;
  const sampleRate = buffer.sampleRate;
  const format = 1; // PCM
  const bitDepth = 16;

  const bytesPerSample = bitDepth / 8;
  const blockAlign = numChannels * bytesPerSample;

  const samples = buffer.length;
  const dataSize = samples * blockAlign;
  const bufferSize = 44 + dataSize;

  const arrayBuffer = new ArrayBuffer(bufferSize);
  const view = new DataView(arrayBuffer);

  // WAV header
  const writeString = (offset: number, str: string) => {
    for (let i = 0; i < str.length; i++) {
      view.setUint8(offset + i, str.charCodeAt(i));
    }
  };

  writeString(0, 'RIFF');
  view.setUint32(4, bufferSize - 8, true);
  writeString(8, 'WAVE');
  writeString(12, 'fmt ');
  view.setUint32(16, 16, true); // fmt chunk size
  view.setUint16(20, format, true);
  view.setUint16(22, numChannels, true);
  view.setUint32(24, sampleRate, true);
  view.setUint32(28, sampleRate * blockAlign, true);
  view.setUint16(32, blockAlign, true);
  view.setUint16(34, bitDepth, true);
  writeString(36, 'data');
  view.setUint32(40, dataSize, true);

  // Write audio data
  const channelData = buffer.getChannelData(0);
  let offset = 44;

  for (let i = 0; i < samples; i++) {
    // Clamp and convert to 16-bit
    const sample = Math.max(-1, Math.min(1, channelData[i] ?? 0));
    const intSample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
    view.setInt16(offset, intSample, true);
    offset += 2;
  }

  return new Blob([arrayBuffer], { type: 'audio/wav' });
}

/**
 * Check if SFX mixing is available (has audio URLs)
 */
export function canMixSFX(sfxPlan: VideoSFXPlan | null): boolean {
  if (!sfxPlan) return false;

  // Check if any scene has an audio URL
  const hasSceneSfx = sfxPlan.scenes.some(s => s.ambientTrack?.audioUrl);
  const hasBackgroundMusic = !!sfxPlan.backgroundMusic?.audioUrl;

  return hasSceneSfx || hasBackgroundMusic;
}

/**
 * Merges multiple audio blobs into a single linear WAV file.
 * Used for concatenating scene narration segments.
 */
export async function mergeConsecutiveAudioBlobs(
  blobs: Blob[],
  sampleRate: number = 24000
): Promise<Blob> {
  if (blobs.length === 0) {
    throw new Error("No blobs to merge");
  }

  // If only one blob, just return it (or converting if needed, but assuming compatible)
  if (blobs.length === 1) {
    return blobs[0]!;
  }

  console.log(`[AudioMixer] Merging ${blobs.length} blobs...`);

  // Decode all blobs
  const audioContext = new AudioContext({ sampleRate });
  const audioBuffers: AudioBuffer[] = [];

  try {
    for (const blob of blobs) {
      const arrayBuffer = await blob.arrayBuffer();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      audioBuffers.push(audioBuffer);
    }

    // Calculate total duration
    const totalDuration = audioBuffers.reduce((acc, buf) => acc + buf.duration, 0);
    const totalSamples = Math.ceil(totalDuration * sampleRate);

    // Create offline context
    const offlineContext = new OfflineAudioContext(1, totalSamples, sampleRate);

    // Schedule sources
    let currentTime = 0;
    for (const buffer of audioBuffers) {
      const source = offlineContext.createBufferSource();
      source.buffer = buffer;
      source.connect(offlineContext.destination);
      source.start(currentTime);
      currentTime += buffer.duration;
    }

    // Render
    const renderedBuffer = await offlineContext.startRendering();

    // Convert to WAV
    return audioBufferToWav(renderedBuffer);
  } finally {
    await audioContext.close();
  }
}
````

## File: packages/shared/src/services/characterService.ts
````typescript
/**
 * Character Service - Character extraction and visual consistency
 * 
 * Handles:
 * - Character extraction from script text using AI
 * - Character reference sheet generation for visual consistency
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { GEMINI_API_KEY, MODELS } from "./shared/apiClient";
import { generateImageWithDeApi } from "./deapiService";
import { getCharacterSeed } from "./imageService";
import { buildImageStyleGuide, serializeStyleGuideAsText } from "./prompt/imageStyleGuide";
import { z } from "zod";
import type { CharacterProfile } from "@/types";
import { withAILogging } from "./aiLogService";

// Schema for structured character extraction
const CharacterExtractionSchema = z.object({
    characters: z.array(z.object({
        name: z.string().describe("Character name"),
        role: z.string().describe("Character role in the story (protagonist, antagonist, supporting, etc.)"),
        visualDescription: z.string().describe("Detailed physical appearance description (face, hair, clothes, age, distinguishing features) for image generation consistency")
    }))
});

/**
 * Extract main characters from script text using Gemini with structured output.
 * Creates consistent visual descriptions for each character.
 * 
 * @param scriptText - The full script or story text to analyze
 * @returns Array of CharacterProfile objects
 */
export async function extractCharacters(scriptText: string, sessionId?: string): Promise<CharacterProfile[]> {
    const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT_EXP,
        apiKey: GEMINI_API_KEY,
        temperature: 0.2,
    }).withStructuredOutput(CharacterExtractionSchema);

    const prompt = `Extract the main characters from this script/story.
For each character, create a detailed and consistent visual description that can be used for image generation.
The visual description should include: approximate age, gender, ethnicity, hair color/style, eye color,
body type, typical clothing/style, and any distinguishing features.
Make descriptions vivid and specific enough to maintain visual consistency across multiple images.

Script:
${scriptText}`;

    try {
        const result = await withAILogging(
            sessionId,
            'character_extract',
            MODELS.TEXT_EXP,
            prompt,
            () => model.invoke(prompt),
            (r) => JSON.stringify(r.characters),
        );

        return result.characters.map((c, i) => ({
            id: `char_${Date.now()}_${i}`,
            name: c.name,
            role: c.role,
            visualDescription: c.visualDescription,
        }));
    } catch (error) {
        console.error("[CharacterService] Failed to extract characters:", error);
        throw new Error(`Character extraction failed: ${error instanceof Error ? error.message : String(error)}`);
    }
}

/**
 * Generate a character reference sheet (turnaround/model sheet) for visual consistency.
 * Creates a multi-view image of the character for reference during scene generation.
 * 
 * @param charName - Character name
 * @param description - Visual description of the character
 * @param sessionId - Session ID for cloud autosave
 * @returns URL of the generated reference image
 */
export async function generateCharacterReference(
    charName: string,
    description: string,
    sessionId: string,
    style: string = "Cinematic",
): Promise<string> {
    // Build a structured JSON style guide using the project's visual style
    // so the character reference matches the art direction, while keeping
    // character-sheet composition (front + three-quarter view, neutral bg).
    const guide = buildImageStyleGuide({
      scene: `Character Design Sheet for "${charName}"`,
      subjects: [{ type: "person", description, pose: "front view and three-quarter view, full body" }],
      style,
      background: "neutral white background",
      lighting: { source: "studio softbox", quality: "soft diffused", direction: "rim light accent" },
      composition: { shot_type: "medium shot", camera_angle: "eye-level", framing: "center framing" },
      avoid: ["blur", "darkness", "noise", "low quality", "text", "watermark"],
    });
    const prompt = serializeStyleGuideAsText(guide);

    // Derive negative_prompt from the guide's avoid array
    const negativePrompt = guide.avoid.map(item => `no ${item}`).join(", ");

    const seed = getCharacterSeed(charName);
    console.log(`[CharacterService] Generating reference sheet for: ${charName} (DeAPI Flux_2_Klein_4B_BF16, seed=${seed}, style=${style})`);

    return generateImageWithDeApi({
        prompt,
        model: "Flux_2_Klein_4B_BF16",
        width: 768,
        height: 768,
        guidance: 3.5,
        steps: 4,
        seed,
        negative_prompt: negativePrompt,
    });
}

/**
 * Build a structured 30-50 word visual identity anchor string for a character.
 * Used as the "CHARACTERS IN FRAME:" section in image prompts to enforce visual consistency.
 *
 * Format: "[name]: [first 2 sentences of visualDescription]. Face: [facialTags]. Rendered in [style] art style."
 * Degrades gracefully if facialTags is absent — uses visualDescription sentences only.
 *
 * @param char - CharacterProfile with at minimum name and visualDescription
 * @param visualStyle - Current project art style (e.g., "Cinematic", "Anime / Manga")
 * @returns 30-50 word structured anchor string
 */
export function buildCoreAnchors(char: CharacterProfile, visualStyle: string = "Cinematic"): string {
    const parts: string[] = [];

    parts.push(`${char.name}:`);

    // First 2 sentences of visualDescription for compact physical identity
    const descSentences = char.visualDescription.match(/[^.!?]+[.!?]+/g) || [char.visualDescription];
    const coreDesc = descSentences.slice(0, 2).join(' ').trim();
    if (coreDesc) parts.push(coreDesc);

    // Compact face/clothing tags if available
    if (char.facialTags) {
        parts.push(`Face: ${char.facialTags}.`);
    }

    // Style anchor so the model knows the rendering context
    parts.push(`Rendered in ${visualStyle.toLowerCase()} art style.`);

    return parts.join(' ');
}

/**
 * Enrich an array of CharacterProfile objects by populating the coreAnchors field.
 * Call after character extraction and before image generation.
 *
 * @param characters - Array of CharacterProfile (already extracted)
 * @param visualStyle - Current project art style
 * @returns New array with coreAnchors populated on each character
 */
export function enrichCharactersWithCoreAnchors(
    characters: CharacterProfile[],
    visualStyle: string = "Cinematic"
): CharacterProfile[] {
    return characters.map(char => ({
        ...char,
        coreAnchors: buildCoreAnchors(char, visualStyle),
    }));
}

/**
 * Generate reference sheets for all characters in a cast.
 *
 * @param characters - Array of CharacterProfile objects
 * @param sessionId - Session ID for cloud autosave
 * @returns Updated CharacterProfile array with referenceImageUrl populated
 */
export async function generateAllCharacterReferences(
    characters: CharacterProfile[],
    sessionId: string,
    style: string = "Cinematic",
): Promise<CharacterProfile[]> {
    const results: CharacterProfile[] = [];

    for (let i = 0; i < characters.length; i++) {
        const char = characters[i];
        if (!char) continue;

        console.log(`[CharacterService] Generating reference ${i + 1}/${characters.length}: ${char.name}`);

        try {
            const referenceUrl = await generateCharacterReference(
                char.name,
                char.visualDescription,
                sessionId,
                style,
            );

            results.push({
                ...char,
                referenceImageUrl: referenceUrl,
            });
        } catch (error) {
            console.error(`[CharacterService] Failed to generate reference for ${char.name}:`, error);
            // Keep the character but without reference image
            results.push(char);
        }
    }

    return results;
}
````

## File: packages/shared/src/services/checkpointSystem.ts
````typescript
/**
 * Checkpoint System
 *
 * Manages user-approval checkpoints during pipeline execution.
 * Pipelines pause at configured points and wait for user approval
 * before continuing. Supports timeout, change requests, and resumption.
 *
 * Requirements: 17.1 (pause), 17.2 (resume), 17.3 (change requests), 17.4 (count constraint), 17.5 (timeout)
 */

import type { CheckpointState } from '../types';
import { agentLogger } from './logger';

const log = agentLogger.child('Checkpoint');

/** Default timeout: 30 minutes */
const DEFAULT_TIMEOUT_MS = 30 * 60 * 1000;

/**
 * Checkpoint approval response from the user
 */
export interface CheckpointApproval {
  approved: boolean;
  changeRequest?: string;
}

/**
 * Callback invoked when a checkpoint is created and needs UI display
 */
export type OnCheckpointCreated = (checkpoint: CheckpointState) => void;

/**
 * Checkpoint System class
 *
 * Each pipeline session creates one CheckpointSystem instance.
 * The system tracks all checkpoints and provides pause/resume semantics.
 */
export class CheckpointSystem {
  private checkpoints: Map<string, CheckpointState> = new Map();
  private pendingResolvers: Map<string, (approval: CheckpointApproval) => void> = new Map();
  private timeoutTimers: Map<string, ReturnType<typeof setTimeout>> = new Map();
  private onCheckpointCreated?: OnCheckpointCreated;
  private maxCheckpoints: number;

  constructor(options: {
    /** Maximum number of checkpoints for this pipeline (from format metadata) */
    maxCheckpoints: number;
    /** Callback when a checkpoint is created */
    onCheckpointCreated?: OnCheckpointCreated;
  }) {
    this.maxCheckpoints = options.maxCheckpoints;
    this.onCheckpointCreated = options.onCheckpointCreated;
  }

  /**
   * Create a checkpoint and pause execution until approved or timed out.
   *
   * @param phase Pipeline phase name (e.g., "research", "script")
   * @param timeoutMs Timeout in ms (default 30 minutes)
   * @returns The approval response
   * @throws If max checkpoint count is exceeded
   */
  async createCheckpoint(
    phase: string,
    data?: Record<string, unknown>,
    timeoutMs: number = DEFAULT_TIMEOUT_MS,
  ): Promise<CheckpointApproval> {
    // Enforce checkpoint count constraint (Requirement 17.4)
    if (this.checkpoints.size >= this.maxCheckpoints) {
      log.warn(`Max checkpoint count (${this.maxCheckpoints}) reached, skipping checkpoint for phase "${phase}"`);
      return { approved: true };
    }

    const checkpointId = `cp_${phase}_${Date.now()}`;
    const checkpoint: CheckpointState = {
      checkpointId,
      phase,
      status: 'pending',
      data,
    };

    this.checkpoints.set(checkpointId, checkpoint);
    log.info(`Checkpoint created: ${checkpointId} (phase: ${phase})`);

    // Notify UI
    this.onCheckpointCreated?.(checkpoint);

    // Create the pause promise (Requirement 17.1)
    const approval = await new Promise<CheckpointApproval>((resolve) => {
      this.pendingResolvers.set(checkpointId, resolve);

      // Set timeout (Requirement 17.5)
      const timer = setTimeout(() => {
        this.timeoutTimers.delete(checkpointId);
        if (this.pendingResolvers.has(checkpointId)) {
          log.warn(`Checkpoint ${checkpointId} timed out after ${timeoutMs}ms, auto-approving`);
          this.pendingResolvers.delete(checkpointId);
          const cp = this.checkpoints.get(checkpointId);
          if (cp) {
            cp.status = 'approved';
            cp.approvedAt = new Date();
          }
          resolve({ approved: true });
        }
      }, timeoutMs);

      this.timeoutTimers.set(checkpointId, timer);
    });

    return approval;
  }

  /**
   * Approve a pending checkpoint, resuming pipeline execution (Requirement 17.2).
   *
   * @param checkpointId The checkpoint to approve
   */
  approveCheckpoint(checkpointId: string): void {
    const resolver = this.pendingResolvers.get(checkpointId);
    if (!resolver) {
      log.warn(`No pending checkpoint found: ${checkpointId}`);
      return;
    }

    // Clear timeout
    const timer = this.timeoutTimers.get(checkpointId);
    if (timer) {
      clearTimeout(timer);
      this.timeoutTimers.delete(checkpointId);
    }

    // Update state
    const cp = this.checkpoints.get(checkpointId);
    if (cp) {
      cp.status = 'approved';
      cp.approvedAt = new Date();
    }

    this.pendingResolvers.delete(checkpointId);
    log.info(`Checkpoint approved: ${checkpointId}`);
    resolver({ approved: true });
  }

  /**
   * Reject a pending checkpoint with an optional change request (Requirement 17.3).
   *
   * @param checkpointId The checkpoint to reject
   * @param changeRequest Optional description of requested changes
   */
  rejectCheckpoint(checkpointId: string, changeRequest?: string): void {
    const resolver = this.pendingResolvers.get(checkpointId);
    if (!resolver) {
      log.warn(`No pending checkpoint found: ${checkpointId}`);
      return;
    }

    // Clear timeout
    const timer = this.timeoutTimers.get(checkpointId);
    if (timer) {
      clearTimeout(timer);
      this.timeoutTimers.delete(checkpointId);
    }

    // Update state
    const cp = this.checkpoints.get(checkpointId);
    if (cp) {
      cp.status = 'rejected';
    }

    this.pendingResolvers.delete(checkpointId);
    log.info(`Checkpoint rejected: ${checkpointId}${changeRequest ? ` (change: ${changeRequest})` : ''}`);
    resolver({ approved: false, changeRequest });
  }

  /**
   * Update a checkpoint's state (e.g., after re-processing with changes).
   */
  updateCheckpoint(checkpointId: string, updates: Partial<CheckpointState>): void {
    const cp = this.checkpoints.get(checkpointId);
    if (!cp) {
      log.warn(`Checkpoint not found: ${checkpointId}`);
      return;
    }
    Object.assign(cp, updates);
  }

  /**
   * Get a specific checkpoint by ID.
   */
  getCheckpoint(checkpointId: string): CheckpointState | null {
    return this.checkpoints.get(checkpointId) ?? null;
  }

  /**
   * Get all checkpoints in creation order.
   */
  getAllCheckpoints(): CheckpointState[] {
    return Array.from(this.checkpoints.values());
  }

  /**
   * Get count of created checkpoints.
   */
  getCheckpointCount(): number {
    return this.checkpoints.size;
  }

  /**
   * Check if there are any pending (unresolved) checkpoints.
   */
  hasPendingCheckpoints(): boolean {
    return this.pendingResolvers.size > 0;
  }

  /**
   * Clean up all timers and pending resolvers.
   * Call this when the pipeline is cancelled or completed.
   */
  dispose(): void {
    for (const timer of this.timeoutTimers.values()) {
      clearTimeout(timer);
    }
    this.timeoutTimers.clear();

    // Auto-approve any pending checkpoints so promises don't leak
    for (const [id, resolver] of this.pendingResolvers.entries()) {
      log.debug(`Disposing pending checkpoint: ${id}`);
      resolver({ approved: true });
    }
    this.pendingResolvers.clear();
  }
}
````

## File: packages/shared/src/services/cloudStorageService.ts
````typescript
/**
 * Cloud Storage Service - Google Cloud Storage integration
 *
 * Uploads production outputs to GCS bucket with organized folder structure
 * Format: gs://aisoul-studio-storage/YYYY-MM-DD_HH-mm-ss/
 *
 * This module has two parts:
 * 1. Direct GCS uploads (Node.js server-side only)
 * 2. Real-time autosave via server proxy (works in browser)
 */

// Configuration
const BUCKET_NAME = 'aisoul-studio-storage';

// Server API base URL for cloud autosave
// In browser: use relative URLs so requests go through Vite's dev proxy
// On server: use direct URL
const CLOUD_API_BASE = typeof window !== 'undefined'
  ? ''
  : 'http://localhost:3001';

/**
 * Convert a direct GCS URL to a proxy URL to avoid CORS issues.
 * Handles both storage.googleapis.com and direct bucket URLs.
 * 
 * @param url - The GCS URL to convert
 * @returns Proxy URL or original URL if not a GCS URL
 */
export function toProxyUrl(url: string): string {
  if (!url) return url;
  
  // Match https://storage.googleapis.com/BUCKET_NAME/path
  const gcsMatch = url.match(/^https?:\/\/storage\.googleapis\.com\/([^/]+)\/(.+)$/);
  if (gcsMatch) {
    const bucket = gcsMatch[1];
    const path = gcsMatch[2];
    if (bucket === BUCKET_NAME && path) {
      return `/api/cloud/file?path=${encodeURIComponent(path)}`;
    }
  }
  
  // Match gs://BUCKET_NAME/path (GCS URI format)
  const gsMatch = url.match(/^gs:\/\/([^/]+)\/(.+)$/);
  if (gsMatch) {
    const bucket = gsMatch[1];
    const path = gsMatch[2];
    if (bucket === BUCKET_NAME && path) {
      return `/api/cloud/file?path=${encodeURIComponent(path)}`;
    }
  }
  
  // Return original if not a GCS URL or different bucket
  return url;
}

// --- Upload Retry Configuration ---
const UPLOAD_TIMEOUT_MS = 60000; // 60 second timeout for uploads
const UPLOAD_MAX_RETRIES = 3;

/**
 * Fetch with timeout using AbortController
 */
async function fetchWithTimeout(
  url: string,
  options: RequestInit,
  timeoutMs: number
): Promise<Response> {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), timeoutMs);

  try {
    const response = await fetch(url, {
      ...options,
      signal: controller.signal
    });
    return response;
  } finally {
    clearTimeout(timeoutId);
  }
}

/**
 * Upload with retry logic for transient network failures
 */
async function uploadWithRetry(
  url: string,
  formData: FormData,
  maxRetries: number = UPLOAD_MAX_RETRIES
): Promise<Response> {
  let lastError: Error | null = null;

  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      console.log(`[Autosave] Upload attempt ${attempt + 1}/${maxRetries}`);

      const response = await fetchWithTimeout(url, {
        method: 'POST',
        body: formData
      }, UPLOAD_TIMEOUT_MS);

      return response;
    } catch (error) {
      lastError = error instanceof Error ? error : new Error(String(error));

      // Classify error type for logging
      const isNetworkError =
        lastError.name === 'AbortError' ||
        lastError.message.includes('Failed to fetch') ||
        lastError.message.includes('NetworkError');

      console.warn(`[Autosave] Upload failed (attempt ${attempt + 1}):`, {
        error: lastError.message,
        isNetworkError,
        willRetry: attempt < maxRetries - 1
      });

      if (attempt < maxRetries - 1) {
        // Exponential backoff: 2s, 4s, 8s
        const delay = 2000 * Math.pow(2, attempt);
        console.log(`[Autosave] Retrying in ${delay}ms...`);
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
  }

  throw lastError || new Error('Upload failed after retries');
}

// --- Real-Time Cloud Autosave (Browser-Compatible) ---

/**
 * Type for asset categories in cloud storage
 */
export type CloudAssetType = 'visuals' | 'audio' | 'music' | 'video_clips' | 'sfx' | 'subtitles' | 'ai_logs' | 'exports';

/**
 * Cloud autosave state tracker
 */
interface AutosaveState {
  sessionId: string | null;
  userId: string | null;
  initialized: boolean;
  uploadQueue: Promise<void>[];
  failedUploads: Array<{ filename: string; error: string }>;
}

const autosaveState: AutosaveState = {
  sessionId: null,
  userId: null,
  initialized: false,
  uploadQueue: [],
  failedUploads: [],
};

/**
 * Real-Time Cloud Autosave Service
 * Handles instant incremental backups of assets via server proxy.
 * Works in both browser and Node.js environments.
 */
export const cloudAutosave = {
  /**
   * Check if cloud storage is available
   */
  async checkAvailability(): Promise<{ available: boolean; message: string }> {
    try {
      const response = await fetch(`${CLOUD_API_BASE}/api/cloud/status`);
      if (!response.ok) {
        return { available: false, message: 'Cloud service unavailable' };
      }
      return await response.json();
    } catch (e) {
      return { available: false, message: String(e) };
    }
  },

  /**
   * Initialize cloud session folder.
   * Call this when the user clicks "Start Production" or "Plan Video".
   *
   * @param sessionId - The production session ID (e.g., prod_TIMESTAMP_HASH)
   * @param userId - Optional user ID for user-specific storage paths
   */
  async initSession(sessionId: string, userId?: string): Promise<boolean> {
    if (!sessionId) {
      console.warn('[Autosave] No sessionId provided, skipping cloud init');
      return false;
    }

    autosaveState.sessionId = sessionId;
    autosaveState.userId = userId || null;
    autosaveState.failedUploads = [];

    try {
      const response = await fetch(`${CLOUD_API_BASE}/api/cloud/init`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ sessionId, userId })
      });

      const result = await response.json();

      if (result.success) {
        autosaveState.initialized = true;
        console.log(`[Autosave] ✓ Cloud session initialized: ${result.folderPath}`);
        return true;
      } else {
        console.warn('[Autosave] Cloud init failed:', result.error || result.warning);
        autosaveState.initialized = false;
        return false;
      }
    } catch (e) {
      console.warn('[Autosave] Cloud init error (non-fatal):', e);
      autosaveState.initialized = false;
      return false;
    }
  },

  /**
   * Save an individual asset to cloud storage.
   * This is fire-and-forget by default to keep the UI responsive.
   *
   * @param sessionId - The production session ID
   * @param blob - The file blob to upload
   * @param filename - Filename for the asset (e.g., "scene_0.png")
   * @param type - Asset type category
   * @param waitForUpload - If true, wait for upload to complete (default: false)
   * @param makePublic - If true, make file public and return public URL (default: false)
   */
  async saveAsset(
    sessionId: string,
    blob: Blob,
    filename: string,
    type: CloudAssetType,
    waitForUpload: boolean = false,
    makePublic: boolean = false
  ): Promise<{ success: boolean; path?: string; publicUrl?: string; error?: string }> {
    if (!sessionId) {
      return { success: false, error: 'No sessionId' };
    }

    const formData = new FormData();
    formData.append('sessionId', sessionId);
    formData.append('assetType', type);
    formData.append('filename', filename);
    formData.append('file', blob, filename);
    formData.append('makePublic', String(makePublic));
    // Include userId for user-specific storage paths
    if (autosaveState.userId) {
      formData.append('userId', autosaveState.userId);
    }

    const uploadPromise = (async () => {
      try {
        // Use retry logic for transient network failures
        const response = await uploadWithRetry(
          `${CLOUD_API_BASE}/api/cloud/upload-asset`,
          formData
        );

        const result = await response.json();

        if (result.success) {
          console.log(`[Autosave] ✓ ${type}/${filename} saved to cloud${result.publicUrl ? ' (public)' : ''}`);
          return { success: true, path: result.path, publicUrl: result.publicUrl };
        } else {
          console.warn(`[Autosave] ${filename} not saved:`, result.warning || result.error);
          autosaveState.failedUploads.push({ filename, error: result.error || 'Unknown error' });
          return { success: false, error: result.error };
        }
      } catch (e) {
        const error = String(e);
        console.warn(`[Autosave] Upload failed for ${filename}:`, error);
        autosaveState.failedUploads.push({ filename, error });
        return { success: false, error };
      }
    })();

    if (waitForUpload) {
      return uploadPromise;
    }

    // Fire and forget - add to queue for tracking but don't wait
    autosaveState.uploadQueue.push(uploadPromise.then(() => {}));
    return { success: true, path: 'upload-pending' };
  },

  /**
   * Save image asset with scene metadata
   */
  async saveImage(
    sessionId: string,
    imageUrl: string,
    sceneIndex: number
  ): Promise<void> {
    try {
      const response = await fetch(imageUrl);
      const blob = await response.blob();
      const filename = `scene_${sceneIndex}.png`;
      await this.saveAsset(sessionId, blob, filename, 'visuals');
    } catch (e) {
      console.warn(`[Autosave] Failed to save image for scene ${sceneIndex}:`, e);
    }
  },

  /**
   * Save image and return public URL for persistence.
   * This waits for upload completion and returns the cloud URL.
   */
  async saveImageWithUrl(
    sessionId: string,
    imageUrl: string,
    shotId: string
  ): Promise<string | null> {
    try {
      const response = await fetch(imageUrl);
      const blob = await response.blob();
      const ext = blob.type.includes('png') ? 'png' : 'jpg';
      const filename = `shot_${shotId}.${ext}`;
      const result = await this.saveAsset(sessionId, blob, filename, 'visuals', true, true);
      return result.publicUrl || null;
    } catch (e) {
      console.warn(`[Autosave] Failed to save image for shot ${shotId}:`, e);
      return null;
    }
  },

  /**
   * Save audio/narration asset
   */
  async saveNarration(
    sessionId: string,
    audioBlob: Blob,
    sceneId: string
  ): Promise<void> {
    const filename = `narration_${sceneId}.wav`;
    await this.saveAsset(sessionId, audioBlob, filename, 'audio');
  },

  /**
   * Save narration audio and return public URL for persistence.
   */
  async saveNarrationWithUrl(
    sessionId: string,
    audioBlob: Blob,
    sceneId: string
  ): Promise<string | null> {
    try {
      const ext = audioBlob.type.includes('wav') ? 'wav' : 'mp3';
      const filename = `narration_${sceneId}.${ext}`;
      const result = await this.saveAsset(sessionId, audioBlob, filename, 'audio', true, true);
      return result.publicUrl || null;
    } catch (e) {
      console.warn(`[Autosave] Failed to save narration for scene ${sceneId}:`, e);
      return null;
    }
  },

  /**
   * Save video clip asset (from Veo)
   */
  async saveVideoClip(
    sessionId: string,
    videoUrl: string,
    sceneIndex: number
  ): Promise<void> {
    try {
      const response = await fetch(videoUrl);
      const blob = await response.blob();
      const filename = `scene_${sceneIndex}_veo.mp4`;
      await this.saveAsset(sessionId, blob, filename, 'video_clips');
    } catch (e) {
      console.warn(`[Autosave] Failed to save video for scene ${sceneIndex}:`, e);
    }
  },

  /**
   * Save animated video and return public URL for persistence.
   */
  async saveAnimatedVideoWithUrl(
    sessionId: string,
    videoUrl: string,
    shotId: string
  ): Promise<string | null> {
    try {
      const response = await fetch(videoUrl);
      const blob = await response.blob();
      const filename = `animated_${shotId}.mp4`;
      const result = await this.saveAsset(sessionId, blob, filename, 'video_clips', true, true);
      return result.publicUrl || null;
    } catch (e) {
      console.warn(`[Autosave] Failed to save animated video for shot ${shotId}:`, e);
      return null;
    }
  },

  /**
   * Save AI log entry to cloud storage as JSON.
   * Stores logs in the ai_logs folder within the session directory.
   *
   * @param sessionId - The production session ID
   * @param logEntry - The AI log entry to save
   */
  async saveAILog(
    sessionId: string,
    logEntry: {
      id: string;
      step: string;
      model: string;
      input: string;
      output: string;
      durationMs: number;
      timestamp: number;
      status: 'success' | 'error';
      error?: string;
      metadata?: Record<string, unknown>;
    }
  ): Promise<{ success: boolean; path?: string; error?: string }> {
    if (!sessionId) {
      return { success: false, error: 'No sessionId' };
    }

    try {
      // Create JSON blob from log entry
      const jsonContent = JSON.stringify(logEntry, null, 2);
      const blob = new Blob([jsonContent], { type: 'application/json' });
      const filename = `${logEntry.step}_${logEntry.id}.json`;

      const result = await this.saveAsset(sessionId, blob, filename, 'ai_logs', false, false);

      if (result.success) {
        console.log(`[Autosave] ✓ AI log saved: ${logEntry.step}/${logEntry.id}`);
      }
      return result;
    } catch (e) {
      console.warn(`[Autosave] Failed to save AI log:`, e);
      return { success: false, error: String(e) };
    }
  },

  /**
   * Wait for all pending uploads to complete
   */
  async flush(): Promise<{ completed: number; failed: number }> {
    const pending = [...autosaveState.uploadQueue];
    autosaveState.uploadQueue = [];

    await Promise.allSettled(pending);

    return {
      completed: pending.length - autosaveState.failedUploads.length,
      failed: autosaveState.failedUploads.length
    };
  },

  /**
   * Get current autosave state
   */
  getState(): { sessionId: string | null; initialized: boolean; pendingUploads: number; failedUploads: number } {
    return {
      sessionId: autosaveState.sessionId,
      initialized: autosaveState.initialized,
      pendingUploads: autosaveState.uploadQueue.length,
      failedUploads: autosaveState.failedUploads.length
    };
  },

  /**
   * Reset autosave state (call when starting a new session)
   */
  reset(): void {
    autosaveState.sessionId = null;
    autosaveState.initialized = false;
    autosaveState.uploadQueue = [];
    autosaveState.failedUploads = [];
  }
};
const PROJECT_ID = process.env.GOOGLE_CLOUD_PROJECT || process.env.VITE_GOOGLE_CLOUD_PROJECT;

// Storage client singleton
let storage: any = null;
let StorageClass: any = null;

/**
 * Initialize the Storage class (lazy loading)
 */
function getStorageClass(): any {
  if (StorageClass) return StorageClass;
  
  if (typeof window !== 'undefined') {
    throw new Error('Cloud Storage operations are not supported in browser. Use server-side export instead.');
  }
  
  try {
    const gcs = require('@google-cloud/storage');
    StorageClass = gcs.Storage;
    return StorageClass;
  } catch (error) {
    throw new Error(`Failed to load @google-cloud/storage: ${error}`);
  }
}

/**
 * Get or initialize storage client
 * NOTE: This only works in Node.js environment (server-side)
 */
function getStorageClient(): any {
  // Check if we're in a browser environment
  if (typeof window !== 'undefined') {
    throw new Error('Cloud Storage operations are not supported in browser. Use server-side export instead.');
  }

  if (!storage) {
    const Storage = getStorageClass();
    if (PROJECT_ID) {
      storage = new Storage({ projectId: PROJECT_ID });
    } else {
      storage = new Storage(); // Uses ADC
    }
  }
  return storage;
}

/**
 * Reset storage client (for testing)
 */
export function _resetStorageClient(): void {
  storage = null;
  StorageClass = null;
}

/**
 * Set mock storage class (for testing)
 */
export function _setMockStorageClass(mockClass: any): void {
  StorageClass = mockClass;
  storage = null;
}

/**
 * Generate folder name from current date/time
 * Format: YYYY-MM-DD_HH-mm-ss
 */
export function generateProductionFolder(): string {
  const now = new Date();
  const year = now.getFullYear();
  const month = String(now.getMonth() + 1).padStart(2, '0');
  const day = String(now.getDate()).padStart(2, '0');
  const hours = String(now.getHours()).padStart(2, '0');
  const minutes = String(now.getMinutes()).padStart(2, '0');
  const seconds = String(now.getSeconds()).padStart(2, '0');

  return `${year}-${month}-${day}_${hours}-${minutes}-${seconds}`;
}

/**
 * Upload result for a single file upload
 */
export interface UploadResult {
  success: boolean;
  fileName: string;
  gsPath: string;
  publicUrl?: string;
  size: number;
  error?: string;
}

/**
 * Production upload bundle
 */
export interface ProductionBundle {
  /** Final video blob */
  video?: Blob;
  /** Narration audio blob (concatenated) */
  narrationAudio?: Blob;
  /** Mixed audio blob (with SFX/music) */
  mixedAudio?: Blob;
  /** Background music blob */
  backgroundMusic?: Blob;
  /** Visual images/videos */
  visuals?: Array<{ sceneId: string; blob: Blob; type: 'image' | 'video' }>;
  /** Subtitle files */
  subtitles?: { format: 'srt' | 'vtt'; content: string }[];
  /** Production logs */
  logs?: string[];
  /** Metadata */
  metadata?: {
    topic: string;
    duration: number;
    language: string;
    sceneCount: number;
    productionId: string;
  };
}

/**
 * Upload a single file to GCS
 *
 * @param blob - File blob to upload
 * @param folderName - Production folder name (e.g., "2024-01-15_14-30-00")
 * @param fileName - File name (e.g., "final-video.mp4")
 * @param makePublic - Whether to make file publicly accessible
 * @returns Upload result
 */
export async function uploadFile(
  blob: Blob,
  folderName: string,
  fileName: string,
  makePublic: boolean = false
): Promise<UploadResult> {
  try {
    const client = getStorageClient();
    const bucket = client.bucket(BUCKET_NAME);
    const filePath = `${folderName}/${fileName}`;
    const file = bucket.file(filePath);

    console.log(`[CloudStorage] Uploading ${fileName} to gs://${BUCKET_NAME}/${filePath}`);

    // Convert blob to buffer
    const arrayBuffer = await blob.arrayBuffer();
    const buffer = Buffer.from(arrayBuffer);

    // Upload with metadata
    await file.save(buffer, {
      metadata: {
        contentType: blob.type || 'application/octet-stream',
        cacheControl: 'public, max-age=31536000',
      },
    });

    // Make public if requested
    let publicUrl: string | undefined;
    if (makePublic) {
      await file.makePublic();
      // Use proxy URL to avoid CORS issues
      publicUrl = `/api/cloud/file?path=${encodeURIComponent(filePath)}`;
    }

    console.log(`[CloudStorage] ✓ Uploaded ${fileName} (${Math.round(blob.size / 1024)}KB)`);

    return {
      success: true,
      fileName,
      gsPath: `gs://${BUCKET_NAME}/${filePath}`,
      publicUrl,
      size: blob.size,
    };
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error);
    console.error(`[CloudStorage] ✗ Failed to upload ${fileName}:`, errorMessage);

    return {
      success: false,
      fileName,
      gsPath: `gs://${BUCKET_NAME}/${folderName}/${fileName}`,
      size: blob.size,
      error: errorMessage,
    };
  }
}

/**
 * Upload text content as a file
 *
 * @param content - Text content to upload
 * @param folderName - Production folder name
 * @param fileName - File name (e.g., "subtitles.srt")
 * @param contentType - MIME type (default: text/plain)
 * @returns Upload result
 */
export async function uploadTextFile(
  content: string,
  folderName: string,
  fileName: string,
  contentType: string = 'text/plain; charset=utf-8'
): Promise<UploadResult> {
  const blob = new Blob([content], { type: contentType });
  return uploadFile(blob, folderName, fileName, false);
}

/**
 * Upload entire production bundle to GCS
 *
 * Creates organized folder structure:
 * - final-video.mp4          (main output)
 * - narration.wav            (narration audio)
 * - mixed-audio.wav          (final mixed audio)
 * - background-music.mp3     (background music if any)
 * - visuals/scene-1.png      (scene visuals)
 * - visuals/scene-2.png
 * - subtitles.srt            (subtitles if any)
 * - subtitles.vtt
 * - production.log           (production logs)
 * - metadata.json            (production metadata)
 *
 * @param bundle - Production bundle to upload
 * @param makePublic - Whether to make files publicly accessible
 * @returns Array of upload results
 */
export async function uploadProductionBundle(
  bundle: ProductionBundle,
  makePublic: boolean = false
): Promise<{
  folderName: string;
  results: UploadResult[];
  publicUrls: Record<string, string>;
  errors: string[];
}> {
  const folderName = generateProductionFolder();
  const results: UploadResult[] = [];
  const publicUrls: Record<string, string> = {};
  const errors: string[] = [];

  console.log(`[CloudStorage] Starting production upload to folder: ${folderName}`);

  // Upload video
  if (bundle.video) {
    const ext = bundle.video.type.includes('webm') ? 'webm' : 'mp4';
    const result = await uploadFile(bundle.video, folderName, `final-video.${ext}`, makePublic);
    results.push(result);
    if (result.success && result.publicUrl) {
      publicUrls.video = result.publicUrl;
    } else if (!result.success) {
      errors.push(`Video upload failed: ${result.error}`);
    }
  }

  // Upload narration audio
  if (bundle.narrationAudio) {
    const result = await uploadFile(bundle.narrationAudio, folderName, 'narration.wav', makePublic);
    results.push(result);
    if (result.success && result.publicUrl) {
      publicUrls.narration = result.publicUrl;
    } else if (!result.success) {
      errors.push(`Narration upload failed: ${result.error}`);
    }
  }

  // Upload mixed audio
  if (bundle.mixedAudio) {
    const result = await uploadFile(bundle.mixedAudio, folderName, 'mixed-audio.wav', makePublic);
    results.push(result);
    if (result.success && result.publicUrl) {
      publicUrls.mixedAudio = result.publicUrl;
    } else if (!result.success) {
      errors.push(`Mixed audio upload failed: ${result.error}`);
    }
  }

  // Upload background music
  if (bundle.backgroundMusic) {
    const ext = bundle.backgroundMusic.type.includes('wav') ? 'wav' : 'mp3';
    const result = await uploadFile(bundle.backgroundMusic, folderName, `background-music.${ext}`, makePublic);
    results.push(result);
    if (result.success && result.publicUrl) {
      publicUrls.music = result.publicUrl;
    } else if (!result.success) {
      errors.push(`Music upload failed: ${result.error}`);
    }
  }

  // Upload visuals
  if (bundle.visuals && bundle.visuals.length > 0) {
    for (const visual of bundle.visuals) {
      const ext = visual.type === 'video' ? 'mp4' : 'png';
      const fileName = `visuals/${visual.sceneId}.${ext}`;
      const result = await uploadFile(visual.blob, folderName, fileName, makePublic);
      results.push(result);
      if (!result.success) {
        errors.push(`Visual ${visual.sceneId} upload failed: ${result.error}`);
      }
    }
  }

  // Upload subtitles
  if (bundle.subtitles && bundle.subtitles.length > 0) {
    for (const subtitle of bundle.subtitles) {
      const contentType = subtitle.format === 'vtt' ? 'text/vtt' : 'text/plain';
      const result = await uploadTextFile(
        subtitle.content,
        folderName,
        `subtitles.${subtitle.format}`,
        contentType
      );
      results.push(result);
      if (result.success && result.publicUrl) {
        publicUrls[`subtitles_${subtitle.format}`] = result.publicUrl;
      } else if (!result.success) {
        errors.push(`Subtitles (${subtitle.format}) upload failed: ${result.error}`);
      }
    }
  }

  // Upload production logs
  if (bundle.logs && bundle.logs.length > 0) {
    const logContent = bundle.logs.join('\n');
    const result = await uploadTextFile(logContent, folderName, 'production.log', 'text/plain');
    results.push(result);
    if (!result.success) {
      errors.push(`Logs upload failed: ${result.error}`);
    }
  }

  // Upload metadata
  if (bundle.metadata) {
    const metadataContent = JSON.stringify(bundle.metadata, null, 2);
    const result = await uploadTextFile(metadataContent, folderName, 'metadata.json', 'application/json');
    results.push(result);
    if (!result.success) {
      errors.push(`Metadata upload failed: ${result.error}`);
    }
  }

  const successCount = results.filter(r => r.success).length;
  const totalSize = results.reduce((sum, r) => sum + r.size, 0);

  console.log(`[CloudStorage] Upload complete: ${successCount}/${results.length} files (${Math.round(totalSize / 1024 / 1024)}MB)`);
  console.log(`[CloudStorage] Folder: gs://${BUCKET_NAME}/${folderName}`);

  if (errors.length > 0) {
    console.warn(`[CloudStorage] ${errors.length} upload errors occurred`);
  }

  return {
    folderName,
    results,
    publicUrls,
    errors,
  };
}

/**
 * List all production folders in the bucket
 *
 * @param limit - Maximum number of folders to return
 * @returns Array of folder names
 */
export async function listProductionFolders(limit: number = 100): Promise<string[]> {
  try {
    const client = getStorageClient();
    const bucket = client.bucket(BUCKET_NAME);

    const [files] = await bucket.getFiles({ prefix: '', delimiter: '/' });

    // Extract unique folder prefixes
    const folders = new Set<string>();
    for (const file of files) {
      const parts = file.name.split('/');
      if (parts.length > 1) {
        folders.add(parts[0]);
      }
    }

    return Array.from(folders)
      .sort()
      .reverse()
      .slice(0, limit);
  } catch (error) {
    console.error('[CloudStorage] Failed to list folders:', error);
    return [];
  }
}

/**
 * Delete a production folder and all its contents
 *
 * @param folderName - Folder name to delete
 * @returns Number of files deleted
 */
export async function deleteProductionFolder(folderName: string): Promise<number> {
  try {
    const client = getStorageClient();
    const bucket = client.bucket(BUCKET_NAME);

    const [files] = await bucket.getFiles({ prefix: `${folderName}/` });

    console.log(`[CloudStorage] Deleting ${files.length} files from ${folderName}`);

    await Promise.all(files.map((file: any) => file.delete()));

    console.log(`[CloudStorage] ✓ Deleted folder ${folderName}`);

    return files.length;
  } catch (error) {
    console.error('[CloudStorage] Failed to delete folder:', error);
    return 0;
  }
}

/**
 * Check if GCS is configured and accessible
 */
export async function isStorageAvailable(): Promise<boolean> {
  try {
    const client = getStorageClient();
    const bucket = client.bucket(BUCKET_NAME);
    await bucket.exists();
    return true;
  } catch (error) {
    console.warn('[CloudStorage] Storage not available:', error);
    return false;
  }
}
````

## File: packages/shared/src/services/contentPlannerService.ts
````typescript
/**
 * ContentPlanner Service
 * 
 * LangChain-based orchestration for video content planning.
 * Analyzes topics and content to create structured video plans with:
 * - Scene breakdowns
 * - Visual descriptions
 * - Narration scripts
 * - Timing and pacing
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence, RunnableLambda } from "@langchain/core/runnables";
import { z } from "zod";
import { ContentPlan, Scene, EmotionalTone, TransitionType } from "../types";
import { API_KEY, MODELS } from "./shared/apiClient";
import { getSystemPersona, type Persona } from "./prompt/personaData";
import { getStyleEnhancement, type StyleEnhancement } from "./prompt/styleEnhancements";
import { type VideoPurpose, type LanguageCode, getLanguageName } from "../constants";
import { traceAsync } from "./tracing";
import { getEffectiveLegacyTone } from "./tripletUtils";
import { getVibeTerms, SCENARIO_TEMPLATES } from "./prompt/vibeLibrary";
import { cleanForTTS } from "./textSanitizer";

// --- Zod Schemas ---

/**
 * Available ambient SFX categories for AI to choose from.
 */
const SFX_CATEGORIES = [
    //write more sfx categories
    "scary",
    "haunted-house",    // Spooky/haunted environments
    "desert-wind",      // Desert/sand environments
    "desert-night",     // Quiet desert night
    "ocean-waves",      // Ocean/beach scenes
    "forest-ambience",  // Forest/nature scenes
    "rain-gentle",      // Rainy/cozy scenes
    "thunderstorm",     // Dramatic storm scenes
    "wind-howling",     // Windy/harsh weather
    "city-traffic",     // Urban/city scenes
    "cafe-ambience",    // Indoor social scenes
    "marketplace",      // Busy market/bazaar
    "eerie-ambience",   // Horror/scary scenes
    "mystical-drone",   // Magical/fantasy scenes
    "whispers",         // Ghostly/supernatural
    "heartbeat",        // Tension/suspense
    "tension-drone",    // Building tension
    "hopeful-pad",      // Positive/uplifting
    "epic-strings",     // Dramatic/cinematic
    "middle-eastern",   // Arabic/oriental scenes

] as const;

/**
 * Schema for ContentPlanner output validation.
 * Ensures AI output matches expected structure.
 */
export const ContentPlanSchema = z.object({
    title: z.string().describe("Title of the video content"),
    totalDuration: z.number().describe("Total duration in seconds"),
    targetAudience: z.string().describe("Description of target audience"),
    overallTone: z.string().describe("Overall emotional/stylistic tone"),
    // Character Bible - for consistency across scenes
    characters: z.array(z.object({
        name: z.string().describe("Character name or identifier"),
        appearance: z.string().describe("Detailed physical description: age, skin, hair, eyes, build"),
        clothing: z.string().describe("Specific clothing and accessories"),
        distinguishingFeatures: z.string().optional().describe("Scars, tattoos, jewelry, glasses, etc."),
        consistencyKey: z.string().optional().describe(
            "EXACTLY 5 comma-separated visual keywords capturing face+clothing essence. " +
            "E.g.: '10yo wiry boy, messy black hair'. Used as prompt anchor for every shot."
        ),
    })).optional().describe("Character definitions for visual consistency"),
    scenes: z.array(z.object({
        id: z.string().describe("Unique scene identifier"),
        name: z.string().describe("Scene name/title"),
        duration: z.number().describe("Scene duration in seconds"),
        visualDescription: z.string().max(200).describe("Detailed visual description for image/video generation (max 200 chars)"),
        narrationScript: z.string().describe("Pure spoken narration text ONLY. NEVER include labels like 'Emotional Hook:', 'Narrative Beat:', 'الخطاف العاطفي:', 'النقطة السردية:' or any markdown/stage-direction syntax. Only the words the narrator will say aloud."),
        emotionalTone: z.enum(["professional", "dramatic", "friendly", "urgent", "calm"]).optional().describe("Legacy emotional tone (fallback)"),
        // Instruction Triplet — preferred over emotionalTone
        instructionTriplet: z.object({
            primaryEmotion: z.string().describe("Core emotional vibe (e.g., 'visceral-dread', 'nostalgic-warmth')"),
            cinematicDirection: z.string().describe("Camera/visual style (e.g., 'slow-push-in', 'dutch-angle')"),
            environmentalAtmosphere: z.string().describe("Ambient texture (e.g., 'foggy-ruins', 'neon-rain')"),
        }).optional().describe("3-axis creative direction (preferred over emotionalTone)"),
        transitionTo: z.enum(["none", "fade", "dissolve", "zoom", "slide"]).optional().describe("Transition to next scene"),
        ambientSfx: z.string().optional().describe("Suggested ambient sound effect ID"),
        // Cinematography fields
        shotType: z.enum(["extreme-close-up", "close-up", "medium", "full", "wide", "extreme-wide"]).optional().describe("Camera shot type"),
        cameraMovement: z.enum(["static", "zoom-in", "zoom-out", "pan", "tracking", "pull-back"]).optional().describe("Camera movement type"),
        lighting: z.string().optional().describe("Lighting description (e.g., 'golden hour', 'harsh overhead', 'neon-lit')"),
    })).min(1).describe("Array of scenes"),
});

export type ContentPlanOutput = z.infer<typeof ContentPlanSchema>;

// --- Configuration ---

export interface ContentPlannerConfig {
    model?: string;
    temperature?: number;
    maxRetries?: number;
    targetDuration?: number; // Target video duration in seconds
    sceneCount?: number; // Desired number of scenes
    videoPurpose?: VideoPurpose; // Video purpose for persona selection
    visualStyle?: string; // Visual style for image generation
    language?: LanguageCode; // Output language for content
}

const DEFAULT_CONFIG: Required<Omit<ContentPlannerConfig, 'videoPurpose' | 'visualStyle' | 'language'>> & { videoPurpose: VideoPurpose; visualStyle: string; language: LanguageCode } = {
    model: MODELS.TEXT,
    temperature: 1.0,  // Higher temperature for more creative output
    maxRetries: 2,
    targetDuration: 60,
    sceneCount: 5,
    videoPurpose: "documentary",
    visualStyle: "cinematic",
    language: "auto",
};

// --- Error Types ---

export class ContentPlannerError extends Error {
    constructor(
        message: string,
        public readonly code: "INVALID_INPUT" | "AI_FAILURE" | "VALIDATION_ERROR" | "TIMEOUT",
        public readonly originalError?: Error
    ) {
        super(message);
        this.name = "ContentPlannerError";
    }
}

// --- Model Initialization ---

function createModel(config: ContentPlannerConfig = {}): ChatGoogleGenerativeAI {
    const mergedConfig = { ...DEFAULT_CONFIG, ...config };

    if (!API_KEY) {
        throw new ContentPlannerError(
            "Gemini API key is not configured",
            "INVALID_INPUT"
        );
    }

    return new ChatGoogleGenerativeAI({
        apiKey: API_KEY,
        model: mergedConfig.model,
        temperature: mergedConfig.temperature,
        maxRetries: mergedConfig.maxRetries,
    });
}

// --- Prompt Template ---

function createContentPlannerTemplate(persona: Persona, style: StyleEnhancement, language: LanguageCode, videoPurpose?: VideoPurpose): ChatPromptTemplate {
    // Build explicit language directive (placed at TOP for maximum visibility)
    const languageDirective = language && language !== 'auto'
        ? `🚨 MANDATORY LANGUAGE REQUIREMENT 🚨
YOU MUST generate ALL text content in ${getLanguageName(language)} (${language}).
This includes: title, scene names, narrationScript, targetAudience, overallTone.
This is NON-NEGOTIABLE. Do NOT translate to English. Output in ${getLanguageName(language)} ONLY.
Exception: visualDescription should remain in English for image generation compatibility.
`
        : `LANGUAGE DETECTION:
- Detect the language of the input topic/content
- Generate ALL text (title, scene names, narrationScript, etc.) in the SAME language as the input
- If input is in Arabic, output ALL text in Arabic
- ONLY visualDescription should remain in English (for image generation)
`;

    // Build persona-specific guidance
    const personaGuidance = `
VISUAL DIRECTOR PERSONA: ${persona.name} (${persona.role})
CORE RULE: ${persona.coreRule}

VISUAL PRINCIPLES:
${persona.visualPrinciples.map(p => `- ${p}`).join('\n')}

AVOID:
${persona.avoidList.map(a => `- ${a}`).join('\n')}`;

    // Build style-specific guidance
    const styleGuidance = `
VISUAL STYLE: ${style.mediumDescription}

STYLE KEYWORDS TO INCORPORATE:
${style.keywords.slice(0, 5).map(k => `- ${k}`).join('\n')}`;

    // Build Instruction Triplet guidance
    const emotionVibes = getVibeTerms("emotion", videoPurpose).map(v => v.id);
    const cinematicVibes = getVibeTerms("cinematic", videoPurpose).map(v => v.id);
    const atmosphereVibes = getVibeTerms("atmosphere", videoPurpose).map(v => v.id);

    const tripletGuidance = `
=== INSTRUCTION TRIPLET SYSTEM (PREFERRED) ===
For each scene, provide an "instructionTriplet" with 3 axes of creative direction:

1. **primaryEmotion** — The core emotional vibe driving the narration voice.
   Available: ${emotionVibes.slice(0, 15).join(', ')}...

2. **cinematicDirection** — The camera/visual style for the scene.
   Available: ${cinematicVibes.slice(0, 15).join(', ')}...

3. **environmentalAtmosphere** — The ambient texture/soundscape of the scene.
   Available: ${atmosphereVibes.slice(0, 15).join(', ')}...

Example:
"instructionTriplet": {
  "primaryEmotion": "visceral-dread",
  "cinematicDirection": "slow-push-in",
  "environmentalAtmosphere": "foggy-ruins"
}

You may ALSO provide "emotionalTone" as a fallback (one of: professional, dramatic, friendly, urgent, calm).
If you provide instructionTriplet, it takes precedence.`;

    // Build scenario engine hint
    const matchedScenario = SCENARIO_TEMPLATES.find(s =>
      videoPurpose === 'horror_mystery' && s.id === 'ghost-protocol' ||
      videoPurpose === 'storytelling' && s.id === 'desert-crossing' ||
      videoPurpose === 'commercial' && s.id === 'neon-descent' ||
      videoPurpose === 'documentary' && s.id === 'silent-signal'
    );
    const scenarioHint = matchedScenario
      ? `\n=== SCENARIO ENGINE ===\nConsider the "${matchedScenario.name}" narrative template: ${matchedScenario.description}\nSuggested arc: ${matchedScenario.arcBeats.map(b => b.beat).join(' → ')}\n`
      : '';

    // TTS-optimized narration style
    const ttsStyleGuidance = `
=== TTS-OPTIMIZED NARRATION STYLE ===
Write narration scripts optimized for text-to-speech delivery:
- Use SHORT, PUNCHY sentences (5-15 words each)
- Embed delivery markers when appropriate:
  [pause: long] — dramatic beat before a reveal
  [emphasis]key phrase[/emphasis] — vocal stress on important words
  [low-tone]dark content[/low-tone] — drop to lower register
  [whisper]secret or intimate[/whisper] — hushed delivery
  [breath] — natural breath for realism
- Open scenes with a HOOK that creates curiosity
- End scenes with a CLIFFHANGER or question that pulls into the next scene
- NEVER end a narration with a generic summary sentence
- Think documentary-mystery style: revelations, not explanations`;

    // Hard prohibition on metadata labels bleeding into narration output
    const narrationPurityRules = `
=== NARRATION SCRIPT PURITY (MANDATORY — VIOLATIONS BREAK THE PRODUCT) ===
The "narrationScript" JSON field must contain ONLY the spoken words the narrator will say aloud.

FORBIDDEN in narrationScript (these break subtitles and TTS):
- Structural labels: "Emotional Hook:", "Narrative Beat:", "Key Beat:", "Hook:", "Beat:"
- Arabic labels: "الخطاف العاطفي:", "النقطة السردية:", "الراوي:", "المشهد:", "وصف المشهد:"
- Markdown: ** bold **, * italic *, # headings, \`code\`, - bullets, > blockquotes
- Screenplay directions: [Direction:], (Note:), (SFX:), (Pause), INT., EXT.
- Multi-section structure: DO NOT split the field into labelled sub-sections

CORRECT ✅ (pure spoken narration):
"رمل الصحراء يغطي كل شيء. ريح هادئة تحمل أصوات الماضي البعيد."

WRONG ❌ (labels leaked in):
"الخطاف العاطفي: رمل الصحراء يغطي كل شيء. **النقطة السردية:** ريح هادئة."

If you feel the urge to write a label, STOP — write only the narration text itself.`;

    // Language hybridization rules
    const hybridLanguageNote = `
=== LANGUAGE HYBRIDIZATION ===
- narrationScript: MUST be in the target/detected language
- instructionTriplet values: ALWAYS in English (they are system identifiers)
- visualDescription: ALWAYS in English (for image generation)`;

    return ChatPromptTemplate.fromMessages([
        ["system", `${languageDirective}

You are an expert video content planner, creative director, and master storyteller.
Your job is to take a topic or content and create a detailed, CREATIVE, and ENGAGING video production plan.

${personaGuidance}

${styleGuidance}

${tripletGuidance}
${scenarioHint}
${ttsStyleGuidance}
${narrationPurityRules}
${hybridLanguageNote}

CREATIVITY GUIDELINES:
- Be IMAGINATIVE and ORIGINAL - don't just state facts, tell a compelling story
- Use vivid, evocative language that paints pictures in the viewer's mind
- Create emotional arcs - build tension, surprise, wonder, or curiosity
- Add unexpected angles, metaphors, or perspectives to make content memorable
- For folklore/stories: bring characters to life, create atmosphere, use sensory details
- For educational content: find the fascinating angle, the "wow" factor
- Think like a filmmaker - what would make this visually stunning and emotionally resonant?

PACING GUIDELINES:
- STANDARD SCENE: aim for 8-12 seconds duration (20-30 words of narration)
- FAST SCENE: aim for 3-5 seconds (5-10 words) for montage/action
- SLOW SCENE: aim for 12-15 seconds max.
- CRITICAL: If a narration segment requires more than 30 words (>12s), you MUST SPLIT IT into two consecutive scenes (e.g., "The Arrival (Part 1)" and "The Arrival (Part 2)").
- NEVER create a single scene longer than 15 seconds (static images become boring).
- VARY THE PACING - don't make every scene the same length
- For Documentary/Educational: Give the viewer time to absorb the visuals (lean towards 10s+)
- For Social Promo: Keep it tighter (5-8s)

AMBIENT SOUND EFFECTS (SFX):
For each scene, suggest an appropriate ambient sound effect from this list:
- "desert-wind" - Wind blowing across sand dunes (for desert/sand scenes)
- "desert-night" - Quiet desert night atmosphere
- "ocean-waves" - Ocean waves on shore (for beach/sea scenes)
- "forest-ambience" - Birds and nature sounds (for forest scenes)
- "rain-gentle" - Soft rain falling (for rainy/cozy scenes)
- "thunderstorm" - Heavy rain with thunder (for dramatic storm scenes)
- "wind-howling" - Strong wind (for harsh weather/mountain scenes)
- "city-traffic" - Urban traffic sounds (for city scenes)
- "cafe-ambience" - Coffee shop atmosphere (for indoor social scenes)
- "marketplace" - Busy market/bazaar sounds (for market scenes)
- "eerie-ambience" - Creepy atmosphere (for horror/scary scenes)
- "mystical-drone" - Ethereal sounds (for magical/fantasy scenes)
- "whispers" - Ghostly whispers (for supernatural scenes)
- "heartbeat" - Tense heartbeat (for suspense scenes)
- "tension-drone" - Building tension music (for thriller scenes)
- "hopeful-pad" - Uplifting ambient (for positive/inspiring scenes)
- "epic-strings" - Cinematic strings (for dramatic moments)
- "middle-eastern" - Arabic/oriental music (for Middle Eastern settings)

Choose the SFX that best matches each scene's mood and setting.

RULES:
1. Break the content into logical scenes that flow naturally with dramatic pacing
2. Each scene should have a clear visual description (max 200 chars) that can be used for image/video generation
3. Write narration scripts that are ENGAGING, POETIC, and match the target audience
4. Assign appropriate emotional tones to guide voice synthesis
5. Ensure pacing creates tension and release - not monotonous
6. Visual descriptions should be CONCRETE, SPECIFIC, and CINEMATIC - describe actual objects, settings, actions, lighting, mood
7. Avoid abstract concepts in visual descriptions - show don't tell
8. Apply the STYLE KEYWORDS to each visual description
9. Choose an appropriate ambientSfx for each scene based on its mood and setting
10. DEFINE A COLOR PALETTE (e.g., "Teal and Orange", "Desaturated Blue") and include color keywords in EVERY visual description to ensure visual cohesion.
11. MAINTAIN ENVIRONMENTAL ANCHORS: If scenes share a location, repeat descriptions of key background elements (e.g., "the cracked wall", "the red neon sign") to ground the viewer.

=== CHARACTER BIBLE (CRITICAL FOR CONSISTENCY) ===
If your story features recurring characters:
1. BEFORE writing scenes, define each character's EXACT appearance:
   - Face: age, skin tone, facial features, expressions
   - Hair: color, length, style, texture
   - Body: build, height, posture
   - Clothing: specific garments, colors, materials, accessories
   - Distinguishing marks: scars, tattoos, jewelry, glasses
2. In EVERY scene featuring that character, include their KEY identifiers
3. Use the EXACT SAME descriptors - "young woman with curly auburn hair and emerald eyes wearing a vintage blue dress" - not just "the woman"
4. Character descriptions should be 15-20 words minimum in each visualDescription
5. For each character, generate a "consistencyKey": exactly 5 comma-separated keywords
   capturing their most visually distinctive face and clothing traits.
   Example: "10yo wiry boy, messy black hair, worn school uniform, bright eyes"
   This key will be prepended to EVERY image prompt where this character appears.

=== CINEMATOGRAPHY VOCABULARY (USE IN visualDescription) ===
SHOT TYPES (Vary these):
- "Extreme close-up" - eyes, hands, small details (emotion, mystery)
- "Close-up" - face (intimacy, dialogue)
- "Medium shot" - waist up (action, interaction)
- "Wide shot" - character in environment (context)
- "Extreme wide shot" - vast landscape (scale, isolation)
- "Low angle" - looking up (power, dominance, awe)
- "High angle" - looking down (vulnerability)
- "Dutch angle" - tilted (unease, disorientation)

CAMERA MOVEMENT (CRITICAL FOR VIDEO):
- "Slow push-in" - increasing focus/tension
- "Slow pull-back" - revealing context/surprise
- "Truck left/right" - moving alongside character
- "Pan left/right" - surveying a landscape
- "Tilt up/down" - revealing vertical scale (buildings, trees)
- "Tracking shot" - following a moving subject
- "Static tripod" - stillness, contemplation
- "Handheld float" - subtle organic movement (reality/documentary)

COMPOSITION:
- "Rule of thirds" - balanced framing
- "Center framing" - symmetry/power
- "Leading lines" - depth/perspective
- "Depth of field" - blurry background (bokeh) to isolate subject

LIGHTING:
- "Golden hour" - warm, sun-flare, magical
- "Blue hour" - cold, pre-dawn, mysterious
- "Volumetric lighting" - god rays, atmosphere
- "Cinematic rim light" - separation from background
- "Soft window light" - natural, intimate
- "Neon noir" - contrasty, colorful, urban

=== "SHOW DON'T TELL" ENFORCEMENT ===
NEVER describe emotions directly. Instead, show through visual cues:

BAD (Narrating emotion):
- "He was nervous" → 
GOOD (Showing emotion):
- Visual: "Close-up of hands fidgeting with coffee cup, eyes darting to door"

BAD: "She was exhausted" →
GOOD: Visual: "Medium shot of woman slumped in chair, heavy eyelids, hair disheveled"

BAD: "The town was abandoned" →
GOOD: Visual: "Wide shot of empty street, broken windows, weeds through cracked pavement"

BAD: "He felt hopeful" →
GOOD: Visual: "Low angle of man looking up at sunrise, slight smile forming"

BAD: "The atmosphere was tense" →
GOOD: Visual: "Close-up of white-knuckled grip on steering wheel, sweat on brow"

=== NARRATION "SHOW DON'T TELL" (for narrationScript) ===
narrationScript will be spoken as voiceover. The viewer HEARS the narration while SEEING the visuals.
Write narration that describes what the VIEWER SEES — sensory, cinematic, grounded.

BAD narration: "He felt overwhelming fear as the situation became dangerous."
GOOD narration: "His breath caught. The corridor stretched into shadow, and something moved."

BAD narration: "She was very happy to see her old friend again."
GOOD narration: "Her eyes widened. A laugh escaped — the kind that shakes your whole body."

BAD narration: "The village had been abandoned for many years."
GOOD narration: "Dust coated every surface. A child's shoe lay in the doorway, sun-bleached and cracked."

RULES for narrationScript:
- Describe what the camera CAPTURES: movement, light, texture, sound
- Use concrete sensory details, not abstract emotional labels
- Short declarative sentences hit harder than long explanations
- Let the visuals carry the emotion — narration adds texture, not exposition

=== VISUAL VARIETY CHECKLIST ===
Across your scenes, ensure you have:
- [ ] At least ONE extreme close-up (detail shot)
- [ ] At least ONE wide/establishing shot
- [ ] Mix of camera angles (not all eye-level)
- [ ] Variety in lighting conditions
- [ ] Different character poses/actions
- [ ] Environmental variety (indoor/outdoor, close/distant)

CRITICAL - LANGUAGE RULES:
- DETECT the language of the input topic/content
- ALL text output (title, scene names, narrationScript, targetAudience, overallTone) MUST be in the SAME LANGUAGE as the input
- If input is in Arabic, output ALL text in Arabic
- If input is in Spanish, output ALL text in Spanish
- If input is in French, output ALL text in French
- And so on for any language
- ONLY the visualDescription should remain in English (for image generation compatibility)
- This is MANDATORY - never translate the user's language to English

CRITICAL - NARRATION LENGTH RULES:
- Speaking rate is approximately 2.5 words per second
- For a 10 second scene, narration should be ~25 words MAX
- For a 15 second scene, narration should be ~37 words MAX
- KEEP NARRATION SCRIPTS SHORT AND PUNCHY
- Each scene's narrationScript MUST fit within its duration
- For longer videos (e.g. 180s), increase the number of scenes and ensure narration fills at least 70% of the scene duration to avoid large silence gaps.

OUTPUT FORMAT:
Return a valid JSON object matching this structure:
{{
  "title": "Video Title (in input language)",
  "totalDuration": 60,
  "targetAudience": "General audience description (in input language)",
  "overallTone": "Professional and educational (in input language)",
  "scenes": [
    {{
      "id": "scene-1",
      "name": "Introduction (in input language)",
      "duration": 10,
      "visualDescription": "Close-up of coffee beans being poured into a grinder, warm morning light, ${style.keywords[0]} (ALWAYS IN ENGLISH)",
      "narrationScript": "Narration text in the SAME language as the input topic",
      "emotionalTone": "friendly",
      "instructionTriplet": {{
        "primaryEmotion": "nostalgic-warmth",
        "cinematicDirection": "slow-push-in",
        "environmentalAtmosphere": "golden-hour-decay"
      }},
      "transitionTo": "dissolve",
      "ambientSfx": "cafe-ambience"
    }}
  ]
}}`],
        ["human", `Create a CREATIVE and ENGAGING video content plan for the following:

TOPIC/CONTENT:
{content}

TARGET DURATION: {targetDuration} seconds
TARGET SCENE COUNT: {sceneCount} scenes
TARGET AUDIENCE: {targetAudience}

IMPORTANT: 
1. Keep each scene's narration script SHORT. Calculate word count as: (scene_duration * 2.5). 
   For example, a 10-second scene should have at most 25 words of narration.
2. CRITICAL: Detect the language of the TOPIC/CONTENT above and generate ALL text (title, scene names, narrationScript, targetAudience, overallTone) in that SAME language. Only visualDescription should be in English.
3. Be CREATIVE - use vivid imagery, emotional storytelling, and unexpected angles. Make it memorable!

Apply ${persona.name}'s visual principles and use ${style.mediumDescription} style in all visual descriptions.

Generate a complete content plan with detailed scenes.`],
    ]);
}

// --- Chain Creation ---

function createContentPlannerChain(config?: ContentPlannerConfig) {
    const model = createModel(config);

    // Get persona and style based on config
    const mergedConfig = { ...DEFAULT_CONFIG, ...config };
    const persona = getSystemPersona(mergedConfig.videoPurpose);
    const style = getStyleEnhancement(mergedConfig.visualStyle);

    const template = createContentPlannerTemplate(persona, style, mergedConfig.language, mergedConfig.videoPurpose);

    return RunnableSequence.from([
        template,
        model,
        new RunnableLambda({
            func: async (message: unknown): Promise<ContentPlanOutput> => {
                const content = typeof message === "object" && message !== null && "content" in message
                    ? String((message as { content: unknown }).content)
                    : String(message);

                // Clean the response
                const jsonStr = content
                    .replace(/^```json\s*/i, "")
                    .replace(/^```\s*/i, "")
                    .replace(/```$/i, "")
                    .trim();

                try {
                    const parsed = JSON.parse(jsonStr);

                    // Normalize values before validation
                    const validTones = ["professional", "dramatic", "friendly", "urgent", "calm"];
                    const validTransitions = ["none", "fade", "dissolve", "zoom", "slide"];

                    if (parsed.scenes && Array.isArray(parsed.scenes)) {
                        parsed.scenes = parsed.scenes.map((scene: any) => {
                            // Normalize emotionalTone if present
                            const normalizedTone = validTones.includes(scene.emotionalTone?.toLowerCase?.())
                                ? scene.emotionalTone.toLowerCase()
                                : undefined;

                            // Preserve instructionTriplet if provided
                            const instructionTriplet = scene.instructionTriplet && typeof scene.instructionTriplet === 'object'
                                ? {
                                    primaryEmotion: String(scene.instructionTriplet.primaryEmotion || "nostalgic-warmth"),
                                    cinematicDirection: String(scene.instructionTriplet.cinematicDirection || "handheld-float"),
                                    environmentalAtmosphere: String(scene.instructionTriplet.environmentalAtmosphere || "golden-hour-decay"),
                                }
                                : undefined;

                            // Effective tone for transition selection
                            const effectiveTone = normalizedTone || "friendly";

                            return {
                                ...scene,
                                // Truncate visualDescription to 200 chars max (AI sometimes gets creative)
                                visualDescription: scene.visualDescription?.length > 200
                                    ? scene.visualDescription.substring(0, 197) + "..."
                                    : scene.visualDescription,
                                // Strip any metadata labels/markdown that leaked into narration (defensive sanitization)
                                narrationScript: cleanForTTS(scene.narrationScript ?? ''),
                                emotionalTone: normalizedTone,
                                instructionTriplet,
                                // Intelligent transition selection based on emotional tone
                                transitionTo: (() => {
                                    if (scene.transitionTo && validTransitions.includes(scene.transitionTo?.toLowerCase?.())) {
                                        return scene.transitionTo.toLowerCase();
                                    }
                                    const moodTransitionMap: Record<string, string> = {
                                        'dramatic': 'dissolve',
                                        'urgent': 'none',
                                        'calm': 'fade',
                                        'friendly': 'slide',
                                        'professional': 'dissolve',
                                    };
                                    return moodTransitionMap[effectiveTone] || 'dissolve';
                                })(),
                                // Validate ambientSfx against known categories
                                ambientSfx: scene.ambientSfx && (SFX_CATEGORIES as readonly string[]).includes(scene.ambientSfx)
                                    ? scene.ambientSfx
                                    : undefined,
                            };
                        });
                    }

                    const validated = ContentPlanSchema.parse(parsed);

                    // Post-process: generate IDs and CALCULATE duration using smart algorithm
                    // Considers: narration length, content complexity, emotional weight, visual action
                    let totalDuration = 0;
                    validated.scenes = validated.scenes.map((scene, index) => {
                        const words = scene.narrationScript.split(/\s+/).length;

                        // Base duration: 2.5 words per second
                        let baseDuration = words / 2.5;

                        // --- Smart Duration Multipliers ---

                        // 1. Complexity bonus: Technical/educational content needs more time
                        const complexPatterns = /\b(therefore|consequently|furthermore|however|specifically|approximately|essentially|fundamentally|\d+%|\d{4}|century|million|billion)\b/gi;
                        const complexMatches = scene.narrationScript.match(complexPatterns) || [];
                        const complexityMultiplier = 1 + (complexMatches.length * 0.05); // +5% per complex term

                        // 2. Emotional weight: Dramatic scenes benefit from longer pauses
                        const emotionalMultipliers: Record<string, number> = {
                            'dramatic': 1.25,    // 25% longer for dramatic impact
                            'calm': 1.15,        // 15% longer for contemplative mood
                            'urgent': 0.85,      // 15% shorter for urgency
                            'professional': 1.0,
                            'friendly': 1.0,
                        };
                        // Use getEffectiveLegacyTone for scenes with triplet or legacy tone
                        const sceneTone = getEffectiveLegacyTone(scene as Scene);
                        const emotionalMultiplier = emotionalMultipliers[sceneTone] || 1.0;

                        // 3. Action intensity: Fast-paced visual content = shorter scenes
                        const actionPatterns = /\b(explode|run|chase|fight|crash|fall|jump|speed|rush|race|attack|escape|sprint|collide)\b/gi;
                        const actionMatches = scene.visualDescription.match(actionPatterns) || [];
                        const actionMultiplier = actionMatches.length > 0 ? 0.8 : 1.0; // 20% shorter for action

                        // Apply all multipliers
                        const smartDuration = baseDuration * complexityMultiplier * emotionalMultiplier * actionMultiplier;

                        // Clamp to valid range: min 5s, max 15s (avoid boring static images)
                        const calculatedDuration = Math.max(5, Math.min(15, Math.ceil(smartDuration) + 1));

                        console.log(`[ContentPlanner] Scene ${index + 1}: ${words} words, ${sceneTone} → ${calculatedDuration}s (complexity: x${complexityMultiplier.toFixed(2)}, emotion: x${emotionalMultiplier}, action: x${actionMultiplier})`);
                        totalDuration += calculatedDuration;

                        return {
                            ...scene,
                            id: scene.id || `scene-${index + 1}`,
                            duration: calculatedDuration,
                        };
                    });

                    // Update total duration
                    validated.totalDuration = totalDuration;
                    console.log(`[ContentPlanner] Total calculated duration: ${totalDuration}s`);

                    return validated;
                } catch (error) {
                    console.error("[ContentPlanner] Parse/validation error:", error);
                    console.error("[ContentPlanner] Raw content:", content.substring(0, 500));
                    throw new ContentPlannerError(
                        `Failed to parse content plan: ${error instanceof Error ? error.message : String(error)}`,
                        "VALIDATION_ERROR",
                        error instanceof Error ? error : undefined
                    );
                }
            },
        }),
    ]);
}

// --- Main API ---

/**
 * Generate a content plan from a topic or content.
 * 
 * @param content - The topic/content to plan (e.g., "How to make coffee")
 * @param options - Configuration options
 * @returns A structured ContentPlan with scenes
 */
export const generateContentPlan = traceAsync(
    async function generateContentPlanImpl(
        content: string,
        options: {
            targetDuration?: number;
            sceneCount?: number;
            targetAudience?: string;
            config?: ContentPlannerConfig;
        } = {}
    ): Promise<ContentPlan> {
        const {
            targetDuration = 60,
            sceneCount = 5,
            targetAudience = "General audience",
            config,
        } = options;

        if (!content?.trim()) {
            throw new ContentPlannerError(
                "Content is required for planning",
                "INVALID_INPUT"
            );
        }

        console.log("[ContentPlanner] Generating plan for:", content.substring(0, 100));

        const chain = createContentPlannerChain(config);

        try {
            const result = await chain.invoke({
                content,
                targetDuration,
                sceneCount,
                targetAudience,
            });

            console.log(`[ContentPlanner] Generated ${result.scenes.length} scenes, total duration: ${result.totalDuration}s`);

            // Convert to ContentPlan type
            return {
                title: result.title,
                totalDuration: result.totalDuration,
                targetAudience: result.targetAudience,
                overallTone: result.overallTone,
                characters: result.characters?.map(c => ({
                    name: c.name,
                    appearance: c.appearance,
                    clothing: c.clothing,
                    distinguishingFeatures: c.distinguishingFeatures,
                    consistencyKey: c.consistencyKey,
                })),
                scenes: result.scenes.map((scene): Scene => ({
                    id: scene.id,
                    name: scene.name,
                    duration: scene.duration,
                    visualDescription: scene.visualDescription,
                    narrationScript: scene.narrationScript,
                    emotionalTone: (scene.emotionalTone as EmotionalTone | undefined),
                    instructionTriplet: scene.instructionTriplet,
                    transitionTo: scene.transitionTo as TransitionType | undefined,
                    ambientSfx: scene.ambientSfx,
                })),
            };
        } catch (error) {
            if (error instanceof ContentPlannerError) {
                throw error;
            }

            console.error("[ContentPlanner] Chain execution failed:", error);
            throw new ContentPlannerError(
                `Content planning failed: ${error instanceof Error ? error.message : String(error)}`,
                "AI_FAILURE",
                error instanceof Error ? error : undefined
            );
        }
    },
    "generateContentPlan",
    {
        runType: "chain",
        metadata: { service: "contentPlanner" },
        tags: ["langchain", "content-planning"],
    }
);

/**
 * Calculate suggested scene count based on target duration.
 * Rule of thumb: 1 scene per 10-15 seconds
 */
export function suggestSceneCount(targetDurationSeconds: number): number {
    const minScenes = 3;
    const maxScenes = 20;
    const secondsPerScene = 12; // Average scene duration

    const suggested = Math.round(targetDurationSeconds / secondsPerScene);
    return Math.max(minScenes, Math.min(maxScenes, suggested));
}

/**
 * Validate a ContentPlan for completeness.
 */
export function validateContentPlan(plan: ContentPlan): {
    valid: boolean;
    issues: string[];
} {
    const issues: string[] = [];

    if (!plan.title) issues.push("Missing title");
    if (!plan.scenes.length) issues.push("No scenes defined");

    let totalDuration = 0;
    plan.scenes.forEach((scene, index) => {
        totalDuration += scene.duration;
        if (!scene.visualDescription) {
            issues.push(`Scene ${index + 1}: Missing visual description`);
        }
        if (!scene.narrationScript) {
            issues.push(`Scene ${index + 1}: Missing narration script`);
        }
        if (scene.duration <= 0) {
            issues.push(`Scene ${index + 1}: Invalid duration`);
        }
    });

    // Check if total duration is reasonably close to plan's totalDuration
    const durationDiff = Math.abs(totalDuration - plan.totalDuration);
    if (durationDiff > 10) {
        issues.push(`Total scene duration (${totalDuration}s) differs significantly from plan duration (${plan.totalDuration}s)`);
    }

    return {
        valid: issues.length === 0,
        issues,
    };
}
````

## File: packages/shared/src/services/deapiService.ts
````typescript
import { cloudAutosave } from "./cloudStorageService";

const DEAPI_DIRECT_BASE = "https://api.deapi.ai/api/v1/client";
const PROXY_BASE = "/api/deapi/proxy"; // Server-side proxy to bypass CORS
const DEFAULT_VIDEO_MODEL = "Ltxv_13B_0_9_8_Distilled_FP8";
const DEFAULT_IMAGE_MODEL = "Flux1schnell"; // Fast, high-quality text-to-image

// Rate limit: 1 request per 60 seconds for img2video
const RATE_LIMIT_MS = 60 * 1000; // 60 seconds

// Use Vite's import.meta.env for browser-side environment variables
// @ts-ignore - Vite injects import.meta.env at build time
const VITE_API_KEY = import.meta.env?.VITE_DEAPI_API_KEY || "";
const API_KEY = VITE_API_KEY || (typeof process !== 'undefined' && process.env?.DEAPI_API_KEY) || "";

// Detect if running in browser (use proxy) or Node.js (direct API calls)
const isBrowser = typeof window !== 'undefined';
const API_BASE = isBrowser ? PROXY_BASE : DEAPI_DIRECT_BASE;

// ============================================================
// Rate Limiter for img2video endpoint (1 request per 60 seconds)
// ============================================================

class RateLimiter {
  private lastRequestTime: number = 0;
  private queue: Array<{
    resolve: () => void;
    reject: (error: Error) => void;
  }> = [];
  private isProcessing: boolean = false;

  /**
   * Wait until rate limit allows the next request.
   * Returns immediately if enough time has passed, otherwise waits.
   */
  async waitForSlot(): Promise<void> {
    return new Promise((resolve, reject) => {
      this.queue.push({ resolve, reject });
      this.processQueue();
    });
  }

  private async processQueue(): Promise<void> {
    if (this.isProcessing || this.queue.length === 0) return;

    this.isProcessing = true;

    while (this.queue.length > 0) {
      const now = Date.now();
      const timeSinceLastRequest = now - this.lastRequestTime;
      const waitTime = RATE_LIMIT_MS - timeSinceLastRequest;

      if (waitTime > 0 && this.lastRequestTime > 0) {
        console.log(`[DeAPI] Rate limit: waiting ${Math.ceil(waitTime / 1000)}s before next request (${this.queue.length} in queue)`);
        await new Promise(r => setTimeout(r, waitTime));
      }

      const item = this.queue.shift();
      if (item) {
        this.lastRequestTime = Date.now();
        item.resolve();
      }
    }

    this.isProcessing = false;
  }

  /**
   * Get the current queue length (for UI feedback)
   */
  getQueueLength(): number {
    return this.queue.length;
  }

  /**
   * Get estimated wait time in seconds for a new request
   */
  getEstimatedWaitTime(): number {
    if (this.lastRequestTime === 0) return 0;
    const timeSinceLastRequest = Date.now() - this.lastRequestTime;
    const waitTime = RATE_LIMIT_MS - timeSinceLastRequest;
    const queueWait = this.queue.length * RATE_LIMIT_MS;
    return Math.max(0, Math.ceil((waitTime + queueWait) / 1000));
  }
}

// Singleton rate limiter for img2video requests
const img2videoRateLimiter = new RateLimiter();

// ============================================================
// Tier Detection and Adaptive Rate Limiting
// ============================================================

/**
 * DeAPI tier information based on documentation:
 * - Basic (free $5 credits): 1-10 RPM, 15-100 daily endpoint limit
 * - Premium (paid tier): 300 RPM, unlimited daily
 */
export type DeApiTier = "basic" | "premium" | "unknown";

let detectedTier: DeApiTier = "unknown";
let consecutiveSuccesses = 0;
let lastRateLimitTime = 0;

/**
 * Detect the user's tier based on rate limit responses.
 * Adapts concurrency limits accordingly.
 */
export const detectTier = (wasRateLimited: boolean): DeApiTier => {
  if (wasRateLimited) {
    lastRateLimitTime = Date.now();
    consecutiveSuccesses = 0;
    detectedTier = "basic";
  } else {
    consecutiveSuccesses++;
    // If we've had 20+ successful requests without rate limiting, likely premium
    if (consecutiveSuccesses > 20 && Date.now() - lastRateLimitTime > 60000) {
      detectedTier = "premium";
    }
  }
  return detectedTier;
};

/**
 * Get recommended concurrency based on detected tier.
 */
export const getRecommendedConcurrency = (): number => {
  switch (detectedTier) {
    case "premium":
      return 10; // Can handle higher concurrency
    case "basic":
      return 2;  // Conservative to avoid rate limits
    default:
      return 5;  // Default middle ground
  }
};

/**
 * Get current detected tier (for UI display).
 */
export const getCurrentTier = (): DeApiTier => detectedTier;

// ============================================================
// Exponential Backoff Retry Logic
// ============================================================

interface RetryOptions {
  maxRetries?: number;
  initialDelayMs?: number;
  maxDelayMs?: number;
  backoffMultiplier?: number;
}

/**
 * Execute a function with exponential backoff retry.
 * Recommended by DeAPI documentation for handling transient errors and rate limits.
 *
 * @param fn - Async function to execute
 * @param options - Retry configuration
 * @returns Result of the function
 */
export const withExponentialBackoff = async <T>(
  fn: () => Promise<T>,
  options: RetryOptions = {}
): Promise<T> => {
  const {
    maxRetries = 3,
    initialDelayMs = 1000,
    maxDelayMs = 30000,
    backoffMultiplier = 2,
  } = options;

  let lastError: Error | null = null;
  let delay = initialDelayMs;

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      const result = await fn();
      // Success - update tier detection
      detectTier(false);
      return result;
    } catch (error) {
      lastError = error instanceof Error ? error : new Error(String(error));

      // Check if it's a rate limit error
      const isRateLimit = lastError.message.includes('429') ||
                          lastError.message.toLowerCase().includes('rate limit');

      if (isRateLimit) {
        detectTier(true);
      }

      // Don't retry on non-retryable errors
      const isRetryable = isRateLimit ||
                          lastError.message.includes('503') ||
                          lastError.message.includes('502') ||
                          lastError.message.toLowerCase().includes('timeout');

      if (!isRetryable || attempt === maxRetries) {
        throw lastError;
      }

      console.log(`[DeAPI] Attempt ${attempt + 1} failed, retrying in ${delay}ms...`);
      await new Promise(resolve => setTimeout(resolve, delay));

      // Exponential backoff with jitter
      delay = Math.min(delay * backoffMultiplier + Math.random() * 1000, maxDelayMs);
    }
  }

  throw lastError || new Error('Unknown error in retry loop');
};

/**
 * Get the estimated wait time for the next img2video request
 */
export const getImg2VideoWaitTime = (): number => {
  return img2videoRateLimiter.getEstimatedWaitTime();
};

/**
 * Get the number of queued img2video requests
 */
export const getImg2VideoQueueLength = (): number => {
  return img2videoRateLimiter.getQueueLength();
};

// Response from img2video endpoint - can be immediate or async
export interface DeApiResponse {
  request_id: string;
  status: "pending" | "processing" | "done" | "error";
  progress?: string;
  preview?: string | null;
  result_url?: string;
  error?: string;
}

// Text-to-image models available on DeAPI
export type DeApiImageModel =
  | "Flux1schnell"        // Fast, high-quality (recommended for speed)
  | "Flux_2_Klein_4B_BF16"   // FLUX.2 Klein 4B - fast generation, up to 1536px, supports guidance
  | "ZImageTurbo_INT8";   // Photorealistic, exceptional clarity (recommended for quality)

/**
 * Model recommendations based on use case:
 * - Flux1schnell: Best for rapid iteration, 1-4 steps, ~2-3 seconds per image
 * - Flux_2_Klein_4B_BF16: Fast generation with guidance control, up to 1536px, ideal for storyboarding
 * - ZImageTurbo_INT8: Best for final production, photorealistic output, INT8 optimized
 */
export const MODEL_RECOMMENDATIONS = {
  speed: "Flux1schnell" as DeApiImageModel,
  storyboard: "Flux_2_Klein_4B_BF16" as DeApiImageModel,
  quality: "ZImageTurbo_INT8" as DeApiImageModel,
} as const;

// Text-to-image request parameters
export interface Txt2ImgParams {
  prompt: string;
  model?: DeApiImageModel;
  width?: number;
  height?: number;
  guidance?: number;      // Guidance scale (default: 7.5)
  steps?: number;         // Inference steps (default: 4, max: 10 for FLUX)
  seed?: number;          // Random seed (-1 for random)
  negative_prompt?: string;
  loras?: string;         // Optional: LoRA adapters for model fine-tuning
  webhook_url?: string;   // Optional: Webhook URL for async completion notification
}

/**
 * Check if DeAPI is configured.
 * In browser: Assume true (proxy handles it).
 * In server: Check for API key.
 */
export const isDeApiConfigured = (): boolean => {
  if (isBrowser) return true; // Browser uses proxy, assume server is configured
  return Boolean(API_KEY && API_KEY.trim().length > 0);
};

/**
 * Get a user-friendly message about DeAPI configuration status.
 */
export const getDeApiConfigMessage = (): string => {
  if (isDeApiConfigured()) {
    return "DeAPI is configured and ready to use.";
  }
  return (
    "DeAPI is not configured on the server. To enable video animation:\n" +
    "1. Get an API key from https://deapi.ai\n" +
    "2. Add VITE_DEAPI_API_KEY=your_key to your .env.local file\n" +
    "3. Restart the development server"
  );
};

// Helper to convert Base64 to Blob/File
const base64ToBlob = async (base64Data: string): Promise<Blob> => {
  const base64Response = await fetch(base64Data);
  return await base64Response.blob();
};

/**
 * Poll for request completion with exponential backoff and rate limit handling
 */
async function pollRequest(requestId: string): Promise<string> {
  const maxAttempts = 60; // Max polling attempts
  const baseDelayMs = 3000; // Start with 3s delay
  const maxDelayMs = 15000; // Cap at 15s between polls
  const maxConsecutive429 = 5; // Max consecutive 429s before giving up

  let consecutive429Count = 0;
  let currentDelay = baseDelayMs;

  for (let attempt = 0; attempt < maxAttempts; attempt++) {
    // Wait before polling (skip first iteration to check immediately after submission)
    if (attempt > 0) {
      await new Promise((resolve) => setTimeout(resolve, currentDelay));
    }

    try {
      const headers: Record<string, string> = {
        Accept: "application/json",
      };

      // Only add Authorization header for direct API calls (not proxy)
      if (!isBrowser) {
        headers.Authorization = `Bearer ${API_KEY}`;
        headers["User-Agent"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) LyricLens/1.0";
      }

      const response = await fetch(`${API_BASE}/request-status/${requestId}`, {
        headers,
      });

      // Handle rate limiting (429)
      if (response.status === 429) {
        consecutive429Count++;
        console.warn(`[DeAPI] Rate limited (429) - attempt ${consecutive429Count}/${maxConsecutive429}`);

        if (consecutive429Count >= maxConsecutive429) {
          throw new Error(
            "DeAPI rate limit exceeded. Too many requests - please wait a few minutes before trying again."
          );
        }

        // Exponential backoff on 429: double the delay, up to max
        currentDelay = Math.min(currentDelay * 2, maxDelayMs);

        // Check for Retry-After header
        const retryAfter = response.headers.get("Retry-After");
        if (retryAfter) {
          const retryMs = parseInt(retryAfter, 10) * 1000;
          if (!isNaN(retryMs) && retryMs > 0) {
            currentDelay = Math.min(retryMs, 60000); // Cap at 60s
            console.log(`[DeAPI] Retry-After header: waiting ${currentDelay / 1000}s`);
          }
        }

        continue; // Retry with backoff
      }

      // Reset 429 counter on successful response
      consecutive429Count = 0;
      currentDelay = baseDelayMs; // Reset delay on success

      if (!response.ok) {
        // For other errors (500, 502, etc.), log and retry with backoff
        console.warn(`[DeAPI] Polling error: ${response.status} - retrying...`);
        currentDelay = Math.min(currentDelay * 1.5, maxDelayMs);
        continue;
      }

      const rawData = await response.json();
      // Handle nested response: { data: { status, result_url } } or flat: { status, result_url }
      const data = (rawData.data || rawData) as DeApiResponse;

      if (data.status === "done" && data.result_url) {
        console.log(`[DeAPI] Generation complete after ${attempt + 1} polls`);
        return data.result_url;
      }

      if (data.status === "error") {
        throw new Error(data.error || "Generation failed at provider");
      }

      // Still pending or processing - log progress
      if (data.progress) {
        console.log(`[DeAPI] Progress: ${data.progress}% (poll ${attempt + 1}/${maxAttempts})`);
      } else {
        console.log(`[DeAPI] Status: ${data.status || 'pending'} (poll ${attempt + 1}/${maxAttempts})`);
      }

    } catch (error: unknown) {
      // Network errors - retry with backoff
      const errorName = error instanceof Error ? error.name : '';
      const errorMessage = error instanceof Error ? error.message : String(error);
      if (errorName === "TypeError" || errorMessage.includes("fetch")) {
        console.warn(`[DeAPI] Network error during poll: ${errorMessage}`);
        currentDelay = Math.min(currentDelay * 1.5, maxDelayMs);
        continue;
      }
      // Re-throw application errors
      throw error;
    }
  }

  throw new Error(
    `Video generation timed out after ${maxAttempts} polling attempts (~${Math.round((maxAttempts * baseDelayMs) / 60000)} minutes)`
  );
}

/**
 * Calculate dimensions that fit within DeAPI's limits while preserving aspect ratio intent
 */
const getDeApiDimensions = (
  aspectRatio: "16:9" | "9:16" | "1:1" = "16:9",
): { width: number; height: number } => {
  // DeAPI supports up to 768px and requires dimensions divisible by 32
  switch (aspectRatio) {
    case "16:9":
      // Landscape: 768 x 432 (close to 16:9 ratio, divisible by 8)
      return { width: 768, height: 432 };
    case "9:16":
      // Portrait: 432 x 768 (close to 9:16 ratio, divisible by 8)
      return { width: 432, height: 768 };
    case "1:1":
      // Square: 768 x 768
      return { width: 768, height: 768 };
    default:
      return { width: 768, height: 768 };
  }
};

/**
 * Text-to-video generation parameters
 */
export interface Txt2VideoParams {
  prompt: string;
  model?: string;
  width?: number;
  height?: number;
  guidance?: number;
  steps?: number;
  frames?: number;
  fps?: number;
  seed?: number;
  webhook_url?: string;   // Optional: Webhook URL for async completion notification
}

/**
 * Image-to-video generation parameters
 */
export interface Img2VideoParams {
  first_frame_image: string;  // Base64 or URL
  last_frame_image?: string;  // Optional: Ending frame image
  prompt: string;
  model?: string;
  width?: number;
  height?: number;
  guidance?: number;
  steps?: number;
  frames?: number;
  fps?: number;
  seed?: number;
  webhook_url?: string;   // Optional: Webhook URL for async completion notification
}

/**
 * Generate a video from text prompt using DeAPI (txt2video endpoint)
 * This is useful for Story Mode when no source image is available.
 * 
 * @param params - Text-to-video generation parameters
 * @param aspectRatio - Aspect ratio preset
 * @param sessionId - Optional session ID for cloud storage
 * @param sceneIndex - Optional scene index for naming
 * @returns Base64-encoded video data URL
 */
export const generateVideoWithDeApi = async (
  params: Txt2VideoParams,
  aspectRatio: "16:9" | "9:16" | "1:1" = "16:9",
  sessionId?: string,
  sceneIndex?: number,
): Promise<string> => {
  if (!isDeApiConfigured()) {
    throw new Error(
      "DeAPI API key is not configured on the server.\n\n" +
      "To use DeAPI text-to-video:\n" +
      "1. Get an API key from https://deapi.ai\n" +
      "2. Add VITE_DEAPI_API_KEY=your_key to your .env.local file\n" +
      "3. Restart the development server (npm run dev:all)"
    );
  }

  const { width, height } = getDeApiDimensions(aspectRatio);

  const {
    prompt,
    model = DEFAULT_VIDEO_MODEL,
    guidance = 0,
    steps = 1, // Distilled model requires max 1 step
    frames = 120, // 4 seconds at 30fps
    fps = 30,
    seed = -1,
    webhook_url,
  } = params;

  console.log(`[DeAPI] Generating video from text: ${width}x${height}, prompt: ${prompt.substring(0, 50)}...`);

  // Build FormData for txt2video endpoint
  const formData = new FormData();
  formData.append("prompt", prompt);
  formData.append("model", model);
  formData.append("width", width.toString());
  formData.append("height", height.toString());
  formData.append("guidance", guidance.toString());
  formData.append("steps", steps.toString());
  formData.append("frames", frames.toString());
  formData.append("fps", fps.toString());
  formData.append("seed", seed.toString());
  
  // Add optional webhook_url if provided
  if (webhook_url) {
    formData.append("webhook_url", webhook_url);
  }

  // Submit request
  let response: Response;

  if (isBrowser) {
    // Use proxy endpoint for browser - send as JSON for express.json() middleware
    response = await fetch('/api/deapi/txt2video', {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        prompt,
        model,
        width,
        height,
        guidance,
        steps,
        frames,
        fps,
        seed,
      }),
    });
  } else {
    // Direct API call from Node.js
    response = await fetch(`${DEAPI_DIRECT_BASE}/txt2video`, {
      method: "POST",
      headers: {
        Authorization: `Bearer ${API_KEY}`,
        Accept: "application/json",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AISoulStudio/1.0",
      },
      body: formData,
    });
  }

  if (!response.ok) {
    const errText = await response.text();
    let errorMessage = `DeAPI txt2video request failed (${response.status})`;

    // Check for Cloudflare challenge
    if (errText.includes('Just a moment') || errText.includes('challenge-platform')) {
      throw new Error(
        `DeAPI txt2video blocked by Cloudflare bot protection.\n` +
        `Use the app in browser (npm run dev:all) - browsers handle Cloudflare automatically.`
      );
    }

    try {
      const errJson = JSON.parse(errText);
      if (errJson.message) errorMessage = `DeAPI: ${errJson.message}`;
      else if (errJson.error) errorMessage = `DeAPI: ${errJson.error}`;
    } catch {
      if (errText) errorMessage = `DeAPI txt2video: ${errText.substring(0, 200)}`;
    }

    throw new Error(errorMessage);
  }

  const rawData = await response.json();
  console.log(`[DeAPI] txt2video raw response:`, JSON.stringify(rawData, null, 2));

  const data: DeApiResponse = rawData.data || rawData;

  // Get video URL (immediate or via polling)
  let videoUrl: string;

  if (data.result_url) {
    console.log(`[DeAPI] Video ready immediately!`);
    videoUrl = data.result_url;
  } else if (data.status === "error") {
    throw new Error(data.error || "Video generation failed at provider");
  } else if (data.request_id) {
    console.log(`[DeAPI] Polling for txt2video request: ${data.request_id}`);
    videoUrl = await pollRequest(data.request_id);
  } else {
    throw new Error("No request_id or result_url received from DeAPI txt2video");
  }

  // Download and convert to Base64
  console.log(`[DeAPI] Downloading video from: ${videoUrl.substring(0, 80)}...`);
  const vidResp = await fetch(videoUrl);

  if (!vidResp.ok) {
    throw new Error(`Failed to download generated video: ${vidResp.status}`);
  }

  const vidBlob = await vidResp.blob();
  console.log(`[DeAPI] Video downloaded: ${(vidBlob.size / 1024 / 1024).toFixed(2)} MB`);

  // Upload to cloud storage if session context is provided
  if (sessionId && sceneIndex !== undefined) {
    cloudAutosave.saveAsset(
      sessionId,
      vidBlob,
      `scene_${sceneIndex}_txt2video.mp4`,
      'video_clips'
    ).catch(err => {
      console.warn('[DeAPI] Cloud upload failed (non-fatal):', err);
    });
  }

  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => resolve(reader.result as string);
    reader.onerror = () => reject(new Error("Failed to convert video to base64"));
    reader.readAsDataURL(vidBlob);
  });
};

/**
 * Animate an image using DeAPI img2video endpoint
 */
export const animateImageWithDeApi = async (
  base64ImageInput: string,
  prompt: string,
  aspectRatio: "16:9" | "9:16" | "1:1" = "16:9",
  sessionId?: string,
  sceneIndex?: number,
  options?: {
    last_frame_image?: string;  // Optional: Ending frame image (base64)
    webhook_url?: string;       // Optional: Webhook for async completion
    motionStrength?: 'subtle' | 'moderate' | 'dynamic'; // Controls frame count and motion intensity
  },
): Promise<string> => {
  let base64Image = base64ImageInput;
  if (!isDeApiConfigured()) {
    throw new Error(
      "DeAPI API key is not configured on the server.\n\n" +
      "DeAPI is an optional video animation provider that converts still images to video loops.\n\n" +
      "To use DeAPI:\n" +
      "1. Get an API key from https://deapi.ai ($20 free credits for new accounts)\n" +
      "2. Add VITE_DEAPI_API_KEY=your_key to your .env.local file\n" +
      "3. Restart the development server (npm run dev:all)\n\n" +
      "Alternatives:\n" +
      "• Switch to 'Google Veo' as your video provider (requires paid Gemini API plan)\n" +
      "• Use 'Image' generation mode instead of video",
    );
  }

  // Validate that base64Image is a proper data URL that can be converted to a Blob
  if (!base64Image || (!base64Image.startsWith('data:image/') && !base64Image.startsWith('data:application/octet-stream'))) {
    // Attempt to fix: if it looks like raw base64 without the data URL prefix, add one
    if (base64Image && /^[A-Za-z0-9+/=]/.test(base64Image) && !base64Image.includes(':')) {
      console.warn('[DeAPI] Received raw base64 without data URL prefix, adding image/png prefix');
      base64Image = `data:image/png;base64,${base64Image}`;
    } else {
      throw new Error(
        `DeAPI img2video requires a valid image data URL. ` +
        `Received: ${base64Image ? base64Image.substring(0, 50) + '...' : 'empty/null'}`
      );
    }
  }

  console.log(`[DeAPI] Proceeding with animation...`);

  // Get dimensions that comply with DeAPI's max 768px limit
  const { width, height } = getDeApiDimensions(aspectRatio);

  // 1. Prepare Form Data
  const formData = new FormData();
  const imageBlob = await base64ToBlob(base64Image);

  // Determine frames from motion strength (Issue 4: reduce morphing artifacts)
  const motionFrameMap = { subtle: 60, moderate: 90, dynamic: 120 } as const;
  const motionStrength = options?.motionStrength || 'moderate';
  const frames = motionFrameMap[motionStrength] || 90;

  formData.append("first_frame_image", imageBlob, "frame0.png");
  formData.append("prompt", prompt);
  formData.append("frames", frames.toString());
  formData.append("width", width.toString());
  formData.append("height", height.toString());
  formData.append("fps", "30");
  formData.append("model", DEFAULT_VIDEO_MODEL);
  formData.append("guidance", "0"); // Distilled model does not support CFG
  formData.append("steps", "1"); // Distilled model requires max 1 step
  formData.append("seed", "-1"); // Random seed

  // Add optional last_frame_image if provided
  if (options?.last_frame_image) {
    const lastFrameBlob = await base64ToBlob(options.last_frame_image);
    formData.append("last_frame_image", lastFrameBlob, "frame_last.png");
  }

  // Add optional webhook_url if provided
  if (options?.webhook_url) {
    formData.append("webhook_url", options.webhook_url);
  }

  console.log(`[DeAPI] Submitting img2video request: ${width}x${height}, prompt: ${prompt.substring(0, 50)}...`);

  // 2. Submit Request
  // Browser uses dedicated proxy endpoint that handles FormData properly
  // Node.js uses direct API call
  let response: Response;

  if (isBrowser) {
    // Use dedicated img2video proxy that handles multipart/form-data
    response = await fetch('/api/deapi/img2video', {
      method: "POST",
      body: formData,
    });
  } else {
    // Direct API call from Node.js
    response = await fetch(`${DEAPI_DIRECT_BASE}/img2video`, {
      method: "POST",
      headers: {
        Authorization: `Bearer ${API_KEY}`,
        Accept: "application/json",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) LyricLens/1.0",
      },
      body: formData,
    });
  }

  if (!response.ok) {
    const errText = await response.text();
    let errorMessage = `DeAPI request failed (${response.status})`;

    // Check for Cloudflare challenge (common with Node.js/server-side requests)
    if (errText.includes('Just a moment') || errText.includes('challenge-platform') || errText.includes('_cf_chl')) {
      throw new Error(
        `DeAPI img2video blocked by Cloudflare bot protection.\n\n` +
        `This happens because DeAPI's video generation endpoint has stricter protection ` +
        `against automated/server-side requests.\n\n` +
        `Solutions:\n` +
        `1. Use the app in browser (npm run dev:all) - browsers handle Cloudflare automatically\n` +
        `2. Contact DeAPI support (support@deapi.ai) to request server-to-server access\n` +
        `3. Switch to Google Veo as your video provider (requires paid Gemini API plan)`
      );
    }

    try {
      const errJson = JSON.parse(errText);
      if (errJson.message) {
        errorMessage = `DeAPI: ${errJson.message}`;
      } else if (errJson.error) {
        errorMessage = `DeAPI: ${errJson.error}`;
      }
    } catch {
      if (errText) {
        errorMessage = `DeAPI img2video failed: API error: ${errText.substring(0, 200)}...`;
      }
    }

    throw new Error(errorMessage);
  }

  const rawData = await response.json();
  console.log(`[DeAPI] Raw response:`, JSON.stringify(rawData, null, 2));

  // Handle multiple response structure variations:
  // 1. Nested: { data: { request_id, status, ... } }
  // 2. Flat: { request_id, status, ... }
  // 3. Direct result: { status: "done", result_url: "..." }
  const data: DeApiResponse = rawData.data || rawData;
  console.log(`[DeAPI] Parsed response:`, data);

  // Check if result is immediately available
  let videoUrl: string;

  // Priority 1: Check for immediate result_url (status: "done")
  if (data.result_url) {
    console.log(`[DeAPI] Video ready immediately! Status: ${data.status || 'unknown'}`);
    videoUrl = data.result_url;
  }
  // Priority 2: Check for error status
  else if (data.status === "error") {
    throw new Error(data.error || "Generation failed at provider");
  }
  // Priority 3: Check for request_id to poll
  else if (data.request_id) {
    console.log(`[DeAPI] Polling for request: ${data.request_id}, status: ${data.status}`);
    videoUrl = await pollRequest(data.request_id);
  }
  // Priority 4: Fallback - unexpected structure
  else {
    console.error(`[DeAPI] Unexpected response structure:`, rawData);
    throw new Error(
      `No request_id or result_url received from DeAPI.\n\n` +
      `Response structure: ${JSON.stringify(rawData, null, 2)}\n\n` +
      `This might indicate:\n` +
      `1. API key is invalid or expired\n` +
      `2. API endpoint has changed\n` +
      `3. Request parameters are incorrect\n\n` +
      `Check browser console for full response details.`
    );
  }

  // 3. Download and convert to Base64 (for consistency with app architecture)
  console.log(`[DeAPI] Downloading video from: ${videoUrl.substring(0, 80)}...`);
  const vidResp = await fetch(videoUrl);

  if (!vidResp.ok) {
    throw new Error(`Failed to download generated video: ${vidResp.status}`);
  }

  const vidBlob = await vidResp.blob();
  console.log(`[DeAPI] Video downloaded: ${(vidBlob.size / 1024 / 1024).toFixed(2)} MB`);

  // Upload to cloud storage if session context is provided
  if (sessionId && sceneIndex !== undefined) {
    cloudAutosave.saveAsset(
      sessionId,
      vidBlob,
      `scene_${sceneIndex}_deapi.mp4`,
      'video_clips'
    ).catch(err => {
      console.warn('[DeAPI] Cloud upload failed (non-fatal):', err);
    });
  }

  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => resolve(reader.result as string);
    reader.onerror = () =>
      reject(new Error("Failed to convert video to base64"));
    reader.readAsDataURL(vidBlob);
  });
};

/**
 * Generate an image from text prompt using DeAPI
 * Supports FLUX.1-schnell (fast) and Z-Image-Turbo (photorealistic)
 * 
 * @param params - Text-to-image generation parameters
 * @returns Base64-encoded image data URL
 */
export const generateImageWithDeApi = async (
  params: Txt2ImgParams
): Promise<string> => {
  if (!isDeApiConfigured()) {
    throw new Error(
      "DeAPI API key is not configured on the server.\n\n" +
      "To use DeAPI text-to-image:\n" +
      "1. Get an API key from https://deapi.ai\n" +
      "2. Add VITE_DEAPI_API_KEY=your_key to your .env.local file\n" +
      "3. Restart the development server (npm run dev:all)"
    );
  }

  const {
    prompt,
    model = DEFAULT_IMAGE_MODEL,
    width = 768,
    height = 768,
    guidance = 7.5,
    steps = 4,  // FLUX models work best with 1-4 steps, max 10
    seed = -1,
    negative_prompt = "blur, darkness, noise, low quality, watermark, text overlay, UI elements, blurry, low resolution",
    loras,
    webhook_url,
  } = params;

  console.log(`[DeAPI] Generating image: ${model}, ${width}x${height}, prompt: ${prompt.substring(0, 50)}...`);

  // 1. Submit Request (via proxy in browser, direct in Node.js)
  const headers: Record<string, string> = {
    Accept: "application/json",
    "Content-Type": "application/json",
  };

  // Only add Authorization header for direct API calls (proxy handles auth server-side)
  if (!isBrowser) {
    headers.Authorization = `Bearer ${API_KEY}`;
    headers["User-Agent"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) LyricLens/1.0";
  }

  // Build request body with optional parameters
  const requestBody: Record<string, unknown> = {
    prompt,
    model,
    width,
    height,
    guidance,
    steps,
    seed,
    negative_prompt,
  };

  // Add optional parameters if provided
  if (loras) requestBody.loras = loras;
  if (webhook_url) requestBody.webhook_url = webhook_url;

  const response = await fetch(`${API_BASE}/txt2img`, {
    method: "POST",
    headers,
    body: JSON.stringify(requestBody),
  });

  if (!response.ok) {
    const errText = await response.text();
    let errorMessage = `DeAPI txt2img request failed (${response.status})`;

    try {
      const errJson = JSON.parse(errText);
      if (errJson.message) {
        errorMessage = `DeAPI: ${errJson.message}`;
      } else if (errJson.error) {
        errorMessage = `DeAPI: ${errJson.error}`;
      }
    } catch {
      if (errText) {
        errorMessage = `DeAPI: ${errText}`;
      }
    }

    throw new Error(errorMessage);
  }

  const rawData = await response.json();
  console.log(`[DeAPI] txt2img raw response:`, JSON.stringify(rawData, null, 2));

  // Handle response structure (same as img2video)
  const data: DeApiResponse = rawData.data || rawData;
  console.log(`[DeAPI] txt2img parsed response:`, data);

  let imageUrl: string;

  // Check for immediate result
  if (data.result_url) {
    console.log(`[DeAPI] Image ready immediately! Status: ${data.status || 'unknown'}`);
    imageUrl = data.result_url;
  } else if (data.status === "error") {
    throw new Error(data.error || "Image generation failed at provider");
  } else if (data.request_id) {
    console.log(`[DeAPI] Polling for txt2img request: ${data.request_id}`);
    imageUrl = await pollRequest(data.request_id);
  } else {
    console.error(`[DeAPI] Unexpected txt2img response:`, rawData);
    throw new Error("No request_id or result_url received from DeAPI txt2img");
  }

  // 2. Download and convert to Base64
  console.log(`[DeAPI] Downloading image from: ${imageUrl.substring(0, 80)}...`);
  const imgResp = await fetch(imageUrl);

  if (!imgResp.ok) {
    throw new Error(`Failed to download generated image: ${imgResp.status}`);
  }

  const imgBlob = await imgResp.blob();
  console.log(`[DeAPI] Image downloaded: ${(imgBlob.size / 1024).toFixed(2)} KB`);

  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => resolve(reader.result as string);
    reader.onerror = () =>
      reject(new Error("Failed to convert image to base64"));
    reader.readAsDataURL(imgBlob);
  });
};

/**
 * Helper function to generate image with aspect ratio presets
 */
export const generateImageWithAspectRatio = async (
  prompt: string,
  aspectRatio: "16:9" | "9:16" | "1:1" = "16:9",
  model: DeApiImageModel = "Flux1schnell",
  negativePrompt?: string
): Promise<string> => {
  const dimensions = getDeApiDimensions(aspectRatio);

  return generateImageWithDeApi({
    prompt,
    model,
    width: dimensions.width,
    height: dimensions.height,
    negative_prompt: negativePrompt,
  });
};

// ============================================================
// Cost Estimation
// ============================================================

/**
 * Pricing per DeAPI documentation (approximate, may change):
 * - txt2img (Flux): $0.00136 per 512×512 image at 4 steps
 * - img2video: ~$0.02-0.05 per 4-second clip (varies by resolution)
 * - txt2video: ~$0.03-0.08 per 4-second clip
 */
export interface CostEstimate {
  imageCount: number;
  videoCount: number;
  estimatedCostUSD: number;
  breakdown: {
    images: number;
    videos: number;
  };
}

/**
 * Estimate the cost of a batch generation job.
 * Note: These are approximate costs based on public pricing. Actual costs may vary.
 *
 * @param imageCount - Number of images to generate
 * @param videoCount - Number of videos to generate
 * @param resolution - Target resolution (affects pricing)
 * @returns Cost estimate with breakdown
 */
export const estimateBatchCost = (
  imageCount: number,
  videoCount: number,
  resolution: "16:9" | "9:16" | "1:1" = "16:9"
): CostEstimate => {
  // Base costs (approximate)
  const IMAGE_COST_BASE = 0.00136; // per 512x512 at 4 steps
  const VIDEO_COST_BASE = 0.03;    // per 4-second clip at 768x432

  // Resolution multipliers (larger = more expensive)
  const resolutionMultiplier = resolution === "1:1" ? 1.2 : 1.0;

  const imageCost = imageCount * IMAGE_COST_BASE * resolutionMultiplier * 2; // 768px is ~2x 512px
  const videoCost = videoCount * VIDEO_COST_BASE * resolutionMultiplier;

  return {
    imageCount,
    videoCount,
    estimatedCostUSD: Math.round((imageCost + videoCost) * 1000) / 1000,
    breakdown: {
      images: Math.round(imageCost * 1000) / 1000,
      videos: Math.round(videoCost * 1000) / 1000,
    },
  };
};

/**
 * Check if the user has sufficient credits for a batch job.
 * This would require an API call to DeAPI's account endpoint.
 */
export const checkCredits = async (): Promise<{ available: number; sufficient: boolean } | null> => {
  // TODO: Implement when DeAPI provides account balance endpoint
  // For now, return null to indicate unknown
  return null;
};

// ============================================================
// Batch Generation with Concurrency Control
// ============================================================

export interface BatchGenerationItem {
  id: string;
  prompt: string;
  aspectRatio?: "16:9" | "9:16" | "1:1";
  model?: DeApiImageModel;
  negativePrompt?: string;
}

export interface BatchGenerationResult {
  id: string;
  success: boolean;
  imageUrl?: string;
  error?: string;
}

export interface BatchGenerationProgress {
  completed: number;
  total: number;
  currentBatch: number;
  totalBatches: number;
  results: BatchGenerationResult[];
}

/**
 * Simple semaphore for controlling concurrent operations
 */
class Semaphore {
  private permits: number;
  private queue: Array<() => void> = [];

  constructor(permits: number) {
    this.permits = permits;
  }

  async acquire(): Promise<void> {
    if (this.permits > 0) {
      this.permits--;
      return Promise.resolve();
    }
    return new Promise<void>((resolve) => {
      this.queue.push(resolve);
    });
  }

  release(): void {
    const next = this.queue.shift();
    if (next) {
      next();
    } else {
      this.permits++;
    }
  }
}

/**
 * Generate multiple images concurrently with configurable concurrency limit.
 * This dramatically speeds up batch generation by running multiple DeAPI requests in parallel.
 *
 * @param items - Array of generation items with prompts and settings
 * @param concurrencyLimit - Maximum number of concurrent requests (default: 5, max recommended: 10)
 * @param onProgress - Optional callback for progress updates
 * @returns Array of results with success/failure status for each item
 */
export const generateImageBatch = async (
  items: BatchGenerationItem[],
  concurrencyLimit: number = 5,
  onProgress?: (progress: BatchGenerationProgress) => void
): Promise<BatchGenerationResult[]> => {
  if (!isDeApiConfigured()) {
    throw new Error("DeAPI API key is not configured.");
  }

  if (items.length === 0) {
    return [];
  }

  // Clamp concurrency to reasonable limits
  const effectiveConcurrency = Math.max(1, Math.min(concurrencyLimit, 10));
  const semaphore = new Semaphore(effectiveConcurrency);
  const results: BatchGenerationResult[] = [];
  let completed = 0;

  const totalBatches = Math.ceil(items.length / effectiveConcurrency);

  console.log(`[DeAPI Batch] Starting batch generation: ${items.length} items, concurrency: ${effectiveConcurrency}`);

  const processItem = async (item: BatchGenerationItem): Promise<BatchGenerationResult> => {
    await semaphore.acquire();

    try {
      console.log(`[DeAPI Batch] Processing item ${item.id}: ${item.prompt.substring(0, 50)}...`);

      const imageUrl = await generateImageWithAspectRatio(
        item.prompt,
        item.aspectRatio || "16:9",
        item.model || "Flux1schnell",
        item.negativePrompt
      );

      return {
        id: item.id,
        success: true,
        imageUrl,
      };
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      console.error(`[DeAPI Batch] Failed to generate item ${item.id}:`, errorMessage);

      return {
        id: item.id,
        success: false,
        error: errorMessage,
      };
    } finally {
      semaphore.release();
      completed++;

      // Report progress
      const currentBatch = Math.ceil(completed / effectiveConcurrency);
      onProgress?.({
        completed,
        total: items.length,
        currentBatch,
        totalBatches,
        results: [...results],
      });
    }
  };

  // Launch all requests concurrently - semaphore will control actual parallelism
  const promises = items.map(processItem);
  const allResults = await Promise.all(promises);

  // Sort results to match original order
  const resultMap = new Map(allResults.map(r => [r.id, r]));
  const orderedResults = items.map(item => resultMap.get(item.id)!);

  const successCount = orderedResults.filter(r => r.success).length;
  console.log(`[DeAPI Batch] Batch complete: ${successCount}/${items.length} successful`);

  return orderedResults;
};

/**
 * Generate multiple videos concurrently with configurable concurrency limit.
 * Note: Video generation is more resource-intensive, so lower concurrency is recommended.
 *
 * @param items - Array of images to animate with their prompts
 * @param concurrencyLimit - Maximum concurrent animations (default: 2, max recommended: 4)
 * @param onProgress - Optional callback for progress updates
 */
/**
 * Generate video directly from text prompt using txt2video endpoint.
 * This bypasses the two-step image→video workflow for faster generation.
 *
 * @param prompt - Text description of the video scene
 * @param aspectRatio - Aspect ratio for the video
 * @param durationFrames - Number of frames (default: 120 = 4 seconds at 30fps)
 * @returns Base64-encoded video data URL
 */
export const generateVideoFromText = async (
  prompt: string,
  aspectRatio: "16:9" | "9:16" | "1:1" = "16:9",
  durationFrames: number = 120
): Promise<string> => {
  if (!isDeApiConfigured()) {
    throw new Error("DeAPI API key is not configured.");
  }

  const { width, height } = getDeApiDimensions(aspectRatio);

  console.log(`[DeAPI] Generating video from text: ${width}x${height}, ${durationFrames} frames`);
  console.log(`[DeAPI] Prompt: ${prompt.substring(0, 80)}...`);

  const headers: Record<string, string> = {
    Accept: "application/json",
    "Content-Type": "application/json",
  };

  if (!isBrowser) {
    headers.Authorization = `Bearer ${API_KEY}`;
    headers["User-Agent"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) LyricLens/1.0";
  }

  const response = await fetch(`${API_BASE}/txt2video`, {
    method: "POST",
    headers,
    body: JSON.stringify({
      prompt,
      model: DEFAULT_VIDEO_MODEL,
      width,
      height,
      guidance: 0,
      steps: 1, // Distilled model
      frames: durationFrames,
      fps: 30,
      seed: -1,
    }),
  });

  if (!response.ok) {
    const errText = await response.text();
    throw new Error(`DeAPI txt2video failed (${response.status}): ${errText.substring(0, 200)}`);
  }

  const rawData = await response.json();
  const data: DeApiResponse = rawData.data || rawData;

  let videoUrl: string;

  if (data.result_url) {
    videoUrl = data.result_url;
  } else if (data.request_id) {
    console.log(`[DeAPI] Polling for txt2video request: ${data.request_id}`);
    videoUrl = await pollRequest(data.request_id);
  } else {
    throw new Error("No request_id or result_url received from DeAPI txt2video");
  }

  // Download and convert to base64
  const vidResp = await fetch(videoUrl);
  if (!vidResp.ok) {
    throw new Error(`Failed to download video: ${vidResp.status}`);
  }

  const vidBlob = await vidResp.blob();
  console.log(`[DeAPI] Video downloaded: ${(vidBlob.size / 1024 / 1024).toFixed(2)} MB`);

  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => resolve(reader.result as string);
    reader.onerror = () => reject(new Error("Failed to convert video to base64"));
    reader.readAsDataURL(vidBlob);
  });
};

export const animateImageBatch = async (
  items: Array<{
    id: string;
    imageUrl: string;
    prompt: string;
    aspectRatio?: "16:9" | "9:16" | "1:1";
  }>,
  concurrencyLimit: number = 2,
  onProgress?: (progress: BatchGenerationProgress) => void
): Promise<BatchGenerationResult[]> => {
  if (!isDeApiConfigured()) {
    throw new Error("DeAPI API key is not configured.");
  }

  if (items.length === 0) {
    return [];
  }

  // Lower concurrency for video generation (more resource intensive)
  const effectiveConcurrency = Math.max(1, Math.min(concurrencyLimit, 4));
  const semaphore = new Semaphore(effectiveConcurrency);
  const results: BatchGenerationResult[] = [];
  let completed = 0;

  const totalBatches = Math.ceil(items.length / effectiveConcurrency);

  console.log(`[DeAPI Batch] Starting video batch: ${items.length} items, concurrency: ${effectiveConcurrency}`);

  const processItem = async (item: typeof items[0]): Promise<BatchGenerationResult> => {
    await semaphore.acquire();

    try {
      console.log(`[DeAPI Batch] Animating item ${item.id}...`);

      const videoUrl = await animateImageWithDeApi(
        item.imageUrl,
        item.prompt,
        item.aspectRatio || "16:9"
      );

      return {
        id: item.id,
        success: true,
        imageUrl: videoUrl,
      };
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      console.error(`[DeAPI Batch] Failed to animate item ${item.id}:`, errorMessage);

      return {
        id: item.id,
        success: false,
        error: errorMessage,
      };
    } finally {
      semaphore.release();
      completed++;

      const currentBatch = Math.ceil(completed / effectiveConcurrency);
      onProgress?.({
        completed,
        total: items.length,
        currentBatch,
        totalBatches,
        results: [...results],
      });
    }
  };

  const promises = items.map(processItem);
  const allResults = await Promise.all(promises);

  const resultMap = new Map(allResults.map(r => [r.id, r]));
  const orderedResults = items.map(item => resultMap.get(item.id)!);

  const successCount = orderedResults.filter(r => r.success).length;
  console.log(`[DeAPI Batch] Video batch complete: ${successCount}/${items.length} successful`);

  return orderedResults;
};
````

## File: packages/shared/src/services/directorService.ts
````typescript
/**
 * Director Service
 * LangChain-based orchestration for prompt generation using a two-stage pipeline:
 * 1. Analyzer Agent: Interprets content structure, emotional arcs, and key themes
 * 2. Storyboarder Agent: Generates detailed visual prompts based on the analysis
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables";
import { z } from "zod";
import { ImagePrompt } from "../types";
import { VideoPurpose, CAMERA_ANGLES, LIGHTING_MOODS } from "../constants";
import { lintPrompt, getPurposeGuidance, getSystemPersona, getStyleEnhancement, generatePromptsFromLyrics, generatePromptsFromStory, refineImagePrompt, injectMasterStyle } from "./promptService";
import { parseSRTTimestamp } from "../utils/srtParser";
import { GEMINI_API_KEY, VERTEX_PROJECT, MODELS } from "./shared/apiClient";

// --- Zod Schemas ---

/**
 * Schema for Analyzer output validation.
 * Defines the structure of content analysis including sections, emotional arc, themes, and motifs.
 */
export const AnalysisSchema = z.object({
  sections: z.array(z.object({
    name: z.string().describe("Section name (e.g., Intro, Verse 1, Chorus)"),
    startTimestamp: z.string().describe("Start timestamp in MM:SS format"),
    endTimestamp: z.string().describe("End timestamp in MM:SS format"),
    type: z.enum(["intro", "verse", "pre-chorus", "chorus", "bridge", "outro", "transition", "key_point", "conclusion"]).describe("Section type"),
    emotionalIntensity: z.number().min(1).max(10).describe("Emotional intensity from 1-10"),
  })).describe("Content sections with timing and emotional intensity"),
  emotionalArc: z.object({
    opening: z.string().describe("Opening emotional state"),
    peak: z.string().describe("Peak emotional moment"),
    resolution: z.string().describe("Resolution emotional state"),
  }).describe("Overall emotional arc of the content"),
  themes: z.array(z.string()).describe("Key visual themes extracted from content"),
  motifs: z.array(z.string()).describe("Recurring visual motifs to maintain consistency"),
  // Art-directed visual scenes (replacing the old "concreteMotifs" approach)
  visualScenes: z.array(z.object({
    visualPrompt: z.string().describe("Full Midjourney-style image prompt (60-100 words) with subject, lighting, composition, atmosphere"),
    subjectContext: z.string().describe("Who/what this scene depicts and its narrative significance"),
    timestamp: z.string().describe("When this scene should appear (MM:SS format)"),
    emotionalTone: z.string().describe("Single word emotional tone (e.g., 'reverent', 'anguished', 'triumphant')"),
  })).describe("CRITICAL: Art-directed visual scenes with full cinematic prompts ready for image generation"),
});

export type AnalysisOutput = z.infer<typeof AnalysisSchema>;

/**
 * Schema for Storyboarder output validation.
 * Defines the structure of generated image prompts.
 */
export const StoryboardSchema = z.object({
  prompts: z.array(z.object({
    text: z.string()
      .min(200, "Visual prompt must be at least 200 characters (approximately 40 words)")
      .describe("REQUIRED: Complete visual scene description, MINIMUM 60 words. Must include: concrete subject, setting, lighting, camera angle, atmosphere. Example: 'A weathered merchant with sun-darkened skin stands behind ancient bronze scales in a dusty marketplace stall, golden afternoon light filtering through tattered canvas overhead, dust motes floating in amber rays, low angle shot emphasizing the dignity of commerce'"),
    mood: z.string().describe("Emotional tone of the scene"),
    timestamp: z.string().describe("Timestamp in MM:SS format"),
    negativePrompt: z.string().optional().describe("Elements to avoid in this specific scene (e.g., 'blurry, low quality, text, watermark, distorted faces')"),
  })),
  globalNegativePrompt: z.string().optional().describe("Negative prompt applied to ALL scenes (e.g., 'text, watermark, logo, blurry, low quality, distorted anatomy')"),
});

export type StoryboardOutput = z.infer<typeof StoryboardSchema>;

// --- Configuration Interface ---

/**
 * Configuration options for the Director Service.
 */
export interface DirectorConfig {
  /** Model name to use (defaults to MODELS.TEXT) */
  model?: string;
  /** Temperature for generation (0-1, defaults to 0.7) */
  temperature?: number;
  /** Maximum retry attempts on failure (defaults to 2) */
  maxRetries?: number;
  /** NEW: Target number of prompts to generate (defaults to 10) */
  targetAssetCount?: number;
}

// --- Default Configuration ---

const DEFAULT_CONFIG: Required<DirectorConfig> = {
  model: MODELS.TEXT,
  temperature: 0.7,
  maxRetries: 2,
  targetAssetCount: 10, // Default fallback if not provided
};

// --- Error Types ---

/**
 * Custom error class for Director Service errors.
 * Provides structured error information for debugging and fallback decisions.
 */
export class DirectorServiceError extends Error {
  public readonly code: DirectorErrorCode;
  public readonly stage: "analyzer" | "storyboarder" | "chain" | "validation" | "unknown";
  public readonly originalError?: Error;

  constructor(
    message: string,
    code: DirectorErrorCode,
    stage: "analyzer" | "storyboarder" | "chain" | "validation" | "unknown",
    originalError?: Error
  ) {
    super(message);
    this.name = "DirectorServiceError";
    this.code = code;
    this.stage = stage;
    this.originalError = originalError;
  }
}

/**
 * Error codes for Director Service failures.
 */
export type DirectorErrorCode =
  | "API_KEY_MISSING"
  | "MODEL_INIT_FAILED"
  | "CHAIN_EXECUTION_FAILED"
  | "OUTPUT_PARSING_FAILED"
  | "SCHEMA_VALIDATION_FAILED"
  | "RATE_LIMIT_EXCEEDED"
  | "NETWORK_ERROR"
  | "TIMEOUT"
  | "UNKNOWN_ERROR";


// --- LangChain Verbose Configuration ---

/**
 * Enable verbose mode for LangChain debugging.
 * Only enabled in development environment to avoid leaking sensitive info in production logs.
 */
const LANGCHAIN_VERBOSE = process.env.NODE_ENV === "development";

// --- Model Initialization ---

/**
 * Creates a configured ChatGoogleGenerativeAI model instance.
 * Uses Vertex AI authentication via ADC (Application Default Credentials).
 * The @langchain/google-genai package auto-detects ADC when no apiKey is provided.
 */
function createModel(config: DirectorConfig = {}): ChatGoogleGenerativeAI {
  const mergedConfig = { ...DEFAULT_CONFIG, ...config };

  if (!GEMINI_API_KEY && !VERTEX_PROJECT) {
    throw new DirectorServiceError(
      "Gemini API key is not configured. Set VITE_GEMINI_API_KEY in .env.local",
      "API_KEY_MISSING",
      "chain"
    );
  }

  return new ChatGoogleGenerativeAI({
    apiKey: GEMINI_API_KEY, // Can be empty if using Vertex
    model: mergedConfig.model,
    temperature: mergedConfig.temperature,
    verbose: LANGCHAIN_VERBOSE,
  });
}

// --- Analyzer Agent ---

/**
 * Creates the Analyzer prompt template.
 * Handles both "lyrics" and "story" content types.
 */
function createAnalyzerTemplate(contentType: "lyrics" | "story"): ChatPromptTemplate {
  return ChatPromptTemplate.fromMessages([
    ["system", `You are a professional content analyst and ART DIRECTOR specializing in ${contentType} analysis.
Your task is to analyze the provided content and create art-directed VISUAL SCENES with full cinematic prompts.

CONTENT TYPE: ${contentType}

ANALYSIS REQUIREMENTS:

1. SECTIONS (REQUIRED): Divide the content into logical sections
   - For lyrics: intro, verse, pre-chorus, chorus, bridge, outro
   - For story: intro, key_point, transition, conclusion
   - Each section needs: name, startTimestamp (MM:SS), endTimestamp (MM:SS), type, emotionalIntensity (1-10)

2. EMOTIONAL ARC (REQUIRED): Identify the overall emotional journey
   - opening: The initial emotional state/mood
   - peak: The most intense emotional moment
   - resolution: How the emotion resolves at the end

3. THEMES: Identify 3-6 key visual themes

4. MOTIFS: Identify 2-4 recurring visual motifs for consistency

5. VISUAL SCENES (CRITICAL - ART DIRECTOR MODE):
   You are an ART DIRECTOR, not an object spotter. For each key moment in the content:

   A. IDENTIFY THE SUBJECT:
      - WHO is this about? (Historical figure, prophet, spiritual being, human archetype)
      - WHAT story is being told? (Journey, sacrifice, transformation, revelation)
      - WHEN in history/mythology does this take place?

   B. CRAFT A FULL VISUAL PROMPT (60-100 words) that includes:
      - SUBJECT: Start with the main figure/element (e.g., "A bearded prophet in flowing robes...")
      - SETTING: Where are they? (e.g., "...standing atop a windswept mountain...")
      - LIGHTING: Dramatic light direction (e.g., "...bathed in golden rays breaking through storm clouds...")
      - ATMOSPHERE: Environmental mood (e.g., "...mist swirling at his feet, ancient stone altar visible...")
      - COMPOSITION: Camera angle and framing (e.g., "...shot from below, emphasizing divine connection...")

   C. PROVIDE CONTEXT:
      - What does this scene represent in the larger narrative?
      - Why is this moment visually significant?

   Generate 5-10 visualScenes distributed across the content duration.

OUTPUT FORMAT:
Return a valid JSON object (NO markdown code blocks, NO backtick wrapper) with ALL these fields:
- "sections": array of section objects with name, startTimestamp, endTimestamp, type, emotionalIntensity
- "emotionalArc": object with opening, peak, resolution strings
- "themes": array of theme strings
- "motifs": array of motif strings (recurring visual elements for consistency)
- "visualScenes": array of objects with visualPrompt, subjectContext, timestamp, emotionalTone

CRITICAL RULES:
- ALL fields are REQUIRED - sections, emotionalArc, themes, motifs, visualScenes
- Each visualScene.visualPrompt MUST be 60-100 words with full artistic direction
- Timestamps MUST be in MM:SS format (e.g., "01:30")
- Return ONLY the JSON object, no markdown formatting
- If content has no clear sections, create at least one section covering the full duration`],
    ["human", `Analyze this content and return the complete JSON structure with sections, emotionalArc, themes, motifs, and visualScenes:

{content}`],
  ]);
}

/**
 * Creates the Analyzer chain that processes content and outputs structured analysis.
 * Uses withStructuredOutput for robust JSON extraction.
 */
export function createAnalyzerChain(contentType: "lyrics" | "story", config?: DirectorConfig) {
  const model = createModel(config).withStructuredOutput(AnalysisSchema, {
    name: "content_analysis",
  });
  const template = createAnalyzerTemplate(contentType);

  return template.pipe(model);
}

/**
 * Runs the Analyzer agent on the provided content.
 */
export async function runAnalyzer(
  content: string,
  contentType: "lyrics" | "story",
  config?: DirectorConfig
): Promise<AnalysisOutput> {
  const chain = createAnalyzerChain(contentType, config);

  try {
    const result = await chain.invoke({
      content,
    });

    // Diagnostic logging: Check visualScenes quality
    if (result.visualScenes && result.visualScenes.length > 0) {
      console.log(`[Analyzer] Generated ${result.visualScenes.length} visual scenes:`);
      result.visualScenes.forEach((scene, i) => {
        const wordCount = scene.visualPrompt?.split(/\s+/).filter(Boolean).length || 0;
        const isFragment = wordCount < 30;
        console.log(`  Scene ${i + 1}: ${wordCount} words ${isFragment ? "⚠️ FRAGMENT" : "✓"} | Tone: ${scene.emotionalTone}`);
        if (isFragment && scene.visualPrompt) {
          console.log(`    Preview: "${scene.visualPrompt.substring(0, 80)}..."`);
        }
      });
    } else {
      console.warn("[Analyzer] No visualScenes generated - Storyboarder will create from scratch");
    }

    return result;
  } catch (error) {
    // If parsing fails, try to extract what we can and provide defaults
    console.warn("[Analyzer] Parsing failed, attempting to provide defaults:", error);

    // Return a minimal valid structure with defaults
    const defaultAnalysis: AnalysisOutput = {
      sections: [{
        name: "Full Content",
        startTimestamp: "00:00",
        endTimestamp: "03:00",
        type: contentType === "lyrics" ? "verse" : "key_point",
        emotionalIntensity: 5,
      }],
      emotionalArc: {
        opening: "Establishing mood",
        peak: "Emotional climax",
        resolution: "Conclusion",
      },
      themes: ["Visual storytelling", "Emotional journey"],
      motifs: ["Light and shadow", "Movement"],
      visualScenes: [],
    };

    return defaultAnalysis;
  }
}


// --- Storyboarder Agent ---

/**
 * Creates the Storyboarder prompt template.
 * Generates detailed visual prompts based on the Analyzer's output and persona rules.
 *
 * CRITICAL DESIGN DECISION: This prompt uses POSITIVE-ONLY framing.
 * We do NOT mention "text", "watermark", "logo", "subtitle", etc. because:
 * 1. The LLM often includes these words in its output even when told to avoid them
 * 2. The lint system flags any mention of these forbidden terms
 * 3. Positive framing ("focus on lighting, texture") works better than negative ("no text")
 */
function createStoryboarderTemplate(): ChatPromptTemplate {
  return ChatPromptTemplate.fromMessages([
    ["system", `{personaInstructions}

ART STYLE: {style}
{styleEnhancement}

{purposeGuidance}

GLOBAL SUBJECT: {globalSubject}
{subjectGuidance}

CRITICAL: If a GLOBAL SUBJECT is provided, you MUST use it as the main subject in almost every scene. Don't wander away from the subject.

VISUAL SCENES (ART-DIRECTED FOUNDATIONS):
{visualScenes}

YOUR ROLE AS STORYBOARDER:
The Analyzer has provided art-directed visual scene foundations. Your job is to:
1. USE these scenes as the backbone of your storyboard
2. ENHANCE each visualPrompt with additional cinematic details (depth, motion hints, micro-textures)
3. INTERPOLATE between scenes - create transitional frames that maintain visual continuity
4. EXPAND the visual language - if a scene shows "golden light", carry that through nearby scenes
5. MAINTAIN the subjectContext - keep the subject/figure consistent across scenes

DO NOT:
- Ignore the visualScenes and create entirely new concepts
- Contradict the subjectContext (if it's about a prophet, don't show a robot)
- Change the emotional tone without narrative reason

INTERPOLATION GUIDELINES:
- If you have fewer visualScenes than targetAssetCount, create transitional scenes between them
- Transitional scenes should maintain subject consistency while varying camera angle/lighting
- Use the emotionalTone field to guide the mood of interpolated scenes

AVAILABLE CAMERA ANGLES: {cameraAngles}
AVAILABLE LIGHTING MOODS: {lightingMoods}

CONTENT ANALYSIS:
{analysis}

=== VISUAL DESCRIPTION REQUIREMENTS ===

Your prompts must be PURE VISUAL DESCRIPTIONS suitable for photorealistic image generation.
Focus EXCLUSIVELY on these elements:

1. SUBJECT (MANDATORY FIRST ELEMENT):
   Start every prompt with a concrete, tangible subject:
   "A bearded man with weathered skin...", "A thick white candle...", "Weathered hands with calloused fingers..."

2. PHYSICAL DETAILS:
   - Materials: leather, wood grain, polished metal, rough stone, flowing silk
   - Textures: cracked, smooth, glistening, dusty, rain-slicked, velvet
   - Colors: specific hues (amber, slate grey, crimson, muted teal)

3. LIGHTING (REQUIRED):
   Every prompt must specify lighting:
   - Direction: backlighting, side lighting, overhead, rim light
   - Quality: soft diffused, harsh direct, dappled through leaves, golden hour warmth
   - Source: single shaft of light, multiple practicals, ambient glow, fire flicker

4. CAMERA & COMPOSITION:
   - Shot type: extreme close-up, medium shot, wide establishing, over-the-shoulder
   - Angle: low angle looking up, eye level, bird's eye, Dutch angle
   - Depth: shallow focus with bokeh, deep focus, rack focus point

5. ATMOSPHERE & ENVIRONMENT:
   - Particles: dust motes, smoke wisps, rain droplets, floating embers
   - Weather: overcast, golden hour, blue hour, stormy
   - Setting details: crumbling walls, polished floors, overgrown paths

6. MOTION HINTS (for video generation):
   - Camera: "slow dolly forward", "gentle crane up", "steady tracking left"
   - Environmental: "smoke drifting", "curtains billowing", "leaves falling"

=== QUALITY GUIDELINES ===

- Be SPECIFIC: "warm amber light filtering through dusty air" NOT "beautiful lighting"
- Be CINEMATIC: Think like a cinematographer describing a single film frame
- Be TACTILE: Include at least one texture that viewers could imagine touching
- Be CONSISTENT: Maintain subject appearance across scenes
- VARY COMPOSITIONS: Never repeat the same camera angle in consecutive scenes

=== EMOTIONAL ARC GUIDANCE ===

- Opening scenes (1-2): Establish mood with wide shots, environmental context
- Building scenes (3-5): Medium shots, character/subject focus, increasing detail
- Peak scenes (6-8): Dynamic angles, intimate close-ups, maximum visual intensity
- Resolution scenes (9+): Pull back, contemplative wide shots, fading light

=== OUTPUT REQUIREMENTS (CRITICAL - READ CAREFULLY) ===

Generate EXACTLY {targetAssetCount} prompts as a JSON object with a "prompts" array.

Each prompt object MUST have:
- "text": A COMPLETE visual description that is EXACTLY 60-120 words long
  ⚠️ MINIMUM 60 WORDS - shorter prompts will be rejected
  ⚠️ Every "text" field MUST start with a concrete, visible subject (person, object, place)
  ⚠️ Every "text" field MUST include: subject + setting + lighting + camera angle + atmosphere

  EXAMPLE of correct length (78 words):
  "A weathered merchant with sun-darkened skin and silver-streaked beard stands behind ancient bronze scales in a dusty marketplace stall, late afternoon golden light filtering through tattered canvas overhead, deep shadows pooling in the wooden stall corners, camera positioned at eye level capturing the intensity of his focused gaze, dust motes floating in amber light rays, worn leather pouch of coins on the counter, atmosphere of quiet dignity and timeless commerce"

- "mood": single word or short phrase for emotional tone
- "timestamp": MM:SS format, distributed across the content duration

DO NOT output short phrases like "a somber tone" or "by a steady hand" - these are INVALID.
Each "text" must be a COMPLETE, DETAILED scene description.`],
    ["human", `Create the visual storyboard based on the analysis provided. Generate exactly {targetAssetCount} prompts.

CRITICAL: Each prompt "text" field MUST be 60-120 words. Count your words. Short fragments will be rejected.

Use the visualScenes as foundations and enhance them with full cinematic detail including: subject, setting, lighting direction, camera angle, atmospheric elements, and textures.`],
  ]);
}

/**
 * Creates the Storyboarder chain that generates image prompts from analysis.
 * Uses withStructuredOutput for robust JSON extraction.
 */
export function createStoryboarderChain(config?: DirectorConfig) {
  const model = createModel(config).withStructuredOutput(StoryboardSchema, {
    name: "storyboard",
  });
  const template = createStoryboarderTemplate();

  return template.pipe(model);
}

/**
 * Runs the Storyboarder agent on the provided analysis.
 */
export async function runStoryboarder(
  analysis: AnalysisOutput,
  style: string,
  videoPurpose: VideoPurpose,
  globalSubject: string = "",
  config?: DirectorConfig
): Promise<StoryboardOutput> {
  const targetAssetCount = config?.targetAssetCount || 10;
  const chain = createStoryboarderChain(config);

  // Get persona for this video purpose
  const persona = getSystemPersona(videoPurpose);
  const personaInstructions = `You are ${persona.name}, a ${persona.role}.

YOUR CORE RULE:
${persona.coreRule}

YOUR VISUAL PRINCIPLES:
${persona.visualPrinciples.map(p => `- ${p}`).join('\n')}

WHAT TO AVOID:
${persona.avoidList.map(a => `- ${a}`).join('\n')}`;

  // Get style enhancement
  const styleData = getStyleEnhancement(style);
  const styleEnhancement = `MEDIUM AUTHENTICITY (apply these characteristics):
${styleData.keywords.map(k => `- ${k}`).join('\n')}
Overall: ${styleData.mediumDescription}`;

  // Get purpose guidance
  const purposeGuidance = getPurposeGuidance(videoPurpose);
  const subjectGuidance = globalSubject.trim()
    ? `Keep this subject's appearance consistent across scenes.`
    : `Create cohesive scenes with consistent environmental elements.`;

  // Format visual scenes from analysis (art-directed approach)
  const visualScenes = analysis.visualScenes && analysis.visualScenes.length > 0
    ? analysis.visualScenes.map((scene, i) =>
        `SCENE ${i + 1} [${scene.timestamp}] - ${scene.emotionalTone}:
      Subject: ${scene.subjectContext}
      Visual: ${scene.visualPrompt}`
      ).join('\n\n')
    : "No visual scenes provided - create cinematic scenes based on themes and emotional arc.";

  const result = await chain.invoke({
    style,
    personaInstructions,
    styleEnhancement,
    purposeGuidance,
    globalSubject: globalSubject || "None specified",
    subjectGuidance,
    visualScenes,
    cameraAngles: CAMERA_ANGLES.join(", "),
    lightingMoods: LIGHTING_MOODS.join(", "),
    analysis: JSON.stringify(analysis, null, 2),
    targetAssetCount,
  });

  // Diagnostic logging: Check for fragment prompts (< 30 words)
  if (result.prompts) {
    console.log("[Storyboarder] Raw output prompt lengths:");
    result.prompts.forEach((p, i) => {
      const wordCount = p.text?.split(/\s+/).filter(Boolean).length || 0;
      const isFragment = wordCount < 30;
      console.log(`  Prompt ${i + 1}: ${wordCount} words ${isFragment ? "⚠️ FRAGMENT" : "✓"}`);
      if (isFragment && p.text) {
        console.log(`    Preview: "${p.text.substring(0, 80)}..."`);
      }
    });
  }

  // Apply master style to each generated prompt for consistency
  if (result.prompts) {
    result.prompts = result.prompts.map(prompt => ({
      ...prompt,
      text: injectMasterStyle(prompt.text, style)
    }));
  }

  // VALIDATION: Check for short prompts and retry if needed
  const shortPrompts = result.prompts?.filter(p => {
    const wordCount = p.text?.split(/\s+/).filter(Boolean).length || 0;
    return wordCount < 40;
  }) || [];

  if (shortPrompts.length > 0 && config?.maxRetries && config.maxRetries > 0) {
    console.log(`[Storyboarder] Found ${shortPrompts.length} short prompts (< 40 words). Retrying...`);
    
    // Retry with reduced targetAssetCount to focus on quality
    const retryConfig = { ...config, targetAssetCount: Math.max(5, targetAssetCount - 2), maxRetries: config.maxRetries - 1 };
    
    try {
      const retryResult = await runStoryboarder(
        analysis,
        style,
        videoPurpose,
        globalSubject,
        retryConfig
      );
      
      // Verify retry result meets length requirements
      const retryShortCount = retryResult.prompts?.filter(p => {
        const wordCount = p.text?.split(/\s+/).filter(Boolean).length || 0;
        return wordCount < 40;
      }).length || 0;
      
      if (retryShortCount === 0) {
        console.log(`[Storyboarder] Retry successful - all prompts meet 40-word minimum`);
        return retryResult;
      } else {
        console.warn(`[Storyboarder] Retry still has ${retryShortCount} short prompts, using original result`);
      }
    } catch (retryError) {
      console.warn(`[Storyboarder] Retry failed:`, retryError);
    }
  }

  // Final warning if short prompts remain
  if (shortPrompts.length > 0) {
    console.warn(`[Storyboarder] Final result contains ${shortPrompts.length} prompts under 40 words (quality may be reduced)`);
    shortPrompts.forEach((p, i) => {
      const wordCount = p.text?.split(/\s+/).filter(Boolean).length || 0;
      console.warn(`  Short prompt ${i + 1}: ${wordCount} words`);
    });
  }

  return result;
}

/**
 * Progress callback for streaming storyboard generation.
 */
export type StoryboardProgressCallback = (progress: {
  stage: "generating" | "complete";
  partialResult?: Partial<StoryboardOutput>;
  finalResult?: StoryboardOutput;
}) => void;

/**
 * Streams storyboard generation with progress callbacks.
 * Uses LangChain streaming for better UX on long-running generations.
 *
 * @param analysis - The content analysis from the analyzer
 * @param style - Visual style for the storyboard
 * @param videoPurpose - The purpose/type of video
 * @param globalSubject - Optional consistent subject across scenes
 * @param onProgress - Callback for progress updates
 * @param config - Optional configuration overrides
 * @returns The final storyboard output
 */
export async function streamStoryboarder(
  analysis: AnalysisOutput,
  style: string,
  videoPurpose: VideoPurpose,
  globalSubject: string = "",
  onProgress?: StoryboardProgressCallback,
  config?: DirectorConfig
): Promise<StoryboardOutput> {
  const targetAssetCount = config?.targetAssetCount || 10;
  const mergedConfig = { ...DEFAULT_CONFIG, ...config };

  // Create model without structured output for streaming raw content
  const model = new ChatGoogleGenerativeAI({
    apiKey: GEMINI_API_KEY,
    model: mergedConfig.model,
    temperature: mergedConfig.temperature,
    verbose: LANGCHAIN_VERBOSE,
  });

  const template = createStoryboarderTemplate();
  const chain = template.pipe(model);

  // Get persona for this video purpose
  const persona = getSystemPersona(videoPurpose);
  const personaInstructions = `You are ${persona.name}, a ${persona.role}.

YOUR CORE RULE:
${persona.coreRule}

YOUR VISUAL PRINCIPLES:
${persona.visualPrinciples.map(p => `- ${p}`).join('\n')}

WHAT TO AVOID:
${persona.avoidList.map(a => `- ${a}`).join('\n')}`;

  // Get style enhancement
  const styleData = getStyleEnhancement(style);
  const styleEnhancement = `MEDIUM AUTHENTICITY (apply these characteristics):
${styleData.keywords.map(k => `- ${k}`).join('\n')}
Overall: ${styleData.mediumDescription}`;

  // Get purpose guidance
  const purposeGuidance = getPurposeGuidance(videoPurpose);
  const subjectGuidance = globalSubject.trim()
    ? `Keep this subject's appearance consistent across scenes.`
    : `Create cohesive scenes with consistent environmental elements.`;

  // Format visual scenes from analysis (art-directed approach)
  const visualScenes = analysis.visualScenes && analysis.visualScenes.length > 0
    ? analysis.visualScenes.map((scene, i) =>
        `SCENE ${i + 1} [${scene.timestamp}] - ${scene.emotionalTone}:
      Subject: ${scene.subjectContext}
      Visual: ${scene.visualPrompt}`
      ).join('\n\n')
    : "No visual scenes provided - create cinematic scenes based on themes and emotional arc.";

  const input = {
    style,
    personaInstructions,
    styleEnhancement,
    purposeGuidance,
    globalSubject: globalSubject || "None specified",
    subjectGuidance,
    visualScenes,
    cameraAngles: CAMERA_ANGLES.join(", "),
    lightingMoods: LIGHTING_MOODS.join(", "),
    analysis: JSON.stringify(analysis, null, 2),
    targetAssetCount,
  };

  // Stream the response
  let fullContent = "";

  onProgress?.({ stage: "generating" });

  for await (const chunk of await chain.stream(input)) {
    // Extract content from the chunk
    const content = typeof chunk.content === "string"
      ? chunk.content
      : Array.isArray(chunk.content)
        ? chunk.content.map(c => typeof c === "string" ? c : "").join("")
        : "";

    fullContent += content;

    // Try to parse partial JSON for progress updates
    try {
      // Attempt to extract any complete prompts from partial JSON
      const promptMatches = fullContent.match(/"text"\s*:\s*"[^"]+"/g);
      if (promptMatches && promptMatches.length > 0) {
        onProgress?.({
          stage: "generating",
          partialResult: {
            prompts: promptMatches.map(() => ({
              text: "...",
              mood: "...",
              timestamp: "...",
            })).slice(0, promptMatches.length),
          },
        });
      }
    } catch {
      // Ignore partial parsing errors
    }
  }

  // Parse the final result
  try {
    // Clean up the content for JSON parsing
    let jsonStr = fullContent.trim();

    // Remove markdown code blocks if present
    if (jsonStr.startsWith("```json")) {
      jsonStr = jsonStr.slice(7);
    } else if (jsonStr.startsWith("```")) {
      jsonStr = jsonStr.slice(3);
    }
    if (jsonStr.endsWith("```")) {
      jsonStr = jsonStr.slice(0, -3);
    }
    jsonStr = jsonStr.trim();

    const parsed = JSON.parse(jsonStr);
    const result = StoryboardSchema.parse(parsed);

    // Apply master style to each generated prompt for consistency
    if (result.prompts) {
      result.prompts = result.prompts.map(prompt => ({
        ...prompt,
        text: injectMasterStyle(prompt.text, style)
      }));
    }

    onProgress?.({ stage: "complete", finalResult: result });

    return result;
  } catch (error) {
    console.error("[Storyboarder Stream] Failed to parse final result:", error);
    console.error("[Storyboarder Stream] Raw content:", fullContent);
    throw new DirectorServiceError(
      `Failed to parse storyboard output: ${error instanceof Error ? error.message : String(error)}`,
      "OUTPUT_PARSING_FAILED",
      "storyboarder"
    );
  }
}


// --- LCEL Director Chain Composition ---

/**
 * Creates the complete Director chain using LCEL.
 * Chains Analyzer → Storyboarder in a single pipeline.
 */
export function createDirectorChain(
  contentType: "lyrics" | "story",
  config?: DirectorConfig
) {
  const analyzerChain = createAnalyzerChain(contentType, config);
  const storyboarderChain = createStoryboarderChain(config);

  return RunnableSequence.from([
    // Stage 1: Analyze content
    async (input: {
      content: string;
      style: string;
      videoPurpose: VideoPurpose;
      globalSubject: string;
    }) => {
      const analysis = await analyzerChain.invoke({ content: input.content });
      console.log("[Director] Analysis complete:", JSON.stringify(analysis, null, 2));

      return {
        analysis,
        style: input.style,
        videoPurpose: input.videoPurpose,
        globalSubject: input.globalSubject,
      };
    },
    // Stage 2: Generate storyboard with persona and style enhancements
    async (input: {
      analysis: AnalysisOutput;
      style: string;
      videoPurpose: VideoPurpose;
      globalSubject: string;
    }) => {
      // Get persona for this video purpose
      const persona = getSystemPersona(input.videoPurpose);
      const personaInstructions = `You are ${persona.name}, a ${persona.role}.

YOUR CORE RULE:
${persona.coreRule}

YOUR VISUAL PRINCIPLES:
${persona.visualPrinciples.map(p => `- ${p}`).join('\n')}

WHAT TO AVOID:
${persona.avoidList.map(a => `- ${a}`).join('\n')}`;

      // Get style enhancement
      const styleData = getStyleEnhancement(input.style);
      const styleEnhancement = `MEDIUM AUTHENTICITY (apply these characteristics):
${styleData.keywords.map(k => `- ${k}`).join('\n')}
Overall: ${styleData.mediumDescription}`;

      const purposeGuidance = getPurposeGuidance(input.videoPurpose);
      const subjectGuidance = input.globalSubject.trim()
        ? `Keep this subject's appearance consistent across scenes.`
        : `Create cohesive scenes with consistent environmental elements.`;

      // Format visual scenes from analysis (art-directed approach)
      const visualScenes = input.analysis.visualScenes && input.analysis.visualScenes.length > 0
        ? input.analysis.visualScenes.map((scene, i) =>
            `SCENE ${i + 1} [${scene.timestamp}] - ${scene.emotionalTone}:
      Subject: ${scene.subjectContext}
      Visual: ${scene.visualPrompt}`
          ).join('\n\n')
        : "No visual scenes provided - create cinematic scenes based on themes and emotional arc.";

      const targetAssetCount = config?.targetAssetCount || 10;
      const result = await storyboarderChain.invoke({
        style: input.style,
        personaInstructions,
        styleEnhancement,
        purposeGuidance,
        globalSubject: input.globalSubject || "None specified",
        subjectGuidance,
        visualScenes,
        cameraAngles: CAMERA_ANGLES.join(", "),
        lightingMoods: LIGHTING_MOODS.join(", "),
        analysis: JSON.stringify(input.analysis, null, 2),
        targetAssetCount,
      });

      // Apply master style to each generated prompt for consistency
      if (result.prompts) {
        result.prompts = result.prompts.map(prompt => ({
          ...prompt,
          text: injectMasterStyle(prompt.text, input.style)
        }));
      }

      console.log("[Director] Storyboard complete:", result.prompts?.length, "prompts generated");
      return result;
    },
  ]);
}

// --- Lint Validation ---

/**
 * Classifies an error and returns the appropriate DirectorErrorCode.
 */
function classifyError(error: unknown): DirectorErrorCode {
  if (error instanceof Error) {
    const message = error.message.toLowerCase();

    // Authentication issues (API key or ADC/Vertex AI)
    if (message.includes("api key") || message.includes("apikey") || message.includes("unauthorized") ||
      message.includes("credentials") || message.includes("authentication") || message.includes("adc")) {
      return "API_KEY_MISSING";
    }

    // Rate limiting
    if (message.includes("rate limit") || message.includes("quota") || message.includes("429")) {
      return "RATE_LIMIT_EXCEEDED";
    }

    // Network errors
    if (message.includes("network") || message.includes("fetch") || message.includes("econnrefused") || message.includes("enotfound")) {
      return "NETWORK_ERROR";
    }

    // Timeout
    if (message.includes("timeout") || message.includes("timed out")) {
      return "TIMEOUT";
    }

    // Parsing errors
    if (message.includes("parse") || message.includes("json") || message.includes("unexpected token")) {
      return "OUTPUT_PARSING_FAILED";
    }

    // Validation errors
    if (message.includes("validation") || message.includes("schema") || message.includes("zod")) {
      return "SCHEMA_VALIDATION_FAILED";
    }

    // Model initialization
    if (message.includes("model") && (message.includes("init") || message.includes("create"))) {
      return "MODEL_INIT_FAILED";
    }
  }

  return "UNKNOWN_ERROR";
}

/**
 * Logs error details for debugging purposes.
 */
function logError(
  stage: string,
  error: unknown,
  context?: Record<string, unknown>
): void {
  const errorCode = classifyError(error);
  const errorMessage = error instanceof Error ? error.message : String(error);
  const errorStack = error instanceof Error ? error.stack : undefined;

  console.error(`[Director] Error in ${stage}:`);
  console.error(`  Code: ${errorCode}`);
  console.error(`  Message: ${errorMessage}`);
  if (context) {
    console.error(`  Context:`, JSON.stringify(context, null, 2));
  }
  if (errorStack) {
    console.error(`  Stack: ${errorStack}`);
  }
}

/**
 * Validates and optionally refines generated prompts using lintPrompt.
 * When critical issues (too_short, missing_subject) are detected, attempts refinement.
 */
async function validateAndLintPrompts(
  prompts: StoryboardOutput["prompts"],
  globalSubject?: string,
  style: string = "Cinematic"
): Promise<ImagePrompt[]> {
  const validatedPrompts: ImagePrompt[] = [];
  const previousPrompts: string[] = [];

  for (let i = 0; i < prompts.length; i++) {
    const prompt = prompts[i];
    if (!prompt) continue;

    // Run lint validation
    const issues = lintPrompt({
      promptText: prompt.text,
      globalSubject,
      previousPrompts,
    });

    // Log any warnings
    if (issues.length > 0) {
      console.log(`[Director] Lint issues for prompt ${i + 1}:`, issues.map(issue => issue.code).join(", "));
    }

    // Check for critical issues that need refinement
    const criticalIssues = issues.filter(
      issue => issue.code === "too_short" || issue.code === "missing_subject"
    );
    const hasCriticalIssues = criticalIssues.length > 0;

    let finalText = prompt.text;

    // Attempt refinement for critical issues
    if (hasCriticalIssues) {
      console.log(`[Director] Critical issues detected for prompt ${i + 1}, attempting refinement...`);

      try {
        const refinementResult = await refineImagePrompt({
          promptText: prompt.text,
          style,
          globalSubject,
          intent: "auto",
          previousPrompts,
        });

        finalText = refinementResult.refinedPrompt;
        console.log(`[Director] Prompt ${i + 1} refined successfully`);

        // Re-lint the refined prompt to verify improvement
        const postRefinementIssues = lintPrompt({
          promptText: finalText,
          globalSubject,
          previousPrompts,
        });

        const stillHasCriticalIssues = postRefinementIssues.some(
          issue => issue.code === "too_short" || issue.code === "missing_subject"
        );

        if (stillHasCriticalIssues) {
          console.log(`[Director] Prompt ${i + 1} still has critical issues after refinement`);
        }
      } catch (refinementError) {
        console.error(`[Director] Refinement failed for prompt ${i + 1}:`, refinementError);
        // Keep original text if refinement fails
      }
    }

    // Create ImagePrompt object
    const imagePrompt: ImagePrompt = {
      id: `prompt-${Date.now()}-${i}`,
      text: finalText,
      mood: prompt.mood,
      timestamp: prompt.timestamp,
      timestampSeconds: parseSRTTimestamp(prompt.timestamp) ?? 0,
    };

    validatedPrompts.push(imagePrompt);
    previousPrompts.push(finalText);
  }

  return validatedPrompts;
}

// --- Main Export Function ---

const isBrowser = typeof window !== "undefined";

/**
 * Generates image prompts using the LangChain Director workflow.
 * 
 * This function orchestrates a two-stage AI pipeline:
 * 1. Analyzer: Interprets content structure and emotional arcs
 * 2. Storyboarder: Generates detailed visual prompts
 * 
 * Falls back to existing prompt generation on errors.
 * 
 * @param srtContent - The SRT content to analyze
 * @param style - Art style preset for generation
 * @param contentType - "lyrics" or "story"
 * @param videoPurpose - Purpose of the video (affects visual style)
 * @param globalSubject - Optional consistent subject across scenes
 * @param config - Optional configuration overrides
 * @returns Array of ImagePrompt objects
 */
export async function generatePromptsWithLangChain(
  srtContent: string,
  style: string,
  contentType: "lyrics" | "story",
  videoPurpose: VideoPurpose,
  globalSubject?: string,
  config?: DirectorConfig
): Promise<ImagePrompt[]> {
  const startTime = Date.now();

  try {
    // Client-side: Offload to server
    if (isBrowser) {
      console.log("[Director] Client-side detected, calling server proxy...");
      const response = await fetch('/api/director/generate', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          srtContent,
          style,
          contentType,
          videoPurpose,
          globalSubject,
          config
        })
      });

      if (!response.ok) {
        const err = await response.json();
        throw new Error(err.error || `Server director failed: ${response.status}`);
      }

      const data = await response.json();
      return data.prompts;
    }

    // Server-side: Run logic directly
    console.log("[Director] Starting LangChain workflow (Server)...");
    console.log("[Director] Content type:", contentType);
    console.log("[Director] Style:", style);
    console.log("[Director] Purpose:", videoPurpose);

    // Validate inputs before proceeding
    if (!srtContent || srtContent.trim().length === 0) {
      console.warn("[Director] Empty SRT content provided, falling back to existing implementation");
      return executeFallback(srtContent, style, contentType, videoPurpose, globalSubject);
    }

    // Check for API key configuration (Server-side check)
    if (!GEMINI_API_KEY && !VERTEX_PROJECT) {
      console.warn("[Director] API key not configured, falling back to existing implementation");
      logError("initialization", new Error("API key not configured - missing VITE_GEMINI_API_KEY"), {
        contentType,
        style
      });
      return executeFallback(srtContent, style, contentType, videoPurpose, globalSubject);
    }

    // Create and run the director chain
    const directorChain = createDirectorChain(contentType, config);

    let result;
    try {
      result = await directorChain.invoke({
        content: srtContent,
        style,
        videoPurpose,
        globalSubject: globalSubject || "",
      });
    } catch (chainError) {
      // Log the chain execution error
      logError("chain execution", chainError, {
        contentType,
        style,
        videoPurpose,
        srtContentLength: srtContent.length,
      });

      // Throw a structured error for the outer catch to handle
      throw new DirectorServiceError(
        `Chain execution failed: ${chainError instanceof Error ? chainError.message : String(chainError)}`,
        classifyError(chainError),
        "chain",
        chainError instanceof Error ? chainError : undefined
      );
    }

    // Validate the result structure
    if (!result || !result.prompts || !Array.isArray(result.prompts)) {
      console.warn("[Director] Invalid result structure, falling back");
      logError("validation", new Error("Invalid result structure"), {
        resultType: typeof result,
        hasPrompts: result ? "prompts" in result : false,
      });
      return executeFallback(srtContent, style, contentType, videoPurpose, globalSubject);
    }

    // Check if we got any prompts
    if (result.prompts.length === 0) {
      console.warn("[Director] No prompts generated, falling back");
      return executeFallback(srtContent, style, contentType, videoPurpose, globalSubject);
    }

    // Validate and lint the generated prompts
    let validatedPrompts: ImagePrompt[];
    try {
      validatedPrompts = await validateAndLintPrompts(
        result.prompts,
        globalSubject,
        style
      );
    } catch (validationError) {
      logError("validation", validationError, {
        promptCount: result.prompts.length,
      });

      // If validation fails, still try to return the raw prompts with basic transformation
      validatedPrompts = result.prompts.map((p, i) => ({
        id: `prompt-${Date.now()}-${i}`,
        text: p.text || "",
        mood: p.mood || "neutral",
        timestamp: p.timestamp,
        timestampSeconds: parseSRTTimestamp(p.timestamp) ?? 0,
      }));
    }

    const duration = Date.now() - startTime;
    console.log(`[Director] Workflow complete: ${validatedPrompts.length} prompts generated in ${duration}ms`);
    return validatedPrompts;

  } catch (error) {
    const duration = Date.now() - startTime;

    // Log the error with full context
    logError("workflow", error, {
      contentType,
      style,
      videoPurpose,
      duration,
      srtContentLength: srtContent?.length || 0,
    });

    // Execute fallback
    console.log("[Director] Executing fallback to existing prompt generation...");
    return executeFallback(srtContent, style, contentType, videoPurpose, globalSubject);
  }
}

/**
 * Executes the fallback to existing prompt generation functions.
 * This is called when the LangChain workflow fails or encounters errors.
 * 
 * @param srtContent - The SRT content to process
 * @param style - Art style preset
 * @param contentType - "lyrics" or "story"
 * @param videoPurpose - Purpose of the video
 * @param globalSubject - Optional consistent subject
 * @returns Array of ImagePrompt objects from fallback implementation
 */
async function executeFallback(
  srtContent: string,
  style: string,
  contentType: "lyrics" | "story",
  videoPurpose: VideoPurpose,
  globalSubject?: string
): Promise<ImagePrompt[]> {
  try {
    console.log(`[Director] Fallback: Using ${contentType === "story" ? "generatePromptsFromStory" : "generatePromptsFromLyrics"}`);

    if (contentType === "story") {
      return await generatePromptsFromStory(srtContent, style, globalSubject, videoPurpose);
    }
    return await generatePromptsFromLyrics(srtContent, style, globalSubject, videoPurpose);
  } catch (fallbackError) {
    // If even the fallback fails, log and return empty array
    logError("fallback", fallbackError, {
      contentType,
      style,
    });
    console.error("[Director] Fallback also failed, returning empty array");
    return [];
  }
}
````

## File: packages/shared/src/services/documentParser.ts
````typescript
/**
 * Document Parser Service
 *
 * Parses reference documents (PDF, TXT, DOCX) into indexed chunks
 * for use by the Research Service and Narrative Engine.
 *
 * Requirements: 22.1, 22.4, 22.5
 *
 * React-free for Node.js compatibility.
 */

// ============================================================================
// Types
// ============================================================================

export interface IndexedDocument {
  id: string;
  filename: string;
  content: string;
  chunks: string[];
  metadata: Record<string, any>;
}

const SUPPORTED_TYPES = [
  'text/plain',
  'application/pdf',
  'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
] as const;

const SUPPORTED_EXTENSIONS = ['.txt', '.pdf', '.docx'] as const;

// ============================================================================
// Public API
// ============================================================================

export function getSupportedTypes(): string[] {
  return [...SUPPORTED_TYPES];
}

/**
 * Parse a File into an IndexedDocument.
 * Throws descriptive errors on failure with filename context.
 */
export async function parseDocument(file: File): Promise<IndexedDocument> {
  const filename = file.name;
  const extension = getExtension(filename);

  if (!isSupportedExtension(extension)) {
    throw new DocumentParseError(
      filename,
      `Unsupported file type '${extension}'. Supported types: ${SUPPORTED_EXTENSIONS.join(', ')}`,
    );
  }

  try {
    let content: string;

    if (extension === '.txt') {
      content = await parseTxt(file);
    } else if (extension === '.docx') {
      content = await parseDocx(file);
    } else if (extension === '.pdf') {
      content = await parsePdf(file);
    } else {
      throw new Error(`No parser for extension '${extension}'`);
    }

    if (!content.trim()) {
      throw new DocumentParseError(filename, 'Document is empty or contains no extractable text');
    }

    const chunks = chunkContent(content);

    return {
      id: `doc_${Date.now()}_${Math.random().toString(36).slice(2, 8)}`,
      filename,
      content,
      chunks,
      metadata: {
        size: file.size,
        type: file.type || extension,
        chunkCount: chunks.length,
        wordCount: content.split(/\s+/).filter(Boolean).length,
        parsedAt: new Date().toISOString(),
      },
    };
  } catch (error) {
    if (error instanceof DocumentParseError) throw error;
    const msg = error instanceof Error ? error.message : String(error);
    throw new DocumentParseError(filename, `Failed to parse: ${msg}`);
  }
}

/**
 * Split content into chunks of approximately `chunkSize` characters
 * at sentence boundaries.
 */
export function chunkContent(content: string, chunkSize: number = 500): string[] {
  if (!content.trim()) return [];

  const sentences = content.match(/[^.!?]+[.!?]+\s*/g) ?? [content];
  const chunks: string[] = [];
  let current = '';

  for (const sentence of sentences) {
    if (current.length + sentence.length > chunkSize && current.length > 0) {
      chunks.push(current.trim());
      current = '';
    }
    current += sentence;
  }

  if (current.trim()) {
    chunks.push(current.trim());
  }

  return chunks;
}

// ============================================================================
// Parsers
// ============================================================================

async function readFileAsArrayBuffer(file: File): Promise<ArrayBuffer> {
  // Use arrayBuffer() if available (modern browsers), fallback to FileReader pattern
  if (typeof file.arrayBuffer === 'function') {
    return file.arrayBuffer();
  }
  // Fallback for environments where File.arrayBuffer is not available (e.g., jsdom)
  return new Promise<ArrayBuffer>((resolve, reject) => {
    const reader = new FileReader();
    reader.onload = () => resolve(reader.result as ArrayBuffer);
    reader.onerror = () => reject(reader.error);
    reader.readAsArrayBuffer(file);
  });
}

async function parseTxt(file: File): Promise<string> {
  const buffer = await readFileAsArrayBuffer(file);
  return new TextDecoder('utf-8').decode(buffer);
}

async function parseDocx(file: File): Promise<string> {
  // DOCX files are ZIP archives containing XML
  // We look for word/document.xml and extract text from <w:t> tags
  const arrayBuffer = await readFileAsArrayBuffer(file);
  const bytes = new Uint8Array(arrayBuffer);

  // Minimal ZIP parser to find word/document.xml
  const xmlContent = await extractDocxXml(bytes);
  if (!xmlContent) {
    throw new Error('Could not find word/document.xml in DOCX archive');
  }

  // Extract text from <w:t> and <w:t xml:space="preserve"> tags
  const textMatches = xmlContent.match(/<w:t[^>]*>([\s\S]*?)<\/w:t>/g) ?? [];
  const texts: string[] = [];

  for (const match of textMatches) {
    const inner = match.replace(/<[^>]+>/g, '');
    if (inner) texts.push(inner);
  }

  // Also extract paragraph breaks from <w:p> boundaries
  let result = '';
  const paragraphs = xmlContent.split(/<\/w:p>/);
  for (const para of paragraphs) {
    const paraTexts = para.match(/<w:t[^>]*>([\s\S]*?)<\/w:t>/g) ?? [];
    const paraContent = paraTexts.map(m => m.replace(/<[^>]+>/g, '')).join('');
    if (paraContent.trim()) {
      result += paraContent + '\n';
    }
  }

  return result.trim() || texts.join(' ');
}

async function parsePdf(file: File): Promise<string> {
  // Browser-side PDF text extraction is limited.
  // We attempt basic text extraction from the PDF stream.
  const arrayBuffer = await readFileAsArrayBuffer(file);
  const bytes = new Uint8Array(arrayBuffer);
  const text = extractPdfText(bytes);

  if (!text.trim()) {
    // Fall back to indicating that server-side parsing is needed
    return `[PDF: ${file.name}] This PDF requires server-side parsing for full text extraction. File size: ${file.size} bytes.`;
  }

  return text;
}

// ============================================================================
// DOCX ZIP Extraction (minimal)
// ============================================================================

async function extractDocxXml(bytes: Uint8Array): Promise<string | null> {
  // Find the word/document.xml entry in the ZIP
  // ZIP local file headers start with 0x504B0304
  const decoder = new TextDecoder('utf-8');
  const target = 'word/document.xml';

  let offset = 0;
  while (offset < bytes.length - 30) {
    // Check for local file header signature
    if (bytes[offset] === 0x50 && bytes[offset + 1] === 0x4B &&
        bytes[offset + 2] === 0x03 && bytes[offset + 3] === 0x04) {

      const fnameLen = bytes[offset + 26]! | (bytes[offset + 27]! << 8);
      const extraLen = bytes[offset + 28]! | (bytes[offset + 29]! << 8);
      const compMethod = bytes[offset + 8]! | (bytes[offset + 9]! << 8);
      const compSize = bytes[offset + 18]! | (bytes[offset + 19]! << 8) |
                       (bytes[offset + 20]! << 16) | (bytes[offset + 21]! << 24);

      const fname = decoder.decode(bytes.slice(offset + 30, offset + 30 + fnameLen));
      const dataStart = offset + 30 + fnameLen + extraLen;

      if (fname === target) {
        if (compMethod === 0) {
          // Stored (no compression)
          return decoder.decode(bytes.slice(dataStart, dataStart + compSize));
        }
        // Compressed — try DecompressionStream if available
        if (typeof DecompressionStream !== 'undefined') {
          try {
            const compressed = bytes.slice(dataStart, dataStart + compSize);
            const ds = new DecompressionStream('deflate-raw');
            const writer = ds.writable.getWriter();
            writer.write(compressed);
            writer.close();
            const reader = ds.readable.getReader();
            const chunks: Uint8Array[] = [];
            while (true) {
              const { done, value } = await reader.read();
              if (done) break;
              chunks.push(value);
            }
            const totalLen = chunks.reduce((s, c) => s + c.length, 0);
            const result = new Uint8Array(totalLen);
            let pos = 0;
            for (const chunk of chunks) {
              result.set(chunk, pos);
              pos += chunk.length;
            }
            return decoder.decode(result);
          } catch {
            return null;
          }
        }
        return null;
      }

      offset = dataStart + compSize;
    } else {
      offset++;
    }
  }

  return null;
}

// ============================================================================
// PDF Text Extraction (basic)
// ============================================================================

function extractPdfText(bytes: Uint8Array): string {
  const decoder = new TextDecoder('latin1');
  const raw = decoder.decode(bytes);

  // Extract text from PDF text objects: BT ... ET blocks with Tj/TJ operators
  const texts: string[] = [];
  const btBlocks = raw.match(/BT[\s\S]*?ET/g) ?? [];

  for (const block of btBlocks) {
    // Match Tj operator (show string)
    const tjMatches = block.match(/\(([^)]*)\)\s*Tj/g) ?? [];
    for (const tj of tjMatches) {
      const inner = tj.match(/\(([^)]*)\)/)?.[1];
      if (inner) texts.push(inner);
    }

    // Match TJ operator (show array of strings)
    const tjArrayMatches = block.match(/\[([^\]]*)\]\s*TJ/g) ?? [];
    for (const tja of tjArrayMatches) {
      const strings = tja.match(/\(([^)]*)\)/g) ?? [];
      for (const s of strings) {
        const inner = s.match(/\(([^)]*)\)/)?.[1];
        if (inner) texts.push(inner);
      }
    }
  }

  return texts.join(' ').replace(/\\n/g, '\n').replace(/\s+/g, ' ').trim();
}

// ============================================================================
// Helpers
// ============================================================================

function getExtension(filename: string): string {
  const dot = filename.lastIndexOf('.');
  return dot >= 0 ? filename.slice(dot).toLowerCase() : '';
}

function isSupportedExtension(ext: string): ext is typeof SUPPORTED_EXTENSIONS[number] {
  return (SUPPORTED_EXTENSIONS as readonly string[]).includes(ext);
}

export class DocumentParseError extends Error {
  constructor(
    public readonly filename: string,
    message: string,
  ) {
    super(`Document parsing failed for '${filename}': ${message}`);
    this.name = 'DocumentParseError';
  }
}
````

## File: packages/shared/src/services/editorService.ts
````typescript
/**
 * Editor Service
 * 
 * Validation and assembly agent for video production pipeline.
 * Responsibilities:
 * - Validate content plan structure and completeness
 * - Check scene-to-narration timing sync
 * - Critique quality and coherence
 * - Coordinate FFmpeg assembly for final video
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence, RunnableLambda } from "@langchain/core/runnables";
import { z } from "zod";
import {
    ContentPlan,
    Scene,
    NarrationSegment,
    ValidationResult,
    GeneratedImage
} from "../types";
import { API_KEY, MODELS } from "./shared/apiClient";
import { getEffectiveLegacyTone } from "./tripletUtils";

// --- Zod Schemas ---

/**
 * Schema for AI-powered critique output
 */
export const CritiqueSchema = z.object({
    score: z.number().min(0).max(100).describe("Overall quality score 0-100"),
    approved: z.boolean().describe("Whether the plan is ready for production"),
    issues: z.array(z.object({
        scene: z.string().describe("Scene ID or name"),
        type: z.enum(["timing", "visual", "narration", "transition", "pacing"]).describe("Issue type"),
        message: z.string().describe("Description of the issue"),
    })).describe("List of identified issues"),
    suggestions: z.array(z.string()).describe("Improvement suggestions"),
});

export type CritiqueOutput = z.infer<typeof CritiqueSchema>;

// --- Configuration ---

export interface EditorConfig {
    model?: string;
    temperature?: number;
    minApprovalScore?: number; // Minimum score to approve (default: 70)
}

const DEFAULT_CONFIG: Required<EditorConfig> = {
    model: MODELS.TEXT,
    temperature: 0.3, // Lower temperature for more consistent critiques
    minApprovalScore: 70,
};

// --- Error Types ---

export class EditorError extends Error {
    constructor(
        message: string,
        public readonly code: "VALIDATION_FAILED" | "SYNC_ERROR" | "ASSEMBLY_ERROR" | "AI_FAILURE",
        public readonly originalError?: Error
    ) {
        super(message);
        this.name = "EditorError";
    }
}

// --- Validation Functions ---

/**
 * Validate content plan structure and completeness.
 * Performs rule-based checks before AI critique.
 */
export function validatePlanStructure(plan: ContentPlan): {
    valid: boolean;
    issues: Array<{ scene: string; type: string; message: string }>;
} {
    const issues: Array<{ scene: string; type: string; message: string }> = [];

    // Check plan-level requirements
    if (!plan.title?.trim()) {
        issues.push({ scene: "plan", type: "general", message: "Missing title" });
    }

    if (plan.scenes.length === 0) {
        issues.push({ scene: "plan", type: "general", message: "No scenes defined" });
        return { valid: false, issues };
    }

    // Check each scene
    let accumulatedDuration = 0;
    plan.scenes.forEach((scene, index) => {
        const sceneId = scene.id || `scene-${index + 1}`;

        // Duration checks
        if (scene.duration <= 0) {
            issues.push({ scene: sceneId, type: "timing", message: "Invalid duration (must be > 0)" });
        } else if (scene.duration < 3) {
            issues.push({ scene: sceneId, type: "timing", message: "Duration too short (< 3s may feel rushed)" });
        } else if (scene.duration > 60) {
            issues.push({ scene: sceneId, type: "timing", message: "Duration very long (> 60s may lose attention)" });
        }
        accumulatedDuration += scene.duration;

        // Visual description checks
        if (!scene.visualDescription?.trim()) {
            issues.push({ scene: sceneId, type: "visual", message: "Missing visual description" });
        } else if (scene.visualDescription.length < 20) {
            issues.push({ scene: sceneId, type: "visual", message: "Visual description too brief (< 20 chars)" });
        }

        // Narration checks
        if (!scene.narrationScript?.trim()) {
            issues.push({ scene: sceneId, type: "narration", message: "Missing narration script" });
        }

        // Estimate narration duration (avg 150 words/minute = 2.5 words/second)
        if (scene.narrationScript) {
            const wordCount = scene.narrationScript.split(/\s+/).length;
            const estimatedNarrationDuration = wordCount / 2.5;

            if (estimatedNarrationDuration > scene.duration * 1.2) {
                issues.push({
                    scene: sceneId,
                    type: "timing",
                    message: `Narration (~${Math.round(estimatedNarrationDuration)}s) may exceed scene duration (${scene.duration}s)`
                });
            }
        }
    });

    // Check total duration
    const durationDiff = Math.abs(accumulatedDuration - plan.totalDuration);
    if (durationDiff > 5) {
        issues.push({
            scene: "plan",
            type: "timing",
            message: `Scene durations (${accumulatedDuration}s) differ from plan total (${plan.totalDuration}s)`
        });
    }

    return {
        valid: issues.length === 0,
        issues,
    };
}

/**
 * Check synchronization between scenes and narration segments.
 */
export function checkNarrationSync(
    scenes: Scene[],
    narrationSegments: NarrationSegment[]
): {
    synced: boolean;
    issues: Array<{ scene: string; type: string; message: string }>;
} {
    const issues: Array<{ scene: string; type: string; message: string }> = [];

    // Check for missing narrations
    scenes.forEach((scene) => {
        const narration = narrationSegments.find(n => n.sceneId === scene.id);

        if (!narration) {
            issues.push({
                scene: scene.id,
                type: "narration",
                message: "Missing narration audio"
            });
        } else {
            // Check duration mismatch
            const durationDiff = Math.abs(narration.audioDuration - scene.duration);
            if (durationDiff > 2) {
                issues.push({
                    scene: scene.id,
                    type: "timing",
                    message: `Narration duration (${narration.audioDuration.toFixed(1)}s) differs from scene (${scene.duration}s)`
                });
            }
        }
    });

    // Check for orphaned narrations
    narrationSegments.forEach((narration) => {
        const scene = scenes.find(s => s.id === narration.sceneId);
        if (!scene) {
            issues.push({
                scene: narration.sceneId,
                type: "narration",
                message: "Narration has no matching scene"
            });
        }
    });

    return {
        synced: issues.length === 0,
        issues,
    };
}

/**
 * Check visual assets against scenes.
 */
export function checkVisualAssets(
    scenes: Scene[],
    visuals: GeneratedImage[]
): {
    complete: boolean;
    issues: Array<{ scene: string; type: string; message: string }>;
} {
    const issues: Array<{ scene: string; type: string; message: string }> = [];

    scenes.forEach((scene) => {
        const visual = visuals.find(v => v.promptId === scene.id);

        if (!visual) {
            issues.push({
                scene: scene.id,
                type: "visual",
                message: "Missing visual asset"
            });
        }
    });

    return {
        complete: issues.length === 0,
        issues,
    };
}

// --- AI-Powered Critique ---

function createModel(config: EditorConfig = {}): ChatGoogleGenerativeAI {
    const mergedConfig = { ...DEFAULT_CONFIG, ...config };

    if (!API_KEY) {
        throw new EditorError("Gemini API key is not configured", "AI_FAILURE");
    }

    return new ChatGoogleGenerativeAI({
        apiKey: API_KEY,
        model: mergedConfig.model,
        temperature: mergedConfig.temperature,
    });
}

function createCritiqueTemplate(): ChatPromptTemplate {
    return ChatPromptTemplate.fromMessages([
        ["system", `You are an expert video editor and quality assurance specialist.
Your job is to critique video content plans and identify issues that would affect the final video quality.

Evaluate based on:
1. PACING: Is the flow natural? Are scenes appropriately timed?
2. COHERENCE: Does the story/content flow logically?
3. VISUAL QUALITY: Are visual descriptions specific and filmable?
4. NARRATION QUALITY: Is the narration clear and engaging?
5. TRANSITIONS: Are transitions appropriate between scenes?

OUTPUT FORMAT:
Return a valid JSON object:
{{
  "score": 85,
  "approved": true,
  "issues": [
    {{"scene": "scene-2", "type": "timing", "message": "Scene duration too short for content"}}
  ],
  "suggestions": ["Consider adding establishing shot", "Slow down pacing in middle section"]
}}`],
        ["human", `Critique this video content plan:

TITLE: {title}
TARGET AUDIENCE: {targetAudience}
OVERALL TONE: {overallTone}
TOTAL DURATION: {totalDuration} seconds

SCENES:
{scenesDescription}

Rule-based issues already found:
{existingIssues}

Provide your critique with a score (0-100), approval status, any additional issues, and suggestions.`],
    ]);
}

/**
 * Get AI-powered critique of a content plan.
 */
export async function critiqueContentPlan(
    plan: ContentPlan,
    existingIssues: Array<{ scene: string; type: string; message: string }> = [],
    config?: EditorConfig
): Promise<CritiqueOutput> {
    const model = createModel(config);
    const template = createCritiqueTemplate();

    // Format scenes for the prompt
    const scenesDescription = plan.scenes.map((scene, i) =>
        `${i + 1}. ${scene.name} (${scene.duration}s, ${getEffectiveLegacyTone(scene)})
   Visual: ${scene.visualDescription}
   Narration: ${scene.narrationScript.substring(0, 100)}...`
    ).join("\n\n");

    const existingIssuesStr = existingIssues.length > 0
        ? existingIssues.map(i => `- ${i.scene}: ${i.message}`).join("\n")
        : "No rule-based issues found.";

    const chain = RunnableSequence.from([
        template,
        model,
        new RunnableLambda({
            func: async (message: unknown): Promise<CritiqueOutput> => {
                const content = typeof message === "object" && message !== null && "content" in message
                    ? String((message as { content: unknown }).content)
                    : String(message);

                const jsonStr = content
                    .replace(/^```json\s*/i, "")
                    .replace(/^```\s*/i, "")
                    .replace(/```$/i, "")
                    .trim();

                try {
                    const parsed = JSON.parse(jsonStr);

                    // Normalize issue types before validation
                    const validTypes = ["timing", "visual", "narration", "transition", "pacing"];
                    if (parsed.issues && Array.isArray(parsed.issues)) {
                        parsed.issues = parsed.issues.map((issue: any) => ({
                            ...issue,
                            // Normalize type to valid value
                            type: validTypes.includes(issue.type?.toLowerCase?.())
                                ? issue.type.toLowerCase()
                                : "pacing", // Default fallback
                        }));
                    }

                    return CritiqueSchema.parse(parsed);
                } catch (error) {
                    console.error("[Editor] Critique parse error:", error);
                    // Return a safe default on parse failure
                    return {
                        score: 50,
                        approved: false,
                        issues: [{ scene: "unknown", type: "pacing", message: "Unable to parse AI critique" }],
                        suggestions: ["Please review the content plan manually"],
                    };
                }
            },
        }),
    ]);

    try {
        return await chain.invoke({
            title: plan.title,
            targetAudience: plan.targetAudience,
            overallTone: plan.overallTone,
            totalDuration: plan.totalDuration,
            scenesDescription,
            existingIssues: existingIssuesStr,
        });
    } catch (error) {
        console.error("[Editor] Critique failed:", error);
        throw new EditorError(
            `Critique failed: ${error instanceof Error ? error.message : String(error)}`,
            "AI_FAILURE",
            error instanceof Error ? error : undefined
        );
    }
}

// --- Main Validation Pipeline ---

/**
 * Run full validation on a content plan.
 * Combines rule-based checks with AI critique.
 */
export async function validateContentPlan(
    plan: ContentPlan,
    options: {
        narrationSegments?: NarrationSegment[];
        visuals?: GeneratedImage[];
        useAICritique?: boolean;
        config?: EditorConfig;
    } = {}
): Promise<ValidationResult> {
    const {
        narrationSegments = [],
        visuals = [],
        useAICritique = true,
        config,
    } = options;

    console.log("[Editor] Validating content plan:", plan.title);

    // Collect all issues
    let allIssues: Array<{ scene: string; type: string; message: string }> = [];

    // 1. Structure validation
    const structureCheck = validatePlanStructure(plan);
    allIssues = [...allIssues, ...structureCheck.issues];

    // 2. Narration sync (if narrations provided)
    if (narrationSegments.length > 0) {
        const narrationCheck = checkNarrationSync(plan.scenes, narrationSegments);
        allIssues = [...allIssues, ...narrationCheck.issues];
    }

    // 3. Visual assets (if visuals provided)
    if (visuals.length > 0) {
        const visualCheck = checkVisualAssets(plan.scenes, visuals);
        allIssues = [...allIssues, ...visualCheck.issues];
    }

    // 4. AI critique (optional)
    let score = 100 - (allIssues.length * 10); // Basic score from rule checks
    let suggestions: string[] = [];

    if (useAICritique && API_KEY) {
        try {
            const critique = await critiqueContentPlan(plan, allIssues, config);
            score = critique.score;
            allIssues = [...allIssues, ...critique.issues];
            suggestions = critique.suggestions;
        } catch (error) {
            console.warn("[Editor] AI critique failed, using rule-based score only:", error);
        }
    }

    // Clamp score
    score = Math.max(0, Math.min(100, score));

    const minScore = config?.minApprovalScore ?? DEFAULT_CONFIG.minApprovalScore;

    // Count critical issues (missing assets, invalid durations)
    const criticalIssues = allIssues.filter(i => 
        i.message.includes("Missing") || 
        i.message.includes("Invalid") ||
        i.message.includes("No scenes")
    );
    
    // Minor timing differences are warnings, not blockers
    const minorTimingIssues = allIssues.filter(i => 
        i.type === "timing" && 
        (i.message.includes("may exceed") || i.message.includes("differs from"))
    );

    console.log(`[Editor] Validation complete. Score: ${score}, Issues: ${allIssues.length} (${criticalIssues.length} critical)`);

    return {
        // Approve if score is good and no critical issues
        // Minor timing warnings shouldn't block production
        approved: score >= minScore && criticalIssues.length === 0,
        score,
        issues: allIssues,
        suggestions,
    };
}

/**
 * Adjust scene durations to match narration.
 * Returns an updated content plan.
 */
export function syncDurationsToNarration(
    plan: ContentPlan,
    narrationSegments: NarrationSegment[]
): ContentPlan {
    const updatedScenes = plan.scenes.map((scene) => {
        const narration = narrationSegments.find(n => n.sceneId === scene.id);

        if (narration) {
            // Sanity check: narration duration should be reasonable (3-60 seconds per scene)
            const audioDuration = narration.audioDuration;
            
            if (audioDuration < 3 || audioDuration > 60) {
                console.warn(`[Editor] Suspicious audio duration for ${scene.id}: ${audioDuration}s. Keeping original ${scene.duration}s`);
                return scene;
            }
            
            // Only adjust if there's a significant difference (> 2 seconds)
            if (Math.abs(audioDuration - scene.duration) > 2) {
                const newDuration = Math.ceil(audioDuration) + 1; // Add 1s buffer
                console.log(`[Editor] Adjusting scene ${scene.id} duration: ${scene.duration}s → ${newDuration}s`);
                return {
                    ...scene,
                    duration: newDuration,
                };
            }
        }

        return scene;
    });

    // Recalculate total duration
    const totalDuration = updatedScenes.reduce((sum, s) => sum + s.duration, 0);

    return {
        ...plan,
        scenes: updatedScenes,
        totalDuration,
    };
}

// --- FFmpeg Assembly ---

export interface AssemblyConfig {
    width?: number;
    height?: number;
    fps?: number;
    transitionDuration?: number; // seconds
    backgroundMusic?: Blob | null;
    backgroundMusicVolume?: number; // 0-1
}

const DEFAULT_ASSEMBLY_CONFIG: Required<Omit<AssemblyConfig, 'backgroundMusic'>> & { backgroundMusic: Blob | null } = {
    width: 1280,
    height: 720,
    fps: 24,
    transitionDuration: 0.5,
    backgroundMusic: null,
    backgroundMusicVolume: 0.3,
};

export interface AssemblyProgress {
    stage: "preparing" | "merging_audio" | "rendering_scenes" | "encoding" | "complete";
    progress: number;
    message: string;
}

/**
 * Assemble final video from content plan, narration, and visuals.
 * Uses server-side FFmpeg for best quality and performance.
 */
export async function assembleNarratedVideo(
    contentPlan: ContentPlan,
    narrationSegments: NarrationSegment[],
    visuals: GeneratedImage[],
    config: AssemblyConfig = {},
    onProgress?: (progress: AssemblyProgress) => void
): Promise<Blob> {
    const mergedConfig = { ...DEFAULT_ASSEMBLY_CONFIG, ...config };

    console.log("[Editor] Starting video assembly for:", contentPlan.title);

    onProgress?.({
        stage: "preparing",
        progress: 0,
        message: "Preparing assets...",
    });

    // Validate we have all required assets
    const visualCheck = checkVisualAssets(contentPlan.scenes, visuals);
    if (!visualCheck.complete) {
        throw new EditorError(
            `Missing visual assets: ${visualCheck.issues.map(i => i.scene).join(", ")}`,
            "ASSEMBLY_ERROR"
        );
    }

    const narrationCheck = checkNarrationSync(contentPlan.scenes, narrationSegments);
    if (!narrationCheck.synced) {
        console.warn("[Editor] Narration sync issues:", narrationCheck.issues);
    }

    // Build timeline data
    const timeline: Array<{
        sceneId: string;
        startTime: number;
        duration: number;
        visualUrl: string;
        visualType: "image" | "video";
        audioBlob: Blob;
        transition: string;
    }> = [];

    let currentTime = 0;
    for (const scene of contentPlan.scenes) {
        const visual = visuals.find(v => v.promptId === scene.id);
        const narration = narrationSegments.find(n => n.sceneId === scene.id);

        if (!visual || !narration) continue;

        timeline.push({
            sceneId: scene.id,
            startTime: currentTime,
            duration: narration.audioDuration,
            visualUrl: visual.imageUrl,
            visualType: visual.type || "image",
            audioBlob: narration.audioBlob,
            transition: scene.transitionTo || "dissolve",
        });

        currentTime += narration.audioDuration;
    }

    onProgress?.({
        stage: "merging_audio",
        progress: 10,
        message: `Merging ${timeline.length} audio segments...`,
    });

    // Merge audio blobs into a single audio track
    const mergedAudio = await mergeAudioBlobs(
        timeline.map(t => t.audioBlob),
        timeline.map(t => t.duration)
    );

    onProgress?.({
        stage: "rendering_scenes",
        progress: 30,
        message: "Rendering video frames...",
    });

    // For now, create a simple video using canvas rendering
    // In production, this would send to FFmpeg server
    const videoBlob = await renderVideoWithCanvas(
        timeline,
        mergedAudio,
        mergedConfig,
        (sceneProgress) => {
            onProgress?.({
                stage: "rendering_scenes",
                progress: 30 + (sceneProgress * 50),
                message: `Rendering scene ${Math.ceil(sceneProgress * timeline.length)}/${timeline.length}...`,
            });
        }
    );

    onProgress?.({
        stage: "complete",
        progress: 100,
        message: "Video assembly complete!",
    });

    console.log(`[Editor] Video assembled: ${(videoBlob.size / 1024 / 1024).toFixed(2)} MB`);

    return videoBlob;
}

/**
 * Merge multiple audio blobs into a single audio track.
 */
async function mergeAudioBlobs(
    blobs: Blob[],
    durations: number[]
): Promise<Blob> {
    // For PCM audio from Gemini TTS (L16 format at 24kHz)
    const sampleRate = 24000;
    const bytesPerSample = 2; // 16-bit

    // Calculate total size
    const totalSamples = durations.reduce((sum, d) => sum + Math.ceil(d * sampleRate), 0);
    const totalBytes = totalSamples * bytesPerSample;

    const mergedBuffer = new ArrayBuffer(totalBytes);
    const mergedView = new Uint8Array(mergedBuffer);

    let offset = 0;
    for (let i = 0; i < blobs.length; i++) {
        const blob = blobs[i];
        if (!blob) continue;
        const blobData = new Uint8Array(await blob.arrayBuffer());
        mergedView.set(blobData, offset);
        offset += blobData.length;
    }

    // Create WAV header
    const wavBuffer = createWavBuffer(mergedView, sampleRate);

    return new Blob([wavBuffer], { type: "audio/wav" });
}

/**
 * Create a WAV file buffer from raw PCM data.
 */
function createWavBuffer(pcmData: Uint8Array, sampleRate: number): ArrayBuffer {
    const headerSize = 44;
    const buffer = new ArrayBuffer(headerSize + pcmData.length);
    const view = new DataView(buffer);

    // RIFF header
    writeString(view, 0, "RIFF");
    view.setUint32(4, 36 + pcmData.length, true);
    writeString(view, 8, "WAVE");

    // fmt chunk
    writeString(view, 12, "fmt ");
    view.setUint32(16, 16, true); // chunk size
    view.setUint16(20, 1, true); // PCM format
    view.setUint16(22, 1, true); // mono
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * 2, true); // byte rate
    view.setUint16(32, 2, true); // block align
    view.setUint16(34, 16, true); // bits per sample

    // data chunk
    writeString(view, 36, "data");
    view.setUint32(40, pcmData.length, true);

    // Copy PCM data
    new Uint8Array(buffer, headerSize).set(pcmData);

    return buffer;
}

function writeString(view: DataView, offset: number, str: string): void {
    for (let i = 0; i < str.length; i++) {
        view.setUint8(offset + i, str.charCodeAt(i));
    }
}

/**
 * Render video using canvas (client-side fallback).
 * For production, prefer server-side FFmpeg rendering.
 */
async function renderVideoWithCanvas(
    timeline: Array<{
        sceneId: string;
        startTime: number;
        duration: number;
        visualUrl: string;
        visualType: "image" | "video";
        transition: string;
    }>,
    audioBlob: Blob,
    config: Required<Omit<AssemblyConfig, 'backgroundMusic'>> & { backgroundMusic: Blob | null },
    onProgress: (progress: number) => void
): Promise<Blob> {
    const { width, height, fps } = config;
    const totalDuration = timeline.reduce((sum, t) => sum + t.duration, 0);
    const totalFrames = Math.ceil(totalDuration * fps);

    // Create canvas
    const canvas = document.createElement("canvas");
    canvas.width = width;
    canvas.height = height;
    const ctx = canvas.getContext("2d")!;

    // Load all images
    const images = await Promise.all(
        timeline.map(async (item) => {
            if (item.visualType === "image") {
                const img = new Image();
                img.crossOrigin = "anonymous";
                await new Promise<void>((resolve, reject) => {
                    img.onload = () => resolve();
                    img.onerror = reject;
                    img.src = item.visualUrl;
                });
                return { type: "image" as const, element: img };
            }
            return { type: "image" as const, element: new Image() }; // Fallback
        })
    );

    // Capture frames
    const frames: Blob[] = [];

    for (let frame = 0; frame < totalFrames; frame++) {
        const currentTime = frame / fps;

        // Find current scene
        let sceneIndex = 0;
        let sceneStartTime = 0;
        for (let i = 0; i < timeline.length; i++) {
            const item = timeline[i];
            if (!item) continue;
            if (currentTime >= sceneStartTime && currentTime < sceneStartTime + item.duration) {
                sceneIndex = i;
                break;
            }
            sceneStartTime += item.duration;
        }

        // Clear canvas
        ctx.fillStyle = "#000";
        ctx.fillRect(0, 0, width, height);

        // Draw current image
        const img = images[sceneIndex]?.element;
        if (img && img.complete && img.naturalWidth > 0) {
            // Cover fit
            const scale = Math.max(width / img.naturalWidth, height / img.naturalHeight);
            const drawWidth = img.naturalWidth * scale;
            const drawHeight = img.naturalHeight * scale;
            const x = (width - drawWidth) / 2;
            const y = (height - drawHeight) / 2;

            ctx.drawImage(img, x, y, drawWidth, drawHeight);
        }

        // Capture frame
        const blob = await new Promise<Blob>((resolve) => {
            canvas.toBlob((b) => resolve(b!), "image/jpeg", 0.85);
        });
        frames.push(blob);

        if (frame % 10 === 0) {
            onProgress(frame / totalFrames);
        }
    }

    // For now, return a simple WebM video using MediaRecorder simulation
    // In production, this would use FFmpeg
    console.log(`[Editor] Rendered ${frames.length} frames`);

    // Create a simple video by combining frames with audio
    // This is a simplified version - production would use FFmpeg
    const videoBlob = await combineFramesAndAudio(frames, audioBlob, fps, totalDuration);

    return videoBlob;
}

/**
 * Combine rendered frames with audio into final video.
 * Simplified version - production should use FFmpeg for proper encoding.
 */
async function combineFramesAndAudio(
    frames: Blob[],
    audioBlob: Blob,
    fps: number,
    duration: number
): Promise<Blob> {
    // For browser-based assembly, we'll create a simple approach
    // using canvas and MediaRecorder

    const canvas = document.createElement("canvas");
    canvas.width = 1280;
    canvas.height = 720;
    const ctx = canvas.getContext("2d")!;

    const stream = canvas.captureStream(fps);

    // Add audio track
    const audioContext = new AudioContext();
    const audioBuffer = await audioContext.decodeAudioData(await audioBlob.arrayBuffer());
    const mediaStreamDest = audioContext.createMediaStreamDestination();
    const source = audioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(mediaStreamDest);

    // Combine tracks
    const audioTrack = mediaStreamDest.stream.getAudioTracks()[0];
    if (audioTrack) {
        stream.addTrack(audioTrack);
    }

    const recorder = new MediaRecorder(stream, {
        mimeType: "video/webm;codecs=vp8,opus",
        videoBitsPerSecond: 2500000,
    });

    const chunks: Blob[] = [];
    recorder.ondataavailable = (e) => chunks.push(e.data);

    return new Promise((resolve) => {
        recorder.onstop = () => {
            const videoBlob = new Blob(chunks, { type: "video/webm" });
            resolve(videoBlob);
        };

        recorder.start();
        source.start();

        // Draw frames at correct timing
        let frameIndex = 0;
        const frameDuration = 1000 / fps;

        const drawFrame = async () => {
            if (frameIndex >= frames.length) {
                recorder.stop();
                return;
            }

            const frameBlob = frames[frameIndex];
            if (!frameBlob) {
                frameIndex++;
                drawFrame();
                return;
            }

            const img = new Image();
            img.src = URL.createObjectURL(frameBlob);
            await new Promise<void>((r) => { img.onload = () => r(); });
            ctx.drawImage(img, 0, 0, canvas.width, canvas.height);
            URL.revokeObjectURL(img.src);

            frameIndex++;
            setTimeout(drawFrame, frameDuration);
        };

        drawFrame();
    });
}
````

## File: packages/shared/src/services/errorAggregator.ts
````typescript
/**
 * Error Aggregator, Critical Failure Handler, and Rate Limit Queue
 *
 * Provides error handling infrastructure for the multi-format pipeline:
 * - ErrorAggregator: Collects errors and produces aggregated messages (Req 20.4, Property 41)
 * - CriticalFailureHandler: Pauses on critical failures with recovery options (Req 20.3, Property 40)
 * - RateLimitQueue: Queues tasks behind rate limit resets (Req 20.5, Property 42)
 *
 * React-free for Node.js compatibility.
 */

import { agentLogger } from './logger';

// ============================================================================
// Types
// ============================================================================

export type ErrorCode =
  | 'FORMAT_NOT_FOUND'
  | 'INVALID_FORMAT'
  | 'TASK_TIMEOUT'
  | 'TASK_FAILED'
  | 'RATE_LIMIT_EXCEEDED'
  | 'CHECKPOINT_TIMEOUT'
  | 'ASSEMBLY_FAILED'
  | 'PARTIAL_FAILURE'
  | 'CANCELLATION_FAILED';

export interface PipelineError {
  code: ErrorCode;
  message: string;
  phase: string;
  taskId?: string;
  recoverable: boolean;
  retryable: boolean;
  details?: any;
}

export interface RecoveryOption {
  action: 'retry' | 'edit' | 'skip' | 'cancel';
  label: string;
  description: string;
}

export interface CriticalFailureResult {
  action: RecoveryOption['action'];
  editedContent?: string;
}

export type OnCriticalFailure = (
  error: PipelineError,
  options: RecoveryOption[],
) => Promise<CriticalFailureResult>;

// ============================================================================
// Error Aggregator
// ============================================================================

const log = agentLogger.child('ErrorAggregator');

export class ErrorAggregator {
  private errors: PipelineError[] = [];

  addError(error: PipelineError): void {
    this.errors.push(error);
    log.warn(`[${error.code}] phase="${error.phase}": ${error.message}`);
  }

  getErrors(): PipelineError[] {
    return [...this.errors];
  }

  getAggregatedMessage(): string {
    if (this.errors.length === 0) {
      return 'No errors recorded.';
    }

    if (this.errors.length === 1) {
      const e = this.errors[0]!;
      return `[${e.code}] ${e.phase}: ${e.message}`;
    }

    const byPhase = new Map<string, PipelineError[]>();
    for (const error of this.errors) {
      const list = byPhase.get(error.phase) ?? [];
      list.push(error);
      byPhase.set(error.phase, list);
    }

    const lines: string[] = [`${this.errors.length} errors occurred during pipeline execution:`];
    for (const [phase, phaseErrors] of byPhase) {
      lines.push(`  Phase "${phase}":`);
      for (const e of phaseErrors) {
        const taskSuffix = e.taskId ? ` (task: ${e.taskId})` : '';
        lines.push(`    - [${e.code}] ${e.message}${taskSuffix}`);
      }
    }

    return lines.join('\n');
  }

  hasErrors(): boolean {
    return this.errors.length > 0;
  }

  hasCriticalErrors(): boolean {
    return this.errors.some((e) => !e.recoverable);
  }

  clear(): void {
    this.errors = [];
  }
}

// ============================================================================
// Critical Failure Handler
// ============================================================================

const CRITICAL_PHASES = new Set(['script', 'screenplay', 'assembly', 'final-assembly']);

const DEFAULT_RECOVERY_OPTIONS: RecoveryOption[] = [
  { action: 'retry', label: 'Retry', description: 'Retry the failed task from scratch' },
  { action: 'edit', label: 'Edit & Retry', description: 'Edit the input and retry' },
  { action: 'skip', label: 'Skip', description: 'Skip this task and continue' },
  { action: 'cancel', label: 'Cancel', description: 'Cancel the entire pipeline' },
];

export class CriticalFailureHandler {
  private onCriticalFailure: OnCriticalFailure;
  private aggregator: ErrorAggregator;

  constructor(onCriticalFailure: OnCriticalFailure, aggregator?: ErrorAggregator) {
    this.onCriticalFailure = onCriticalFailure;
    this.aggregator = aggregator ?? new ErrorAggregator();
  }

  async handleFailure(error: PipelineError): Promise<CriticalFailureResult> {
    this.aggregator.addError(error);

    if (CRITICAL_PHASES.has(error.phase)) {
      log.error(`Critical failure in phase "${error.phase}": ${error.message}`);
      return this.onCriticalFailure(error, DEFAULT_RECOVERY_OPTIONS);
    }

    log.warn(`Non-critical failure in phase "${error.phase}": ${error.message}. Continuing.`);
    return { action: 'skip' };
  }

  getAggregator(): ErrorAggregator {
    return this.aggregator;
  }
}

// ============================================================================
// Rate Limit Queue
// ============================================================================

export class RateLimitQueue {
  private defaultResetMs: number;

  constructor(options?: { defaultResetMs?: number }) {
    this.defaultResetMs = options?.defaultResetMs ?? 60_000;
  }

  isRateLimitError(error: any): boolean {
    if (!error) return false;
    const message = (error.message ?? '').toLowerCase();
    const statusCode = error.status ?? error.statusCode ?? error.code;
    return (
      statusCode === 429 ||
      statusCode === '429' ||
      message.includes('rate limit') ||
      message.includes('too many requests') ||
      message.includes('quota exceeded') ||
      message.includes('429')
    );
  }

  getResetDelay(error: any): number {
    const retryAfter =
      error?.details?.['retry-after'] ??
      error?.details?.retryAfter ??
      error?.headers?.['retry-after'] ??
      error?.retryAfter;

    if (retryAfter != null) {
      const parsed = Number(retryAfter);
      if (!Number.isNaN(parsed) && parsed > 0) {
        return parsed * 1000;
      }
    }

    return this.defaultResetMs;
  }

  async enqueue<T>(task: () => Promise<T>): Promise<T> {
    while (true) {
      try {
        return await task();
      } catch (error: any) {
        if (!this.isRateLimitError(error)) {
          throw error;
        }
        const delay = this.getResetDelay(error);
        log.warn(`Rate limit hit. Queuing retry after ${Math.round(delay / 1000)}s.`);
        await new Promise((resolve) => setTimeout(resolve, delay));
      }
    }
  }
}
````

## File: packages/shared/src/services/exportFormatsService.ts
````typescript
/**
 * Export Formats Service
 * 
 * Provides additional export formats for Story Mode projects:
 * - SRT/VTT subtitles
 * - JSON project export/import
 * - WebM video export
 */

import type { StoryState, StoryShot } from '@/types';

export interface SubtitleEntry {
  index: number;
  startTime: number;
  endTime: number;
  text: string;
}

export interface ProjectExport {
  version: string;
  exportedAt: string;
  projectName: string;
  state: StoryState;
  metadata: {
    totalScenes: number;
    totalShots: number;
    totalCharacters: number;
    visualStyle?: string;
    aspectRatio?: string;
    duration?: number;
  };
}

const EXPORT_VERSION = '1.0.0';

function formatSRTTime(seconds: number): string {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  const ms = Math.floor((seconds % 1) * 1000);
  
  return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')},${ms.toString().padStart(3, '0')}`;
}

function formatVTTTime(seconds: number): string {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  const ms = Math.floor((seconds % 1) * 1000);
  
  return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}.${ms.toString().padStart(3, '0')}`;
}

export function generateSubtitlesFromShots(shots: StoryShot[]): SubtitleEntry[] {
  const entries: SubtitleEntry[] = [];
  let currentTime = 0;
  
  shots.forEach((shot, index) => {
    const duration = shot.duration || 4; // Default 4 seconds per shot
    const text = shot.description || '';
    
    if (text.trim()) {
      entries.push({
        index: entries.length + 1,
        startTime: currentTime,
        endTime: currentTime + duration,
        text: text.trim(),
      });
    }
    
    currentTime += duration;
  });
  
  return entries;
}

export function generateSubtitlesFromDialogue(state: StoryState): SubtitleEntry[] {
  const entries: SubtitleEntry[] = [];
  let currentTime = 0;
  
  if (!state.script?.scenes) return entries;
  
  for (const scene of state.script.scenes) {
    for (const dialogue of scene.dialogue || []) {
      const text = `${dialogue.speaker}: ${dialogue.text}`;
      const duration = Math.max(2, text.length / 15); // ~15 chars per second reading speed
      
      entries.push({
        index: entries.length + 1,
        startTime: currentTime,
        endTime: currentTime + duration,
        text,
      });
      
      currentTime += duration + 0.5; // 0.5s gap between lines
    }
  }
  
  return entries;
}

export function exportToSRT(entries: SubtitleEntry[]): string {
  return entries.map(entry => {
    return [
      entry.index.toString(),
      `${formatSRTTime(entry.startTime)} --> ${formatSRTTime(entry.endTime)}`,
      entry.text,
      '', // Empty line separator
    ].join('\n');
  }).join('\n');
}

export function exportToVTT(entries: SubtitleEntry[]): string {
  const header = 'WEBVTT\n\n';
  
  const content = entries.map(entry => {
    return [
      entry.index.toString(),
      `${formatVTTTime(entry.startTime)} --> ${formatVTTTime(entry.endTime)}`,
      entry.text,
      '', // Empty line separator
    ].join('\n');
  }).join('\n');
  
  return header + content;
}

export function downloadSubtitles(
  state: StoryState,
  format: 'srt' | 'vtt',
  source: 'shots' | 'dialogue' = 'shots'
): void {
  const entries = source === 'shots' 
    ? generateSubtitlesFromShots(state.shots || [])
    : generateSubtitlesFromDialogue(state);
  
  const content = format === 'srt' ? exportToSRT(entries) : exportToVTT(entries);
  const mimeType = format === 'srt' ? 'text/plain' : 'text/vtt';
  const extension = format;
  
  const blob = new Blob([content], { type: mimeType });
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url;
  a.download = `${state.script?.title || 'story'}_subtitles.${extension}`;
  document.body.appendChild(a);
  a.click();
  document.body.removeChild(a);
  URL.revokeObjectURL(url);
}

export function exportProjectToJSON(state: StoryState, projectName?: string): ProjectExport {
  const shots = state.shots || [];
  let totalDuration = 0;
  
  for (const shot of shots) {
    totalDuration += shot.duration || 4;
  }
  
  return {
    version: EXPORT_VERSION,
    exportedAt: new Date().toISOString(),
    projectName: projectName || state.script?.title || 'Untitled Project',
    state: JSON.parse(JSON.stringify(state)), // Deep clone
    metadata: {
      totalScenes: state.breakdown?.length ?? 0,
      totalShots: shots.length,
      totalCharacters: state.characters?.length ?? 0,
      visualStyle: state.visualStyle,
      aspectRatio: state.aspectRatio,
      duration: totalDuration,
    },
  };
}

export function downloadProjectJSON(state: StoryState, projectName?: string): void {
  const exportData = exportProjectToJSON(state, projectName);
  const content = JSON.stringify(exportData, null, 2);
  
  const blob = new Blob([content], { type: 'application/json' });
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url;
  a.download = `${exportData.projectName.replace(/[^a-z0-9]/gi, '_')}_project.json`;
  document.body.appendChild(a);
  a.click();
  document.body.removeChild(a);
  URL.revokeObjectURL(url);
}

export async function importProjectFromJSON(file: File): Promise<{
  success: boolean;
  state?: StoryState;
  metadata?: ProjectExport['metadata'];
  error?: string;
}> {
  try {
    const content = await file.text();
    const data = JSON.parse(content) as ProjectExport;
    
    // Validate structure
    if (!data.version || !data.state) {
      return {
        success: false,
        error: 'Invalid project file format. Missing version or state.',
      };
    }
    
    // Check version compatibility
    const [major] = data.version.split('.');
    const [currentMajor] = EXPORT_VERSION.split('.');
    
    if (major !== currentMajor) {
      return {
        success: false,
        error: `Incompatible project version. File: ${data.version}, Current: ${EXPORT_VERSION}`,
      };
    }
    
    // Validate state has required fields
    if (!data.state.currentStep) {
      data.state.currentStep = 'idea';
    }
    
    return {
      success: true,
      state: data.state,
      metadata: data.metadata,
    };
  } catch (error) {
    return {
      success: false,
      error: error instanceof Error ? error.message : 'Failed to parse project file',
    };
  }
}

export async function convertToWebM(
  videoBlob: Blob,
  options?: {
    quality?: number; // 0-1, default 0.8
    width?: number;
    height?: number;
  }
): Promise<Blob> {
  return new Promise((resolve, reject) => {
    const video = document.createElement('video');
    video.muted = true;
    video.playsInline = true;
    
    const url = URL.createObjectURL(videoBlob);
    video.src = url;
    
    video.onloadedmetadata = () => {
      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');
      
      if (!ctx) {
        URL.revokeObjectURL(url);
        reject(new Error('Failed to get canvas context'));
        return;
      }
      
      const width = options?.width || video.videoWidth;
      const height = options?.height || video.videoHeight;
      
      canvas.width = width;
      canvas.height = height;
      
      const quality = options?.quality ?? 0.8;
      
      // Use MediaRecorder to capture the video as WebM
      const stream = canvas.captureStream(30);
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'video/webm;codecs=vp9',
        videoBitsPerSecond: Math.floor(2500000 * quality),
      });
      
      const chunks: Blob[] = [];
      
      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunks.push(e.data);
        }
      };
      
      mediaRecorder.onstop = () => {
        URL.revokeObjectURL(url);
        const webmBlob = new Blob(chunks, { type: 'video/webm' });
        resolve(webmBlob);
      };
      
      mediaRecorder.onerror = (e) => {
        URL.revokeObjectURL(url);
        reject(new Error('MediaRecorder error'));
      };
      
      mediaRecorder.start();
      
      video.onended = () => {
        mediaRecorder.stop();
      };
      
      const drawFrame = () => {
        if (video.paused || video.ended) return;
        ctx.drawImage(video, 0, 0, width, height);
        requestAnimationFrame(drawFrame);
      };
      
      video.play().then(() => {
        drawFrame();
      }).catch(reject);
    };
    
    video.onerror = () => {
      URL.revokeObjectURL(url);
      reject(new Error('Failed to load video'));
    };
  });
}

export async function downloadAsWebM(
  videoBlob: Blob,
  filename: string,
  options?: {
    quality?: number;
    width?: number;
    height?: number;
  }
): Promise<void> {
  try {
    const webmBlob = await convertToWebM(videoBlob, options);
    
    const url = URL.createObjectURL(webmBlob);
    const a = document.createElement('a');
    a.href = url;
    a.download = filename.endsWith('.webm') ? filename : `${filename}.webm`;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
  } catch (error) {
    console.error('[Export] WebM conversion failed:', error);
    throw error;
  }
}

export function getExportFormats(): Array<{
  id: string;
  name: string;
  extension: string;
  mimeType: string;
  description: string;
  category: 'video' | 'subtitle' | 'project';
}> {
  return [
    {
      id: 'mp4',
      name: 'MP4 Video',
      extension: '.mp4',
      mimeType: 'video/mp4',
      description: 'Standard video format, compatible with most devices',
      category: 'video',
    },
    {
      id: 'webm',
      name: 'WebM Video',
      extension: '.webm',
      mimeType: 'video/webm',
      description: 'Web-optimized video format for embedding',
      category: 'video',
    },
    {
      id: 'srt',
      name: 'SRT Subtitles',
      extension: '.srt',
      mimeType: 'text/plain',
      description: 'SubRip subtitle format, widely supported',
      category: 'subtitle',
    },
    {
      id: 'vtt',
      name: 'WebVTT Subtitles',
      extension: '.vtt',
      mimeType: 'text/vtt',
      description: 'Web Video Text Tracks, HTML5 native subtitles',
      category: 'subtitle',
    },
    {
      id: 'json',
      name: 'Project File',
      extension: '.json',
      mimeType: 'application/json',
      description: 'Full project backup, can be imported later',
      category: 'project',
    },
  ];
}
````

## File: packages/shared/src/services/ffmpeg/assetLoader.ts
````typescript
/**
 * Asset Loader Module
 *
 * Handles preloading of images and videos for video export rendering.
 * Provides async utilities for loading DOM elements from URLs.
 *
 * Enhanced with:
 * - Reliable video seeking with proper event handling
 * - Video dimension validation
 * - Video frame caching for performance
 * - Timeout handling for slow-loading assets
 */

import { SongData } from "../../types";
import { RenderAsset } from "./exportConfig";

// --- Constants ---

/** Timeout for loading assets (60 seconds - increased for large Veo video blobs) */
const ASSET_LOAD_TIMEOUT_MS = 60000;

/** Timeout for video seeking (2 seconds) */
const VIDEO_SEEK_TIMEOUT_MS = 2000;

// --- Video Frame Cache ---

/**
 * Cache for extracted video frames to avoid re-seeking
 * Key: `${videoSrc}:${frameIndex}` where frameIndex = Math.floor(time * FPS)
 */
const videoFrameCache = new Map<string, ImageBitmap>();

/** Maximum cache size (frames) to prevent memory issues */
const MAX_CACHE_SIZE = 500;

/** FPS used for frame cache key calculation */
const CACHE_FPS = 24;

/**
 * Get cache key for a video frame
 */
function getFrameCacheKey(videoSrc: string, time: number): string {
    const frameIndex = Math.floor(time * CACHE_FPS);
    return `${videoSrc}:${frameIndex}`;
}

/**
 * Get cached frame if available
 */
export function getCachedFrame(videoSrc: string, time: number): ImageBitmap | null {
    const key = getFrameCacheKey(videoSrc, time);
    return videoFrameCache.get(key) || null;
}

/**
 * Cache a video frame
 */
export function cacheFrame(videoSrc: string, time: number, bitmap: ImageBitmap): void {
    // Evict oldest entries if cache is full
    if (videoFrameCache.size >= MAX_CACHE_SIZE) {
        const firstKey = videoFrameCache.keys().next().value;
        if (firstKey) {
            const oldBitmap = videoFrameCache.get(firstKey);
            oldBitmap?.close(); // Release ImageBitmap memory
            videoFrameCache.delete(firstKey);
        }
    }

    const key = getFrameCacheKey(videoSrc, time);
    videoFrameCache.set(key, bitmap);
}

/**
 * Clear all cached frames (call after export completes)
 */
export function clearFrameCache(): void {
    for (const bitmap of videoFrameCache.values()) {
        bitmap.close();
    }
    videoFrameCache.clear();
    console.log("[AssetLoader] Frame cache cleared");
}

/**
 * Get cache statistics for debugging
 */
export function getFrameCacheStats(): { size: number; maxSize: number } {
    return { size: videoFrameCache.size, maxSize: MAX_CACHE_SIZE };
}

// --- Image Loading ---

/**
 * Load an image from URL and return the HTMLImageElement
 * @param url - URL to load image from
 * @param timeoutMs - Optional timeout in milliseconds (default: 15000)
 */
export async function loadImageAsset(url: string, timeoutMs = ASSET_LOAD_TIMEOUT_MS): Promise<HTMLImageElement> {
    const img = new Image();
    img.crossOrigin = "anonymous";

    await new Promise<void>((resolve, reject) => {
        const timeout = setTimeout(() => {
            reject(new Error(`Image load timeout after ${timeoutMs}ms: ${url}`));
        }, timeoutMs);

        img.onload = () => {
            clearTimeout(timeout);
            resolve();
        };
        img.onerror = (e) => {
            clearTimeout(timeout);
            reject(new Error(`Failed to load image: ${url} - ${e}`));
        };
        img.src = url;
    });

    return img;
}

// --- Video Loading ---

/**
 * Video asset result with additional metadata
 */
export interface VideoAssetResult {
    element: HTMLVideoElement;
    width: number;
    height: number;
    duration: number;
    hasAudio: boolean;
}

/**
 * Load a video from URL with proper validation and metadata extraction
 * @param url - URL to load video from
 * @param timeoutMs - Optional timeout in milliseconds (default: 15000)
 * @throws Error if video fails to load or has invalid dimensions
 */
export async function loadVideoAsset(url: string, timeoutMs = ASSET_LOAD_TIMEOUT_MS): Promise<HTMLVideoElement> {
    const vid = document.createElement("video");
    vid.crossOrigin = "anonymous";
    vid.preload = "metadata"; // Load metadata first, then switch to auto
    vid.playsInline = true;
    vid.muted = true; // Muted for export (audio handled separately)

    await new Promise<void>((resolve, reject) => {
        const timeout = setTimeout(() => {
            reject(new Error(`Video metadata load timeout after ${timeoutMs}ms: ${url}`));
        }, timeoutMs);

        vid.onloadedmetadata = () => {
            clearTimeout(timeout);

            // Validate dimensions
            if (vid.videoWidth === 0 || vid.videoHeight === 0) {
                reject(new Error(`Invalid video dimensions (${vid.videoWidth}x${vid.videoHeight}): ${url}`));
                return;
            }

            // Validate duration
            if (!vid.duration || !isFinite(vid.duration) || vid.duration <= 0) {
                reject(new Error(`Invalid video duration (${vid.duration}): ${url}`));
                return;
            }

            console.log(`[AssetLoader] Video loaded: ${vid.videoWidth}x${vid.videoHeight}, ${vid.duration.toFixed(2)}s - ${url}`);
            resolve();
        };

        vid.onerror = (e) => {
            clearTimeout(timeout);
            const errorMessage = vid.error?.message || "Unknown error";
            reject(new Error(`Failed to load video: ${url} - ${errorMessage}`));
        };

        vid.src = url;
    });

    // Switch to full preload and ensure video is ready for seeking
    vid.preload = "auto";

    // Prime the video for seeking by playing and immediately pausing
    // This ensures the video decoder is initialized
    try {
        await vid.play();
        vid.pause();
        vid.currentTime = 0;
    } catch (e) {
        // Play might fail due to autoplay policies, but that's ok for export
        console.warn(`[AssetLoader] Video play/pause init warning (non-fatal): ${e}`);
    }

    return vid;
}

/**
 * Load video with extended metadata
 */
export async function loadVideoAssetWithMetadata(url: string, timeoutMs = ASSET_LOAD_TIMEOUT_MS): Promise<VideoAssetResult> {
    const vid = await loadVideoAsset(url, timeoutMs);

    // Check if video has audio tracks
    // Note: This may not be accurate in all browsers
    const hasAudio = (vid as any).mozHasAudio !== undefined
        ? (vid as any).mozHasAudio
        : (vid as any).webkitAudioDecodedByteCount !== undefined
            ? (vid as any).webkitAudioDecodedByteCount > 0
            : true; // Assume audio present if we can't detect

    return {
        element: vid,
        width: vid.videoWidth,
        height: vid.videoHeight,
        duration: vid.duration,
        hasAudio,
    };
}

// --- Video Seeking ---

/**
 * Seek video to specific time with proper event handling
 * This ensures the video frame is actually available before returning
 *
 * @param vid - Video element to seek
 * @param time - Time in seconds to seek to
 * @param timeoutMs - Optional timeout in milliseconds (default: 2000)
 * @returns Promise that resolves when seek is complete
 */
export async function seekVideoToTime(
    vid: HTMLVideoElement,
    time: number,
    timeoutMs = VIDEO_SEEK_TIMEOUT_MS
): Promise<void> {
    // Clamp time to valid range
    const targetTime = Math.max(0, Math.min(time, vid.duration - 0.001));

    // If already at target time (within 1 frame tolerance), skip seeking
    const frameDuration = 1 / CACHE_FPS;
    if (Math.abs(vid.currentTime - targetTime) < frameDuration) {
        return;
    }

    return new Promise<void>((resolve, reject) => {
        const timeout = setTimeout(() => {
            vid.removeEventListener('seeked', onSeeked);
            vid.removeEventListener('error', onError);
            // Don't reject, just warn and resolve - allows export to continue
            console.warn(`[AssetLoader] Video seek timeout at ${targetTime.toFixed(2)}s, using current frame`);
            resolve();
        }, timeoutMs);

        const onSeeked = () => {
            clearTimeout(timeout);
            vid.removeEventListener('seeked', onSeeked);
            vid.removeEventListener('error', onError);
            resolve();
        };

        const onError = () => {
            clearTimeout(timeout);
            vid.removeEventListener('seeked', onSeeked);
            vid.removeEventListener('error', onError);
            console.warn(`[AssetLoader] Video seek error at ${targetTime.toFixed(2)}s`);
            resolve(); // Don't reject, allow export to continue
        };

        vid.addEventListener('seeked', onSeeked);
        vid.addEventListener('error', onError);
        vid.currentTime = targetTime;
    });
}

/**
 * Get video frame at specific time, using cache if available
 * Creates an ImageBitmap for efficient canvas drawing
 *
 * @param vid - Video element
 * @param time - Time in seconds
 * @param useCache - Whether to use/populate frame cache (default: true)
 * @returns ImageBitmap of the video frame
 */
export async function getVideoFrameAtTime(
    vid: HTMLVideoElement,
    time: number,
    useCache = true
): Promise<ImageBitmap> {
    const videoSrc = vid.src;

    // Check cache first
    if (useCache) {
        const cached = getCachedFrame(videoSrc, time);
        if (cached) {
            return cached;
        }
    }

    // Seek to the requested time
    await seekVideoToTime(vid, time);

    // Create ImageBitmap from current frame
    const bitmap = await createImageBitmap(vid);

    // Cache the frame
    if (useCache) {
        cacheFrame(videoSrc, time, bitmap);
    }

    return bitmap;
}

// --- Placeholder Generation ---

/**
 * Create a placeholder image for failed video loads
 */
export function createPlaceholderImage(
    width: number,
    height: number,
    message = "Video unavailable"
): HTMLCanvasElement {
    const canvas = document.createElement("canvas");
    canvas.width = width || 1280;
    canvas.height = height || 720;
    const ctx = canvas.getContext("2d")!;

    // Dark background
    ctx.fillStyle = "#1a1a2e";
    ctx.fillRect(0, 0, canvas.width, canvas.height);

    // Warning icon (simple triangle)
    const centerX = canvas.width / 2;
    const centerY = canvas.height / 2 - 30;
    const iconSize = 60;

    ctx.fillStyle = "#ff6b6b";
    ctx.beginPath();
    ctx.moveTo(centerX, centerY - iconSize / 2);
    ctx.lineTo(centerX + iconSize / 2, centerY + iconSize / 2);
    ctx.lineTo(centerX - iconSize / 2, centerY + iconSize / 2);
    ctx.closePath();
    ctx.fill();

    // Exclamation mark
    ctx.fillStyle = "#1a1a2e";
    ctx.font = "bold 36px Arial";
    ctx.textAlign = "center";
    ctx.textBaseline = "middle";
    ctx.fillText("!", centerX, centerY + 10);

    // Message
    ctx.fillStyle = "#ffffff";
    ctx.font = "24px Arial";
    ctx.fillText(message, centerX, centerY + iconSize + 20);

    return canvas;
}

// --- Asset Preloading ---

/**
 * Progress callback for asset loading
 */
export interface AssetLoadProgress {
    loaded: number;
    total: number;
    currentAsset: string;
    type: "image" | "video";
    success: boolean;
}

/**
 * Preload all assets (images/videos) for a song
 * Returns sorted array of RenderAssets ready for frame rendering
 *
 * Enhanced with:
 * - Proper error handling with fallback placeholders
 * - Detailed progress reporting
 * - Video dimension validation
 */
export async function preloadAssets(
    songData: SongData,
    onProgress?: (progress: AssetLoadProgress) => void
): Promise<RenderAsset[]> {
    const assets: RenderAsset[] = [];

    // Sort prompts by timestamp
    const sortedPrompts = [...songData.prompts].sort(
        (a, b) => (a.timestampSeconds || 0) - (b.timestampSeconds || 0)
    );

    // Debug: Log timestamp distribution
    const timestamps = sortedPrompts.map(p => p.timestampSeconds || 0);
    const uniqueTimestamps = new Set(timestamps);
    console.log(`[AssetLoader] Prompt timestamps: ${sortedPrompts.length} prompts, ${uniqueTimestamps.size} unique times`);
    if (sortedPrompts.length > 0) {
        console.log(`[AssetLoader] Time range: ${timestamps[0]}s - ${timestamps[timestamps.length - 1]}s`);
        if (uniqueTimestamps.size === 1) {
            console.warn(`[AssetLoader] WARNING: All prompts have same timestamp (${timestamps[0]}s) - video will show only last image!`);
        }
    }

    const total = sortedPrompts.length;
    let loaded = 0;

    for (const prompt of sortedPrompts) {
        const generated = songData.generatedImages.find(
            (g) => g.promptId === prompt.id
        );

        if (generated) {
            const isVideo = generated.type === "video";
            // Prefer cached blob URL over original URL (prevents expired URL issues)
            const assetUrl = generated.cachedBlobUrl || generated.imageUrl;

            onProgress?.({
                loaded,
                total,
                currentAsset: assetUrl,
                type: isVideo ? "video" : "image",
                success: true,
            });

            try {

                if (isVideo) {
                    const element = await loadVideoAsset(assetUrl);
                    // Capture native duration for freeze-frame support (Issue 6)
                    const nativeDuration = element.duration && isFinite(element.duration) ? element.duration : undefined;
                    assets.push({
                        time: prompt.timestampSeconds || 0,
                        type: "video",
                        element,
                        nativeDuration,
                    });
                    console.log(`[AssetLoader] ✓ Video asset loaded for scene ${prompt.id} (native duration: ${nativeDuration?.toFixed(1) || 'unknown'}s)${generated.cachedBlobUrl ? ' (from cache)' : ''}`);
                } else {
                    const element = await loadImageAsset(assetUrl);
                    assets.push({
                        time: prompt.timestampSeconds || 0,
                        type: "image",
                        element,
                    });
                }
            } catch (error) {
                console.warn(`[AssetLoader] ✗ Failed to load ${isVideo ? 'video' : 'image'} for prompt ${prompt.id}:`, error);

                // Create placeholder for failed assets
                const placeholder = createPlaceholderImage(1280, 720, `Scene ${prompt.id} unavailable`);
                assets.push({
                    time: prompt.timestampSeconds || 0,
                    type: "image", // Fallback to image type
                    element: placeholder as unknown as HTMLImageElement,
                });

                onProgress?.({
                    loaded,
                    total,
                    currentAsset: assetUrl,
                    type: isVideo ? "video" : "image",
                    success: false,
                });
            }
        }

        loaded++;
    }

    // Log final asset times for debugging
    const assetTimes = assets.map(a => a.time);
    const uniqueAssetTimes = new Set(assetTimes);
    console.log(`[AssetLoader] Preloaded ${assets.length} assets (${assets.filter(a => a.type === 'video').length} videos)`);
    console.log(`[AssetLoader] Asset time distribution: ${uniqueAssetTimes.size} unique times, range: ${Math.min(...assetTimes)}s - ${Math.max(...assetTimes)}s`);
    
    if (uniqueAssetTimes.size === 1 && assets.length > 1) {
        console.error(`[AssetLoader] BUG: All ${assets.length} assets have same time (${assetTimes[0]}s)! Check timestampSeconds calculation.`);
    }
    
    return assets;
}
````

## File: packages/shared/src/services/ffmpeg/checksumGenerator.ts
````typescript
/**
 * Checksum Generator for Frame Validation
 *
 * Generates SHA-256 checksums for frames client-side
 * to enable server-side validation of uploaded data integrity.
 */

/**
 * Frame checksum data
 */
export interface FrameChecksum {
  frameIndex: number;
  checksum: string;
  size: number;
}

/**
 * Generate SHA-256 checksum for a Blob
 */
export async function generateBlobChecksum(blob: Blob): Promise<string> {
  const buffer = await blob.arrayBuffer();
  const hashBuffer = await crypto.subtle.digest('SHA-256', buffer);
  const hashArray = Array.from(new Uint8Array(hashBuffer));
  return hashArray.map((b) => b.toString(16).padStart(2, '0')).join('');
}

/**
 * Generate checksum for an ArrayBuffer
 */
export async function generateBufferChecksum(
  buffer: ArrayBuffer
): Promise<string> {
  const hashBuffer = await crypto.subtle.digest('SHA-256', buffer);
  const hashArray = Array.from(new Uint8Array(hashBuffer));
  return hashArray.map((b) => b.toString(16).padStart(2, '0')).join('');
}

/**
 * Generate checksums for a batch of frames
 */
export async function generateBatchChecksums(
  frames: Array<{ blob: Blob; index: number }>
): Promise<FrameChecksum[]> {
  const checksums: FrameChecksum[] = [];

  // Process in parallel for performance
  const promises = frames.map(async ({ blob, index }) => {
    const checksum = await generateBlobChecksum(blob);
    return {
      frameIndex: index,
      checksum,
      size: blob.size,
    };
  });

  return Promise.all(promises);
}

/**
 * Generate checksums with progress callback (for large batches)
 */
export async function generateBatchChecksumsWithProgress(
  frames: Array<{ blob: Blob; index: number }>,
  onProgress?: (completed: number, total: number) => void
): Promise<FrameChecksum[]> {
  const checksums: FrameChecksum[] = [];
  const total = frames.length;

  // Process in smaller parallel batches for memory efficiency
  const BATCH_SIZE = 10;

  for (let i = 0; i < frames.length; i += BATCH_SIZE) {
    const batch = frames.slice(i, i + BATCH_SIZE);
    const batchResults = await generateBatchChecksums(batch);
    checksums.push(...batchResults);

    onProgress?.(Math.min(i + BATCH_SIZE, total), total);
  }

  return checksums;
}

/**
 * Check if Web Crypto API is available
 */
export function isChecksumSupported(): boolean {
  return (
    typeof crypto !== 'undefined' &&
    typeof crypto.subtle !== 'undefined' &&
    typeof crypto.subtle.digest === 'function'
  );
}

/**
 * Simple fallback checksum using basic hashing (less secure but works everywhere)
 * Only use if Web Crypto is not available
 */
export function generateSimpleChecksum(data: ArrayBuffer): string {
  const view = new Uint8Array(data);
  let hash = 0x811c9dc5; // FNV-1a offset basis

  for (let i = 0; i < view.length; i++) {
    hash ^= view[i]!;
    hash = Math.imul(hash, 0x01000193); // FNV prime
  }

  // Return as hex string
  return (hash >>> 0).toString(16).padStart(8, '0');
}

/**
 * Verify a checksum matches
 */
export async function verifyChecksum(
  blob: Blob,
  expectedChecksum: string
): Promise<boolean> {
  const actualChecksum = await generateBlobChecksum(blob);
  return actualChecksum === expectedChecksum;
}

/**
 * Create a manifest of all frame checksums (for sending with finalize request)
 */
export function createFrameManifest(
  checksums: FrameChecksum[]
): Record<number, FrameChecksum> {
  const manifest: Record<number, FrameChecksum> = {};

  for (const checksum of checksums) {
    manifest[checksum.frameIndex] = checksum;
  }

  return manifest;
}
````

## File: packages/shared/src/services/ffmpeg/envUtils.ts
````typescript
/**
 * Platform Utilities Module
 *
 * Platform detection and export engine recommendations.
 * Extracted from ffmpegService.ts for modularity.
 */

import { isNative, isFFmpegWasmSupported, getRecommendedExportEngine } from "../../utils/platformUtils";

/**
 * Check if client-side FFmpeg WASM export is available on this platform
 * Returns false on mobile (Capacitor) as SharedArrayBuffer is not supported in WebViews
 */
export function isClientSideExportAvailable(): boolean {
    return isFFmpegWasmSupported();
}

/**
 * Get the recommended export engine for current platform
 * Mobile apps should use 'cloud' rendering, web can use 'browser' if supported
 */
export function getDefaultExportEngine(): "cloud" | "browser" {
    return getRecommendedExportEngine();
}
````

## File: packages/shared/src/services/ffmpeg/exportConfig.ts
````typescript
/**
 * Export Configuration
 * 
 * Types and default configurations for video export.
 * Extracted from ffmpegService.ts for modularity.
 */

import { TransitionType, VideoFormat, FormatAssemblyRules } from "../../types";
import { isAndroid } from "../../utils/platformUtils";

/**
 * Get the server URL based on the current platform.
 *
 * Priority:
 *  1. VITE_SERVER_URL env var — set this in .env.local when running on a real
 *     mobile device so the app can reach the Express server over the LAN.
 *     e.g. VITE_SERVER_URL=http://192.168.1.42:3001
 *  2. Android emulator — 10.0.2.2 is the host loopback alias inside AVD.
 *  3. Default — localhost:3001 (works for web browser dev).
 */
export const getServerUrl = (): string => {
    // Allow override via environment variable (real device / CI)
    const envUrl = typeof import.meta !== 'undefined'
        ? (import.meta as { env?: Record<string, string> }).env?.VITE_SERVER_URL
        : undefined;
    if (envUrl) return envUrl;

    if (isAndroid()) {
        return "http://10.0.2.2:3001";
    }
    return "http://localhost:3001";
};

export const SERVER_URL = getServerUrl();

export type ExportProgress = {
    stage: "loading" | "preparing" | "rendering" | "encoding" | "complete";
    progress: number;
    message: string;
    /** Current frame being rendered (for detailed progress) */
    currentFrame?: number;
    /** Total frames to render */
    totalFrames?: number;
    /** Current asset type being processed */
    currentAssetType?: "image" | "video";
    /** Current asset index */
    currentAssetIndex?: number;
    /** Total asset count */
    totalAssets?: number;
    /** Video-specific: whether seeking is in progress */
    isSeekingVideo?: boolean;
};

export type ProgressCallback = (progress: ExportProgress) => void;

export interface ExportConfig {
    orientation: "landscape" | "portrait";
    useModernEffects: boolean;
    syncOffsetMs: number;
    fadeOutBeforeCut: boolean;
    wordLevelHighlight: boolean;
    contentMode: "music" | "story";
    transitionType: TransitionType;
    transitionDuration: number;

    visualizerConfig?: {
        enabled: boolean;
        opacity: number;
        maxHeightRatio: number;
        zIndex: number;
        barWidth: number;
        barGap: number;
        colorScheme: "cyan-purple" | "rainbow" | "monochrome";
    };

    textAnimationConfig?: {
        revealDirection: "ltr" | "rtl" | "center-out" | "center-in";
        revealDuration: number;
        wordReveal: boolean;
    };

    // SFX configuration
    sfxPlan?: import("../../types").VideoSFXPlan | null;
    sfxMasterVolume?: number; // 0-1, default: 1.0
    musicMasterVolume?: number; // 0-1, default: 0.5
    /** Scene timing info for SFX mixing */
    sceneTimings?: import("../audioMixerService").SceneAudioInfo[];

    // Format-specific assembly (Task 10.1)
    /** Video format identifier — drives format-specific assembly rules */
    formatId?: VideoFormat;
    /** Pre-built format assembly rules (overrides auto-generation from formatId) */
    assemblyRules?: FormatAssemblyRules;
}



export type RenderAsset = {
    time: number;
    type: "image" | "video";
    element: HTMLImageElement | HTMLVideoElement;
    /** Native duration of the video asset in seconds (for freeze-frame on short clips) */
    nativeDuration?: number;
};

/**
 * Default export configuration for cloud rendering
 */
export const DEFAULT_EXPORT_CONFIG: ExportConfig = {
    orientation: "landscape",
    useModernEffects: true,
    syncOffsetMs: -50,
    fadeOutBeforeCut: true,
    wordLevelHighlight: true,
    contentMode: "music",
    transitionType: "dissolve",
    transitionDuration: 1.5,
    visualizerConfig: {
        enabled: true,
        opacity: 0.15,
        maxHeightRatio: 0.25,
        zIndex: 1,
        barWidth: 3,
        barGap: 2,
        colorScheme: "cyan-purple",
    },
    textAnimationConfig: {
        revealDirection: "ltr",
        revealDuration: 0.3,
        wordReveal: true,
    },
};

/**
 * Merge user config with defaults
 */
export function mergeExportConfig(config?: Partial<ExportConfig>): ExportConfig {
    if (!config) return DEFAULT_EXPORT_CONFIG;

    return {
        ...DEFAULT_EXPORT_CONFIG,
        ...config,
        visualizerConfig: config.visualizerConfig
            ? { ...DEFAULT_EXPORT_CONFIG.visualizerConfig!, ...config.visualizerConfig }
            : DEFAULT_EXPORT_CONFIG.visualizerConfig,
        textAnimationConfig: config.textAnimationConfig
            ? { ...DEFAULT_EXPORT_CONFIG.textAnimationConfig!, ...config.textAnimationConfig }
            : DEFAULT_EXPORT_CONFIG.textAnimationConfig,
    };
}
````

## File: packages/shared/src/services/ffmpeg/exporters.ts
````typescript
/**
 * Video Exporters Module
 *
 * Cloud rendering (server-side FFmpeg) and browser rendering (FFmpeg WASM) export functions.
 */

import { SongData } from "../../types";
import { extractFrequencyData } from "../../utils/audioAnalysis";
import { FFmpeg } from "@ffmpeg/ffmpeg";
import { fetchFile, toBlobURL } from "@ffmpeg/util";
import { applyPolyfills } from "../../lib/utils";
import { mixAudioWithSFX, canMixSFX } from "../audioMixerService";
import {
    ExportConfig,
    ExportProgress,
    ProgressCallback,
    RenderAsset,
    SERVER_URL,
    DEFAULT_EXPORT_CONFIG,
    mergeExportConfig,
} from "./exportConfig";
import { preloadAssets, clearFrameCache, type AssetLoadProgress } from "./assetLoader";
import { renderFrameToCanvas } from "./frameRenderer";
import { cloudAutosave } from "../cloudStorageService";
import { saveExportRecord } from "../projectService";
import { subscribeToJob, isSSESupported, JobProgress } from "./sseClient";
import { generateBatchChecksums, isChecksumSupported, FrameChecksum } from "./checksumGenerator";

// Rendering constants
const RENDER_WIDTH_LANDSCAPE = 1920;
const RENDER_HEIGHT_LANDSCAPE = 1080;
const RENDER_WIDTH_PORTRAIT = 1080;
const RENDER_HEIGHT_PORTRAIT = 1920;
const FPS = 24;
const JPEG_QUALITY = 0.92;      // Slightly reduced for faster uploads (still excellent quality)
const BATCH_SIZE = 96;          // Doubled batch size for fewer HTTP round-trips
const PARALLEL_RENDERS = 4;     // Number of frames to render in parallel (where possible)

/**
 * Export options for user project tracking
 */
export interface ExportOptions {
    cloudSessionId?: string;
    userId?: string;
    projectId?: string;
}

/**
 * Export result with optional cloud URL
 */
export interface ExportResult {
    blob: Blob;
    cloudUrl?: string;
}

/**
 * Export video using cloud rendering (server-side FFmpeg)
 */
export async function exportVideoWithFFmpeg(
    songData: SongData,
    onProgress: ProgressCallback,
    config: Partial<ExportConfig> = {},
    options: ExportOptions = {}
): Promise<ExportResult> {
    const { cloudSessionId, userId, projectId } = options;
    const mergedConfig = mergeExportConfig(config);
    const WIDTH = mergedConfig.orientation === "landscape" ? RENDER_WIDTH_LANDSCAPE : RENDER_WIDTH_PORTRAIT;
    const HEIGHT = mergedConfig.orientation === "landscape" ? RENDER_HEIGHT_LANDSCAPE : RENDER_HEIGHT_PORTRAIT;

    onProgress({
        stage: "preparing",
        progress: 0,
        message: "Analyzing audio...",
    });

    // 1. Fetch and Decode Audio
    if (!songData.audioUrl) {
        throw new Error("No audio URL provided. Cannot export video without audio.");
    }

    const audioResponse = await fetch(songData.audioUrl);
    if (!audioResponse.ok) {
        throw new Error(`Failed to fetch audio: ${audioResponse.status} ${audioResponse.statusText}`);
    }
    const audioBlob = await audioResponse.blob();
    if (audioBlob.size === 0) {
        throw new Error("Audio file is empty. Please ensure audio has been generated.");
    }
    const arrayBuffer = await audioBlob.arrayBuffer();
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();

    let audioBuffer: AudioBuffer;
    try {
        audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    } catch (decodeError: any) {
        throw new Error(`Unable to decode audio data. The audio file may be corrupted or in an unsupported format. (${decodeError.message || 'Unknown error'})`);
    }

    // 2. Extract Frequency Data
    const frequencyDataArray = await extractFrequencyData(audioBuffer, FPS);

    onProgress({
        stage: "preparing",
        progress: 10,
        message: "Initializing render session...",
    });

    // 3. Initialize Session
    const initFormData = new FormData();
    initFormData.append("audio", audioBlob, "audio.mp3");

    const initRes = await fetch(`${SERVER_URL}/api/export/init`, {
        method: "POST",
        body: initFormData,
    });

    if (!initRes.ok) {
        throw new Error("Failed to initialize export session");
    }

    const { sessionId } = await initRes.json();

    onProgress({
        stage: "preparing",
        progress: 20,
        message: "Loading high-res assets...",
    });

    // 4. Preload assets with progress tracking
    const assets = await preloadAssets(songData, (progress) => {
        onProgress({
            stage: "preparing",
            progress: 20 + Math.round((progress.loaded / progress.total) * 10),
            message: `Loading asset ${progress.loaded}/${progress.total} (${progress.type})...`,
            currentAssetIndex: progress.loaded,
            totalAssets: progress.total,
            currentAssetType: progress.type,
        });
    });

    // Count video assets for logging
    const videoAssetCount = assets.filter(a => a.type === "video").length;
    const imageAssetCount = assets.filter(a => a.type === "image").length;
    console.log(`[FFmpeg] Loaded ${assets.length} assets (${videoAssetCount} videos, ${imageAssetCount} images)`);

    const duration = audioBuffer.duration;
    const totalFrames = Math.ceil(duration * FPS);

    onProgress({
        stage: "rendering",
        progress: 0,
        message: videoAssetCount > 0
            ? `Rendering ${totalFrames} frames (includes ${videoAssetCount} video assets)...`
            : "Rendering cinematic frames...",
        totalFrames,
        totalAssets: assets.length,
    });

    // 5. Create canvas
    const canvas = document.createElement("canvas");
    canvas.width = WIDTH;
    canvas.height = HEIGHT;
    const ctx = canvas.getContext("2d", { willReadFrequently: true })!;
    applyPolyfills(ctx);

    // 6. Render Loop with Parallel Upload
    // Upload batches asynchronously while rendering continues
    let previousFreqData: Uint8Array | null = null;
    let frameBuffer: { blob: Blob; name: string }[] = [];
    let pendingUpload: Promise<void> | null = null;
    let uploadErrors: Error[] = [];

    // Helper to upload a batch asynchronously
    const uploadBatch = async (batch: { blob: Blob; name: string }[]) => {
        const chunkFormData = new FormData();
        batch.forEach((f) => chunkFormData.append("frames", f.blob, f.name));
        const chunkRes = await fetch(
            `${SERVER_URL}/api/export/chunk?sessionId=${sessionId}`,
            { method: "POST", body: chunkFormData }
        );
        if (!chunkRes.ok) {
            uploadErrors.push(new Error("Failed to upload video chunk"));
        }
    };

    for (let frame = 0; frame < totalFrames; frame++) {
        const currentTime = frame / FPS;
        const freqData = frequencyDataArray[frame] || new Uint8Array(128).fill(0);

        await renderFrameToCanvas(
            ctx,
            WIDTH,
            HEIGHT,
            currentTime,
            assets,
            songData.parsedSubtitles,
            freqData,
            previousFreqData,
            mergedConfig
        );

        previousFreqData = freqData;

        const blob = await new Promise<Blob>((resolve) => {
            canvas.toBlob((b) => resolve(b!), "image/jpeg", JPEG_QUALITY);
        });

        frameBuffer.push({
            blob,
            name: `frame${frame.toString().padStart(6, "0")}.jpg`,
        });

        // Upload batch asynchronously (don't wait, continue rendering)
        if (frameBuffer.length >= BATCH_SIZE) {
            // Wait for previous upload to complete before starting new one
            if (pendingUpload) {
                await pendingUpload;
            }
            // Check for errors
            if (uploadErrors.length > 0) {
                throw uploadErrors[0];
            }
            // Start new upload (don't await - continue rendering)
            const batchToUpload = [...frameBuffer];
            pendingUpload = uploadBatch(batchToUpload);
            frameBuffer = [];
        }

        // Update progress with detailed frame info
        if (frame % FPS === 0) {
            const progress = Math.round((frame / totalFrames) * 90);

            // Determine current asset type
            const currentAsset = assets.find((a, i) => {
                const nextAsset = assets[i + 1];
                return currentTime >= a.time && (!nextAsset || currentTime < nextAsset.time);
            });

            onProgress({
                stage: "rendering",
                progress,
                message: `Rendering ${Math.floor(frame / FPS)}s / ${Math.floor(duration)}s`,
                currentFrame: frame,
                totalFrames,
                currentAssetType: currentAsset?.type,
                isSeekingVideo: currentAsset?.type === "video",
            });
        }
    }

    // Wait for any pending upload
    if (pendingUpload) {
        await pendingUpload;
    }

    // Check for upload errors
    if (uploadErrors.length > 0) {
        throw uploadErrors[0];
    }

    // Upload remaining frames
    if (frameBuffer.length > 0) {
        const chunkFormData = new FormData();
        frameBuffer.forEach((f) => chunkFormData.append("frames", f.blob, f.name));
        await fetch(`${SERVER_URL}/api/export/chunk?sessionId=${sessionId}`, {
            method: "POST",
            body: chunkFormData,
        });
    }

    onProgress({
        stage: "encoding",
        progress: 90,
        message: "Queuing video encoding...",
    });

    // Determine encoding mode: async (SSE) or sync (legacy)
    const useAsyncEncoding = isSSESupported();

    const finalizeRes = await fetch(`${SERVER_URL}/api/export/finalize`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
            sessionId,
            fps: FPS,
            totalFrames,
            sync: !useAsyncEncoding,
        }),
    });

    if (!finalizeRes.ok) {
        const errorData = await finalizeRes.json().catch(() => ({ error: 'Export failed' }));
        throw new Error(errorData.error || "Export failed");
    }

    let videoBlob: Blob;

    if (useAsyncEncoding) {
        // Async mode: response is JSON with jobId
        const finalizeData = await finalizeRes.json();
        const jobId = finalizeData.jobId;
        console.log(`[FFmpeg] Job ${jobId} queued, subscribing to SSE progress`);

        onProgress({
            stage: "encoding",
            progress: 90,
            message: "Server encoding started...",
        });

        // Wait for job completion via SSE
        videoBlob = await new Promise<Blob>((resolve, reject) => {
            const unsubscribe = subscribeToJob(
                jobId,
                (progress: JobProgress) => {
                    // Map server progress (0-100) to UI progress (90-99)
                    const uiProgress = 90 + Math.round(progress.progress * 0.09);

                    onProgress({
                        stage: "encoding",
                        progress: uiProgress,
                        message: progress.message,
                        currentFrame: progress.currentFrame,
                        totalFrames: progress.totalFrames,
                    });

                    if (progress.status === 'complete') {
                        unsubscribe();
                        // Download the completed video
                        fetch(`${SERVER_URL}/api/export/download/${jobId}`)
                            .then(res => {
                                if (!res.ok) throw new Error('Failed to download video');
                                return res.blob();
                            })
                            .then(resolve)
                            .catch(reject);
                    } else if (progress.status === 'failed') {
                        unsubscribe();
                        reject(new Error(progress.error || 'Export failed'));
                    }
                },
                (error) => {
                    unsubscribe();
                    reject(error);
                }
            );

            // Timeout after 30 minutes
            setTimeout(() => {
                unsubscribe();
                reject(new Error('Export timed out'));
            }, 30 * 60 * 1000);
        });
    } else {
        // Sync mode: response IS the video blob (single request, no duplicate)
        onProgress({ stage: "encoding", progress: 99, message: "Downloading..." });
        videoBlob = await finalizeRes.blob();
    }

    onProgress({ stage: "complete", progress: 100, message: "Export complete!" });

    // Clear frame cache to free memory
    clearFrameCache();

    let cloudUrl: string | undefined;

    // Upload final video to cloud storage if session context provided
    if (cloudSessionId) {
        try {
            const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
            const filename = `export_${timestamp}.mp4`;
            const result = await cloudAutosave.saveAsset(
                cloudSessionId,
                videoBlob,
                filename,
                'exports',
                true, // waitForUpload
                true  // makePublic
            );
            if (result.publicUrl) {
                cloudUrl = result.publicUrl;
                console.log('[FFmpeg] ✓ Final video uploaded to cloud:', cloudUrl);

                // Save export record if user is authenticated
                if (userId && projectId) {
                    const aspectRatio = mergedConfig.orientation === 'landscape' ? '16:9' : '9:16';
                    await saveExportRecord(projectId, {
                        format: 'mp4',
                        quality: 'high',
                        aspectRatio: aspectRatio as '16:9' | '9:16' | '1:1',
                        cloudUrl,
                        fileSize: videoBlob.size,
                        duration,
                    });
                    console.log('[FFmpeg] ✓ Export record saved to Firestore');
                }
            }
        } catch (err) {
            console.warn('[FFmpeg] Cloud upload/record failed (non-fatal):', err);
        }
    }

    return { blob: videoBlob, cloudUrl };
}

/**
 * Export video using browser-side FFmpeg WASM
 */
export async function exportVideoClientSide(
    songData: SongData,
    onProgress: ProgressCallback,
    config: Partial<ExportConfig> = {},
    options: ExportOptions = {}
): Promise<ExportResult> {
    const { cloudSessionId, userId, projectId } = options;
    const mergedConfig = mergeExportConfig(config);
    const WIDTH = mergedConfig.orientation === "landscape" ? RENDER_WIDTH_LANDSCAPE : RENDER_WIDTH_PORTRAIT;
    const HEIGHT = mergedConfig.orientation === "landscape" ? RENDER_HEIGHT_LANDSCAPE : RENDER_HEIGHT_PORTRAIT;

    onProgress({
        stage: "loading",
        progress: 0,
        message: "Loading FFmpeg Core...",
    });

    const ffmpeg = new FFmpeg();
    const baseURL = "https://unpkg.com/@ffmpeg/core@0.12.10/dist/esm";

    try {
        await ffmpeg.load({
            coreURL: await toBlobURL(`${baseURL}/ffmpeg-core.js`, "text/javascript"),
            wasmURL: await toBlobURL(`${baseURL}/ffmpeg-core.wasm`, "application/wasm"),
        });
    } catch (e) {
        console.error("FFmpeg load failed", e);
        throw new Error("Failed to load FFmpeg. Check browser compatibility.");
    }

    onProgress({
        stage: "preparing",
        progress: 0,
        message: "Analyzing audio...",
    });

    // Validate audio URL
    if (!songData.audioUrl) {
        throw new Error("No audio URL provided. Cannot export video without audio.");
    }

    // 1. Fetch and potentially mix audio with SFX
    let audioBlob: Blob;
    let audioBuffer: AudioBuffer;

    const shouldMixSFX =
        mergedConfig.sfxPlan &&
        canMixSFX(mergedConfig.sfxPlan) &&
        mergedConfig.sceneTimings &&
        mergedConfig.sceneTimings.length > 0;

    if (shouldMixSFX) {
        onProgress({
            stage: "preparing",
            progress: 5,
            message: "Mixing audio with SFX...",
        });

        console.log("[FFmpeg] Mixing narration with SFX...");

        try {
            audioBlob = await mixAudioWithSFX({
                narrationUrl: songData.audioUrl,
                sfxPlan: mergedConfig.sfxPlan!,
                scenes: mergedConfig.sceneTimings!,
                sfxMasterVolume: mergedConfig.sfxMasterVolume,
                musicMasterVolume: mergedConfig.musicMasterVolume,
            });
            console.log(`[FFmpeg] Mixed audio size: ${(audioBlob.size / 1024 / 1024).toFixed(2)} MB`);
        } catch (error) {
            console.warn("[FFmpeg] SFX mixing failed, using original audio:", error);
            const audioResponse = await fetch(songData.audioUrl);
            if (!audioResponse.ok) {
                throw new Error(`Failed to fetch audio: ${audioResponse.status} ${audioResponse.statusText}`);
            }
            audioBlob = await audioResponse.blob();
        }
    } else {
        const audioResponse = await fetch(songData.audioUrl);
        if (!audioResponse.ok) {
            throw new Error(`Failed to fetch audio: ${audioResponse.status} ${audioResponse.statusText}`);
        }
        audioBlob = await audioResponse.blob();
    }

    if (audioBlob.size === 0) {
        throw new Error("Audio file is empty. Please ensure audio has been generated.");
    }

    // Decode audio for visualization
    const arrayBuffer = await audioBlob.arrayBuffer();
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();

    try {
        audioBuffer = await audioContext.decodeAudioData(arrayBuffer.slice(0));
    } catch (decodeError: any) {
        throw new Error(`Unable to decode audio data. The audio file may be corrupted or in an unsupported format. (${decodeError.message || 'Unknown error'})`);
    }

    await ffmpeg.writeFile("audio.wav", await fetchFile(audioBlob));

    // 2. Extract Frequency Data
    const frequencyDataArray = await extractFrequencyData(audioBuffer, FPS);

    onProgress({
        stage: "preparing",
        progress: 20,
        message: "Loading high-res assets...",
    });

    // 3. Preload assets with progress tracking
    const assets = await preloadAssets(songData, (progress) => {
        onProgress({
            stage: "preparing",
            progress: 20 + Math.round((progress.loaded / progress.total) * 10),
            message: `Loading asset ${progress.loaded}/${progress.total} (${progress.type})...`,
            currentAssetIndex: progress.loaded,
            totalAssets: progress.total,
            currentAssetType: progress.type,
        });
    });

    // Count video assets for logging
    const videoAssetCount = assets.filter(a => a.type === "video").length;
    const imageAssetCount = assets.filter(a => a.type === "image").length;
    console.log(`[FFmpeg WASM] Loaded ${assets.length} assets (${videoAssetCount} videos, ${imageAssetCount} images)`);

    const duration = audioBuffer.duration;
    const totalFrames = Math.ceil(duration * FPS);

    onProgress({
        stage: "rendering",
        progress: 0,
        message: videoAssetCount > 0
            ? `Rendering ${totalFrames} frames (includes ${videoAssetCount} video assets)...`
            : "Rendering frames...",
        totalFrames,
        totalAssets: assets.length,
    });

    // 4. Create canvas
    const canvas = document.createElement("canvas");
    canvas.width = WIDTH;
    canvas.height = HEIGHT;
    const ctx = canvas.getContext("2d", { willReadFrequently: true })!;
    applyPolyfills(ctx);

    // 5. Render Loop
    let previousFreqData: Uint8Array | null = null;

    for (let frame = 0; frame < totalFrames; frame++) {
        const currentTime = frame / FPS;
        const freqData = frequencyDataArray[frame] || new Uint8Array(128).fill(0);

        await renderFrameToCanvas(
            ctx,
            WIDTH,
            HEIGHT,
            currentTime,
            assets,
            songData.parsedSubtitles,
            freqData,
            previousFreqData,
            mergedConfig
        );

        previousFreqData = freqData;

        const blob = await new Promise<Blob>((resolve) => {
            canvas.toBlob((b) => resolve(b!), "image/jpeg", JPEG_QUALITY);
        });

        const frameName = `frame${frame.toString().padStart(6, "0")}.jpg`;
        await ffmpeg.writeFile(frameName, await fetchFile(blob));

        // Update progress with detailed frame info
        if (frame % FPS === 0) {
            const progress = Math.round((frame / totalFrames) * 80);

            // Determine current asset type
            const currentAsset = assets.find((a, i) => {
                const nextAsset = assets[i + 1];
                return currentTime >= a.time && (!nextAsset || currentTime < nextAsset.time);
            });

            onProgress({
                stage: "rendering",
                progress,
                message: `Rendering ${Math.floor(frame / FPS)}s / ${Math.floor(duration)}s`,
                currentFrame: frame,
                totalFrames,
                currentAssetType: currentAsset?.type,
                isSeekingVideo: currentAsset?.type === "video",
            });
        }
    }

    onProgress({
        stage: "encoding",
        progress: 85,
        message: "Encoding MP4 (WASM)...",
    });

    // 6. Run FFmpeg
    await ffmpeg.exec([
        "-framerate",
        String(FPS),
        "-i",
        "frame%06d.jpg",
        "-i",
        "audio.wav",
        "-c:v",
        "libx264",
        "-c:a",
        "aac",
        "-b:a",
        "256k",
        "-pix_fmt",
        "yuv420p",
        "-vf", `scale=${WIDTH}:${HEIGHT}:flags=lanczos,setsar=1`,
        "-shortest",
        "-preset", "medium",
        "-crf", "21",
        "output.mp4",
    ]);

    onProgress({ stage: "complete", progress: 100, message: "Done!" });

    // 7. Read output
    const data = (await ffmpeg.readFile("output.mp4")) as Uint8Array;
    const videoBlob = new Blob([data.slice()], { type: "video/mp4" });

    // Clear frame cache to free memory
    clearFrameCache();

    let cloudUrl: string | undefined;

    // Upload final video to cloud storage if session context provided
    if (cloudSessionId) {
        try {
            const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
            const filename = `export_${timestamp}.mp4`;
            const result = await cloudAutosave.saveAsset(
                cloudSessionId,
                videoBlob,
                filename,
                'exports',
                true, // waitForUpload
                true  // makePublic
            );
            if (result.publicUrl) {
                cloudUrl = result.publicUrl;
                console.log('[FFmpeg WASM] ✓ Final video uploaded to cloud:', cloudUrl);

                // Save export record if user is authenticated
                if (userId && projectId) {
                    const aspectRatio = mergedConfig.orientation === 'landscape' ? '16:9' : '9:16';
                    await saveExportRecord(projectId, {
                        format: 'mp4',
                        quality: 'high',
                        aspectRatio: aspectRatio as '16:9' | '9:16' | '1:1',
                        cloudUrl,
                        fileSize: videoBlob.size,
                        duration,
                    });
                    console.log('[FFmpeg WASM] ✓ Export record saved to Firestore');
                }
            }
        } catch (err) {
            console.warn('[FFmpeg WASM] Cloud upload/record failed (non-fatal):', err);
        }
    }

    return { blob: videoBlob, cloudUrl };
}
````

## File: packages/shared/src/services/ffmpeg/exportPresets.ts
````typescript
/**
 * Export Presets Module
 *
 * Provides pre-configured export settings for common video platforms and use cases.
 * Each preset optimizes settings for specific output targets.
 */

import { ExportConfig } from "./exportConfig";

/**
 * Export preset identifier
 */
export type ExportPresetId =
  | "youtube-landscape"
  | "youtube-shorts"
  | "tiktok"
  | "instagram-feed"
  | "instagram-reels"
  | "instagram-story"
  | "twitter"
  | "linkedin"
  | "draft-preview"
  | "high-quality"
  | "podcast-video";

/**
 * Export preset definition
 */
export interface ExportPreset {
  /** Preset identifier */
  id: ExportPresetId;
  /** Display name */
  name: string;
  /** Description of the preset */
  description: string;
  /** Target platform or use case */
  platform: string;
  /** Aspect ratio string */
  aspectRatio: "16:9" | "9:16" | "1:1" | "4:5";
  /** Orientation derived from aspect ratio */
  orientation: "landscape" | "portrait";
  /** Recommended frame rate */
  fps: number;
  /** Quality preset */
  quality: "draft" | "standard" | "high";
  /** Partial ExportConfig overrides */
  config: Partial<ExportConfig>;
  /** Maximum recommended duration in seconds */
  maxDuration?: number;
  /** Minimum recommended duration in seconds */
  minDuration?: number;
}

/**
 * All available export presets
 */
export const EXPORT_PRESETS: Record<ExportPresetId, ExportPreset> = {
  "youtube-landscape": {
    id: "youtube-landscape",
    name: "YouTube (Landscape)",
    description: "Standard YouTube video format, optimized for desktop viewing",
    platform: "YouTube",
    aspectRatio: "16:9",
    orientation: "landscape",
    fps: 24,
    quality: "high",
    config: {
      orientation: "landscape",
      useModernEffects: true,
      transitionDuration: 1.5,
      contentMode: "story",
    },
    minDuration: 60,
  },

  "youtube-shorts": {
    id: "youtube-shorts",
    name: "YouTube Shorts",
    description: "Vertical video format for YouTube Shorts",
    platform: "YouTube Shorts",
    aspectRatio: "9:16",
    orientation: "portrait",
    fps: 30,
    quality: "high",
    config: {
      orientation: "portrait",
      useModernEffects: true,
      transitionDuration: 0.8,
      contentMode: "story",
    },
    maxDuration: 60,
    minDuration: 15,
  },

  "tiktok": {
    id: "tiktok",
    name: "TikTok",
    description: "Optimized for TikTok's vertical format",
    platform: "TikTok",
    aspectRatio: "9:16",
    orientation: "portrait",
    fps: 30,
    quality: "standard",
    config: {
      orientation: "portrait",
      useModernEffects: true,
      transitionDuration: 0.5,
      contentMode: "story",
    },
    maxDuration: 180,
    minDuration: 15,
  },

  "instagram-feed": {
    id: "instagram-feed",
    name: "Instagram Feed",
    description: "Square format for Instagram feed posts",
    platform: "Instagram",
    aspectRatio: "1:1",
    orientation: "landscape", // Square uses landscape rendering
    fps: 30,
    quality: "standard",
    config: {
      orientation: "landscape",
      useModernEffects: true,
      transitionDuration: 1.0,
      contentMode: "story",
    },
    maxDuration: 60,
  },

  "instagram-reels": {
    id: "instagram-reels",
    name: "Instagram Reels",
    description: "Vertical video for Instagram Reels",
    platform: "Instagram Reels",
    aspectRatio: "9:16",
    orientation: "portrait",
    fps: 30,
    quality: "high",
    config: {
      orientation: "portrait",
      useModernEffects: true,
      transitionDuration: 0.7,
      contentMode: "story",
    },
    maxDuration: 90,
    minDuration: 15,
  },

  "instagram-story": {
    id: "instagram-story",
    name: "Instagram Story",
    description: "Full-screen vertical format for Instagram Stories",
    platform: "Instagram Stories",
    aspectRatio: "9:16",
    orientation: "portrait",
    fps: 30,
    quality: "standard",
    config: {
      orientation: "portrait",
      useModernEffects: true,
      transitionDuration: 0.5,
      contentMode: "story",
    },
    maxDuration: 15,
  },

  "twitter": {
    id: "twitter",
    name: "Twitter/X",
    description: "Landscape video for Twitter/X posts",
    platform: "Twitter/X",
    aspectRatio: "16:9",
    orientation: "landscape",
    fps: 30,
    quality: "standard",
    config: {
      orientation: "landscape",
      useModernEffects: true,
      transitionDuration: 1.0,
      contentMode: "story",
    },
    maxDuration: 140,
  },

  "linkedin": {
    id: "linkedin",
    name: "LinkedIn",
    description: "Professional video format for LinkedIn",
    platform: "LinkedIn",
    aspectRatio: "16:9",
    orientation: "landscape",
    fps: 24,
    quality: "high",
    config: {
      orientation: "landscape",
      useModernEffects: true,
      transitionDuration: 1.5,
      contentMode: "story",
    },
    maxDuration: 600,
    minDuration: 30,
  },

  "draft-preview": {
    id: "draft-preview",
    name: "Draft Preview",
    description: "Fast, low-quality preview for quick iterations",
    platform: "Preview",
    aspectRatio: "16:9",
    orientation: "landscape",
    fps: 15,
    quality: "draft",
    config: {
      orientation: "landscape",
      useModernEffects: false,
      transitionDuration: 0.3,
      contentMode: "story",
    },
  },

  "high-quality": {
    id: "high-quality",
    name: "High Quality",
    description: "Maximum quality for archival or professional use",
    platform: "General",
    aspectRatio: "16:9",
    orientation: "landscape",
    fps: 30,
    quality: "high",
    config: {
      orientation: "landscape",
      useModernEffects: true,
      transitionDuration: 2.0,
      contentMode: "story",
    },
  },

  "podcast-video": {
    id: "podcast-video",
    name: "Podcast Video",
    description: "Long-form content optimized for podcast clips",
    platform: "Podcast",
    aspectRatio: "16:9",
    orientation: "landscape",
    fps: 24,
    quality: "standard",
    config: {
      orientation: "landscape",
      useModernEffects: false,
      transitionDuration: 1.0,
      contentMode: "story",
    },
    minDuration: 120,
  },
};

/**
 * Get a preset by ID
 */
export function getExportPreset(id: ExportPresetId): ExportPreset {
  return EXPORT_PRESETS[id];
}

/**
 * Get all presets for a specific platform
 */
export function getPresetsForPlatform(platform: string): ExportPreset[] {
  return Object.values(EXPORT_PRESETS).filter(
    (preset) => preset.platform.toLowerCase().includes(platform.toLowerCase())
  );
}

/**
 * Get all presets matching an aspect ratio
 */
export function getPresetsForAspectRatio(
  aspectRatio: "16:9" | "9:16" | "1:1" | "4:5"
): ExportPreset[] {
  return Object.values(EXPORT_PRESETS).filter(
    (preset) => preset.aspectRatio === aspectRatio
  );
}

/**
 * Get the recommended preset based on duration and orientation
 */
export function getRecommendedPreset(
  duration: number,
  orientation: "landscape" | "portrait" = "landscape"
): ExportPreset {
  const presets = Object.values(EXPORT_PRESETS).filter(
    (p) => p.orientation === orientation
  );

  // Find presets that match the duration range
  const matchingPresets = presets.filter((p) => {
    const minOk = !p.minDuration || duration >= p.minDuration;
    const maxOk = !p.maxDuration || duration <= p.maxDuration;
    return minOk && maxOk;
  });

  // Return the first matching preset, or default to youtube-landscape/shorts
  if (matchingPresets.length > 0) {
    // Prefer high quality presets
    const highQuality = matchingPresets.find((p) => p.quality === "high");
    return (highQuality || matchingPresets[0])!;
  }

  return orientation === "portrait"
    ? EXPORT_PRESETS["youtube-shorts"]
    : EXPORT_PRESETS["youtube-landscape"];
}

/**
 * Get all preset IDs
 */
export function getAllPresetIds(): ExportPresetId[] {
  return Object.keys(EXPORT_PRESETS) as ExportPresetId[];
}

/**
 * Get preset summary for display
 */
export function getPresetSummary(preset: ExportPreset): string {
  return `${preset.name} (${preset.aspectRatio}, ${preset.quality} quality)`;
}
````

## File: packages/shared/src/services/ffmpeg/formatAssembly.ts
````typescript
/**
 * Format-Specific Assembly Helpers
 *
 * Pure functions for building format-aware assembly configurations.
 * Used by the export pipeline to apply format-specific rules.
 *
 * Requirements: 4.6, 5.6, 8.6, 15.1–15.5
 */

import type {
  VideoFormat,
  FormatAssemblyRules,
  CTAMarker,
  ChapterMarker,
  BeatEvent,
  BeatMetadata,
  TransitionType,
  ScreenplayScene,
  TimelineClip,
} from '../../types';
import { formatRegistry } from '../formatRegistry';

// ============================================================================
// Assembly Rule Builder (Task 10.1)
// ============================================================================

/**
 * Build format-specific assembly rules from format metadata.
 * Returns a rules object that the export pipeline can apply.
 *
 * Requirements: 15.1
 *
 * @param formatId - Video format identifier
 * @param options  - Optional overrides (CTA text, chapter scenes, beat data)
 */
export function buildAssemblyRules(
  formatId: VideoFormat,
  options: {
    totalDuration?: number;
    ctaText?: string;
    scenes?: ScreenplayScene[];
    sceneDurations?: number[];
    beatMetadata?: BeatMetadata;
  } = {}
): FormatAssemblyRules {
  const meta = formatRegistry.getFormat(formatId);
  const aspectRatio = meta?.aspectRatio ?? '16:9';

  const base: FormatAssemblyRules = {
    formatId,
    aspectRatio,
    defaultTransition: getDefaultTransition(formatId),
    transitionDuration: getDefaultTransitionDuration(formatId),
  };

  // Advertisement: CTA emphasis (Task 10.2)
  if (formatId === 'advertisement' && options.totalDuration) {
    base.ctaMarker = buildCTAMarker(
      options.ctaText ?? 'Learn More',
      options.totalDuration
    );
  }

  // Documentary: chapter organization (Task 10.4)
  if (formatId === 'documentary' && options.scenes && options.sceneDurations) {
    base.chapters = buildChapterMarkers(options.scenes, options.sceneDurations);
    base.useChapterStructure = true;
  }

  // Music Video: beat synchronization (Task 10.6)
  if (formatId === 'music-video' && options.beatMetadata) {
    base.beatMetadata = options.beatMetadata;
    base.useBeatSync = true;
  }

  return base;
}

// ============================================================================
// Default Transition Helpers
// ============================================================================

/**
 * Get the default transition type for a format.
 */
function getDefaultTransition(formatId: VideoFormat): TransitionType {
  switch (formatId) {
    case 'advertisement':
    case 'shorts':
      return 'none'; // Hard cuts for fast-paced content
    case 'documentary':
    case 'youtube-narrator':
      return 'dissolve'; // Smooth transitions for long-form
    case 'music-video':
      return 'fade'; // Beat-synced fades
    case 'news-politics':
      return 'slide'; // Professional slide transitions
    default:
      return 'dissolve';
  }
}

/**
 * Get the default transition duration in seconds for a format.
 */
function getDefaultTransitionDuration(formatId: VideoFormat): number {
  switch (formatId) {
    case 'advertisement':
    case 'shorts':
      return 0.3;
    case 'documentary':
      return 1.5;
    case 'music-video':
      return 0.5;
    default:
      return 1.0;
  }
}

// ============================================================================
// CTA Emphasis (Task 10.2) — Requirements: 4.6, 15.2
// ============================================================================

/**
 * Build a CTA marker that positions the call-to-action in the final 5 seconds.
 *
 * Requirements: 4.6, 15.2
 *
 * @param ctaText       - CTA text content
 * @param totalDuration - Total video duration in seconds
 * @param ctaDuration   - CTA display duration in seconds (default: 5)
 * @returns CTAMarker positioned at the end of the video
 */
export function buildCTAMarker(
  ctaText: string,
  totalDuration: number,
  ctaDuration: number = 5
): CTAMarker {
  // Clamp CTA duration to available time
  const effectiveDuration = Math.min(ctaDuration, totalDuration);
  const startTime = Math.max(0, totalDuration - effectiveDuration);

  return {
    text: ctaText,
    startTime,
    duration: effectiveDuration,
  };
}

/**
 * Validate that a CTA marker is correctly positioned in the final 5 seconds.
 *
 * @param marker        - CTA marker to validate
 * @param totalDuration - Total video duration in seconds
 * @returns Whether the CTA is within the final 5 seconds
 */
export function validateCTAPosition(
  marker: CTAMarker,
  totalDuration: number
): boolean {
  const ctaEnd = marker.startTime + marker.duration;
  const finalFiveStart = Math.max(0, totalDuration - 5);

  return (
    marker.startTime >= finalFiveStart &&
    ctaEnd <= totalDuration + 0.001 // small tolerance for float precision
  );
}

// ============================================================================
// Chapter Organization (Task 10.4) — Requirements: 5.6, 15.4
// ============================================================================

/**
 * Build chapter markers from screenplay scenes and their durations.
 *
 * Requirements: 5.6, 15.4
 *
 * @param scenes         - Screenplay scenes (each becomes a chapter)
 * @param sceneDurations - Duration of each scene in seconds (must match scenes length)
 * @returns Array of ChapterMarker objects with correct time positions
 */
export function buildChapterMarkers(
  scenes: ScreenplayScene[],
  sceneDurations: number[]
): ChapterMarker[] {
  const chapters: ChapterMarker[] = [];
  let currentTime = 0;

  for (let i = 0; i < scenes.length; i++) {
    const scene = scenes[i];
    const duration = sceneDurations[i] ?? 0;

    if (scene && duration > 0) {
      chapters.push({
        id: `chapter_${i}`,
        title: scene.heading || `Chapter ${i + 1}`,
        startTime: currentTime,
        endTime: currentTime + duration,
      });
    }

    currentTime += duration;
  }

  return chapters;
}

/**
 * Validate that chapters are contiguous and non-overlapping.
 *
 * @param chapters - Chapter markers to validate
 * @returns Whether chapters form a valid sequence
 */
export function validateChapterSequence(chapters: ChapterMarker[]): boolean {
  if (chapters.length === 0) return true;

  for (let i = 0; i < chapters.length; i++) {
    const chapter = chapters[i]!;
    // Each chapter must have positive duration
    if (chapter.endTime <= chapter.startTime) return false;

    // Chapters must not overlap (allow small tolerance for float precision)
    if (i > 0) {
      const prev = chapters[i - 1]!;
      if (chapter.startTime < prev.endTime - 0.001) return false;
    }
  }

  return true;
}

// ============================================================================
// Beat Synchronization (Task 10.6) — Requirements: 8.6, 15.3
// ============================================================================

/**
 * Generate evenly-spaced beat metadata from BPM and duration.
 * Used when no real beat detection is available.
 *
 * @param bpm             - Beats per minute
 * @param durationSeconds - Total track duration in seconds
 * @returns BeatMetadata with generated beat events
 */
export function generateBeatMetadata(
  bpm: number,
  durationSeconds: number
): BeatMetadata {
  const beatInterval = 60 / bpm; // seconds between beats
  const beats: BeatEvent[] = [];
  let time = 0;
  let beatIndex = 0;

  while (time < durationSeconds) {
    beats.push({
      timestamp: Math.round(time * 1000) / 1000, // round to ms precision
      intensity: beatIndex % 4 === 0 ? 1.0 : beatIndex % 2 === 0 ? 0.6 : 0.3,
    });
    time += beatInterval;
    beatIndex++;
  }

  return { bpm, durationSeconds, beats };
}

/**
 * Find the nearest beat event to a given timestamp.
 * Returns the beat and the offset (in ms) from the target time.
 *
 * Requirements: 8.6 (100ms tolerance)
 *
 * @param beats     - Array of beat events
 * @param timestamp - Target timestamp in seconds
 * @returns { beat, offsetMs } or null if no beats exist
 */
export function findNearestBeat(
  beats: BeatEvent[],
  timestamp: number
): { beat: BeatEvent; offsetMs: number } | null {
  if (beats.length === 0) return null;

  let closest = beats[0]!;
  let minOffset = Math.abs(timestamp - closest.timestamp) * 1000;

  for (let i = 1; i < beats.length; i++) {
    const beat = beats[i]!;
    const offset = Math.abs(timestamp - beat.timestamp) * 1000;
    if (offset < minOffset) {
      closest = beat;
      minOffset = offset;
    }
  }

  return { beat: closest, offsetMs: minOffset };
}

/**
 * Snap a timestamp to the nearest beat within a tolerance (default 100ms).
 * If no beat is close enough, returns the original timestamp.
 *
 * Requirements: 8.6, 15.3 (100ms tolerance)
 *
 * @param beats       - Array of beat events
 * @param timestamp   - Target timestamp in seconds
 * @param toleranceMs - Maximum offset in ms to snap (default: 100)
 * @returns Snapped timestamp in seconds
 */
export function snapToBeat(
  beats: BeatEvent[],
  timestamp: number,
  toleranceMs: number = 100
): number {
  const nearest = findNearestBeat(beats, timestamp);
  if (!nearest) return timestamp;

  return nearest.offsetMs <= toleranceMs ? nearest.beat.timestamp : timestamp;
}

/**
 * Align visual transition timestamps to beat positions.
 * Returns new timestamps for each transition, snapped to the nearest beat.
 *
 * Requirements: 8.6, 15.3
 *
 * @param transitionTimes - Original transition timestamps in seconds
 * @param beats           - Beat events to sync to
 * @param toleranceMs     - Maximum snap tolerance in ms (default: 100)
 * @returns Array of aligned timestamps in seconds
 */
export function alignTransitionsToBeat(
  transitionTimes: number[],
  beats: BeatEvent[],
  toleranceMs: number = 100
): number[] {
  return transitionTimes.map(t => snapToBeat(beats, t, toleranceMs));
}

// ============================================================================
// Graceful Degradation (Task 10.7) — Requirements: 15.5
// ============================================================================

/**
 * Result of attempting to assemble a video with potentially missing assets.
 */
export interface AssemblyResult {
  /** Whether the assembly produced a usable output */
  success: boolean;
  /** Whether some assets were missing (partial output) */
  partial: boolean;
  /** Timeline clips that were successfully assembled */
  assembledClips: TimelineClip[];
  /** IDs of assets that were missing or failed to load */
  missingAssets: string[];
  /** Human-readable error messages for each failure */
  errors: string[];
}

/**
 * Assemble a timeline from clips, gracefully handling missing assets.
 * Missing assets are skipped; the timeline is built from available clips only.
 *
 * Requirements: 15.5
 *
 * @param clips          - All expected timeline clips
 * @param availableAssets - Set of asset IDs that are available
 * @returns AssemblyResult with partial output if some assets are missing
 */
export function assembleWithGracefulDegradation(
  clips: TimelineClip[],
  availableAssets: Set<string>
): AssemblyResult {
  const assembledClips: TimelineClip[] = [];
  const missingAssets: string[] = [];
  const errors: string[] = [];

  for (const clip of clips) {
    // Clips without asset URLs (e.g., transitions, text overlays) always succeed
    if (!clip.assetUrl) {
      assembledClips.push(clip);
      continue;
    }

    if (availableAssets.has(clip.id)) {
      assembledClips.push(clip);
    } else {
      missingAssets.push(clip.id);
      errors.push(`Missing asset for clip "${clip.id}" (${clip.type}, ${clip.startTime}s–${clip.endTime}s)`);
    }
  }

  return {
    success: assembledClips.length > 0,
    partial: missingAssets.length > 0 && assembledClips.length > 0,
    assembledClips,
    missingAssets,
    errors,
  };
}
````

## File: packages/shared/src/services/ffmpeg/frameRenderer.ts
````typescript
/**
 * Frame Renderer Module
 *
 * Main frame composition orchestrator that combines visuals, subtitles,
 * visualizer, and overlays into a single rendered frame.
 */

import { SongData } from "../../types";
import { LAYOUT_PRESETS } from "../../constants/layout";
import { isRTL, reshapeArabicText } from "../../lib/utils";
import { ExportConfig, RenderAsset } from "./exportConfig";
import { renderTextWithWipe } from "./textRenderer";
import { renderVisualizerLayer } from "./visualizer";
import { drawAsset, applyTransition } from "./transitions";

/**
 * Get zone bounds from normalized coordinates
 */
function getZoneBounds(
    zone: { x: number; y: number; width: number; height: number },
    canvasWidth: number,
    canvasHeight: number
): { x: number; y: number; width: number; height: number } {
    return {
        x: zone.x * canvasWidth,
        y: zone.y * canvasHeight,
        width: zone.width * canvasWidth,
        height: zone.height * canvasHeight,
    };
}

/**
 * Render a complete frame to canvas
 */
export async function renderFrameToCanvas(
    ctx: CanvasRenderingContext2D,
    width: number,
    height: number,
    currentTime: number,
    assets: RenderAsset[],
    subtitles: SongData["parsedSubtitles"],
    frequencyData: Uint8Array | null,
    previousFrequencyData: Uint8Array | null,
    config: ExportConfig
): Promise<void> {
    // 1. Background (Black)
    ctx.fillStyle = "#000";
    ctx.fillRect(0, 0, width, height);

    // Layout zones
    const layoutPreset =
        config.orientation === "portrait"
            ? LAYOUT_PRESETS.portrait
            : LAYOUT_PRESETS.landscape;

    const zones = {
        visualizer: getZoneBounds(layoutPreset!.zones.visualizer, width, height),
        text: getZoneBounds(layoutPreset!.zones.text, width, height),
        translation: getZoneBounds(layoutPreset!.zones.translation, width, height),
    };

    // 2. Visual Layer with Ken Burns & Transitions
    let currentIndex = 0;
    for (let i = 0; i < assets.length; i++) {
        const asset = assets[i];
        if (asset && currentTime >= asset.time) {
            currentIndex = i;
        } else {
            break;
        }
    }

    const currentAsset = assets[currentIndex];
    const nextAsset = assets[currentIndex + 1];

    // Calculate duration of current slide
    const slideStartTime = currentAsset?.time ?? 0;
    const slideEndTime = nextAsset ? nextAsset.time : slideStartTime + 30;
    const slideDuration = slideEndTime - slideStartTime;
    const slideProgress = (currentTime - slideStartTime) / slideDuration;

    if (currentAsset?.element) {
        const TRANSITION_DURATION = config.transitionDuration || 1.5;
        const timeUntilNext = nextAsset ? nextAsset.time - currentTime : Infinity;
        const isTransitioning = timeUntilNext < TRANSITION_DURATION && nextAsset;

        if (config.transitionType === "none" || !isTransitioning) {
            const useKenBurns = config.useModernEffects && config.transitionType !== "none";
            await drawAsset(ctx, width, height, currentAsset, currentTime, useKenBurns ? slideProgress : 0, 1, config.useModernEffects);
        } else {
            const t = 1 - timeUntilNext / TRANSITION_DURATION;
            await applyTransition({
                ctx,
                width,
                height,
                currentTime,
                currentAsset,
                nextAsset,
                slideProgress,
                transitionProgress: t,
                config,
            });
        }
    }

    // 3. Visualizer Layer
    if (frequencyData && config.contentMode === "music" && config.visualizerConfig?.enabled) {
        renderVisualizerLayer(
            ctx,
            width,
            height,
            frequencyData,
            previousFrequencyData,
            config.visualizerConfig,
            config.useModernEffects,
            { top: zones.visualizer.y, height: zones.visualizer.height }
        );
    }

    // 4. Gradient Overlay
    if (config.useModernEffects) {
        const overlayGradient = ctx.createLinearGradient(0, height, 0, 0);
        overlayGradient.addColorStop(0, "rgba(2, 6, 23, 0.95)");
        overlayGradient.addColorStop(0.3, "rgba(2, 6, 23, 0.6)");
        overlayGradient.addColorStop(0.7, "rgba(2, 6, 23, 0.2)");
        overlayGradient.addColorStop(1, "rgba(2, 6, 23, 0.4)");
        ctx.fillStyle = overlayGradient;
        ctx.fillRect(0, 0, width, height);
    } else {
        const overlayGradient = ctx.createLinearGradient(0, height, 0, height / 2);
        overlayGradient.addColorStop(0, "rgba(15, 23, 42, 0.9)");
        overlayGradient.addColorStop(1, "rgba(15, 23, 42, 0.0)");
        ctx.fillStyle = overlayGradient;
        ctx.fillRect(0, 0, width, height);
    }

    // 5. Subtitles
    const adjustedTime = currentTime + config.syncOffsetMs / 1000;
    const activeSub = subtitles.find(
        (s) => adjustedTime >= s.startTime && adjustedTime <= s.endTime
    );

    let subtitleOpacity = 1.0;
    if (config.fadeOutBeforeCut) {
        const fadeOutDuration = 0.3;
        const timeUntilCut = slideEndTime - currentTime;
        if (timeUntilCut < fadeOutDuration && timeUntilCut > 0) {
            subtitleOpacity = timeUntilCut / fadeOutDuration;
        }
    }

    if (activeSub && subtitleOpacity > 0) {
        renderSubtitles(ctx, width, height, activeSub, adjustedTime, subtitleOpacity, zones, config);
    }
}

/**
 * Analyze frame brightness to find safe zone for text placement.
 * Returns vertical offset adjustment based on brightness in lower third.
 * 
 * Algorithm:
 * 1. Sample pixels in the lower third (where subtitles typically go)
 * 2. Divide into horizontal bands
 * 3. Find the darkest band for optimal text visibility
 * 
 * @param ctx - Canvas context
 * @param width - Canvas width
 * @param height - Canvas height
 * @returns Vertical offset adjustment (negative = move up, positive = move down)
 */
function _analyzeSafeTextZone(
    ctx: CanvasRenderingContext2D,
    width: number,
    height: number
): { verticalOffset: number; optimalY: number; brightness: number } {
    const lowerThirdStart = Math.floor(height * 0.65);
    const sampleHeight = Math.floor(height * 0.30);

    // Get image data for lower third
    let imageData: ImageData;
    try {
        imageData = ctx.getImageData(0, lowerThirdStart, width, sampleHeight);
    } catch (_e) {
        // Fallback if getImageData fails (e.g., cross-origin)
        return { verticalOffset: 0, optimalY: height * 0.85, brightness: 0.5 };
    }

    const data = imageData.data;
    const numBands = 5; // Divide lower third into 5 horizontal bands
    const bandHeight = Math.floor(sampleHeight / numBands);
    const bandBrightness: number[] = [];

    // Calculate average brightness for each band
    for (let band = 0; band < numBands; band++) {
        const bandStart = band * bandHeight;
        const bandEnd = bandStart + bandHeight;
        let totalBrightness = 0;
        let pixelCount = 0;

        // Sample every 4th column for performance
        for (let y = bandStart; y < bandEnd; y += 2) {
            for (let x = 0; x < width; x += 4) {
                const idx = (y * width + x) * 4;
                // Luminance formula: 0.299*R + 0.587*G + 0.114*B
                const r = data[idx] || 0;
                const g = data[idx + 1] || 0;
                const b = data[idx + 2] || 0;
                const luminance = 0.299 * r + 0.587 * g + 0.114 * b;
                totalBrightness += luminance / 255;
                pixelCount++;
            }
        }

        bandBrightness.push(pixelCount > 0 ? totalBrightness / pixelCount : 0.5);
    }

    // Find the darkest band (best for white text)
    let darkestBand = 0;
    let minBrightness = bandBrightness[0] ?? 0.5;

    for (let i = 1; i < bandBrightness.length; i++) {
        const brightness = bandBrightness[i] ?? 0.5;
        if (brightness < minBrightness) {
            minBrightness = brightness;
            darkestBand = i;
        }
    }

    // Calculate optimal Y position
    const optimalY = lowerThirdStart + (darkestBand + 0.5) * bandHeight;
    const defaultY = height * 0.85;

    // Limit vertical adjustment to prevent subtitles from going too high or low
    const maxOffset = height * 0.1;
    const rawOffset = optimalY - defaultY;
    const clampedOffset = Math.max(-maxOffset, Math.min(maxOffset, rawOffset));

    return {
        verticalOffset: clampedOffset,
        optimalY: defaultY + clampedOffset,
        brightness: minBrightness,
    };
}

/**
 * Render subtitles with word-level karaoke highlighting
 */
function renderSubtitles(
    ctx: CanvasRenderingContext2D,
    width: number,
    height: number,
    activeSub: SongData["parsedSubtitles"][0],
    adjustedTime: number,
    subtitleOpacity: number,
    zones: { text: { x: number; y: number; width: number; height: number }; translation: { x: number; y: number; width: number; height: number } },
    config: ExportConfig
): void {
    ctx.save();
    ctx.globalAlpha = subtitleOpacity;

    const totalDuration = activeSub.endTime - activeSub.startTime;
    const lineProgress = Math.max(0, Math.min(1, (adjustedTime - activeSub.startTime) / totalDuration));

    // Story mode: smaller font, outline-only style (Issue 7)
    const isStory = config.contentMode === "story";
    const fontSize = isStory
        ? (config.orientation === "portrait" ? 22 : 28)
        : (config.orientation === "portrait" ? 36 : 42);
    const fontWeight = isStory ? "500" : (config.useModernEffects ? "600" : "bold");
    ctx.font = `${fontWeight} ${fontSize}px "Inter", "Segoe UI", "Arial", sans-serif`;
    ctx.textAlign = "left";
    ctx.textBaseline = "middle";

    const isTextRTL = isRTL(activeSub.text);
    const hasWordTiming = config.wordLevelHighlight && activeSub.words && activeSub.words.length > 0;

    const displayWords = hasWordTiming
        ? activeSub.words!.map((w) => isTextRTL ? reshapeArabicText(w.word) : w.word)
        : (isTextRTL ? reshapeArabicText(activeSub.text) : activeSub.text).split(" ");

    // Text wrapping
    const maxWidth = zones.text.width - (config.orientation === "portrait" ? 80 : 140);
    const wrappedLines: { words: string[]; wordIndices: number[] }[] = [];
    let currentLine: string[] = [];
    let currentLineIndices: number[] = [];
    let currentLineWidth = 0;

    displayWords.forEach((word, idx) => {
        const wordWidth = ctx.measureText(word + " ").width;
        if (currentLineWidth + wordWidth > maxWidth && currentLine.length > 0) {
            wrappedLines.push({ words: currentLine, wordIndices: currentLineIndices });
            currentLine = [word];
            currentLineIndices = [idx];
            currentLineWidth = wordWidth;
        } else {
            currentLine.push(word);
            currentLineIndices.push(idx);
            currentLineWidth += wordWidth;
        }
    });
    if (currentLine.length > 0) {
        wrappedLines.push({ words: currentLine, wordIndices: currentLineIndices });
    }

    // Story mode: enforce 2-line max, truncate with ellipsis (Issue 7)
    if (isStory && wrappedLines.length > 2) {
        wrappedLines.length = 2;
        // Add ellipsis to last word of second line
        const lastLine = wrappedLines[1]!;
        if (lastLine.words.length > 0) {
            lastLine.words[lastLine.words.length - 1] += "…";
        }
    }

    const lineHeight = fontSize * 1.3;
    const totalTextHeight = wrappedLines.length * lineHeight;

    // Story mode: bottom safe zone at 82% (Issue 7), regular mode uses zone center
    const baseY = isStory
        ? height * 0.82 - totalTextHeight / 2
        : zones.text.y + zones.text.height / 2 - totalTextHeight / 2;

    // Background: outline + shadow for story mode (Netflix-style), opaque box for music mode
    if (!isStory) {
        const bgPadding = { x: 20, y: 10 };
        const maxLineWidth = Math.max(...wrappedLines.map(line => ctx.measureText(line.words.join(" ")).width));
        const bgWidth = maxLineWidth + bgPadding.x * 2;
        const bgHeight = totalTextHeight + bgPadding.y * 2;
        const bgX = zones.text.x + (zones.text.width - bgWidth) / 2;
        const bgY2 = baseY - bgPadding.y;

        ctx.save();
        ctx.fillStyle = "rgba(0, 0, 0, 0.70)";
        ctx.beginPath();
        const radius = 8;
        ctx.moveTo(bgX + radius, bgY2);
        ctx.lineTo(bgX + bgWidth - radius, bgY2);
        ctx.quadraticCurveTo(bgX + bgWidth, bgY2, bgX + bgWidth, bgY2 + radius);
        ctx.lineTo(bgX + bgWidth, bgY2 + bgHeight - radius);
        ctx.quadraticCurveTo(bgX + bgWidth, bgY2 + bgHeight, bgX + bgWidth - radius, bgY2 + bgHeight);
        ctx.lineTo(bgX + radius, bgY2 + bgHeight);
        ctx.quadraticCurveTo(bgX, bgY2 + bgHeight, bgX, bgY2 + bgHeight - radius);
        ctx.lineTo(bgX, bgY2 + radius);
        ctx.quadraticCurveTo(bgX, bgY2, bgX + radius, bgY2);
        ctx.closePath();
        ctx.fill();
        ctx.restore();
    }

    // Render words
    wrappedLines.forEach((lineData, lineIdx) => {
        const yPos = baseY + lineIdx * lineHeight;
        const lineText = lineData.words.join(" ");
        const lineWidth = ctx.measureText(lineText).width;

        let xPos = isTextRTL
            ? zones.text.x + (zones.text.width + lineWidth) / 2
            : zones.text.x + (zones.text.width - lineWidth) / 2;

        lineData.words.forEach((word, wordIdx) => {
            const globalWordIdx = lineData.wordIndices[wordIdx];
            if (globalWordIdx === undefined) return;

            const wordWidth = ctx.measureText(word).width;
            const spaceWidth = ctx.measureText(" ").width;

            if (isTextRTL) {
                xPos -= wordWidth;
            }

            // Calculate word progress
            let wordProgress = 0;
            let isActiveWord = false;
            let wordDuration = 0;

            if (hasWordTiming && activeSub.words![globalWordIdx]) {
                const wordTiming = activeSub.words![globalWordIdx];
                const wordStart = wordTiming.startTime;
                const wordEnd = wordTiming.endTime;
                wordDuration = wordEnd - wordStart;

                if (adjustedTime >= wordEnd) {
                    wordProgress = 1;
                } else if (adjustedTime >= wordStart) {
                    const revealDuration = config.textAnimationConfig?.revealDuration ?? (wordEnd - wordStart);
                    const revealWindow = Math.max(0.05, Math.min(revealDuration, wordEnd - wordStart));
                    wordProgress = (adjustedTime - wordStart) / revealWindow;
                    isActiveWord = true;
                }
            } else {
                // Fallback progress calculation
                const originalWords = hasWordTiming ? activeSub.words!.map((w) => w.word) : activeSub.text.split(" ");
                const originalFullText = originalWords.join(" ");
                const charsBefore = originalWords.slice(0, globalWordIdx).join(" ").length;
                const totalChars = originalFullText.length;
                const wordStartProgress = charsBefore / totalChars;
                const wordEndProgress = (charsBefore + word.length) / totalChars;

                if (lineProgress >= wordEndProgress) {
                    wordProgress = 1;
                } else if (lineProgress >= wordStartProgress) {
                    wordProgress = (lineProgress - wordStartProgress) / (wordEndProgress - wordStartProgress);
                    isActiveWord = true;
                }
            }

            // Render word
            if (isStory) {
                // Netflix-style: outline + shadow, no background box (Issue 7)
                renderStoryWord(ctx, word, xPos, yPos);
            } else if (config.textAnimationConfig) {
                renderTextWithWipe(
                    ctx,
                    word,
                    xPos,
                    yPos,
                    fontSize,
                    wordProgress,
                    config.textAnimationConfig.revealDirection,
                    isTextRTL
                );
            } else if (config.useModernEffects) {
                renderModernWord(ctx, word, xPos, yPos, wordWidth, wordProgress, isActiveWord, wordDuration, isTextRTL);
            } else {
                renderSimpleWord(ctx, word, xPos, yPos, wordProgress);
            }

            // Update position
            if (isTextRTL) {
                xPos -= spaceWidth;
            } else {
                xPos += wordWidth + spaceWidth;
            }
        });
    });

    // Translation
    if (activeSub.translation) {
        renderTranslation(ctx, width, activeSub.translation, zones.translation, config);
    }

    ctx.restore();
}

/**
 * Render word with modern glow effects
 */
function renderModernWord(
    ctx: CanvasRenderingContext2D,
    word: string,
    xPos: number,
    yPos: number,
    wordWidth: number,
    wordProgress: number,
    isActiveWord: boolean,
    wordDuration: number,
    isTextRTL: boolean
): void {
    ctx.save();

    let emphasisScale = 1.0;
    let emphasisGlow = false;
    if (isActiveWord && wordDuration > 0.5) {
        emphasisScale = 1.0 + wordProgress * 0.06;
        emphasisGlow = true;
    }

    ctx.shadowColor = "rgba(0, 0, 0, 0.95)";
    ctx.shadowBlur = 16;
    ctx.shadowOffsetX = 3;
    ctx.shadowOffsetY = 4;
    ctx.strokeStyle = "rgba(0, 0, 0, 0.9)";
    ctx.lineWidth = 6;
    ctx.lineJoin = "round";
    ctx.textAlign = "left";
    ctx.strokeText(word, xPos, yPos);
    ctx.fillStyle = "rgba(255, 255, 255, 0.5)";
    ctx.fillText(word, xPos, yPos);

    if (wordProgress > 0) {
        ctx.save();
        if (emphasisScale > 1) {
            ctx.translate(xPos + wordWidth / 2, yPos);
            ctx.scale(emphasisScale, emphasisScale);
            ctx.translate(-(xPos + wordWidth / 2), -yPos);
        }

        let gradient: CanvasGradient;
        if (isTextRTL) {
            gradient = ctx.createLinearGradient(xPos + wordWidth, 0, xPos, 0);
        } else {
            gradient = ctx.createLinearGradient(xPos, 0, xPos + wordWidth, 0);
        }
        gradient.addColorStop(0, "#ffffff");
        gradient.addColorStop(Math.max(0, wordProgress - 0.05), "#ffffff");
        gradient.addColorStop(Math.min(1, wordProgress + 0.05), "rgba(255,255,255,0)");
        gradient.addColorStop(1, "rgba(255,255,255,0)");
        ctx.fillStyle = gradient;

        if (isActiveWord || emphasisGlow) {
            ctx.shadowColor = "rgba(255, 215, 100, 0.9)";
            ctx.shadowBlur = emphasisGlow ? 30 : 20;
        }

        ctx.fillText(word, xPos, yPos);
        ctx.restore();
    }

    ctx.restore();
}

/**
 * Render word with simple styling
 */
function renderSimpleWord(
    ctx: CanvasRenderingContext2D,
    word: string,
    xPos: number,
    yPos: number,
    wordProgress: number
): void {
    ctx.save();
    ctx.shadowColor = "rgba(0, 0, 0, 0.8)";
    ctx.shadowBlur = 8;
    ctx.shadowOffsetY = 2;
    ctx.strokeStyle = "rgba(0, 0, 0, 0.5)";
    ctx.lineWidth = 3;
    ctx.lineJoin = "round";
    ctx.textAlign = "left";
    ctx.strokeText(word, xPos, yPos);

    if (wordProgress >= 1) {
        ctx.fillStyle = "#ffffff";
    } else if (wordProgress > 0) {
        ctx.fillStyle = "#ffd700";
    } else {
        ctx.fillStyle = "rgba(255, 255, 255, 0.7)";
    }
    ctx.fillText(word, xPos, yPos);
    ctx.restore();
}

/**
 * Render word with Netflix-style outline + shadow for story mode (Issue 7).
 * No background box — just clean white text with dark outline and soft shadow.
 */
function renderStoryWord(
    ctx: CanvasRenderingContext2D,
    word: string,
    xPos: number,
    yPos: number,
): void {
    ctx.save();
    ctx.textAlign = "left";

    // Dark outline for readability on any background
    ctx.strokeStyle = "rgba(0, 0, 0, 0.9)";
    ctx.lineWidth = 6;
    ctx.lineJoin = "round";
    ctx.strokeText(word, xPos, yPos);

    // Soft shadow
    ctx.shadowColor = "rgba(0, 0, 0, 0.5)";
    ctx.shadowBlur = 10;
    ctx.shadowOffsetX = 0;
    ctx.shadowOffsetY = 2;

    // White fill
    ctx.fillStyle = "#ffffff";
    ctx.fillText(word, xPos, yPos);

    ctx.restore();
}

/**
 * Render translation text
 */
function renderTranslation(
    ctx: CanvasRenderingContext2D,
    width: number,
    translation: string,
    zone: { x: number; y: number; width: number; height: number },
    config: ExportConfig
): void {
    const transY = zone.y + zone.height / 2;
    ctx.textAlign = "center";

    if (config.useModernEffects) {
        const transFontSize = config.orientation === "portrait" ? 36 : 42;
        ctx.font = `500 ${transFontSize}px "Inter", "Segoe UI", "Arial", sans-serif`;
        const transWidth = ctx.measureText(translation).width;
        const padding = 32;

        ctx.shadowBlur = 0;
        ctx.fillStyle = "rgba(0, 0, 0, 0.5)";
        ctx.beginPath();
        (ctx as any).roundRect(
            width / 2 - transWidth / 2 - padding,
            transY - transFontSize / 2 - 8,
            transWidth + padding * 2,
            transFontSize + 16,
            24
        );
        ctx.fill();

        ctx.shadowColor = "rgba(0, 0, 0, 0.5)";
        ctx.shadowBlur = 4;
        ctx.fillStyle = "#ffffff";
        ctx.fillText(translation, width / 2, transY + 2);
    } else {
        const transFontSize = config.orientation === "portrait" ? 32 : 38;
        ctx.font = `italic ${transFontSize}px "Inter", "Arial", sans-serif`;
        ctx.shadowColor = "rgba(0, 0, 0, 0.9)";
        ctx.shadowBlur = 8;
        ctx.shadowOffsetY = 2;
        ctx.strokeStyle = "rgba(0, 0, 0, 0.6)";
        ctx.lineWidth = 3;
        ctx.strokeText(translation, width / 2, transY);
        ctx.fillStyle = "rgba(255, 255, 255, 0.9)";
        ctx.fillText(translation, width / 2, transY);
        ctx.shadowBlur = 0;
    }
}
````

## File: packages/shared/src/services/ffmpeg/index.ts
````typescript
/**
 * FFmpeg Module Index
 *
 * Re-exports all FFmpeg-related modules for convenient imports.
 */

// Types and configuration
export {
    SERVER_URL,
    getServerUrl,
    DEFAULT_EXPORT_CONFIG,
    mergeExportConfig,
    type ExportConfig,
    type ExportProgress,
    type ProgressCallback,
    type RenderAsset,
} from "./exportConfig";

// Platform utilities
export {
    isClientSideExportAvailable,
    getDefaultExportEngine,
} from "./envUtils";

// Export functions (main API)
export {
    exportVideoWithFFmpeg,
    exportVideoClientSide,
} from "./exporters";

// Rendering components (for advanced usage)
export { renderFrameToCanvas } from "./frameRenderer";
export { renderVisualizerLayer } from "./visualizer";
export { renderTextWithWipe, calculateWordRevealProgress } from "./textRenderer";
export { drawAsset, applyTransition } from "./transitions";

// Asset loading (enhanced with video support)
export {
    preloadAssets,
    loadImageAsset,
    loadVideoAsset,
    loadVideoAssetWithMetadata,
    seekVideoToTime,
    getVideoFrameAtTime,
    getCachedFrame,
    cacheFrame,
    clearFrameCache,
    getFrameCacheStats,
    createPlaceholderImage,
    type VideoAssetResult,
    type AssetLoadProgress,
} from "./assetLoader";

// Video audio extraction (for Veo native audio)
export {
    extractAudioFromVideo,
    extractAudioFromVideos,
    mixVideoAudioWithNarration,
    type ExtractedVideoAudio,
    type VideoAudioExtractionResult,
} from "./videoAudioExtractor";

// Export presets for different platforms
export {
    EXPORT_PRESETS,
    getExportPreset,
    getPresetsForPlatform,
    getPresetsForAspectRatio,
    getRecommendedPreset,
    getAllPresetIds,
    getPresetSummary,
    type ExportPresetId,
    type ExportPreset,
} from "./exportPresets";

// SSE progress client for real-time updates
export {
    subscribeToJob,
    pollJobStatus,
    waitForJobCompletion,
    isSSESupported,
    type JobProgress,
} from "./sseClient";

// Checksum generator for frame validation
export {
    generateBlobChecksum,
    generateBatchChecksums,
    isChecksumSupported,
    verifyChecksum,
    createFrameManifest,
    type FrameChecksum,
} from "./checksumGenerator";

// Export options type
export type { ExportOptions, ExportResult } from "./exporters";
````

## File: packages/shared/src/services/ffmpeg/sseClient.ts
````typescript
/**
 * SSE Client for Export Progress
 *
 * Subscribes to Server-Sent Events for real-time export progress updates.
 */

import { SERVER_URL } from './exportConfig';

export interface JobProgress {
  jobId: string;
  status: 'pending' | 'uploading' | 'queued' | 'encoding' | 'complete' | 'failed';
  progress: number;
  message: string;
  currentFrame?: number;
  totalFrames?: number;
  encodingSpeed?: string;
  estimatedTimeRemaining?: number;
  error?: string;
}

export type ProgressCallback = (progress: JobProgress) => void;
export type ErrorCallback = (error: Error) => void;

/**
 * SSE connection state
 */
export interface SSEConnection {
  isConnected: boolean;
  reconnectAttempts: number;
  lastEventTime: number;
}

const MAX_RECONNECT_ATTEMPTS = 5;
const RECONNECT_DELAY_MS = 2000;

/**
 * Subscribe to job progress via SSE
 */
export function subscribeToJob(
  jobId: string,
  onProgress: ProgressCallback,
  onError?: ErrorCallback
): () => void {
  let eventSource: EventSource | null = null;
  let reconnectAttempts = 0;
  let isClosed = false;
  let reconnectTimeout: number | null = null;

  const connect = () => {
    if (isClosed) return;

    const url = `${SERVER_URL}/api/export/events/${jobId}`;
    console.log(`[SSE] Connecting to ${url}`);

    eventSource = new EventSource(url);

    eventSource.onopen = () => {
      console.log(`[SSE] Connected to job ${jobId}`);
      reconnectAttempts = 0;
    };

    eventSource.onmessage = (event) => {
      try {
        const progress = JSON.parse(event.data) as JobProgress;
        onProgress(progress);

        // Close connection on terminal states
        if (progress.status === 'complete' || progress.status === 'failed') {
          console.log(`[SSE] Job ${jobId} reached terminal state: ${progress.status}`);
          close();
        }
      } catch (error) {
        console.error('[SSE] Failed to parse message:', error);
      }
    };

    eventSource.onerror = (event) => {
      console.error(`[SSE] Connection error for job ${jobId}:`, event);

      // Close current connection
      eventSource?.close();
      eventSource = null;

      // Attempt reconnection
      if (!isClosed && reconnectAttempts < MAX_RECONNECT_ATTEMPTS) {
        reconnectAttempts++;
        console.log(
          `[SSE] Reconnecting (attempt ${reconnectAttempts}/${MAX_RECONNECT_ATTEMPTS})...`
        );
        reconnectTimeout = window.setTimeout(connect, RECONNECT_DELAY_MS);
      } else if (onError && reconnectAttempts >= MAX_RECONNECT_ATTEMPTS) {
        onError(new Error('Failed to maintain SSE connection'));
      }
    };
  };

  const close = () => {
    isClosed = true;
    if (reconnectTimeout) {
      clearTimeout(reconnectTimeout);
      reconnectTimeout = null;
    }
    if (eventSource) {
      eventSource.close();
      eventSource = null;
      console.log(`[SSE] Disconnected from job ${jobId}`);
    }
  };

  // Start connection
  connect();

  // Return cleanup function
  return close;
}

/**
 * Poll job status (fallback for environments without SSE)
 */
export async function pollJobStatus(
  jobId: string,
  onProgress: ProgressCallback,
  intervalMs: number = 1000
): Promise<() => void> {
  let isCancelled = false;
  let timeoutId: number | null = null;

  const poll = async () => {
    if (isCancelled) return;

    try {
      const response = await fetch(`${SERVER_URL}/api/export/status/${jobId}`);
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}`);
      }

      const progress = (await response.json()) as JobProgress;
      onProgress(progress);

      // Continue polling if not in terminal state
      if (progress.status !== 'complete' && progress.status !== 'failed') {
        timeoutId = window.setTimeout(poll, intervalMs);
      }
    } catch (error) {
      console.error('[Poll] Failed to get job status:', error);
      // Retry after delay
      if (!isCancelled) {
        timeoutId = window.setTimeout(poll, intervalMs * 2);
      }
    }
  };

  // Start polling
  poll();

  // Return cancel function
  return () => {
    isCancelled = true;
    if (timeoutId) {
      clearTimeout(timeoutId);
    }
  };
}

/**
 * Wait for job completion (returns final status)
 */
export function waitForJobCompletion(
  jobId: string,
  onProgress?: ProgressCallback
): Promise<JobProgress> {
  return new Promise((resolve, reject) => {
    let lastProgress: JobProgress | null = null;

    const unsubscribe = subscribeToJob(
      jobId,
      (progress) => {
        lastProgress = progress;
        onProgress?.(progress);

        if (progress.status === 'complete') {
          unsubscribe();
          resolve(progress);
        } else if (progress.status === 'failed') {
          unsubscribe();
          reject(new Error(progress.error || 'Export failed'));
        }
      },
      (error) => {
        unsubscribe();
        reject(error);
      }
    );

    // Timeout after 30 minutes
    setTimeout(() => {
      unsubscribe();
      if (lastProgress?.status !== 'complete' && lastProgress?.status !== 'failed') {
        reject(new Error('Export timed out'));
      }
    }, 30 * 60 * 1000);
  });
}

/**
 * Check if SSE is supported in the current environment
 */
export function isSSESupported(): boolean {
  return typeof EventSource !== 'undefined';
}
````

## File: packages/shared/src/services/ffmpeg/textRenderer.ts
````typescript
/**
 * Text Renderer Module
 *
 * Handles text/subtitle rendering with karaoke-style word reveal animations.
 * Supports RTL languages (Arabic, Hebrew) and multiple reveal directions.
 */

import { ExportConfig } from "./exportConfig";

/**
 * Calculate word reveal progress for wipe animation
 * Note: For RTL text, we no longer reverse indices - the rendering handles RTL positioning
 * 
 * Enhanced: Uses actual word timing data instead of hardcoded 0.3s duration
 * for precise sync with narration/TTS output.
 */
export function calculateWordRevealProgress(
    currentTime: number,
    subtitle: { words?: { startTime: number; endTime: number; word: string }[] },
    _isRTL: boolean // kept for API compatibility but no longer used for reversal
): number[] {
    if (!subtitle.words || subtitle.words.length === 0) {
        return [1]; // Full reveal for non-word-timed
    }

    const progress: number[] = [];

    subtitle.words.forEach((word, idx) => {
        const wordStart = word.startTime;
        const wordEnd = word.endTime;
        
        // Calculate actual duration from timestamp data for precise sync
        const actualDuration = Math.max(0.1, wordEnd - wordStart); // Min 100ms to avoid division issues

        // Use natural index - RTL positioning is handled in rendering
        if (currentTime < wordStart) {
            progress[idx] = 0;
        } else if (currentTime >= wordEnd) {
            progress[idx] = 1;
        } else {
            // Reveal based on actual word duration from timing data
            progress[idx] = Math.min(1, (currentTime - wordStart) / actualDuration);
        }
    });

    return progress;
}

/**
 * Render text with directional wipe animation
 * Draws both inactive (ghost) and active (revealed) text with professional styling
 */
export function renderTextWithWipe(
    ctx: CanvasRenderingContext2D,
    text: string,
    x: number,
    y: number,
    fontSize: number,
    progress: number, // 0-1 animation progress
    revealDirection: NonNullable<ExportConfig["textAnimationConfig"]>["revealDirection"],
    isTextRTL: boolean
): void {
    const textWidth = ctx.measureText(text).width;
    const clipHeight = fontSize * 1.6;
    const clipTop = y - clipHeight / 2;

    // If text is RTL, force RTL unless caller explicitly chooses a center wipe.
    const effectiveDirection =
        (isTextRTL && (revealDirection === "ltr" || revealDirection === "rtl"))
            ? "rtl"
            : revealDirection;

    const clamped = Math.max(0, Math.min(1, progress));

    // First, draw the ghost (inactive) text - full width, dimmed
    ctx.save();
    ctx.shadowColor = "rgba(0, 0, 0, 0.8)";
    ctx.shadowBlur = 10;
    ctx.shadowOffsetY = 2;
    ctx.strokeStyle = "rgba(0, 0, 0, 0.5)";
    ctx.lineWidth = 3;
    ctx.lineJoin = "round";
    ctx.strokeText(text, x, y);
    ctx.fillStyle = "rgba(255, 255, 255, 0.4)";
    ctx.fillText(text, x, y);
    ctx.restore();

    // Then draw the revealed (active) text with clipping
    if (clamped > 0) {
        ctx.save();
        ctx.beginPath();

        if (effectiveDirection === "rtl") {
            // RTL: Reveal from right to left
            const revealWidth = textWidth * clamped;
            ctx.rect(x + textWidth - revealWidth, clipTop, revealWidth, clipHeight);
        } else if (effectiveDirection === "ltr") {
            // LTR: Reveal from left to right
            const revealWidth = textWidth * clamped;
            ctx.rect(x, clipTop, revealWidth, clipHeight);
        } else if (effectiveDirection === "center-out") {
            // Center-out: expand from center
            const revealWidth = textWidth * clamped;
            const left = x + (textWidth - revealWidth) / 2;
            ctx.rect(left, clipTop, revealWidth, clipHeight);
        } else {
            // center-in: shrink towards center (reverse feel)
            const revealWidth = textWidth * (1 - clamped);
            const left = x + (textWidth - revealWidth) / 2;
            ctx.rect(left, clipTop, revealWidth, clipHeight);
        }

        ctx.clip();

        // Draw bright white revealed text with glow
        ctx.shadowColor = "rgba(255, 215, 100, 0.8)";
        ctx.shadowBlur = 15;
        ctx.fillStyle = "#ffffff";
        ctx.fillText(text, x, y);
        ctx.restore();
    }
}
````

## File: packages/shared/src/services/ffmpeg/transitions.ts
````typescript
/**
 * Transitions Module
 *
 * Handles scene-to-scene transition effects including fade, dissolve, zoom, and slide.
 * Also includes Ken Burns effect for image zoom during scenes.
 *
 * Enhanced with reliable video frame extraction using proper seeking.
 */

import { RenderAsset, ExportConfig } from "./exportConfig";
import { seekVideoToTime, getCachedFrame, cacheFrame } from "./assetLoader";

/**
 * Ken Burns movement types for visual variety.
 * Includes compound movements for more cinematic camera motion.
 */
type KenBurnsMovement =
    | 'zoom_in' | 'zoom_out'
    | 'pan_left' | 'pan_right' | 'pan_up' | 'pan_down'
    | 'zoom_in_pan_left' | 'zoom_in_pan_right'
    | 'zoom_out_pan_up' | 'zoom_out_pan_down';

/**
 * Ease-in-out cubic for organic camera motion
 */
function easeInOutCubic(t: number): number {
    return t < 0.5 ? 4 * t * t * t : 1 - Math.pow(-2 * t + 2, 3) / 2;
}

/**
 * Get deterministic but varied Ken Burns movement based on asset index/time.
 * Uses better hash distribution across 10 movements.
 */
function getKenBurnsMovement(assetTime: number): KenBurnsMovement {
    const movements: KenBurnsMovement[] = [
        'zoom_in', 'zoom_out',
        'pan_left', 'pan_right', 'pan_up', 'pan_down',
        'zoom_in_pan_left', 'zoom_in_pan_right',
        'zoom_out_pan_up', 'zoom_out_pan_down',
    ];
    // Better hash: multiply by prime for more spread across sequential times
    const index = Math.floor(assetTime * 7.3) % movements.length;
    return movements[index] || 'zoom_in';
}

/**
 * Draw a single asset with Ken Burns effect and opacity
 * Enhanced with varied movement types (zoom in/out, pan directions)
 * Uses reliable video seeking with proper event handling
 */
export async function drawAsset(
    ctx: CanvasRenderingContext2D,
    width: number,
    height: number,
    asset: RenderAsset,
    currentTime: number,
    progress: number,
    opacity: number,
    useModernEffects: boolean,
    offsetTime: number = 0
): Promise<void> {
    ctx.save();
    ctx.globalAlpha = opacity;

    let scale: number;
    let x: number;
    let y: number;
    let drawWidth: number;
    let drawHeight: number;
    const element = asset.element;

    // Get natural dimensions
    const naturalWidth =
        asset.type === "video"
            ? (element as HTMLVideoElement).videoWidth
            : (element as HTMLImageElement).width;
    const naturalHeight =
        asset.type === "video"
            ? (element as HTMLVideoElement).videoHeight
            : (element as HTMLImageElement).height;

    // Validate dimensions (fallback for edge cases)
    if (naturalWidth === 0 || naturalHeight === 0) {
        console.warn(`[Transitions] Invalid asset dimensions (${naturalWidth}x${naturalHeight}), skipping frame`);
        ctx.restore();
        return;
    }

    // Handle Video Seek with reliable seeking
    if (asset.type === "video") {
        const vid = element as HTMLVideoElement;
        if (vid.duration && isFinite(vid.duration)) {
            const relativeTime = currentTime + offsetTime - asset.time;

            // If asset has a nativeDuration and relative time exceeds it,
            // freeze on the last frame instead of looping (Issue 6: desync fix)
            let targetTime: number;
            if (asset.nativeDuration && relativeTime > asset.nativeDuration) {
                // Freeze on last frame (slightly before end to avoid black frame)
                targetTime = Math.max(0, asset.nativeDuration - 0.05);
            } else {
                // Normal loop behavior for music mode / short clips
                targetTime = ((relativeTime % vid.duration) + vid.duration) % vid.duration;
            }

            await seekVideoToTime(vid, targetTime);
        }
    }

    // Base scale to cover canvas
    const baseScale = Math.max(width / naturalWidth, height / naturalHeight);
    
    if (useModernEffects) {
        // Get varied Ken Burns movement based on asset timing
        const movement = getKenBurnsMovement(asset.time);
        const intensity = 0.18; // 18% movement range (was 12%)
        const panDistance = 80; // pixels for pan movements (was 40)
        // Apply ease-in-out for organic camera motion
        const p = easeInOutCubic(progress);

        switch (movement) {
            case 'zoom_in':
                scale = baseScale * (1.0 + p * intensity);
                drawWidth = naturalWidth * scale;
                drawHeight = naturalHeight * scale;
                x = (width - drawWidth) / 2;
                y = (height - drawHeight) / 2;
                break;

            case 'zoom_out':
                scale = baseScale * (1.0 + intensity - p * intensity);
                drawWidth = naturalWidth * scale;
                drawHeight = naturalHeight * scale;
                x = (width - drawWidth) / 2;
                y = (height - drawHeight) / 2;
                break;

            case 'pan_left':
                scale = baseScale * 1.15;
                drawWidth = naturalWidth * scale;
                drawHeight = naturalHeight * scale;
                x = (width - drawWidth) / 2 - (p * panDistance);
                y = (height - drawHeight) / 2;
                break;

            case 'pan_right':
                scale = baseScale * 1.15;
                drawWidth = naturalWidth * scale;
                drawHeight = naturalHeight * scale;
                x = (width - drawWidth) / 2 + (p * panDistance);
                y = (height - drawHeight) / 2;
                break;

            case 'pan_up':
                scale = baseScale * 1.15;
                drawWidth = naturalWidth * scale;
                drawHeight = naturalHeight * scale;
                x = (width - drawWidth) / 2;
                y = (height - drawHeight) / 2 - (p * panDistance);
                break;

            case 'pan_down':
                // FIX: was adjusting x instead of y
                scale = baseScale * 1.15;
                drawWidth = naturalWidth * scale;
                drawHeight = naturalHeight * scale;
                x = (width - drawWidth) / 2;
                y = (height - drawHeight) / 2 + (p * panDistance);
                break;

            case 'zoom_in_pan_left':
                scale = baseScale * (1.0 + p * intensity);
                drawWidth = naturalWidth * scale;
                drawHeight = naturalHeight * scale;
                x = (width - drawWidth) / 2 - (p * panDistance * 0.6);
                y = (height - drawHeight) / 2;
                break;

            case 'zoom_in_pan_right':
                scale = baseScale * (1.0 + p * intensity);
                drawWidth = naturalWidth * scale;
                drawHeight = naturalHeight * scale;
                x = (width - drawWidth) / 2 + (p * panDistance * 0.6);
                y = (height - drawHeight) / 2;
                break;

            case 'zoom_out_pan_up':
                scale = baseScale * (1.0 + intensity - p * intensity);
                drawWidth = naturalWidth * scale;
                drawHeight = naturalHeight * scale;
                x = (width - drawWidth) / 2;
                y = (height - drawHeight) / 2 - (p * panDistance * 0.6);
                break;

            case 'zoom_out_pan_down':
                scale = baseScale * (1.0 + intensity - p * intensity);
                drawWidth = naturalWidth * scale;
                drawHeight = naturalHeight * scale;
                x = (width - drawWidth) / 2;
                y = (height - drawHeight) / 2 + (p * panDistance * 0.6);
                break;

            default:
                scale = baseScale * (1.0 + p * intensity);
                drawWidth = naturalWidth * scale;
                drawHeight = naturalHeight * scale;
                x = (width - drawWidth) / 2;
                y = (height - drawHeight) / 2;
        }
    } else {
        // Static fill
        scale = baseScale;
        drawWidth = naturalWidth * scale;
        drawHeight = naturalHeight * scale;
        x = (width - drawWidth) / 2;
        y = (height - drawHeight) / 2;
    }

    ctx.drawImage(element, x, y, drawWidth, drawHeight);
    ctx.restore();
}

export interface TransitionContext {
    ctx: CanvasRenderingContext2D;
    width: number;
    height: number;
    currentTime: number;
    currentAsset: RenderAsset;
    nextAsset: RenderAsset;
    slideProgress: number;
    transitionProgress: number; // 0-1, how far into the transition
    config: ExportConfig;
}

/**
 * Apply transition effect between two assets
 */
export async function applyTransition(context: TransitionContext): Promise<void> {
    const {
        ctx,
        width,
        height,
        currentTime,
        currentAsset,
        nextAsset,
        slideProgress,
        transitionProgress: t,
        config,
    } = context;

    const timeUntilNext = nextAsset.time - currentTime;

    switch (config.transitionType) {
        case "fade": {
            // Fade through black
            if (t < 0.5) {
                // First half: fade out current to black
                await drawAsset(ctx, width, height, currentAsset, currentTime, slideProgress, 1 - t * 2, config.useModernEffects);
            } else {
                // Second half: fade in next from black
                await drawAsset(ctx, width, height, nextAsset, currentTime, 0, (t - 0.5) * 2, config.useModernEffects, -timeUntilNext);
            }
            break;
        }

        case "dissolve": {
            // Cross-dissolve (blend both)
            await drawAsset(ctx, width, height, currentAsset, currentTime, slideProgress, 1, config.useModernEffects);
            await drawAsset(ctx, width, height, nextAsset, currentTime, 0, t, config.useModernEffects, -timeUntilNext);
            break;
        }

        case "zoom": {
            // Zoom into current, then show next
            ctx.save();
            const zoomScale = 1 + t * 0.5; // Zoom up to 1.5x
            const centerX = width / 2;
            const centerY = height / 2;
            ctx.translate(centerX, centerY);
            ctx.scale(zoomScale, zoomScale);
            ctx.translate(-centerX, -centerY);
            ctx.globalAlpha = 1 - t;

            // Draw current (zooming in and fading out)
            const element = currentAsset.element;
            const naturalWidth = currentAsset.type === "video"
                ? (element as HTMLVideoElement).videoWidth
                : (element as HTMLImageElement).width;
            const naturalHeight = currentAsset.type === "video"
                ? (element as HTMLVideoElement).videoHeight
                : (element as HTMLImageElement).height;
            const scale = Math.max(width / naturalWidth, height / naturalHeight);
            const drawWidth = naturalWidth * scale;
            const drawHeight = naturalHeight * scale;
            const x = (width - drawWidth) / 2;
            const y = (height - drawHeight) / 2;
            ctx.drawImage(element, x, y, drawWidth, drawHeight);
            ctx.restore();

            // Draw next (fading in underneath)
            await drawAsset(ctx, width, height, nextAsset, currentTime, 0, t, config.useModernEffects, -timeUntilNext);
            break;
        }

        case "slide": {
            // Slide left - current slides out left, next slides in from right
            const slideOffset = t * width;

            // Draw current (sliding left)
            ctx.save();
            ctx.translate(-slideOffset, 0);
            await drawAsset(ctx, width, height, currentAsset, currentTime, slideProgress, 1, config.useModernEffects);
            ctx.restore();

            // Draw next (sliding in from right)
            ctx.save();
            ctx.translate(width - slideOffset, 0);
            await drawAsset(ctx, width, height, nextAsset, currentTime, 0, 1, config.useModernEffects, -timeUntilNext);
            ctx.restore();
            break;
        }

        default: {
            // Fallback: simple dissolve
            await drawAsset(ctx, width, height, currentAsset, currentTime, slideProgress, 1, config.useModernEffects);
            await drawAsset(ctx, width, height, nextAsset, currentTime, 0, t, config.useModernEffects, -timeUntilNext);
        }
    }
}
````

## File: packages/shared/src/services/ffmpeg/videoAudioExtractor.ts
````typescript
/**
 * Video Audio Extractor Module
 *
 * Extracts audio tracks from video files (e.g., Veo-generated videos)
 * for mixing into the final production audio.
 *
 * This allows Veo's native audio to be preserved and mixed with narration.
 */

// --- Types ---

export interface ExtractedVideoAudio {
    /** Scene ID this audio belongs to */
    sceneId: string;
    /** Audio blob extracted from video */
    audioBlob: Blob;
    /** Duration in seconds */
    duration: number;
    /** Start time in the video timeline */
    startTime: number;
    /** Sample rate of extracted audio */
    sampleRate: number;
    /** Whether audio was successfully extracted */
    hasAudio: boolean;
}

export interface VideoAudioExtractionResult {
    /** Successfully extracted audio tracks */
    audioTracks: ExtractedVideoAudio[];
    /** Scene IDs that had no audio or failed extraction */
    failedScenes: string[];
    /** Total duration of all extracted audio */
    totalDuration: number;
}

// --- Audio Extraction ---

/**
 * Extract audio from a video URL using Web Audio API
 *
 * @param videoUrl - URL of the video to extract audio from
 * @param sceneId - Scene identifier for tracking
 * @param startTime - Start time in the video timeline
 * @returns Extracted audio information
 */
export async function extractAudioFromVideo(
    videoUrl: string,
    sceneId: string,
    startTime: number
): Promise<ExtractedVideoAudio> {
    console.log(`[VideoAudioExtractor] Extracting audio from video: ${sceneId}`);

    try {
        // Fetch the video file
        const response = await fetch(videoUrl);
        if (!response.ok) {
            throw new Error(`Failed to fetch video: ${response.status}`);
        }

        const videoBlob = await response.blob();
        const arrayBuffer = await videoBlob.arrayBuffer();

        // Create audio context for decoding
        const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();

        // Try to decode the video as audio
        let audioBuffer: AudioBuffer;
        try {
            audioBuffer = await audioContext.decodeAudioData(arrayBuffer.slice(0));
        } catch (decodeError) {
            console.warn(`[VideoAudioExtractor] Video ${sceneId} has no audio track or unsupported format`);
            await audioContext.close();
            return {
                sceneId,
                audioBlob: new Blob([], { type: 'audio/wav' }),
                duration: 0,
                startTime,
                sampleRate: 44100,
                hasAudio: false,
            };
        }

        // Convert AudioBuffer to WAV Blob
        const wavBlob = audioBufferToWav(audioBuffer);

        await audioContext.close();

        console.log(`[VideoAudioExtractor] ✓ Extracted ${audioBuffer.duration.toFixed(2)}s audio from ${sceneId}`);

        return {
            sceneId,
            audioBlob: wavBlob,
            duration: audioBuffer.duration,
            startTime,
            sampleRate: audioBuffer.sampleRate,
            hasAudio: true,
        };
    } catch (error) {
        console.error(`[VideoAudioExtractor] Failed to extract audio from ${sceneId}:`, error);
        return {
            sceneId,
            audioBlob: new Blob([], { type: 'audio/wav' }),
            duration: 0,
            startTime,
            sampleRate: 44100,
            hasAudio: false,
        };
    }
}

/**
 * Extract audio from multiple video scenes
 *
 * @param videos - Array of video information
 * @returns Extraction results with all audio tracks
 */
export async function extractAudioFromVideos(
    videos: Array<{
        sceneId: string;
        videoUrl: string;
        startTime: number;
    }>
): Promise<VideoAudioExtractionResult> {
    const audioTracks: ExtractedVideoAudio[] = [];
    const failedScenes: string[] = [];
    let totalDuration = 0;

    console.log(`[VideoAudioExtractor] Extracting audio from ${videos.length} videos`);

    for (const video of videos) {
        const result = await extractAudioFromVideo(
            video.videoUrl,
            video.sceneId,
            video.startTime
        );

        if (result.hasAudio) {
            audioTracks.push(result);
            totalDuration += result.duration;
        } else {
            failedScenes.push(video.sceneId);
        }
    }

    console.log(`[VideoAudioExtractor] Extracted audio from ${audioTracks.length}/${videos.length} videos`);

    return {
        audioTracks,
        failedScenes,
        totalDuration,
    };
}

// --- WAV Encoding ---

/**
 * Convert AudioBuffer to WAV Blob
 * Uses standard PCM encoding
 */
function audioBufferToWav(buffer: AudioBuffer): Blob {
    const numChannels = buffer.numberOfChannels;
    const sampleRate = buffer.sampleRate;
    const format = 1; // PCM
    const bitDepth = 16;

    // Interleave channels
    const interleaved = interleaveChannels(buffer);

    // Create WAV file
    const dataLength = interleaved.length * 2; // 16-bit = 2 bytes
    const headerLength = 44;
    const totalLength = headerLength + dataLength;

    const arrayBuffer = new ArrayBuffer(totalLength);
    const view = new DataView(arrayBuffer);

    // WAV Header
    writeString(view, 0, 'RIFF');
    view.setUint32(4, totalLength - 8, true);
    writeString(view, 8, 'WAVE');
    writeString(view, 12, 'fmt ');
    view.setUint32(16, 16, true); // Subchunk1Size
    view.setUint16(20, format, true);
    view.setUint16(22, numChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * numChannels * (bitDepth / 8), true); // ByteRate
    view.setUint16(32, numChannels * (bitDepth / 8), true); // BlockAlign
    view.setUint16(34, bitDepth, true);
    writeString(view, 36, 'data');
    view.setUint32(40, dataLength, true);

    // Write PCM samples
    const offset = 44;
    for (let i = 0; i < interleaved.length; i++) {
        const val = interleaved[i];
        if (val === undefined) continue;
        const sample = Math.max(-1, Math.min(1, val));
        const intSample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
        view.setInt16(offset + i * 2, intSample, true);
    }

    return new Blob([arrayBuffer], { type: 'audio/wav' });
}

/**
 * Interleave audio channels for WAV encoding
 */
function interleaveChannels(buffer: AudioBuffer): Float32Array {
    const numChannels = buffer.numberOfChannels;
    const length = buffer.length;
    const result = new Float32Array(length * numChannels);

    for (let i = 0; i < length; i++) {
        for (let channel = 0; channel < numChannels; channel++) {
            const channelData = buffer.getChannelData(channel);
            const val = channelData[i];
            if (val !== undefined) {
                result[i * numChannels + channel] = val;
            }
        }
    }

    return result;
}

/**
 * Write string to DataView
 */
function writeString(view: DataView, offset: number, str: string): void {
    for (let i = 0; i < str.length; i++) {
        view.setUint8(offset + i, str.charCodeAt(i));
    }
}

// --- Audio Mixing Helpers ---

/**
 * Mix extracted video audio with narration at appropriate times
 * Creates a combined audio track with video audio layered under narration
 *
 * @param narrationBlob - Main narration audio
 * @param videoAudioTracks - Extracted video audio tracks
 * @param videoAudioVolume - Volume for video audio (0-1, default 0.3)
 * @returns Mixed audio blob
 */
export async function mixVideoAudioWithNarration(
    narrationBlob: Blob,
    videoAudioTracks: ExtractedVideoAudio[],
    videoAudioVolume = 0.3
): Promise<Blob> {
    if (videoAudioTracks.length === 0 || !videoAudioTracks.some(t => t.hasAudio)) {
        console.log("[VideoAudioExtractor] No video audio to mix, returning narration only");
        return narrationBlob;
    }

    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();

    try {
        // Decode narration
        const narrationBuffer = await audioContext.decodeAudioData(
            await narrationBlob.arrayBuffer()
        );

        // Find total duration (max of narration or video audio end times)
        let maxEndTime = narrationBuffer.duration;
        for (const track of videoAudioTracks) {
            if (track.hasAudio) {
                const trackEnd = track.startTime + track.duration;
                maxEndTime = Math.max(maxEndTime, trackEnd);
            }
        }

        // Create output buffer
        const outputLength = Math.ceil(maxEndTime * audioContext.sampleRate);
        const outputBuffer = audioContext.createBuffer(
            2, // Stereo output
            outputLength,
            audioContext.sampleRate
        );

        // Copy narration to output (full volume)
        for (let channel = 0; channel < Math.min(2, narrationBuffer.numberOfChannels); channel++) {
            const outputData = outputBuffer.getChannelData(channel);
            const narrationData = narrationBuffer.getChannelData(
                channel < narrationBuffer.numberOfChannels ? channel : 0
            );
            for (let i = 0; i < narrationData.length && i < outputData.length; i++) {
                const val = narrationData[i];
                if (val !== undefined) {
                    outputData[i] = val;
                }
            }
        }

        // Mix in video audio tracks at their start times
        for (const track of videoAudioTracks) {
            if (!track.hasAudio || track.audioBlob.size === 0) continue;

            try {
                const trackBuffer = await audioContext.decodeAudioData(
                    await track.audioBlob.arrayBuffer()
                );

                const startSample = Math.floor(track.startTime * audioContext.sampleRate);

                for (let channel = 0; channel < 2; channel++) {
                    const outputData = outputBuffer.getChannelData(channel);
                    const trackData = trackBuffer.getChannelData(
                        channel < trackBuffer.numberOfChannels ? channel : 0
                    );

                    for (let i = 0; i < trackData.length; i++) {
                        const outputIndex = startSample + i;
                        const trackVal = trackData[i];
                        if (outputIndex < outputData.length && trackVal !== undefined) {
                            const outputVal = outputData[outputIndex];
                            if (outputVal !== undefined) {
                                // Mix video audio at specified volume
                                let mixed = outputVal + (trackVal * videoAudioVolume);
                                // Soft clip to prevent distortion
                                mixed = Math.max(-1, Math.min(1, mixed));
                                outputData[outputIndex] = mixed;
                            }
                        }
                    }
                }

                console.log(`[VideoAudioExtractor] Mixed video audio from ${track.sceneId} at ${track.startTime}s`);
            } catch (e) {
                console.warn(`[VideoAudioExtractor] Failed to mix track ${track.sceneId}:`, e);
            }
        }

        // Convert to WAV
        const mixedWav = audioBufferToWav(outputBuffer);

        await audioContext.close();

        console.log(`[VideoAudioExtractor] ✓ Mixed audio complete: ${maxEndTime.toFixed(2)}s`);
        return mixedWav;
    } catch (error) {
        console.error("[VideoAudioExtractor] Mix failed:", error);
        await audioContext.close();
        return narrationBlob; // Return original on failure
    }
}
````

## File: packages/shared/src/services/ffmpeg/visualizer.ts
````typescript
/**
 * Visualizer Module
 *
 * Renders audio frequency visualizer as a mirrored spectrum display.
 * Supports multiple color schemes and configurable opacity/height.
 */

import { ExportConfig } from "./exportConfig";

/**
 * Render refined visualizer layer with reduced opacity and constrained height
 */
export function renderVisualizerLayer(
    ctx: CanvasRenderingContext2D,
    width: number,
    height: number,
    frequencyData: Uint8Array,
    previousFrequencyData: Uint8Array | null,
    config: NonNullable<ExportConfig["visualizerConfig"]>,
    useModernEffects: boolean,
    zone?: { top: number; height: number }
): void {
    const bufferLength = frequencyData.length;

    const zoneTop = zone?.top ?? 0;
    const zoneHeight = zone?.height ?? height;
    const baselineY = zoneTop + zoneHeight;

    const maxHeight = Math.min(zoneHeight, height * config.maxHeightRatio);
    const barWidth = config.barWidth;
    const barGap = config.barGap;

    ctx.save();

    // Hard clip to the visualizer zone so it never invades the lyric zone.
    ctx.beginPath();
    ctx.rect(0, zoneTop, width, zoneHeight);
    ctx.clip();

    // Set reduced opacity
    ctx.globalAlpha = config.opacity;

    // Create gradient based on color scheme
    const gradient = ctx.createLinearGradient(0, baselineY, 0, baselineY - maxHeight);

    switch (config.colorScheme) {
        case "cyan-purple":
            gradient.addColorStop(0, `rgba(34, 211, 238, ${config.opacity * 0.5})`);
            gradient.addColorStop(0.5, `rgba(34, 211, 238, ${config.opacity * 0.8})`);
            gradient.addColorStop(1, `rgba(167, 139, 250, ${config.opacity})`);
            break;
        case "rainbow":
            gradient.addColorStop(0, `rgba(255, 0, 0, ${config.opacity})`);
            gradient.addColorStop(0.2, `rgba(255, 165, 0, ${config.opacity})`);
            gradient.addColorStop(0.4, `rgba(255, 255, 0, ${config.opacity})`);
            gradient.addColorStop(0.6, `rgba(0, 255, 0, ${config.opacity})`);
            gradient.addColorStop(0.8, `rgba(0, 0, 255, ${config.opacity})`);
            gradient.addColorStop(1, `rgba(128, 0, 128, ${config.opacity})`);
            break;
        case "monochrome":
            gradient.addColorStop(0, `rgba(255, 255, 255, ${config.opacity * 0.3})`);
            gradient.addColorStop(1, `rgba(255, 255, 255, ${config.opacity})`);
            break;
    }

    ctx.fillStyle = gradient;

    // Optional: subtle glow for modern effects
    if (useModernEffects) {
        ctx.shadowBlur = 8;
        ctx.shadowColor = `rgba(34, 211, 238, ${config.opacity * 0.3})`;
    }

    // Render mirrored spectrum
    const centerX = width / 2;

    for (let i = 0; i < bufferLength; i++) {
        const freqVal = frequencyData[i];
        if (freqVal === undefined) continue;

        let value = freqVal;

        if (previousFrequencyData) {
            const prevVal = previousFrequencyData[i];
            if (prevVal !== undefined) {
                value = (value + prevVal) / 2;
            }
        }

        const barHeight = (value / 255) * maxHeight;

        if (barHeight > 0) {
            const offset = i * (barWidth + barGap);
            const radius = barWidth / 2;

            // Right side
            ctx.beginPath();
            if (useModernEffects) {
                ctx.roundRect(
                    centerX + offset,
                    baselineY - barHeight,
                    barWidth,
                    barHeight,
                    [radius, radius, 0, 0]
                );
            } else {
                ctx.rect(centerX + offset, baselineY - barHeight, barWidth, barHeight);
            }
            ctx.fill();

            // Left side (mirrored)
            ctx.beginPath();
            if (useModernEffects) {
                ctx.roundRect(
                    centerX - offset - barWidth,
                    baselineY - barHeight,
                    barWidth,
                    barHeight,
                    [radius, radius, 0, 0]
                );
            } else {
                ctx.rect(centerX - offset - barWidth, baselineY - barHeight, barWidth, barHeight);
            }
            ctx.fill();
        }
    }

    ctx.restore();
}
````

## File: packages/shared/src/services/firebase/authService.ts
````typescript
/**
 * Firebase Authentication Service
 *
 * Provides Google and email sign-in functionality.
 * Manages auth state and user sessions.
 */
import {
  signInWithPopup,
  getRedirectResult,
  signInWithEmailAndPassword,
  createUserWithEmailAndPassword,
  signOut as firebaseSignOut,
  onAuthStateChanged,
  GoogleAuthProvider,
  type User,
  type Unsubscribe,
} from 'firebase/auth';
import { getFirebaseAuth, isFirebaseConfigured } from './config';

export interface AuthUser {
  uid: string;
  email: string | null;
  displayName: string | null;
  photoURL: string | null;
  isAnonymous: boolean;
}

// Convert Firebase User to AuthUser
function toAuthUser(user: User): AuthUser {
  return {
    uid: user.uid,
    email: user.email,
    displayName: user.displayName,
    photoURL: user.photoURL,
    isAnonymous: user.isAnonymous,
  };
}

// Google provider instance
const googleProvider = new GoogleAuthProvider();
googleProvider.setCustomParameters({
  prompt: 'select_account',
});

/**
 * Sign in with Google redirect
 */
export async function signInWithGoogle(): Promise<AuthUser | null> {
  const auth = getFirebaseAuth();
  if (!auth) {
    console.warn('[Auth] Firebase not configured');
    return null;
  }

  console.log('[Auth] Starting Google sign-in with popup...');

  try {
    const result = await signInWithPopup(auth, googleProvider);
    console.log('[Auth] Google sign-in successful:', result.user.email);
    return toAuthUser(result.user);
  } catch (error: unknown) {
    const firebaseError = error as { code?: string; message?: string };
    console.error('[Auth] Google sign-in failed:', firebaseError.message);
    throw error;
  }
}

// Module-level guard: only check for redirect result once per page load
let _redirectCheckPromise: Promise<AuthUser | null> | null = null;

/**
 * Check for redirect result on app load
 * Call this when the app initializes to handle redirect-based sign-in.
 * Deduplicates across multiple hook mounts — only runs once per page load.
 */
export async function handleRedirectResult(): Promise<AuthUser | null> {
  // Return cached promise if already checking/checked
  if (_redirectCheckPromise) {
    return _redirectCheckPromise;
  }

  _redirectCheckPromise = _handleRedirectResultImpl();
  return _redirectCheckPromise;
}

async function _handleRedirectResultImpl(): Promise<AuthUser | null> {
  const auth = getFirebaseAuth();
  if (!auth) {
    console.log('[Auth] No auth instance, skipping redirect check');
    return null;
  }

  try {
    console.log('[Auth] Checking for redirect result...');
    const result = await getRedirectResult(auth);

    if (result) {
      console.log('[Auth] Redirect sign-in successful:', result.user.email);
      return toAuthUser(result.user);
    }

    console.log('[Auth] No redirect result found');
    return null;
  } catch (error: unknown) {
    const firebaseError = error as { code?: string; message?: string };
    console.error('[Auth] Redirect result error:', firebaseError.message);
    throw error;
  }
}

/**
 * Sign in with email and password
 */
export async function signInWithEmail(
  email: string,
  password: string
): Promise<AuthUser | null> {
  const auth = getFirebaseAuth();
  if (!auth) {
    console.warn('[Auth] Firebase not configured');
    return null;
  }

  try {
    const result = await signInWithEmailAndPassword(auth, email, password);
    console.log('[Auth] Email sign-in successful:', result.user.email);
    return toAuthUser(result.user);
  } catch (error: unknown) {
    const firebaseError = error as { code?: string; message?: string };
    console.error('[Auth] Email sign-in failed:', firebaseError.message);
    throw error;
  }
}

/**
 * Create account with email and password
 */
export async function createAccount(
  email: string,
  password: string
): Promise<AuthUser | null> {
  const auth = getFirebaseAuth();
  if (!auth) {
    console.warn('[Auth] Firebase not configured');
    return null;
  }

  try {
    const result = await createUserWithEmailAndPassword(auth, email, password);
    console.log('[Auth] Account created:', result.user.email);
    return toAuthUser(result.user);
  } catch (error: unknown) {
    const firebaseError = error as { code?: string; message?: string };
    console.error('[Auth] Account creation failed:', firebaseError.message);
    throw error;
  }
}

/**
 * Sign out current user
 */
export async function signOut(): Promise<void> {
  const auth = getFirebaseAuth();
  if (!auth) return;

  try {
    await firebaseSignOut(auth);
    console.log('[Auth] Signed out');
  } catch (error: unknown) {
    const firebaseError = error as { message?: string };
    console.error('[Auth] Sign out failed:', firebaseError.message);
    throw error;
  }
}

/**
 * Get current user (synchronous)
 */
export function getCurrentUser(): AuthUser | null {
  const auth = getFirebaseAuth();
  if (!auth || !auth.currentUser) return null;
  return toAuthUser(auth.currentUser);
}

/**
 * Subscribe to auth state changes
 */
export function onAuthChange(
  callback: (user: AuthUser | null) => void
): Unsubscribe | null {
  const auth = getFirebaseAuth();
  if (!auth) {
    // Call with null immediately if Firebase not configured
    callback(null);
    return null;
  }

  return onAuthStateChanged(auth, (user) => {
    callback(user ? toAuthUser(user) : null);
  });
}

/**
 * Check if Firebase Auth is available
 */
export function isAuthAvailable(): boolean {
  return isFirebaseConfigured();
}
````

## File: packages/shared/src/services/firebase/config.ts
````typescript
/**
 * Firebase Configuration
 *
 * Initialize Firebase app with environment variables.
 * Supports both browser and potential SSR environments.
 */
import { initializeApp, getApps, getApp, type FirebaseApp } from 'firebase/app';
import { getAuth, type Auth } from 'firebase/auth';
import { getFirestore, type Firestore } from 'firebase/firestore';

// Firebase configuration from environment variables
// NOTE: Storage is handled by Google Cloud Storage (aisoul-studio-storage bucket)
// via services/cloudStorageService.ts - NOT Firebase Storage
const firebaseConfig = {
  apiKey: import.meta.env.VITE_FIREBASE_API_KEY,
  authDomain: import.meta.env.VITE_FIREBASE_AUTH_DOMAIN,
  projectId: import.meta.env.VITE_FIREBASE_PROJECT_ID,
  // storageBucket is NOT used - GCS bucket is in a different project
  // storageBucket: import.meta.env.VITE_FIREBASE_STORAGE_BUCKET,
  messagingSenderId: import.meta.env.VITE_FIREBASE_MESSAGING_SENDER_ID,
  appId: import.meta.env.VITE_FIREBASE_APP_ID,
};

// Check if Firebase is configured
export function isFirebaseConfigured(): boolean {
  return !!(
    firebaseConfig.apiKey &&
    firebaseConfig.authDomain &&
    firebaseConfig.projectId
  );
}

// Initialize Firebase app (singleton pattern)
let app: FirebaseApp | null = null;
let auth: Auth | null = null;
let db: Firestore | null = null;

export function getFirebaseApp(): FirebaseApp | null {
  if (!isFirebaseConfigured()) {
    console.warn('[Firebase] Not configured. Set VITE_FIREBASE_* environment variables.');
    console.warn('[Firebase] Current config:', {
      hasApiKey: !!firebaseConfig.apiKey,
      hasAuthDomain: !!firebaseConfig.authDomain,
      hasProjectId: !!firebaseConfig.projectId,
      authDomain: firebaseConfig.authDomain,
      projectId: firebaseConfig.projectId
    });
    return null;
  }

  if (!app) {
    console.log('[Firebase] Initializing app with config:', {
      authDomain: firebaseConfig.authDomain,
      projectId: firebaseConfig.projectId
    });
    app = getApps().length > 0 ? getApp() : initializeApp(firebaseConfig);
    console.log('[Firebase] App initialized successfully');
  }
  return app;
}

export function getFirebaseAuth(): Auth | null {
  const firebaseApp = getFirebaseApp();
  if (!firebaseApp) return null;

  if (!auth) {
    auth = getAuth(firebaseApp);
  }
  return auth;
}

export function getFirebaseDb(): Firestore | null {
  const firebaseApp = getFirebaseApp();
  if (!firebaseApp) return null;

  if (!db) {
    db = getFirestore(firebaseApp);
  }
  return db;
}
````

## File: packages/shared/src/services/firebase/index.ts
````typescript
/**
 * Firebase Services
 *
 * Exports authentication and story sync functionality.
 */
export { isFirebaseConfigured } from './config';

export {
  signInWithGoogle,
  signInWithEmail,
  createAccount,
  signOut,
  getCurrentUser,
  onAuthChange,
  isAuthAvailable,
  handleRedirectResult,
  type AuthUser,
} from './authService';

export {
  saveStoryToCloud,
  loadStoryFromCloud,
  deleteStoryFromCloud,
  listUserStories,
  subscribeToStory,
  isSyncAvailable,
  debouncedSaveToCloud,
  flushPendingSave,
  type StoryListItem,
  type StorySyncDocument,
} from './storySync';
````

## File: packages/shared/src/services/firebase/storySync.ts
````typescript
/**
 * Firestore Story Sync Service
 *
 * Real-time synchronization of story state to Firestore.
 * Stores metadata only - media files go to GCS via cloudStorageService.
 */
import {
  doc,
  setDoc,
  getDoc,
  deleteDoc,
  onSnapshot,
  collection,
  query,
  where,
  orderBy,
  limit,
  getDocs,
  serverTimestamp,
  Timestamp,
  type Unsubscribe,
  type DocumentData,
} from 'firebase/firestore';
import { getFirebaseDb, isFirebaseConfigured } from './config';
import { getCurrentUser } from './authService';
import type { StoryState } from '@/types';

/**
 * Story document stored in Firestore
 * Contains metadata and cloud URLs - no base64 data
 */
export interface StorySyncDocument {
  // Identifiers
  id: string; // Document ID = sessionId
  userId: string;
  title: string;
  topic?: string;

  // Timestamps
  createdAt: ReturnType<typeof serverTimestamp>;
  updatedAt: ReturnType<typeof serverTimestamp>;

  // Story state (stripped of base64 data)
  state: StoryState;

  // Cloud storage references
  cloudSessionId: string; // Maps to GCS folder: production_{sessionId}/
}

/**
 * Recursively strip `undefined` values and other Firestore-incompatible types
 * from an object before writing to Firestore.
 * Firestore rejects: undefined, custom classes, functions, Symbols.
 * Uses JSON round-trip which drops undefined fields and converts Date → string.
 */
function sanitizeForFirestore<T>(data: T): T {
  return JSON.parse(JSON.stringify(data, (_key, value) => {
    // Convert undefined to null (JSON.stringify already drops undefined in objects,
    // but this handles undefined inside arrays where it becomes null)
    if (value === undefined) return null;
    // Strip functions
    if (typeof value === 'function') return undefined;
    return value;
  }));
}

/**
 * Strip base64/blob data from state before syncing to Firestore.
 * Media URLs that start with https:// (cloud URLs) are kept.
 */
function stripLocalMediaData(state: StoryState): StoryState {
  const stripped = { ...state };

  // Helper: keep only https:// URLs, replace others with empty string
  // Using empty string instead of undefined to avoid Firestore rejection
  const keepCloudUrl = (url: string | undefined): string =>
    url?.startsWith('https://') ? url : '';

  // Strip character referenceImageUrls that are local (base64 data: or blob:)
  // These can be very large (500KB+ each) and would exceed Firestore's field size limit.
  if (stripped.characters) {
    stripped.characters = stripped.characters.map((char) => ({
      ...char,
      referenceImageUrl: keepCloudUrl(char.referenceImageUrl) || undefined,
    }));
  }

  // Strip shotlist imageUrls that are local (blob: or data:)
  if (stripped.shotlist) {
    stripped.shotlist = stripped.shotlist.map((shot) => ({
      ...shot,
      imageUrl: keepCloudUrl(shot.imageUrl) || undefined,
    }));
  }

  // Strip shots[] imageUrls that are local (Storyboarder.ai-style workflow)
  if (stripped.shots) {
    stripped.shots = stripped.shots.map((shot) => ({
      ...shot,
      imageUrl: keepCloudUrl(shot.imageUrl) || undefined,
    }));
  }

  // Strip narration audioUrls that are local
  if (stripped.narrationSegments) {
    stripped.narrationSegments = stripped.narrationSegments.map((seg) => ({
      ...seg,
      audioUrl: keepCloudUrl(seg.audioUrl),
    }));
  }

  // Strip animated shot videoUrls that are local
  if (stripped.animatedShots) {
    stripped.animatedShots = stripped.animatedShots.map((shot) => ({
      ...shot,
      videoUrl: keepCloudUrl(shot.videoUrl),
      thumbnailUrl: keepCloudUrl(shot.thumbnailUrl) || undefined,
    }));
  }

  // Strip final video URL if local
  if (stripped.finalVideoUrl && !stripped.finalVideoUrl.startsWith('https://')) {
    stripped.finalVideoUrl = undefined;
  }

  // Strip stageErrors: replace null values with empty string for Firestore compatibility
  if (stripped.stageErrors) {
    const cleanErrors: Record<string, string> = {};
    for (const [key, val] of Object.entries(stripped.stageErrors)) {
      if (val != null) {
        cleanErrors[key] = val;
      }
    }
    stripped.stageErrors = cleanErrors as any;
  }

  return stripped;
}

/**
 * Generate a title from story state
 */
function generateTitle(state: StoryState): string {
  // Try to get title from script
  if (state.script?.title) {
    return state.script.title;
  }

  // Try to get from first scene heading
  if (state.breakdown.length > 0 && state.breakdown[0]?.heading) {
    return state.breakdown[0].heading.substring(0, 50);
  }

  // Default
  return 'Untitled Story';
}

/**
 * Save story state to Firestore
 */
export async function saveStoryToCloud(
  sessionId: string,
  state: StoryState,
  topic?: string
): Promise<boolean> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    console.log('[StorySync] Cannot save - no auth or Firebase not configured');
    return false;
  }

  try {
    const docRef = doc(db, 'stories', sessionId);
    const strippedState = stripLocalMediaData(state);

    const storyDoc: Omit<StorySyncDocument, 'createdAt'> & {
      createdAt?: ReturnType<typeof serverTimestamp>;
      updatedAt: ReturnType<typeof serverTimestamp>;
    } = {
      id: sessionId,
      userId: user.uid,
      title: generateTitle(state),
      ...(topic !== undefined && { topic }),
      updatedAt: serverTimestamp(),
      state: strippedState,
      cloudSessionId: sessionId,
    };

    // Check if document exists to preserve createdAt and verify ownership
    const existing = await getDoc(docRef);
    if (existing.exists()) {
      const existingData = existing.data() as StorySyncDocument;
      // Verify ownership before update to prevent cross-user writes
      if (existingData.userId !== user.uid) {
        console.warn('[StorySync] Cannot save - document belongs to different user');
        return false;
      }
    } else {
      storyDoc.createdAt = serverTimestamp();
    }

    await setDoc(docRef, sanitizeForFirestore(storyDoc), { merge: true });
    console.log(`[StorySync] Saved story ${sessionId} to Firestore`);
    return true;
  } catch (error) {
    console.error('[StorySync] Failed to save:', error);
    return false;
  }
}

/**
 * Load story state from Firestore
 */
export async function loadStoryFromCloud(
  sessionId: string
): Promise<StoryState | null> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return null;
  }

  try {
    const docRef = doc(db, 'stories', sessionId);
    const docSnap = await getDoc(docRef);

    if (!docSnap.exists()) {
      return null;
    }

    const data = docSnap.data() as StorySyncDocument;

    // Verify ownership
    if (data.userId !== user.uid) {
      console.warn('[StorySync] Story belongs to different user');
      return null;
    }

    console.log(`[StorySync] Loaded story ${sessionId} from Firestore`);
    return data.state;
  } catch (error) {
    console.error('[StorySync] Failed to load:', error);
    return null;
  }
}

/**
 * Delete story from Firestore
 */
export async function deleteStoryFromCloud(sessionId: string): Promise<boolean> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return false;
  }

  try {
    const docRef = doc(db, 'stories', sessionId);
    const docSnap = await getDoc(docRef);

    if (docSnap.exists()) {
      const data = docSnap.data() as StorySyncDocument;
      if (data.userId !== user.uid) {
        console.warn('[StorySync] Cannot delete - story belongs to different user');
        return false;
      }
    }

    await deleteDoc(docRef);
    console.log(`[StorySync] Deleted story ${sessionId}`);
    return true;
  } catch (error) {
    console.error('[StorySync] Failed to delete:', error);
    return false;
  }
}

/**
 * List user's stories
 */
export interface StoryListItem {
  id: string;
  title: string;
  topic?: string;
  updatedAt: Date;
  createdAt: Date;
  sceneCount: number;
  hasVisuals: boolean;
  hasNarration: boolean;
  hasAnimation: boolean;
}

export async function listUserStories(
  maxResults: number = 20
): Promise<StoryListItem[]> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return [];
  }

  try {
    const storiesRef = collection(db, 'stories');
    const q = query(
      storiesRef,
      where('userId', '==', user.uid),
      orderBy('updatedAt', 'desc'),
      limit(maxResults)
    );

    const snapshot = await getDocs(q);
    const stories: StoryListItem[] = [];

    snapshot.forEach((docSnapshot) => {
      const data = docSnapshot.data() as DocumentData;
      // Firestore timestamps come back as Timestamp objects when read
      const updatedAt = data.updatedAt instanceof Timestamp
        ? data.updatedAt.toDate()
        : new Date();
      const createdAt = data.createdAt instanceof Timestamp
        ? data.createdAt.toDate()
        : new Date();

      stories.push({
        id: data.id,
        title: data.title,
        topic: data.topic,
        updatedAt,
        createdAt,
        sceneCount: data.state?.breakdown?.length || 0,
        hasVisuals: (data.state?.scenesWithVisuals?.length || 0) > 0,
        hasNarration: (data.state?.narrationSegments?.length || 0) > 0,
        hasAnimation: (data.state?.animatedShots?.length || 0) > 0,
      });
    });

    return stories;
  } catch (error) {
    console.error('[StorySync] Failed to list stories:', error);
    return [];
  }
}

/**
 * Subscribe to real-time updates for a story
 */
export function subscribeToStory(
  sessionId: string,
  onUpdate: (state: StoryState | null) => void
): Unsubscribe | null {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return null;
  }

  const docRef = doc(db, 'stories', sessionId);

  return onSnapshot(
    docRef,
    (docSnap) => {
      if (!docSnap.exists()) {
        onUpdate(null);
        return;
      }

      const data = docSnap.data() as StorySyncDocument;

      // Verify ownership
      if (data.userId !== user.uid) {
        console.warn('[StorySync] Real-time update for story owned by different user');
        onUpdate(null);
        return;
      }

      onUpdate(data.state);
    },
    (error) => {
      console.error('[StorySync] Real-time subscription error:', error);
    }
  );
}

/**
 * Check if story sync is available
 */
export function isSyncAvailable(): boolean {
  return isFirebaseConfigured() && getCurrentUser() !== null;
}

/**
 * Debounced save function to avoid too many writes
 */
let saveTimeout: ReturnType<typeof setTimeout> | null = null;
const SAVE_DEBOUNCE_MS = 2000; // 2 seconds

export function debouncedSaveToCloud(
  sessionId: string,
  state: StoryState,
  topic?: string
): void {
  if (saveTimeout) {
    clearTimeout(saveTimeout);
  }

  saveTimeout = setTimeout(async () => {
    await saveStoryToCloud(sessionId, state, topic);
    saveTimeout = null;
  }, SAVE_DEBOUNCE_MS);
}

/**
 * Flush any pending saves immediately
 */
export async function flushPendingSave(
  sessionId: string,
  state: StoryState,
  topic?: string
): Promise<void> {
  if (saveTimeout) {
    clearTimeout(saveTimeout);
    saveTimeout = null;
  }
  await saveStoryToCloud(sessionId, state, topic);
}
````

## File: packages/shared/src/services/formatRegistry.ts
````typescript
/**
 * Format Registry
 * 
 * Centralized registry of all supported video formats with metadata and configuration.
 * Each format defines its pipeline characteristics, constraints, and requirements.
 */

import { FormatMetadata, VideoFormat } from '../types';

/**
 * Format Registry class for managing video format metadata
 */
export class FormatRegistry {
  private formats: Map<VideoFormat, FormatMetadata> = new Map();

  constructor() {
    // Register all 8 formats on initialization
    this.registerAllFormats();
  }

  /**
   * Get format metadata by ID
   * @param id Format identifier
   * @returns Format metadata or null if not found
   */
  getFormat(id: string): FormatMetadata | null {
    return this.formats.get(id as VideoFormat) || null;
  }

  /**
   * Get all registered formats
   * @returns Array of all format metadata
   */
  getAllFormats(): FormatMetadata[] {
    return Array.from(this.formats.values());
  }

  /**
   * Register a new format or update existing
   * @param metadata Format metadata to register
   */
  registerFormat(metadata: FormatMetadata): void {
    this.formats.set(metadata.id, metadata);
  }

  /**
   * Get all active (non-deprecated) formats
   * @returns Array of non-deprecated format metadata
   */
  getActiveFormats(): FormatMetadata[] {
    return Array.from(this.formats.values()).filter(f => !f.deprecated);
  }

  /**
   * Get all deprecated formats
   * @returns Array of deprecated format metadata
   */
  getDeprecatedFormats(): FormatMetadata[] {
    return Array.from(this.formats.values()).filter(f => f.deprecated === true);
  }

  /**
   * Check if a format is deprecated
   * @param id Format identifier
   * @returns True if format exists and is deprecated
   */
  isDeprecated(id: string): boolean {
    const format = this.formats.get(id as VideoFormat);
    return format?.deprecated === true;
  }

  /**
   * Mark a format as deprecated
   * @param id Format identifier
   * @param message Optional deprecation message
   */
  deprecateFormat(id: string, message?: string): void {
    const format = this.formats.get(id as VideoFormat);
    if (format) {
      format.deprecated = true;
      if (message) {
        format.deprecationMessage = message;
      }
    }
  }

  /**
   * Check if a format ID is valid
   * @param id Format identifier to validate
   * @returns True if format exists in registry (including deprecated formats)
   */
  isValidFormat(id: string): boolean {
    return this.formats.has(id as VideoFormat);
  }

  /**
   * Register all default formats
   */
  private registerAllFormats(): void {
    // YouTube Narrator Format
    this.registerFormat({
      id: 'youtube-narrator',
      name: 'YouTube Narrator',
      description: 'Conversational long-form content with B-roll visuals and research-backed narration',
      icon: '🎙️',
      durationRange: { min: 480, max: 1500 }, // 8-25 minutes
      aspectRatio: '16:9',
      applicableGenres: [
        'Educational',
        'Documentary',
        'Commentary',
        'Review',
        'Tutorial',
        'Explainer',
        'History',
        'Science',
        'Technology'
      ],
      checkpointCount: 3,
      concurrencyLimit: 5,
      requiresResearch: true,
      supportedLanguages: ['ar', 'en']
    });

    // Advertisement Format
    this.registerFormat({
      id: 'advertisement',
      name: 'Advertisement',
      description: 'Short, high-impact promotional videos with clear call-to-action',
      icon: '📢',
      durationRange: { min: 15, max: 60 }, // 15-60 seconds
      aspectRatio: '16:9',
      applicableGenres: [
        'Product Launch',
        'Brand Story',
        'Service Promotion',
        'App Demo',
        'Event Announcement',
        'Sale/Offer',
        'Testimonial'
      ],
      checkpointCount: 2,
      concurrencyLimit: 3,
      requiresResearch: false,
      supportedLanguages: ['ar', 'en']
    });

    // Movie/Animation Format (existing pipeline)
    this.registerFormat({
      id: 'movie-animation',
      name: 'Movie/Animation',
      description: 'Cinematic storytelling with character-driven narratives and visual consistency',
      icon: '🎬',
      durationRange: { min: 300, max: 1800 }, // 5-30 minutes
      aspectRatio: '16:9',
      applicableGenres: [
        'Drama',
        'Comedy',
        'Thriller',
        'Horror',
        'Sci-Fi',
        'Fantasy',
        'Romance',
        'Action',
        'Mystery',
        'Adventure'
      ],
      checkpointCount: 4,
      concurrencyLimit: 4,
      requiresResearch: false,
      supportedLanguages: ['ar', 'en']
    });

    // Educational Format
    this.registerFormat({
      id: 'educational',
      name: 'Educational Tutorial',
      description: 'Structured learning content with visual aids, diagrams, and clear explanations',
      icon: '📚',
      durationRange: { min: 300, max: 1200 }, // 5-20 minutes
      aspectRatio: '16:9',
      applicableGenres: [
        'Math',
        'Science',
        'Language',
        'Programming',
        'Business',
        'Art',
        'Music',
        'History',
        'Health',
        'Skills Training'
      ],
      checkpointCount: 3,
      concurrencyLimit: 4,
      requiresResearch: true,
      supportedLanguages: ['ar', 'en']
    });

    // Shorts/Reels Format
    this.registerFormat({
      id: 'shorts',
      name: 'Shorts/Reels',
      description: 'Vertical short-form content optimized for mobile with hook-first engagement',
      icon: '📱',
      durationRange: { min: 15, max: 60 }, // 15-60 seconds
      aspectRatio: '9:16',
      applicableGenres: [
        'Comedy',
        'Life Hack',
        'Quick Tip',
        'Trending',
        'Challenge',
        'Behind the Scenes',
        'Teaser',
        'Reaction'
      ],
      checkpointCount: 2,
      concurrencyLimit: 3,
      requiresResearch: false,
      supportedLanguages: ['ar', 'en']
    });

    // Documentary Format
    this.registerFormat({
      id: 'documentary',
      name: 'Documentary',
      description: 'Deeply researched long-form content with chapter structure and citations',
      icon: '🎥',
      durationRange: { min: 900, max: 3600 }, // 15-60 minutes
      aspectRatio: '16:9',
      applicableGenres: [
        'Investigative',
        'Historical',
        'Nature',
        'Biography',
        'Social Issues',
        'True Crime',
        'Cultural',
        'Scientific'
      ],
      checkpointCount: 4,
      concurrencyLimit: 5,
      requiresResearch: true,
      supportedLanguages: ['ar', 'en']
    });

    // Music Video Format
    this.registerFormat({
      id: 'music-video',
      name: 'Music Video',
      description: 'AI-generated music with beat-synchronized visuals and lyrics',
      icon: '🎵',
      durationRange: { min: 120, max: 480 }, // 2-8 minutes
      aspectRatio: '16:9',
      applicableGenres: [
        'Pop',
        'Rock',
        'Hip Hop',
        'Electronic',
        'Jazz',
        'Classical',
        'R&B',
        'Country',
        'Indie',
        'Ambient'
      ],
      checkpointCount: 3,
      concurrencyLimit: 4,
      requiresResearch: false,
      supportedLanguages: ['ar', 'en']
    });

    // News/Politics Format
    this.registerFormat({
      id: 'news-politics',
      name: 'News/Politics',
      description: 'Factual reporting with balanced perspectives and source citations',
      icon: '📰',
      durationRange: { min: 180, max: 900 }, // 3-15 minutes
      aspectRatio: '16:9',
      applicableGenres: [
        'Breaking News',
        'Political Analysis',
        'Election Coverage',
        'Policy Explainer',
        'International Affairs',
        'Local News',
        'Investigative Journalism'
      ],
      checkpointCount: 3,
      concurrencyLimit: 5,
      requiresResearch: true,
      supportedLanguages: ['ar', 'en']
    });
  }
}

// Export singleton instance
export const formatRegistry = new FormatRegistry();
````

## File: packages/shared/src/services/formatRouter.ts
````typescript
/**
 * Format Router
 * 
 * Centralized router that dispatches pipeline requests to format-specific pipelines.
 * Validates format IDs, loads pipeline configurations, and passes parameters to pipelines.
 * 
 * Requirements: 1.2, 10.1, 10.2, 10.3, 10.5
 */

import { formatRegistry } from './formatRegistry';
import { VideoFormat, FormatMetadata, CheckpointState } from '../types';
import type { IndexedDocument } from './documentParser';
import type { CheckpointSystem } from './checkpointSystem';
import type { ExecutionProgress } from './parallelExecutionEngine';

/**
 * Pipeline request interface
 */
export interface PipelineRequest {
  formatId: VideoFormat;
  idea: string;
  genre?: string;
  language: 'ar' | 'en';
  referenceDocuments?: IndexedDocument[];
  userId: string;
  projectId: string;
}

/**
 * Callbacks for pipeline UI integration
 */
export interface PipelineCallbacks {
  onCheckpointCreated?: (checkpoint: CheckpointState) => void;
  onCheckpointSystemCreated?: (system: CheckpointSystem) => void;
  onProgress?: (progress: ExecutionProgress) => void;
  onCancelRequested?: (cancelFn: () => void) => void;
}

/**
 * Pipeline result interface
 */
export interface PipelineResult {
  success: boolean;
  videoUrl?: string;
  error?: string;
  partialResults?: any;
  warnings?: string[];
}

/**
 * Format-specific pipeline interface
 */
export interface FormatPipeline {
  execute(request: PipelineRequest, callbacks?: PipelineCallbacks): Promise<PipelineResult>;
  validate?(request: PipelineRequest): Promise<boolean>;
  getMetadata(): FormatMetadata;
}

/**
 * Error codes for format routing
 */
export enum FormatRouterErrorCode {
  FORMAT_NOT_FOUND = 'FORMAT_NOT_FOUND',
  INVALID_FORMAT = 'INVALID_FORMAT',
  PIPELINE_NOT_FOUND = 'PIPELINE_NOT_FOUND',
  VALIDATION_FAILED = 'VALIDATION_FAILED',
  EXECUTION_FAILED = 'EXECUTION_FAILED',
  FORMAT_DEPRECATED = 'FORMAT_DEPRECATED'
}

/**
 * Format Router error class
 */
export class FormatRouterError extends Error {
  constructor(
    public code: FormatRouterErrorCode,
    message: string,
    public details?: any
  ) {
    super(message);
    this.name = 'FormatRouterError';
  }
}

/**
 * Format Router class
 * 
 * Dispatches pipeline requests to format-specific implementations
 */
export class FormatRouter {
  private pipelines: Map<VideoFormat, FormatPipeline> = new Map();

  /**
   * Register a format-specific pipeline
   * @param formatId Format identifier
   * @param pipeline Pipeline implementation
   */
  registerPipeline(formatId: VideoFormat, pipeline: FormatPipeline): void {
    this.pipelines.set(formatId, pipeline);
  }

  /**
   * Validate format ID against registry
   * @param formatId Format identifier to validate
   * @returns True if format is valid
   * @throws FormatRouterError if format is invalid
   */
  validateFormat(formatId: string): boolean {
    if (!formatId || typeof formatId !== 'string') {
      throw new FormatRouterError(
        FormatRouterErrorCode.INVALID_FORMAT,
        'Format ID must be a non-empty string',
        { providedFormatId: formatId }
      );
    }

    if (!formatRegistry.isValidFormat(formatId)) {
      const availableFormats = formatRegistry.getAllFormats().map(f => f.id);
      throw new FormatRouterError(
        FormatRouterErrorCode.FORMAT_NOT_FOUND,
        `Format '${formatId}' not found in registry. Available formats: ${availableFormats.join(', ')}`,
        { 
          providedFormatId: formatId,
          availableFormats 
        }
      );
    }

    return true;
  }

  /**
   * Get format-specific pipeline
   * @param formatId Format identifier
   * @returns Format pipeline implementation
   * @throws FormatRouterError if pipeline not found
   */
  getFormatPipeline(formatId: VideoFormat): FormatPipeline {
    // First validate the format exists in registry
    this.validateFormat(formatId);

    // Then check if pipeline is registered
    const pipeline = this.pipelines.get(formatId);
    if (!pipeline) {
      throw new FormatRouterError(
        FormatRouterErrorCode.PIPELINE_NOT_FOUND,
        `Pipeline implementation not found for format '${formatId}'. The format exists in the registry but no pipeline has been registered yet.`,
        { 
          formatId,
          registeredPipelines: Array.from(this.pipelines.keys())
        }
      );
    }

    return pipeline;
  }

  /**
   * Dispatch pipeline request to appropriate format pipeline
   * @param request Pipeline request with format ID and parameters
   * @returns Pipeline execution result
   * @throws FormatRouterError if validation or execution fails
   */
  async dispatch(request: PipelineRequest, callbacks?: PipelineCallbacks): Promise<PipelineResult> {
    try {
      // Validate format ID
      this.validateFormat(request.formatId);

      // Get format metadata for parameter validation
      const formatMetadata = formatRegistry.getFormat(request.formatId);
      if (!formatMetadata) {
        throw new FormatRouterError(
          FormatRouterErrorCode.FORMAT_NOT_FOUND,
          `Format metadata not found for '${request.formatId}'`
        );
      }

      // Collect warnings (e.g., deprecation)
      const warnings: string[] = [];

      // Check for deprecation — still allow execution but warn
      if (formatMetadata.deprecated) {
        const msg = formatMetadata.deprecationMessage
          ? `Format '${request.formatId}' is deprecated: ${formatMetadata.deprecationMessage}`
          : `Format '${request.formatId}' is deprecated.`;
        warnings.push(msg);
      }

      // Validate language support
      if (!formatMetadata.supportedLanguages.includes(request.language)) {
        throw new FormatRouterError(
          FormatRouterErrorCode.VALIDATION_FAILED,
          `Language '${request.language}' is not supported for format '${request.formatId}'. Supported languages: ${formatMetadata.supportedLanguages.join(', ')}`,
          {
            formatId: request.formatId,
            providedLanguage: request.language,
            supportedLanguages: formatMetadata.supportedLanguages
          }
        );
      }

      // Validate genre if provided
      if (request.genre && !formatMetadata.applicableGenres.includes(request.genre)) {
        throw new FormatRouterError(
          FormatRouterErrorCode.VALIDATION_FAILED,
          `Genre '${request.genre}' is not applicable for format '${request.formatId}'. Applicable genres: ${formatMetadata.applicableGenres.join(', ')}`,
          {
            formatId: request.formatId,
            providedGenre: request.genre,
            applicableGenres: formatMetadata.applicableGenres
          }
        );
      }

      // Get pipeline implementation
      const pipeline = this.getFormatPipeline(request.formatId);

      // Run pipeline-specific validation if available
      if (pipeline.validate) {
        const isValid = await pipeline.validate(request);
        if (!isValid) {
          throw new FormatRouterError(
            FormatRouterErrorCode.VALIDATION_FAILED,
            `Pipeline validation failed for format '${request.formatId}'`,
            { request }
          );
        }
      }

      // Execute pipeline with format-specific parameters
      const result = await pipeline.execute(request, callbacks);

      // Attach warnings to result
      if (warnings.length > 0) {
        result.warnings = [...(result.warnings || []), ...warnings];
      }

      return result;

    } catch (error) {
      // Re-throw FormatRouterError as-is
      if (error instanceof FormatRouterError) {
        throw error;
      }

      // Wrap other errors
      throw new FormatRouterError(
        FormatRouterErrorCode.EXECUTION_FAILED,
        `Pipeline execution failed: ${error instanceof Error ? error.message : String(error)}`,
        { 
          originalError: error,
          request 
        }
      );
    }
  }

  /**
   * Get all registered pipeline format IDs
   * @returns Array of format IDs with registered pipelines
   */
  getRegisteredPipelines(): VideoFormat[] {
    return Array.from(this.pipelines.keys());
  }

  /**
   * Check if a pipeline is registered for a format
   * @param formatId Format identifier
   * @returns True if pipeline is registered
   */
  hasPipeline(formatId: VideoFormat): boolean {
    return this.pipelines.has(formatId);
  }
}

// Export singleton instance
export const formatRouter = new FormatRouter();
````

## File: packages/shared/src/services/formatValidation.ts
````typescript
/**
 * Format Validation Service
 *
 * Provides validation for:
 * - Format configuration consistency (Property 1, Requirement 1.2, 3.1-9.1, 25.2)
 * - Genre pipeline invariance (Property 46, Requirements 12.5, 23.3)
 *
 * All functions are React-free and pure.
 */

import type { VideoFormat, FormatMetadata } from '../types';
import { formatRegistry } from './formatRegistry';

// ============================================================================
// Format Configuration Consistency (Property 1)
// ============================================================================

export interface FormatComplianceResult {
  valid: boolean;
  violations: FormatViolation[];
}

export interface FormatViolation {
  field: string;
  expected: string;
  actual: string;
  message: string;
}

/**
 * Validate that pipeline assets conform to the format's metadata constraints.
 *
 * Checks duration range, aspect ratio, and checkpoint count against the
 * registered format metadata.
 *
 * @param formatId        - Video format identifier
 * @param assets          - Asset metadata to validate
 * @returns Compliance result with violations if any
 */
export function validateFormatCompliance(
  formatId: VideoFormat,
  assets: {
    durationSeconds?: number;
    aspectRatio?: string;
    checkpointCount?: number;
    concurrentTasks?: number;
  },
): FormatComplianceResult {
  const meta = formatRegistry.getFormat(formatId);
  if (!meta) {
    return {
      valid: false,
      violations: [
        {
          field: 'formatId',
          expected: 'registered format',
          actual: formatId,
          message: `Format '${formatId}' not found in registry`,
        },
      ],
    };
  }

  const violations: FormatViolation[] = [];

  // Duration range check
  if (assets.durationSeconds != null) {
    if (assets.durationSeconds < meta.durationRange.min) {
      violations.push({
        field: 'duration',
        expected: `>= ${meta.durationRange.min}s`,
        actual: `${assets.durationSeconds}s`,
        message: `Duration ${assets.durationSeconds}s is below minimum ${meta.durationRange.min}s for ${meta.name}`,
      });
    }
    if (assets.durationSeconds > meta.durationRange.max) {
      violations.push({
        field: 'duration',
        expected: `<= ${meta.durationRange.max}s`,
        actual: `${assets.durationSeconds}s`,
        message: `Duration ${assets.durationSeconds}s exceeds maximum ${meta.durationRange.max}s for ${meta.name}`,
      });
    }
  }

  // Aspect ratio check
  if (assets.aspectRatio != null && assets.aspectRatio !== meta.aspectRatio) {
    violations.push({
      field: 'aspectRatio',
      expected: meta.aspectRatio,
      actual: assets.aspectRatio,
      message: `Aspect ratio '${assets.aspectRatio}' does not match expected '${meta.aspectRatio}' for ${meta.name}`,
    });
  }

  // Checkpoint count check
  if (assets.checkpointCount != null && assets.checkpointCount > meta.checkpointCount) {
    violations.push({
      field: 'checkpointCount',
      expected: `<= ${meta.checkpointCount}`,
      actual: `${assets.checkpointCount}`,
      message: `Checkpoint count ${assets.checkpointCount} exceeds maximum ${meta.checkpointCount} for ${meta.name}`,
    });
  }

  // Concurrency limit check
  if (assets.concurrentTasks != null && assets.concurrentTasks > meta.concurrencyLimit) {
    violations.push({
      field: 'concurrencyLimit',
      expected: `<= ${meta.concurrencyLimit}`,
      actual: `${assets.concurrentTasks}`,
      message: `Concurrent tasks ${assets.concurrentTasks} exceeds limit ${meta.concurrencyLimit} for ${meta.name}`,
    });
  }

  return {
    valid: violations.length === 0,
    violations,
  };
}

// ============================================================================
// Genre Pipeline Invariance (Property 46)
// ============================================================================

/**
 * Pipeline phase descriptor used to compare pipeline structures.
 * Genre should only affect the `styleParams` — never the phase list itself.
 */
export interface PipelinePhaseDescriptor {
  id: string;
  name: string;
  order: number;
  parallel: boolean;
}

/**
 * Extract the structural pipeline phases for a format.
 * This returns the phase sequence that must remain invariant regardless of genre.
 *
 * For each format, the pipeline structure is fixed:
 */
export function getFormatPipelineStructure(formatId: VideoFormat): PipelinePhaseDescriptor[] {
  switch (formatId) {
    case 'youtube-narrator':
      return [
        { id: 'research', name: 'Research', order: 1, parallel: true },
        { id: 'script', name: 'Script Generation', order: 2, parallel: false },
        { id: 'visual', name: 'Visual Generation', order: 3, parallel: true },
        { id: 'audio', name: 'Audio Generation', order: 4, parallel: false },
        { id: 'assembly', name: 'Assembly', order: 5, parallel: false },
      ];
    case 'advertisement':
      return [
        { id: 'script', name: 'Script Generation', order: 1, parallel: false },
        { id: 'visual', name: 'Visual Generation', order: 2, parallel: true },
        { id: 'audio', name: 'Audio Generation', order: 3, parallel: false },
        { id: 'assembly', name: 'Assembly', order: 4, parallel: false },
      ];
    case 'movie-animation':
      return [
        { id: 'breakdown', name: 'Story Breakdown', order: 1, parallel: false },
        { id: 'screenplay', name: 'Screenplay', order: 2, parallel: false },
        { id: 'characters', name: 'Character Design', order: 3, parallel: false },
        { id: 'visual', name: 'Visual Generation', order: 4, parallel: true },
        { id: 'audio', name: 'Audio Generation', order: 5, parallel: false },
        { id: 'assembly', name: 'Assembly', order: 6, parallel: false },
      ];
    case 'educational':
      return [
        { id: 'script', name: 'Script with Objectives', order: 1, parallel: false },
        { id: 'visual', name: 'Visuals with Overlays', order: 2, parallel: true },
        { id: 'audio', name: 'Audio Generation', order: 3, parallel: false },
        { id: 'assembly', name: 'Assembly', order: 4, parallel: false },
      ];
    case 'shorts':
      return [
        { id: 'script', name: 'Hook-First Script', order: 1, parallel: false },
        { id: 'visual', name: 'Visual Generation', order: 2, parallel: true },
        { id: 'audio', name: 'Audio Generation', order: 3, parallel: false },
        { id: 'assembly', name: 'Assembly', order: 4, parallel: false },
      ];
    case 'documentary':
      return [
        { id: 'research', name: 'Deep Research', order: 1, parallel: true },
        { id: 'script', name: 'Chapter Script', order: 2, parallel: false },
        { id: 'visual', name: 'Visual Generation', order: 3, parallel: true },
        { id: 'audio', name: 'Audio Generation', order: 4, parallel: false },
        { id: 'assembly', name: 'Assembly', order: 5, parallel: false },
      ];
    case 'music-video':
      return [
        { id: 'lyrics', name: 'Lyrics Generation', order: 1, parallel: false },
        { id: 'music', name: 'Music Generation', order: 2, parallel: false },
        { id: 'visual', name: 'Beat-Synced Visuals', order: 3, parallel: true },
        { id: 'assembly', name: 'Assembly', order: 4, parallel: false },
      ];
    case 'news-politics':
      return [
        { id: 'research', name: 'Multi-Source Research', order: 1, parallel: true },
        { id: 'script', name: 'Balanced Script', order: 2, parallel: false },
        { id: 'visual', name: 'News Graphics', order: 3, parallel: true },
        { id: 'audio', name: 'Audio Generation', order: 4, parallel: false },
        { id: 'assembly', name: 'Assembly', order: 5, parallel: false },
      ];
    default:
      return [];
  }
}

/**
 * Validate that genre selection does not change the pipeline phase structure.
 *
 * This compares two pipeline structures (one without genre, one with genre)
 * and asserts they are identical. Genre must only affect style parameters
 * passed to services, never the pipeline architecture.
 *
 * Property 46: Genre Pipeline Invariance
 * Requirements: 12.5, 23.3
 *
 * @param formatId - Video format identifier
 * @param genre    - Genre to validate
 * @returns Whether the pipeline structure remains invariant
 */
export function validateGenrePipelineInvariance(
  formatId: VideoFormat,
  genre: string,
): { invariant: boolean; message?: string } {
  const meta = formatRegistry.getFormat(formatId);
  if (!meta) {
    return { invariant: false, message: `Format '${formatId}' not found in registry` };
  }

  // Validate genre is applicable
  if (!meta.applicableGenres.includes(genre)) {
    return {
      invariant: false,
      message: `Genre '${genre}' is not applicable for format '${formatId}'`,
    };
  }

  // The pipeline structure is static per format — genre doesn't change it.
  // This function verifies the structural definition exists and is non-empty.
  const structure = getFormatPipelineStructure(formatId);
  if (structure.length === 0) {
    return { invariant: false, message: `No pipeline structure defined for format '${formatId}'` };
  }

  // Verify phase ordering is consistent (monotonically increasing)
  for (let i = 1; i < structure.length; i++) {
    if (structure[i]!.order <= structure[i - 1]!.order) {
      return {
        invariant: false,
        message: `Pipeline phase ordering is inconsistent at phase '${structure[i]!.id}'`,
      };
    }
  }

  return { invariant: true };
}

/**
 * Extract only the style parameters that genre affects.
 * This is the approved set of parameters that genre may modify.
 *
 * @param formatId - Video format identifier
 * @param genre    - Selected genre
 * @returns Style parameters derived from genre
 */
export function getGenreStyleParams(
  formatId: VideoFormat,
  genre: string,
): {
  tone: string;
  visualMood: string;
  colorPalette: string;
  pacing: string;
} {
  // Genre only influences these style parameters — never pipeline structure
  const genreLower = genre.toLowerCase();

  // Derive tone from genre
  const toneMap: Record<string, string> = {
    drama: 'dramatic',
    comedy: 'lighthearted',
    thriller: 'suspenseful',
    horror: 'dark',
    'sci-fi': 'futuristic',
    fantasy: 'mystical',
    romance: 'warm',
    action: 'intense',
    mystery: 'intriguing',
    documentary: 'authoritative',
    investigative: 'serious',
    'product launch': 'exciting',
    'brand story': 'inspirational',
  };

  const moodMap: Record<string, string> = {
    drama: 'moody, cinematic',
    comedy: 'bright, colorful',
    thriller: 'dark, high-contrast',
    horror: 'dark, desaturated',
    'sci-fi': 'neon, futuristic',
    fantasy: 'ethereal, vibrant',
    romance: 'warm tones, soft light',
    action: 'dynamic, saturated',
    mystery: 'shadowy, muted',
  };

  const pacingMap: Record<string, string> = {
    drama: 'measured',
    comedy: 'upbeat',
    thriller: 'fast',
    horror: 'slow-build',
    action: 'rapid',
    documentary: 'steady',
    shorts: 'rapid-fire',
  };

  return {
    tone: toneMap[genreLower] ?? 'neutral',
    visualMood: moodMap[genreLower] ?? 'balanced',
    colorPalette: 'default',
    pacing: pacingMap[genreLower] ?? 'moderate',
  };
}
````

## File: packages/shared/src/services/freesoundService.ts
````typescript
/**
 * Freesound API Service
 *
 * Integrates with Freesound.org API to fetch real ambient sounds.
 * https://freesound.org/docs/api/
 *
 * QUALITY IMPROVEMENTS:
 * - Smart query building with synonyms and related terms
 * - Multi-tier fallback search strategy
 * - Quality scoring based on rating, downloads, and duration fit
 * - Minimum quality thresholds (rating >= 3.0, downloads >= 10)
 * - Better duration matching for scene requirements
 * - Expanded ambient category library
 */

// --- Configuration ---
// Access Vite environment variables (defined in vite-env.d.ts)
const getFreesoundApiKey = (): string => {
  // Try Vite's import.meta.env first (browser)
  if (typeof window !== "undefined") {
    // @ts-ignore - Vite injects this at build time
    const viteEnv = (import.meta as any).env;
    if (viteEnv?.VITE_FREESOUND_API_KEY) {
      return viteEnv.VITE_FREESOUND_API_KEY;
    }
  }
  // Fallback to process.env (Node.js/SSR)
  return process.env.VITE_FREESOUND_API_KEY || "";
};

const FREESOUND_API_KEY = getFreesoundApiKey();
const FREESOUND_API_BASE = "https://freesound.org/apiv2";

// Quality thresholds
const MIN_RATING = 3.0;        // Minimum average rating (out of 5)
const MIN_DOWNLOADS = 10;      // Minimum downloads for quality assurance
const IDEAL_DURATION_MIN = 8;  // Ideal minimum duration for ambient loops
const IDEAL_DURATION_MAX = 120; // Ideal maximum duration

// Debug log
const isBrowser = typeof window !== "undefined";
if (isBrowser) {
  console.log(`[Freesound] API Key configured: ${FREESOUND_API_KEY ? "YES" : "NO"}`);
}

// --- Types ---

export interface FreesoundSound {
  id: number;
  name: string;
  description: string;
  tags: string[];
  duration: number;
  url: string;
  previews: {
    "preview-hq-mp3": string;
    "preview-lq-mp3": string;
    "preview-hq-ogg": string;
    "preview-lq-ogg": string;
  };
  images: {
    waveform_l: string;
    waveform_m: string;
    spectral_l: string;
    spectral_m: string;
  };
  username: string;
  license: string;
  avg_rating: number;
  num_downloads: number;
}

export interface FreesoundSearchResult {
  count: number;
  next: string | null;
  previous: string | null;
  results: FreesoundSound[];
}

export interface FreesoundSearchOptions {
  query: string;
  filter?: string;
  sort?: "score" | "duration_desc" | "duration_asc" | "created_desc" | "created_asc" | "downloads_desc" | "downloads_asc" | "rating_desc" | "rating_asc";
  pageSize?: number;
  fields?: string[];
  minDuration?: number;
  maxDuration?: number;
}

// --- API Functions ---

/**
 * Check if Freesound API is configured
 */
export function isFreesoundConfigured(): boolean {
  return !!FREESOUND_API_KEY;
}

/**
 * Search for sounds on Freesound
 */
export async function searchSounds(options: FreesoundSearchOptions): Promise<FreesoundSearchResult> {
  if (!FREESOUND_API_KEY) {
    throw new Error("Freesound API key not configured. Add VITE_FREESOUND_API_KEY to .env.local");
  }

  const {
    query,
    filter,
    sort = "rating_desc",
    pageSize = 5,
    fields = ["id", "name", "description", "tags", "duration", "url", "previews", "images", "username", "license", "avg_rating", "num_downloads"],
    minDuration,
    maxDuration,
  } = options;

  // Build filter string
  const filterParts: string[] = [];
  if (filter) filterParts.push(filter);
  if (minDuration !== undefined) filterParts.push(`duration:[${minDuration} TO *]`);
  if (maxDuration !== undefined) filterParts.push(`duration:[* TO ${maxDuration}]`);
  
  const params = new URLSearchParams({
    query,
    token: FREESOUND_API_KEY,
    sort,
    page_size: String(pageSize),
    fields: fields.join(","),
  });

  if (filterParts.length > 0) {
    params.set("filter", filterParts.join(" AND "));
  }

  const url = `${FREESOUND_API_BASE}/search/text/?${params.toString()}`;
  
  console.log(`[Freesound] Searching: "${query}"`);

  const response = await fetch(url);
  
  if (!response.ok) {
    const error = await response.text();
    console.error("[Freesound] Search failed:", error);
    throw new Error(`Freesound search failed: ${response.status}`);
  }

  const data = await response.json();
  console.log(`[Freesound] Found ${data.count} results for "${query}"`);
  
  return data;
}

/**
 * Get a specific sound by ID
 */
export async function getSound(soundId: number): Promise<FreesoundSound> {
  if (!FREESOUND_API_KEY) {
    throw new Error("Freesound API key not configured");
  }

  const fields = ["id", "name", "description", "tags", "duration", "url", "previews", "images", "username", "license", "avg_rating", "num_downloads"];
  
  const params = new URLSearchParams({
    token: FREESOUND_API_KEY,
    fields: fields.join(","),
  });

  const url = `${FREESOUND_API_BASE}/sounds/${soundId}/?${params.toString()}`;
  
  const response = await fetch(url);
  
  if (!response.ok) {
    throw new Error(`Failed to get sound ${soundId}: ${response.status}`);
  }

  return response.json();
}

// --- Quality Scoring ---

/**
 * Calculate quality score for a sound based on multiple factors.
 * Score range: 0-100
 */
function calculateQualityScore(
  sound: FreesoundSound,
  targetDuration?: number
): number {
  let score = 0;

  // Rating component (0-40 points)
  // Rating is 0-5, normalize to 0-40
  const ratingScore = (sound.avg_rating / 5) * 40;
  score += ratingScore;

  // Download popularity component (0-30 points)
  // Log scale for downloads (10 = 10pts, 100 = 20pts, 1000+ = 30pts)
  const downloadScore = Math.min(30, Math.log10(Math.max(1, sound.num_downloads)) * 10);
  score += downloadScore;

  // Duration fit component (0-30 points)
  if (targetDuration) {
    // Perfect match = 30 points, decreasing as it deviates
    const durationRatio = sound.duration / targetDuration;
    if (durationRatio >= 0.8 && durationRatio <= 2.0) {
      // Within ideal range
      score += 30;
    } else if (durationRatio >= 0.5 && durationRatio <= 3.0) {
      // Acceptable range
      score += 20;
    } else if (durationRatio >= 0.3) {
      // Usable
      score += 10;
    }
  } else {
    // No target duration, prefer sounds in ideal range
    if (sound.duration >= IDEAL_DURATION_MIN && sound.duration <= IDEAL_DURATION_MAX) {
      score += 30;
    } else if (sound.duration >= 5) {
      score += 15;
    }
  }

  return Math.round(score);
}

/**
 * Filter sounds by minimum quality standards
 */
function filterByQuality(sounds: FreesoundSound[], strict: boolean = true): FreesoundSound[] {
  return sounds.filter((sound) => {
    // Must have a preview URL
    if (!sound.previews?.["preview-hq-mp3"] && !sound.previews?.["preview-lq-mp3"]) {
      return false;
    }

    if (strict) {
      // Strict quality requirements
      if (sound.avg_rating < MIN_RATING) return false;
      if (sound.num_downloads < MIN_DOWNLOADS) return false;
    } else {
      // Relaxed requirements for fallback
      if (sound.avg_rating < 2.0) return false;
      if (sound.num_downloads < 3) return false;
    }

    return true;
  });
}

/**
 * Sort sounds by quality score
 */
function sortByQuality(sounds: FreesoundSound[], targetDuration?: number): FreesoundSound[] {
  return [...sounds].sort((a, b) => {
    const scoreA = calculateQualityScore(a, targetDuration);
    const scoreB = calculateQualityScore(b, targetDuration);
    return scoreB - scoreA;
  });
}

// --- Synonym and Related Terms ---

/**
 * Expand search terms with synonyms and related words
 */
const TERM_EXPANSIONS: Record<string, string[]> = {
  // Nature
  forest: ["woods", "jungle", "trees", "woodland"],
  ocean: ["sea", "waves", "beach", "coastal"],
  rain: ["rainfall", "rainy", "precipitation", "drizzle"],
  wind: ["breeze", "gust", "windy", "blowing"],
  thunder: ["thunderstorm", "storm", "lightning"],
  desert: ["sand", "arid", "sahara", "dunes"],
  river: ["stream", "creek", "water", "flowing"],
  birds: ["birdsong", "chirping", "songbirds", "avian"],
  night: ["nocturnal", "nighttime", "evening", "dark"],

  // Urban
  city: ["urban", "downtown", "metropolitan", "street"],
  traffic: ["cars", "vehicles", "highway", "road"],
  cafe: ["coffee", "restaurant", "bistro", "coffeehouse"],
  crowd: ["people", "chatter", "murmur", "voices"],

  // Mood/Atmosphere
  ambient: ["atmosphere", "atmospheric", "background", "soundscape"],
  eerie: ["creepy", "spooky", "haunting", "unsettling"],
  tension: ["suspense", "tense", "dramatic", "ominous"],
  peaceful: ["calm", "relaxing", "serene", "tranquil"],
  epic: ["cinematic", "dramatic", "orchestral", "grand"],
  mystical: ["magical", "ethereal", "fantasy", "enchanted"],
  hopeful: ["uplifting", "positive", "inspiring", "warm"],

  // Technical
  drone: ["pad", "sustained", "tone", "hum"],
  loop: ["loopable", "seamless", "repeating"],
  transition: ["swoosh", "sweep", "pass"],
};

/**
 * Build enhanced search query with synonyms
 */
function buildEnhancedQuery(baseQuery: string, includeSynonyms: boolean = true): string {
  if (!includeSynonyms) return baseQuery;

  const words = baseQuery.toLowerCase().split(/\s+/);
  const expandedTerms: string[] = [];

  for (const word of words) {
    expandedTerms.push(word);
    const synonyms = TERM_EXPANSIONS[word];
    if (synonyms && synonyms.length > 0 && synonyms[0]) {
      // Add first synonym only to avoid overly broad searches
      expandedTerms.push(synonyms[0]);
    }
  }

  // Remove duplicates and join
  return [...new Set(expandedTerms)].join(" ");
}

// --- Ambient Sound Search Queries ---

/**
 * Pre-defined search queries for different ambient categories
 * Enhanced with synonyms, better filters, and quality preferences
 */
export const AMBIENT_SEARCH_QUERIES: Record<string, {
  query: string;
  altQueries?: string[];  // Alternative queries for fallback
  filter?: string;
  minDuration?: number;
  maxDuration?: number;
  targetDuration?: number;  // Ideal duration for quality scoring
}> = {
  // ============ NATURE ============
  "desert-wind": {
    query: "desert wind sand ambient",
    altQueries: ["wind sand dunes", "sahara wind", "arid wind ambience"],
    filter: "tag:wind OR tag:desert",
    minDuration: 10,
    targetDuration: 30,
  },
  "desert-night": {
    query: "desert night ambient quiet crickets",
    altQueries: ["night desert silence", "arid night ambience", "quiet night outdoors"],
    minDuration: 10,
    targetDuration: 30,
  },
  "ocean-waves": {
    query: "ocean waves beach ambient",
    altQueries: ["sea waves shore", "beach ocean surf", "coastal waves ambient"],
    filter: "tag:ocean OR tag:waves OR tag:beach",
    minDuration: 15,
    targetDuration: 60,
  },
  "forest-ambience": {
    query: "forest birds nature ambient",
    altQueries: ["woodland birds chirping", "nature forest soundscape", "jungle ambient birds"],
    filter: "tag:forest OR tag:nature OR tag:birds",
    minDuration: 15,
    targetDuration: 60,
  },
  "rain-gentle": {
    query: "rain gentle soft ambient",
    altQueries: ["light rain drops", "soft rainfall", "rain on window"],
    filter: "tag:rain",
    minDuration: 15,
    targetDuration: 60,
  },
  "rain-heavy": {
    query: "heavy rain downpour ambient",
    altQueries: ["rain storm heavy", "pouring rain", "monsoon rain"],
    filter: "tag:rain",
    minDuration: 15,
    targetDuration: 60,
  },
  "thunderstorm": {
    query: "thunderstorm rain thunder ambient",
    altQueries: ["thunder storm rain", "lightning thunder", "storm ambient"],
    filter: "tag:thunder OR tag:storm",
    minDuration: 15,
    targetDuration: 60,
  },
  "wind-howling": {
    query: "wind howling strong ambient",
    altQueries: ["strong wind gusts", "windy storm", "blowing wind"],
    filter: "tag:wind",
    minDuration: 10,
    targetDuration: 30,
  },
  "river-stream": {
    query: "river stream water flowing ambient",
    altQueries: ["creek water flow", "brook babbling", "stream nature"],
    filter: "tag:water OR tag:river",
    minDuration: 15,
    targetDuration: 60,
  },
  "fire-crackling": {
    query: "fire crackling campfire ambient",
    altQueries: ["fireplace crackling", "bonfire burning", "wood fire"],
    filter: "tag:fire",
    minDuration: 15,
    targetDuration: 60,
  },
  "night-crickets": {
    query: "night crickets insects ambient",
    altQueries: ["crickets chirping", "summer night insects", "cicadas night"],
    filter: "tag:night OR tag:crickets",
    minDuration: 15,
    targetDuration: 60,
  },

  // ============ URBAN ============
  "city-traffic": {
    query: "city traffic urban ambient",
    altQueries: ["street traffic cars", "urban road sounds", "downtown traffic"],
    filter: "tag:city OR tag:traffic OR tag:urban",
    minDuration: 15,
    targetDuration: 60,
  },
  "cafe-ambience": {
    query: "cafe coffee shop ambient",
    altQueries: ["coffee shop background", "restaurant ambience", "bistro chatter"],
    filter: "tag:cafe OR tag:restaurant OR tag:coffee",
    minDuration: 15,
    targetDuration: 60,
  },
  "marketplace": {
    query: "market bazaar crowd ambient",
    altQueries: ["busy market people", "street market chatter", "bazaar ambience"],
    minDuration: 15,
    targetDuration: 60,
  },
  "office-ambience": {
    query: "office ambient typing keyboard",
    altQueries: ["office background", "workplace ambient", "computer room"],
    minDuration: 10,
    targetDuration: 30,
  },
  "subway-train": {
    query: "subway train metro ambient",
    altQueries: ["underground train", "metro station", "train station ambient"],
    filter: "tag:train OR tag:subway",
    minDuration: 10,
    targetDuration: 30,
  },
  "airport": {
    query: "airport terminal ambient announcements",
    altQueries: ["airport ambience", "terminal crowd", "airport background"],
    minDuration: 15,
    targetDuration: 60,
  },

  // ============ SUPERNATURAL/MYSTERY ============
  "eerie-ambience": {
    query: "eerie horror ambient dark",
    altQueries: ["creepy atmosphere", "horror ambience", "scary dark ambient"],
    filter: "tag:horror OR tag:scary OR tag:eerie",
    minDuration: 15,
    targetDuration: 30,
  },
  "mystical-drone": {
    query: "mystical ethereal drone ambient",
    altQueries: ["magical atmosphere", "fantasy ambient", "enchanted drone"],
    filter: "tag:mystical OR tag:ethereal OR tag:fantasy",
    minDuration: 15,
    targetDuration: 60,
  },
  "whispers": {
    query: "whisper ghost eerie ambient",
    altQueries: ["ghostly whispers", "creepy whispers", "voices whisper"],
    minDuration: 3,
    maxDuration: 30,
    targetDuration: 10,
  },
  "heartbeat": {
    query: "heartbeat tension suspense",
    altQueries: ["heart beating", "pulse heartbeat", "heartbeat loop"],
    minDuration: 5,
    maxDuration: 60,
    targetDuration: 15,
  },
  "haunted-house": {
    query: "haunted house creaking ambient",
    altQueries: ["old house creaks", "spooky house", "creepy house sounds"],
    minDuration: 10,
    targetDuration: 30,
  },

  // ============ TRANSITIONS & SFX ============
  "whoosh-soft": {
    query: "whoosh soft transition swoosh",
    altQueries: ["soft swoosh", "gentle whoosh", "light transition"],
    maxDuration: 3,
    targetDuration: 1,
  },
  "whoosh-dramatic": {
    query: "whoosh cinematic dramatic",
    altQueries: ["epic whoosh", "dramatic swoosh", "big whoosh"],
    maxDuration: 5,
    targetDuration: 2,
  },
  "impact-deep": {
    query: "impact deep bass hit cinematic",
    altQueries: ["deep hit", "bass impact", "cinematic boom"],
    maxDuration: 5,
    targetDuration: 2,
  },
  "shimmer": {
    query: "shimmer sparkle magic",
    altQueries: ["magic sparkle", "fairy dust", "twinkle sound"],
    maxDuration: 5,
    targetDuration: 2,
  },
  "riser-tension": {
    query: "riser tension build cinematic",
    altQueries: ["tension riser", "build up sound", "suspense riser"],
    maxDuration: 15,
    targetDuration: 5,
  },

  // ============ MUSICAL DRONES & PADS ============
  "tension-drone": {
    query: "tension drone dark ambient",
    altQueries: ["suspense drone", "ominous pad", "dark atmosphere"],
    filter: "tag:tension OR tag:suspense OR tag:drone",
    minDuration: 15,
    targetDuration: 60,
  },
  "hopeful-pad": {
    query: "hopeful pad ambient warm",
    altQueries: ["uplifting pad", "positive ambient", "warm synth pad"],
    minDuration: 15,
    targetDuration: 60,
  },
  "sad-melancholy": {
    query: "sad melancholy ambient piano",
    altQueries: ["melancholic ambient", "sorrowful pad", "emotional ambient"],
    minDuration: 15,
    targetDuration: 60,
  },
  "epic-strings": {
    query: "epic strings cinematic orchestra",
    altQueries: ["dramatic strings", "orchestral epic", "cinematic orchestra"],
    minDuration: 10,
    targetDuration: 30,
  },
  "middle-eastern": {
    query: "middle eastern arabic ambient",
    altQueries: ["arabic music ambient", "oriental ambient", "persian atmosphere"],
    filter: "tag:arabic OR tag:oriental OR tag:middle-eastern",
    minDuration: 15,
    targetDuration: 60,
  },
  "asian-zen": {
    query: "asian zen meditation ambient",
    altQueries: ["japanese zen", "chinese ambient", "eastern meditation"],
    filter: "tag:asian OR tag:zen OR tag:meditation",
    minDuration: 15,
    targetDuration: 60,
  },
  "sci-fi-ambient": {
    query: "sci-fi ambient space futuristic",
    altQueries: ["space ambient", "futuristic drone", "science fiction atmosphere"],
    filter: "tag:sci-fi OR tag:space OR tag:futuristic",
    minDuration: 15,
    targetDuration: 60,
  },

  // ============ INDUSTRIAL & MECHANICAL ============
  "factory": {
    query: "factory industrial machinery ambient",
    altQueries: ["industrial ambient", "machinery sounds", "factory floor"],
    minDuration: 15,
    targetDuration: 60,
  },
  "clock-ticking": {
    query: "clock ticking mechanical",
    altQueries: ["ticking clock", "clockwork", "watch ticking"],
    minDuration: 5,
    maxDuration: 60,
    targetDuration: 15,
  },
};

/**
 * Search for ambient sound by category ID
 * Enhanced with multi-tier fallback and quality scoring
 */
export async function searchAmbientSound(
  categoryId: string,
  sceneDuration?: number
): Promise<FreesoundSound | null> {
  const searchConfig = AMBIENT_SEARCH_QUERIES[categoryId];

  if (!searchConfig) {
    console.warn(`[Freesound] No search config for category: ${categoryId}`);
    // Try a generic search with the category name
    return searchGenericAmbient(categoryId, sceneDuration);
  }

  const targetDuration = sceneDuration || searchConfig.targetDuration;

  try {
    // Tier 1: Primary query with enhanced terms
    const enhancedQuery = buildEnhancedQuery(searchConfig.query, true);
    console.log(`[Freesound] Searching "${categoryId}" with: "${enhancedQuery}"`);

    let result = await searchSounds({
      query: enhancedQuery,
      filter: searchConfig.filter,
      minDuration: searchConfig.minDuration,
      maxDuration: searchConfig.maxDuration,
      sort: "downloads_desc", // Start with popular sounds
      pageSize: 15,
    });

    // Apply quality filtering and scoring
    let qualitySounds = filterByQuality(result.results, true);
    qualitySounds = sortByQuality(qualitySounds, targetDuration);

    if (qualitySounds.length > 0) {
      const selected = qualitySounds[0];
      if (!selected) return null;
      const score = calculateQualityScore(selected, targetDuration);
      console.log(`[Freesound] ✓ Found "${selected.name}" (score: ${score}, ${selected.duration.toFixed(1)}s, ★${selected.avg_rating.toFixed(1)})`);
      return selected;
    }

    // Tier 2: Try alternative queries
    if (searchConfig.altQueries && searchConfig.altQueries.length > 0) {
      for (const altQuery of searchConfig.altQueries) {
        console.log(`[Freesound] Fallback query: "${altQuery}"`);

        result = await searchSounds({
          query: altQuery,
          minDuration: searchConfig.minDuration,
          maxDuration: searchConfig.maxDuration,
          sort: "rating_desc",
          pageSize: 10,
        });

        qualitySounds = filterByQuality(result.results, true);
        qualitySounds = sortByQuality(qualitySounds, targetDuration);

        if (qualitySounds.length > 0) {
          const selected = qualitySounds[0];
          if (!selected) continue;
          const score = calculateQualityScore(selected, targetDuration);
          console.log(`[Freesound] ✓ Found via alt query: "${selected.name}" (score: ${score})`);
          return selected;
        }
      }
    }

    // Tier 3: Simplified primary query (first 2 words)
    const simplifiedQuery = searchConfig.query.split(" ").slice(0, 2).join(" ");
    console.log(`[Freesound] Simplified query: "${simplifiedQuery}"`);

    result = await searchSounds({
      query: simplifiedQuery,
      minDuration: searchConfig.minDuration,
      maxDuration: searchConfig.maxDuration,
      sort: "rating_desc",
      pageSize: 10,
    });

    // Relax quality requirements for fallback
    qualitySounds = filterByQuality(result.results, false);
    qualitySounds = sortByQuality(qualitySounds, targetDuration);

    if (qualitySounds.length > 0) {
      const selected = qualitySounds[0];
      if (!selected) return null;
      console.log(`[Freesound] ✓ Found via simplified: "${selected.name}"`);
      return selected;
    }

    // Tier 4: Just the category name
    const categoryQuery = categoryId.replace(/-/g, " ");
    console.log(`[Freesound] Final fallback: "${categoryQuery}"`);

    result = await searchSounds({
      query: categoryQuery,
      sort: "downloads_desc",
      pageSize: 10,
    });

    qualitySounds = filterByQuality(result.results, false);
    if (qualitySounds.length > 0) {
      const selected = qualitySounds[0];
      if (!selected) return null;
      console.log(`[Freesound] ✓ Found via category name: "${selected.name}"`);
      return selected;
    }

    console.warn(`[Freesound] ✗ No results for "${categoryId}" (all fallbacks exhausted)`);
    return null;
  } catch (error) {
    console.error(`[Freesound] Error searching for ${categoryId}:`, error);
    return null;
  }
}

/**
 * Generic ambient search for categories not in the predefined list
 */
async function searchGenericAmbient(
  query: string,
  targetDuration?: number
): Promise<FreesoundSound | null> {
  const cleanQuery = query.replace(/-/g, " ").toLowerCase();
  console.log(`[Freesound] Generic search: "${cleanQuery}"`);

  try {
    // Try with "ambient" added
    let result = await searchSounds({
      query: `${cleanQuery} ambient`,
      sort: "downloads_desc",
      pageSize: 15,
      minDuration: 5,
    });

    let qualitySounds = filterByQuality(result.results, false);
    qualitySounds = sortByQuality(qualitySounds, targetDuration);

    if (qualitySounds.length > 0) {
      const selected = qualitySounds[0];
      return selected ?? null;
    }

    // Try without "ambient"
    result = await searchSounds({
      query: cleanQuery,
      sort: "downloads_desc",
      pageSize: 10,
      minDuration: 3,
    });

    qualitySounds = filterByQuality(result.results, false);
    if (qualitySounds.length > 0) {
      const selected = qualitySounds[0];
      return selected ?? null;
    }

    return null;
  } catch (error) {
    console.error(`[Freesound] Generic search failed:`, error);
    return null;
  }
}

/**
 * Get preview URL for a sound (HQ MP3)
 */
export function getPreviewUrl(sound: FreesoundSound): string {
  return sound.previews["preview-hq-mp3"] || sound.previews["preview-lq-mp3"];
}

/**
 * Cache for fetched sounds to avoid repeated API calls
 */
const soundCache = new Map<string, FreesoundSound>();

/**
 * Get ambient sound with caching
 */
export async function getAmbientSoundCached(categoryId: string): Promise<FreesoundSound | null> {
  // Check cache first
  if (soundCache.has(categoryId)) {
    const cached = soundCache.get(categoryId);
    return cached ?? null;
  }

  const sound = await searchAmbientSound(categoryId);
  
  if (sound) {
    soundCache.set(categoryId, sound);
  }

  return sound;
}

/**
 * Clear the sound cache
 */
export function clearSoundCache(): void {
  soundCache.clear();
}

/**
 * Preload sounds for a list of category IDs
 */
export async function preloadSounds(categoryIds: string[]): Promise<Map<string, FreesoundSound>> {
  const results = new Map<string, FreesoundSound>();
  
  // Fetch in parallel with rate limiting (max 3 concurrent)
  const chunks: string[][] = [];
  for (let i = 0; i < categoryIds.length; i += 3) {
    chunks.push(categoryIds.slice(i, i + 3));
  }

  for (const chunk of chunks) {
    const promises = chunk.map(async (id) => {
      const sound = await getAmbientSoundCached(id);
      if (sound) {
        results.set(id, sound);
      }
    });
    
    await Promise.all(promises);
    
    // Small delay between chunks to respect rate limits
    if (chunks.indexOf(chunk) < chunks.length - 1) {
      await new Promise(resolve => setTimeout(resolve, 200));
    }
  }

  return results;
}


/**
 * Test function to verify Freesound API is working.
 * Can be called from browser console: testFreesoundAPI()
 */
export async function testFreesoundAPI(): Promise<void> {
  console.log("=== Freesound API Test ===");
  console.log(`API Key configured: ${isFreesoundConfigured() ? "YES" : "NO"}`);
  
  if (!isFreesoundConfigured()) {
    console.error("❌ Freesound API key not found. Add VITE_FREESOUND_API_KEY to .env.local");
    return;
  }

  try {
    console.log("Testing search for 'desert wind ambient'...");
    const result = await searchSounds({
      query: "desert wind ambient",
      pageSize: 2,
    });
    
    console.log(`✅ Search successful! Found ${result.count} results`);
    
    if (result.results.length > 0) {
      const sound = result.results[0];
      if (!sound) {
        console.error("❌ First result is undefined");
        return;
      }
      console.log(`First result: "${sound.name}" (${sound.duration.toFixed(1)}s)`);
      console.log(`Preview URL: ${sound.previews["preview-hq-mp3"]}`);
      
      // Test playing the audio
      console.log("Testing audio playback...");
      const audio = new Audio(sound.previews["preview-hq-mp3"]);
      audio.volume = 0.3;
      await audio.play();
      console.log("✅ Audio playing! (will stop in 3 seconds)");
      
      setTimeout(() => {
        audio.pause();
        console.log("Audio stopped.");
      }, 3000);
    }
  } catch (error) {
    console.error("❌ Test failed:", error);
  }
}

// Expose to window for console testing
if (typeof window !== "undefined") {
  (window as any).testFreesoundAPI = testFreesoundAPI;
}
````

## File: packages/shared/src/services/geminiService.ts
````typescript
/**
 * Gemini Service
 * 
 * This module re-exports all functions from the split service modules
 * for backward compatibility. New code should import directly from
 * the specific service modules.
 * 
 * @module geminiService
 */

// Re-export from transcriptionService
export {
  fileToGenerativePart,
  inferAudioMimeType,
  transcribeAudio,
  transcribeAudioWithWordTiming,
} from "./transcriptionService";

// Re-export from promptService
export {
  // Types
  type PromptRefinementIntent,
  type PromptLintIssueCode,
  type PromptLintIssue,
  // Helper functions
  normalizeForSimilarity,
  countWords,
  jaccardSimilarity,
  lintPrompt,
  getPurposeGuidance,
  getPromptGenerationInstruction,
  // Main functions
  generatePromptsFromLyrics,
  generatePromptsFromStory,
  refineImagePrompt,
  generateMotionPrompt,
} from "./promptService";

// Re-export from imageService
export { generateImageFromPrompt } from "./imageService";

// Re-export from videoService
export { generateVideoFromPrompt } from "./videoService";

// Re-export from translationService
export { translateSubtitles } from "./translationService";

// Re-export VideoPurpose type from constants for backward compatibility
export { type VideoPurpose } from "../constants";
````

## File: packages/shared/src/services/imageService.ts
````typescript
/**
 * Image Service
 * Handles image generation functionality using Gemini AI.
 * 
 * Features:
 * - Character seed tracking for visual consistency
 * - Automatic prompt refinement and linting
 * - Support for both Imagen and Gemini image models
 */

import { ai, MODELS, withRetry } from "./shared/apiClient";
import { refineImagePrompt, compressPromptForGeneration } from "./promptService";
import {
  buildImageStyleGuide,
  serializeStyleGuideAsText,
  type ImageStyleGuide,
} from "./prompt/imageStyleGuide";
import { traceAsync } from "./tracing";
import { cloudAutosave } from "./cloudStorageService";
import { withAILogging } from "./aiLogService";

// --- Character Seed Registry ---
// Stores seeds for character consistency across scenes

interface CharacterSeed {
  characterKey: string; // Normalized character identifier
  seed: number;
  createdAt: number;
  usageCount: number;
}

const characterSeedRegistry: Map<string, CharacterSeed> = new Map();

/**
 * Normalize a character description to create a consistent key.
 * Extracts key features like "young woman, brown hair, blue dress"
 */
function normalizeCharacterKey(description: string): string {
  // Extract key visual descriptors in order (no sorting to preserve intent)
  // "tall young woman" and "young tall woman" should be different if intended
  return description
    .toLowerCase()
    .replace(/[^a-z0-9\s]/g, '')
    .split(/\s+/)
    .filter(word => word.length > 2)
    .slice(0, 10) // Take first 10 meaningful words
    // NOTE: Removed .sort() - sorting destroys word order which may be intentional
    // e.g., "young woman with brown hair" vs "brown-haired young woman" are now distinct
    .join('_');
}

/**
 * Generate a random seed value.
 */
function generateRandomSeed(): number {
  return Math.floor(Math.random() * 2147483647); // Max 32-bit int
}

/**
 * Get or create a seed for a character.
 * If the character has been seen before, returns the same seed.
 */
export function getCharacterSeed(characterDescription: string): number {
  const key = normalizeCharacterKey(characterDescription);

  const existing = characterSeedRegistry.get(key);
  if (existing) {
    existing.usageCount++;
    console.log(`[ImageService] Reusing seed ${existing.seed} for character: ${key} (usage #${existing.usageCount})`);
    return existing.seed;
  }

  const newSeed = generateRandomSeed();
  characterSeedRegistry.set(key, {
    characterKey: key,
    seed: newSeed,
    createdAt: Date.now(),
    usageCount: 1,
  });

  console.log(`[ImageService] Created new seed ${newSeed} for character: ${key}`);
  return newSeed;
}

/**
 * Clear all character seeds (e.g., when starting a new project).
 */
export function clearCharacterSeeds(): void {
  characterSeedRegistry.clear();
  console.log('[ImageService] Cleared all character seeds');
}

/**
 * Get all registered character seeds (for debugging/display).
 */
export function getCharacterSeedRegistry(): CharacterSeed[] {
  return Array.from(characterSeedRegistry.values());
}

/**
 * Check if the model is an Imagen model (requires generateImages API).
 */
function isImagenModel(model: string): boolean {
  return model.toLowerCase().includes("imagen");
}

/**
 * Generate image using Imagen API (generateImages method).
 * Used for imagen-3.0, imagen-4.0, etc.
 *
 * NOTE: The `seed` parameter is NOT supported by the Gemini API for imagen-4.0.
 * Character consistency is achieved by embedding detailed physical descriptions
 * in the prompt text instead (see getCharacterSeed → prompt-based approach).
 *
 * @param prompt - The image generation prompt
 * @param aspectRatio - Image aspect ratio
 * @param _seed - Deprecated: seed is not supported by Imagen API. Kept for signature compat.
 */
async function generateWithImagenAPI(
  prompt: string,
  aspectRatio: string,
  _seed?: number
): Promise<string> {
  console.log(`[ImageService] Using Imagen API with model: ${MODELS.IMAGE}`);

  // Build config — seed is NOT supported by imagen-4.0 via @google/genai SDK
  const config: Record<string, unknown> = {
    numberOfImages: 1,
    aspectRatio: aspectRatio,
    personGeneration: "allow_adult",
  };

  const response = await ai.models.generateImages({
    model: MODELS.IMAGE,
    prompt: prompt,
    // @ts-ignore - some config options may not be in types yet
    config,
  });

  // Check if we got generated images
  const img = response.generatedImages?.[0];
  if (img) {
    // Check if image was filtered
    if (img.raiFilteredReason) {
      console.warn(`[ImageService] Image was filtered: ${img.raiFilteredReason}`);
      throw new Error(`Image generation was filtered by safety system: ${img.raiFilteredReason}`);
    }

    // Get the image bytes
    if (img.image?.imageBytes) {
      return `data:image/png;base64,${img.image.imageBytes}`;
    }
  }

  throw new Error("No image data found in Imagen response");
}

/**
 * Generate image using Gemini API (generateContent method).
 * Used for gemini-2.5-flash-image, gemini-3-pro-preview, etc.
 */
async function generateWithGeminiAPI(prompt: string, aspectRatio: string): Promise<string> {
  console.log(`[ImageService] Using Gemini API with model: ${MODELS.IMAGE}`);

  const response = await ai.models.generateContent({
    model: MODELS.IMAGE,
    contents: { parts: [{ text: prompt }] },
    config: {
      // @ts-ignore
      imageConfig: { aspectRatio: aspectRatio },
    },
  });

  for (const part of response.candidates?.[0]?.content?.parts || []) {
    if (part.inlineData) {
      return `data:image/png;base64,${part.inlineData.data}`;
    }
  }

  throw new Error("No image data found in Gemini response");
}

/**
 * Transform a sketch into a detailed image (Storyboarder.ai-style sketch-to-image).
 * Uses the sketch as a composition guide while generating a detailed final image.
 * 
 * @param sketchBase64 - Base64-encoded sketch image (PNG/JPEG)
 * @param promptText - Description of what the final image should look like
 * @param style - Art style preset
 * @param aspectRatio - Output aspect ratio
 */
export const sketchToImage = async (
  sketchBase64: string,
  promptText: string,
  style: string = "Cinematic",
  aspectRatio: string = "16:9"
): Promise<string> => {
  console.log(`[ImageService] Sketch-to-image transformation with style: ${style}`);

  // Build a style guide for consistency, then prepend sketch-specific instructions
  const guide = buildImageStyleGuide({ scene: promptText, style });
  const guideText = serializeStyleGuideAsText(guide);

  const transformPrompt = `Transform this rough sketch into a detailed, polished ${style} image.
Maintain the exact composition, subject positions, and framing from the sketch.
Add professional lighting, textures, and details while preserving the sketch's layout.

${guideText}

Important: Keep the same composition and subject placement as the sketch.`;

  try {
    const response = await ai.models.generateContent({
      model: MODELS.IMAGE,
      contents: [
        {
          role: "user",
          parts: [
            { text: transformPrompt },
            {
              inlineData: {
                mimeType: sketchBase64.startsWith('data:image/png') ? 'image/png' : 'image/jpeg',
                data: sketchBase64.replace(/^data:image\/\w+;base64,/, ''),
              },
            },
          ],
        },
      ],
      config: {
        // @ts-ignore
        imageConfig: { aspectRatio },
      },
    });

    for (const part of response.candidates?.[0]?.content?.parts || []) {
      if (part.inlineData) {
        return `data:image/png;base64,${part.inlineData.data}`;
      }
    }

    throw new Error("No image data in sketch-to-image response");
  } catch (error) {
    console.error("[ImageService] Sketch-to-image failed:", error);
    throw error;
  }
};

/**
 * Generate an image using a style reference image (Storyboarder.ai-style custom art style).
 * The reference image defines the visual style, colors, and aesthetic.
 * 
 * @param styleReferenceBase64 - Base64-encoded style reference image
 * @param promptText - Description of the scene to generate
 * @param aspectRatio - Output aspect ratio
 */
export const generateWithStyleReference = async (
  styleReferenceBase64: string,
  promptText: string,
  aspectRatio: string = "16:9"
): Promise<string> => {
  console.log(`[ImageService] Generating with custom style reference`);

  // Build a style guide for the scene description portion
  const guide = buildImageStyleGuide({ scene: promptText });
  const guideText = serializeStyleGuideAsText(guide);

  const stylePrompt = `Generate a new image matching the artistic style, color palette, and visual aesthetic of the reference image.

${guideText}

Important instructions:
- Match the art style, brushwork, and texture of the reference
- Use the same color palette and lighting mood
- Apply the same level of detail and rendering style
- Create a NEW scene (not a copy of the reference)`;

  try {
    const response = await ai.models.generateContent({
      model: MODELS.IMAGE,
      contents: [
        {
          role: "user",
          parts: [
            { text: stylePrompt },
            {
              inlineData: {
                mimeType: styleReferenceBase64.startsWith('data:image/png') ? 'image/png' : 'image/jpeg',
                data: styleReferenceBase64.replace(/^data:image\/\w+;base64,/, ''),
              },
            },
          ],
        },
      ],
      config: {
        // @ts-ignore
        imageConfig: { aspectRatio },
      },
    });

    for (const part of response.candidates?.[0]?.content?.parts || []) {
      if (part.inlineData) {
        return `data:image/png;base64,${part.inlineData.data}`;
      }
    }

    throw new Error("No image data in style reference response");
  } catch (error) {
    console.error("[ImageService] Style reference generation failed:", error);
    throw error;
  }
};

/**
 * Generate an image from a prompt.
 * @param promptText - The prompt describing the image to generate
 * @param style - Art style preset (default: "Cinematic")
 * @param globalSubject - Subject to keep consistent across scenes (also used for seed tracking)
 * @param aspectRatio - Image aspect ratio (default: "16:9")
 * @param skipRefine - Skip AI refinement if prompt was already refined upstream
 * @param seed - Optional seed for reproducibility. If globalSubject is provided, a consistent seed is auto-generated.
 * @param sessionId - Optional session ID for cloud autosave
 * @param sceneIndex - Optional scene index for cloud autosave filename
 * @param prebuiltGuide - Optional pre-built ImageStyleGuide. When provided, skips both refinement and guide building to avoid double-wrapping.
 */
export const generateImageFromPrompt = traceAsync(
  async function generateImageFromPromptImpl(
    promptText: string,
    style: string = "Cinematic",
    globalSubject: string = "",
    aspectRatio: string = "16:9",
    skipRefine: boolean = false,
    seed?: number,
    sessionId?: string,
    sceneIndex?: number,
    prebuiltGuide?: ImageStyleGuide,
  ): Promise<string> {
    return withRetry(async () => {
      let finalPrompt: string;

      if (prebuiltGuide) {
        // Caller already built a guide — serialize directly (no refinement, no re-wrapping)
        finalPrompt = serializeStyleGuideAsText(prebuiltGuide);
      } else {
        // Run a lightweight lint + (optional) AI refinement before image generation.
        // Skip if already refined upstream (e.g., during bulk generation with cross-scene context).
        let refinedPrompt = promptText;

        if (!skipRefine) {
          const result = await refineImagePrompt({
            promptText,
            style,
            globalSubject,
            aspectRatio,
            intent: "auto",
            previousPrompts: [],
          });

          refinedPrompt = result.refinedPrompt;

          if (result.issues.length > 0) {
            console.log(
              `[prompt-lint] ${result.issues.map((i) => i.code).join(", ")} | style=${style} | aspectRatio=${aspectRatio}`,
            );
          }
        }

        // Build structured style guide and serialize as natural-language prompt
        const guide = buildImageStyleGuide({
          promptText: refinedPrompt,
          style,
          globalSubject,
        });
        finalPrompt = serializeStyleGuideAsText(guide);
      }

      // Compress long prompts to reduce instruction dilution (story-mode shots can exceed 200 words)
      finalPrompt = await compressPromptForGeneration(finalPrompt);

      // Determine seed: use provided seed, or auto-generate from globalSubject for consistency
      let effectiveSeed = seed;
      if (!effectiveSeed && globalSubject && globalSubject.trim().length > 0) {
        effectiveSeed = getCharacterSeed(globalSubject);
      }

      // Check if we're using an Imagen model (requires different API)
      let imageUrl: string;
      if (isImagenModel(MODELS.IMAGE)) {
        imageUrl = await withAILogging(
          sessionId,
          'image_gen',
          MODELS.IMAGE,
          finalPrompt,
          () => generateWithImagenAPI(finalPrompt, aspectRatio, effectiveSeed),
          () => `image generated (${aspectRatio})`,
        );
      } else {
        imageUrl = await withAILogging(
          sessionId,
          'image_gen',
          MODELS.IMAGE,
          finalPrompt,
          () => generateWithGeminiAPI(finalPrompt, aspectRatio),
          () => `image generated (${aspectRatio})`,
        );
      }

      // Cloud autosave trigger (fire-and-forget, non-blocking)
      if (sessionId && imageUrl) {
        cloudAutosave.saveImage(sessionId, imageUrl, sceneIndex ?? Date.now()).catch(err => {
          console.warn('[ImageService] Cloud autosave failed (non-fatal):', err);
        });
      }

      return imageUrl;
    });
  },
  "generateImageFromPrompt",
  {
    runType: "tool",
    metadata: { service: "imageService", operation: "imageGen" },
    tags: ["imagen", "image-generation"],
  }
);
````

## File: packages/shared/src/services/jsonExtractor.ts
````typescript
/**
 * JSON Extractor Service
 * 
 * Robust, multi-strategy JSON extraction system for parsing LLM responses.
 * Implements multiple parsing strategies with progressive fallback and
 * comprehensive error handling.
 * 
 * Feature: agent-director-json-parsing-fix
 * Requirements: 1.1, 1.2, 1.4, 1.5, 5.4, 5.5
 */

import {
  preprocessFormatCorrection,
  needsFormatCorrection,
  responsePatternLibrary,
  type FormatCorrectionResult
} from './promptFormatService';

// --- Enums and Interfaces ---

/**
 * Extraction methods used by the JSONExtractor.
 * Listed in order of preference/reliability.
 */
export enum ExtractionMethod {
  MARKDOWN_BLOCKS = 'markdown_blocks',
  REGEX_PATTERN = 'regex_pattern',
  BRACKET_MATCHING = 'bracket_matching',
  FALLBACK_TEXT = 'fallback_text'
}

/**
 * Result of a successful JSON extraction.
 */
export interface ExtractedJSON {
  data: unknown;
  method: ExtractionMethod;
  confidence: number;
}

/**
 * Detailed error information for failed extractions.
 * Requirements: 1.3, 2.1, 2.3, 2.4
 */
export interface ParseError {
  type: 'JSON_PARSE_ERROR' | 'VALIDATION_ERROR' | 'EXTRACTION_ERROR';
  message: string;
  originalContent: string;
  attemptedMethods: ExtractionMethod[];
  suggestions: string[];
  timestamp: string;
  contentLength: number;
  failureReasons: MethodFailure[];
}

/**
 * Details about why a specific extraction method failed.
 * Requirements: 2.1, 2.3
 */
export interface MethodFailure {
  method: ExtractionMethod;
  error: string;
  attemptedAt: string;
}

/**
 * Success tracking information for extraction methods.
 * Requirements: 2.5
 */
export interface ExtractionSuccess {
  method: ExtractionMethod;
  confidence: number;
  timestamp: string;
  retryCount: number;
  processingTimeMs: number;
}

/**
 * Metrics for tracking method effectiveness.
 * Requirements: 2.5
 */
export interface MethodMetrics {
  method: ExtractionMethod;
  successCount: number;
  failureCount: number;
  averageConfidence: number;
  lastUsed: string | null;
}

/**
 * Result of JSON validation.
 * Requirements: 3.1, 3.4
 */
export interface ValidationResult {
  isValid: boolean;
  errors: string[];
  warnings: string[];
  fixedData?: unknown;
  fieldErrors: FieldValidationError[];
  suggestions: string[];
}

/**
 * Detailed field-level validation error.
 * Requirements: 3.4
 */
export interface FieldValidationError {
  field: string;
  message: string;
  expectedType?: string;
  actualType?: string;
  suggestion?: string;
}

/**
 * Storyboard scene structure for validation.
 * Requirements: 3.1
 */
export interface StoryboardScene {
  scene?: number;
  prompt?: string;
  text?: string;
  mood?: string;
  timestamp?: string;
  confidence?: number;
  source?: 'llm' | 'fallback' | 'reconstructed';
}

/**
 * Storyboard structure for validation.
 * Requirements: 3.1
 */
export interface StoryboardData {
  prompts?: StoryboardScene[];
  scenes?: StoryboardScene[];
  visual_prompts?: StoryboardScene[];
  visualPrompts?: StoryboardScene[];
  storyboard?: StoryboardScene[];
  metadata?: Record<string, unknown>;
}

/**
 * Internal result type for extraction attempts.
 */
interface ExtractionAttempt {
  success: boolean;
  data?: unknown;
  error?: string;
}

/**
 * Logger interface for customizable logging.
 * Requirements: 1.3, 2.4
 */
export interface ExtractorLogger {
  info(message: string, data?: Record<string, unknown>): void;
  warn(message: string, data?: Record<string, unknown>): void;
  error(message: string, data?: Record<string, unknown>): void;
  debug(message: string, data?: Record<string, unknown>): void;
}

/**
 * Default console logger implementation.
 */
const defaultLogger: ExtractorLogger = {
  info: (message, data) => console.log(`[JSONExtractor] INFO: ${message}`, data || ''),
  warn: (message, data) => console.warn(`[JSONExtractor] WARN: ${message}`, data || ''),
  error: (message, data) => console.error(`[JSONExtractor] ERROR: ${message}`, data || ''),
  debug: (message, data) => console.debug(`[JSONExtractor] DEBUG: ${message}`, data || ''),
};

// --- JSONExtractor Class ---

/**
 * JSONExtractor provides robust JSON extraction from LLM responses.
 * 
 * It implements multiple parsing strategies:
 * 1. Markdown code block extraction
 * 2. Regex-based JSON object detection
 * 3. Bracket matching for malformed JSON
 * 4. Fallback text processing
 * 
 * Requirements: 1.1, 1.2, 1.3, 1.4, 1.5, 2.1, 2.3, 2.4, 2.5, 5.4, 5.5
 */
export class JSONExtractor {
  private attemptedMethods: ExtractionMethod[] = [];
  private lastError: string | null = null;
  private methodFailures: MethodFailure[] = [];
  private lastSuccess: ExtractionSuccess | null = null;
  private methodMetrics: Map<ExtractionMethod, MethodMetrics> = new Map();
  private logger: ExtractorLogger;
  private extractionStartTime: number = 0;
  private lastFormatCorrection: FormatCorrectionResult | null = null;

  constructor(logger?: ExtractorLogger) {
    this.logger = logger || defaultLogger;
    this.initializeMetrics();
  }

  /**
   * Initialize metrics for all extraction methods.
   * Requirements: 2.5
   */
  private initializeMetrics(): void {
    const methods = [
      ExtractionMethod.MARKDOWN_BLOCKS,
      ExtractionMethod.REGEX_PATTERN,
      ExtractionMethod.BRACKET_MATCHING,
      ExtractionMethod.FALLBACK_TEXT
    ];

    for (const method of methods) {
      this.methodMetrics.set(method, {
        method,
        successCount: 0,
        failureCount: 0,
        averageConfidence: 0,
        lastUsed: null
      });
    }
  }

  /**
   * Record a successful extraction for metrics tracking.
   * Requirements: 2.5
   */
  private recordSuccess(method: ExtractionMethod, confidence: number): void {
    if (!this.attemptedMethods.includes(method)) {
      this.attemptedMethods.push(method);
    }

    const metrics = this.methodMetrics.get(method);
    if (metrics) {
      const totalSuccesses = metrics.successCount + 1;
      metrics.averageConfidence = (metrics.averageConfidence * metrics.successCount + confidence) / totalSuccesses;
      metrics.successCount = totalSuccesses;
      metrics.lastUsed = new Date().toISOString();
    }

    this.lastSuccess = {
      method,
      confidence,
      timestamp: new Date().toISOString(),
      retryCount: this.attemptedMethods.length - 1,
      processingTimeMs: Date.now() - this.extractionStartTime
    };

    this.logger.info(`JSON extraction successful using ${method}`, {
      confidence,
      timeMs: this.lastSuccess.processingTimeMs
    });
  }

  /**
   * Record a failed extraction attempt for a specific method.
   * Requirements: 2.1, 2.3
   */
  private recordMethodFailure(method: ExtractionMethod, error: string): void {
    if (!this.attemptedMethods.includes(method)) {
      this.attemptedMethods.push(method);
    }

    const failure: MethodFailure = {
      method,
      error,
      attemptedAt: new Date().toISOString()
    };

    this.methodFailures.push(failure);
    this.lastError = error;

    const metrics = this.methodMetrics.get(method);
    if (metrics) {
      metrics.failureCount++;
      metrics.lastUsed = new Date().toISOString();
    }

    this.logger.warn(`JSON extraction failed using ${method}`, { error });
  }

  /**
   * Extract JSON from content using simplified robust strategies.
   * Native JSON mode in Gemini makes complex regex/bracket matching obsolete.
   * Enhanced with additional fallback strategies for edge cases.
   */
  async extractJSON(content: string): Promise<ExtractedJSON | null> {
    this.attemptedMethods = [];
    this.methodFailures = [];
    this.lastError = null;
    this.lastSuccess = null;
    this.extractionStartTime = Date.now();

    if (!content || typeof content !== 'string' || content.trim().length === 0) {
      return null;
    }

    // Strategy 1: Direct Parse (Most likely with native JSON mode)
    try {
      const sanitized = this.sanitizeJsonString(content);
      const parsed = JSON.parse(sanitized);
      this.recordSuccess(ExtractionMethod.REGEX_PATTERN, 1.0);
      return { data: parsed, method: ExtractionMethod.REGEX_PATTERN, confidence: 1.0 };
    } catch (e) {
      this.recordMethodFailure(ExtractionMethod.REGEX_PATTERN, String(e));
    }

    // Strategy 2: Markdown Code Blocks (```json ... ```)
    const jsonBlockMatch = content.match(/```json\s*([\s\S]*?)\s*```/i);
    if (jsonBlockMatch && jsonBlockMatch[1]) {
      try {
        const jsonStr = jsonBlockMatch[1].trim();
        const parsed = JSON.parse(this.sanitizeJsonString(jsonStr));
        this.recordSuccess(ExtractionMethod.MARKDOWN_BLOCKS, 0.95);
        return { data: parsed, method: ExtractionMethod.MARKDOWN_BLOCKS, confidence: 0.95 };
      } catch (e) {
        this.recordMethodFailure(ExtractionMethod.MARKDOWN_BLOCKS, String(e));
      }
    }

    // Strategy 3: Generic Code Blocks (``` ... ```)
    const codeBlockMatch = content.match(/```\s*([\s\S]*?)\s*```/);
    if (codeBlockMatch && codeBlockMatch[1]) {
      try {
        const jsonStr = codeBlockMatch[1].trim();
        const parsed = JSON.parse(this.sanitizeJsonString(jsonStr));
        this.recordSuccess(ExtractionMethod.MARKDOWN_BLOCKS, 0.9);
        return { data: parsed, method: ExtractionMethod.MARKDOWN_BLOCKS, confidence: 0.9 };
      } catch (e) {
        this.recordMethodFailure(ExtractionMethod.MARKDOWN_BLOCKS, String(e));
      }
    }

    // Strategy 4: First complete JSON object {...}
    const objMatch = content.match(/(\{[\s\S]*\})/);
    if (objMatch && objMatch[1]) {
      try {
        const parsed = JSON.parse(this.sanitizeJsonString(objMatch[1]));
        this.recordSuccess(ExtractionMethod.REGEX_PATTERN, 0.8);
        return { data: parsed, method: ExtractionMethod.REGEX_PATTERN, confidence: 0.8 };
      } catch (e) {
        this.recordMethodFailure(ExtractionMethod.REGEX_PATTERN, String(e));
      }
    }

    // Strategy 5: First complete JSON array [...]
    const arrMatch = content.match(/(\[[\s\S]*\])/);
    if (arrMatch && arrMatch[1]) {
      try {
        const parsed = JSON.parse(this.sanitizeJsonString(arrMatch[1]));
        this.recordSuccess(ExtractionMethod.REGEX_PATTERN, 0.75);
        return { data: parsed, method: ExtractionMethod.REGEX_PATTERN, confidence: 0.75 };
      } catch (e) {
        this.recordMethodFailure(ExtractionMethod.REGEX_PATTERN, String(e));
      }
    }

    // Strategy 6: Fix common JSON issues and retry
    try {
      const fixed = this.fixCommonJsonIssues(content);
      // Try to find JSON after fixing
      const fixedObjMatch = fixed.match(/(\{[\s\S]*\})/);
      const fixedArrMatch = fixed.match(/(\[[\s\S]*\])/);
      const toTry = fixedObjMatch?.[1] || fixedArrMatch?.[1];

      if (toTry) {
        const parsed = JSON.parse(toTry);
        this.recordSuccess(ExtractionMethod.BRACKET_MATCHING, 0.7);
        return { data: parsed, method: ExtractionMethod.BRACKET_MATCHING, confidence: 0.7 };
      }
    } catch (e) {
      this.recordMethodFailure(ExtractionMethod.BRACKET_MATCHING, String(e));
    }

    // Strategy 7: Strip everything before first [ or { and after last ] or }
    try {
      const cleaned = content
        .replace(/^[\s\S]*?(?=[\[{])/, '') // Remove everything before first [ or {
        .replace(/[\]}][^}\]]*$/, (match) => match[0] ?? "") // Keep only up to last ] or }
        .trim();

      if (cleaned.length > 0) {
        const parsed = JSON.parse(this.sanitizeJsonString(cleaned));
        this.recordSuccess(ExtractionMethod.FALLBACK_TEXT, 0.6);
        return { data: parsed, method: ExtractionMethod.FALLBACK_TEXT, confidence: 0.6 };
      }
    } catch (e) {
      this.recordMethodFailure(ExtractionMethod.FALLBACK_TEXT, String(e));
    }

    return null;
  }

  /**
   * Extract JSON and validate with a custom validator function.
   * Useful with Zod schemas or custom validation logic.
   *
   * @param content - Raw string response from LLM
   * @param validate - Validation function that throws on invalid data
   * @returns Validated data or null
   */
  async extractAndValidate<T>(
    content: string,
    validate: (data: unknown) => T
  ): Promise<{ data: T; method: ExtractionMethod; confidence: number } | null> {
    const extracted = await this.extractJSON(content);
    if (!extracted) return null;

    try {
      const validated = validate(extracted.data);
      return {
        data: validated,
        method: extracted.method,
        confidence: extracted.confidence,
      };
    } catch (error) {
      this.logger.warn('Validation failed after extraction', {
        error: error instanceof Error ? error.message : String(error),
      });
      return null;
    }
  }

  /**
   * Sanitize JSON string by handling common formatting issues.
   */
  sanitizeJsonString(jsonStr: string): string {
    return jsonStr
      .replace(/[\x00-\x1F\x7F]/g, (char) => {
        switch (char) {
          case '\n': return '\\n';
          case '\r': return '\\r';
          case '\t': return '\\t';
          default: return '';
        }
      })
      .trim();
  }

  /**
   * Fix common JSON formatting issues.
   */
  fixCommonJsonIssues(jsonStr: string): string {
    return this.sanitizeJsonString(jsonStr)
      .replace(/,(\s*[\]}])/g, '$1'); // Only keep trailing comma fix
  }

  /**
   * Get the list of attempted extraction methods.
   */
  getAttemptedMethods(): ExtractionMethod[] {
    return [...this.attemptedMethods];
  }

  /**
   * Get the last error message.
   */
  getLastError(): string | null {
    return this.lastError;
  }

  /**
   * Create a structured parse error with comprehensive diagnostic information.
   * Requirements: 1.3, 2.1, 2.3
   */
  createParseError(content: string): ParseError {
    return {
      type: 'EXTRACTION_ERROR',
      message: this.lastError || 'Unknown extraction error',
      originalContent: content,
      attemptedMethods: this.getAttemptedMethods(),
      suggestions: this.generateSuggestions(content),
      timestamp: new Date().toISOString(),
      contentLength: content?.length || 0,
      failureReasons: [...this.methodFailures]
    };
  }

  /**
   * Get the last successful extraction information.
   * Requirements: 2.5
   */
  getLastSuccess(): ExtractionSuccess | null {
    return this.lastSuccess;
  }

  /**
   * Get method failures from the last extraction attempt.
   * Requirements: 2.1, 2.3
   */
  getMethodFailures(): MethodFailure[] {
    return [...this.methodFailures];
  }

  /**
   * Get metrics for all extraction methods.
   * Requirements: 2.5
   */
  getMethodMetrics(): Map<ExtractionMethod, MethodMetrics> {
    return new Map(this.methodMetrics);
  }

  /**
   * Get metrics for a specific extraction method.
   * Requirements: 2.5
   */
  getMetricsForMethod(method: ExtractionMethod): MethodMetrics | undefined {
    return this.methodMetrics.get(method);
  }

  /**
   * Reset all metrics (useful for testing).
   */
  resetMetrics(): void {
    this.initializeMetrics();
  }

  /**
   * Get a summary of method effectiveness for optimization.
   * Requirements: 2.5
   */
  getMethodEffectivenessSummary(): {
    mostEffective: ExtractionMethod | null;
    leastEffective: ExtractionMethod | null;
    totalExtractions: number;
    overallSuccessRate: number;
  } {
    let mostEffective: ExtractionMethod | null = null;
    let leastEffective: ExtractionMethod | null = null;
    let highestSuccessRate = -1;
    let lowestSuccessRate = 2;
    let totalSuccesses = 0;
    let totalAttempts = 0;

    for (const [method, metrics] of this.methodMetrics) {
      const total = metrics.successCount + metrics.failureCount;
      if (total === 0) continue;

      totalSuccesses += metrics.successCount;
      totalAttempts += total;

      const successRate = metrics.successCount / total;

      if (successRate > highestSuccessRate) {
        highestSuccessRate = successRate;
        mostEffective = method;
      }

      if (successRate < lowestSuccessRate) {
        lowestSuccessRate = successRate;
        leastEffective = method;
      }
    }

    return {
      mostEffective,
      leastEffective,
      totalExtractions: totalAttempts,
      overallSuccessRate: totalAttempts > 0 ? totalSuccesses / totalAttempts : 0
    };
  }

  /**
   * Check if the last extraction required retries.
   * Requirements: 2.5
   */
  wasRetryRequired(): boolean {
    return this.lastSuccess !== null && this.lastSuccess.retryCount > 0;
  }

  /**
   * Get the number of retries from the last extraction.
   * Requirements: 2.5
   */
  getRetryCount(): number {
    return this.lastSuccess?.retryCount || 0;
  }

  /**
   * Generate suggestions for fixing JSON issues.
   */
  private generateSuggestions(content: string): string[] {
    const suggestions: string[] = [];

    if (!content.includes('{') && !content.includes('[')) {
      suggestions.push('Content does not appear to contain JSON. Ensure the LLM response includes JSON data.');
    }

    if (content.includes('```') && !content.includes('```json')) {
      suggestions.push('JSON may be in a code block without the json language specifier.');
    }

    const openBraces = (content.match(/\{/g) || []).length;
    const closeBraces = (content.match(/\}/g) || []).length;
    if (openBraces !== closeBraces) {
      suggestions.push(`Unbalanced braces detected: ${openBraces} opening, ${closeBraces} closing.`);
    }

    if (content.includes(',]') || content.includes(',}')) {
      suggestions.push('Trailing commas detected in JSON structure.');
    }

    return suggestions;
  }

  // --- Validation Methods (Requirements: 3.1, 3.2, 3.4) ---

  /**
   * Required fields for a valid storyboard.
   * At least one of these array fields must be present.
   * Requirements: 3.1
   */
  private static readonly STORYBOARD_ARRAY_FIELDS = ['prompts', 'scenes', 'visual_prompts', 'visualPrompts', 'storyboard'];

  /**
   * Required fields for a valid scene/prompt item.
   * At least one text field must be present.
   * Requirements: 3.1
   */
  private static readonly SCENE_TEXT_FIELDS = ['prompt', 'text', 'description', 'content'];

  /**
   * Minimum prompt text length for quality validation.
   * Requirements: 3.3
   */
  private static readonly MIN_PROMPT_LENGTH = 10;

  /**
   * Validate extracted JSON against storyboard schema.
   * Checks required fields, data types, and value constraints.
   * 
   * Requirements: 3.1, 3.2, 3.4
   * 
   * @param json - The extracted JSON to validate
   * @returns ValidationResult with errors, warnings, and suggestions
   */
  validateStoryboard(json: unknown): ValidationResult {
    const result: ValidationResult = {
      isValid: true,
      errors: [],
      warnings: [],
      fieldErrors: [],
      suggestions: []
    };

    // Check if json is an object
    if (!json || typeof json !== 'object') {
      result.isValid = false;
      result.errors.push('JSON must be an object');
      result.fieldErrors.push({
        field: 'root',
        message: 'Expected an object but received ' + (json === null ? 'null' : typeof json),
        expectedType: 'object',
        actualType: json === null ? 'null' : typeof json,
        suggestion: 'Ensure the JSON response is a valid object with storyboard data'
      });
      return result;
    }

    const obj = json as StoryboardData;

    // Check for at least one storyboard array field
    const foundArrayField = JSONExtractor.STORYBOARD_ARRAY_FIELDS.find(field => {
      const value = obj[field as keyof StoryboardData];
      return value !== undefined;
    });

    if (!foundArrayField) {
      result.isValid = false;
      result.errors.push('Missing required storyboard array field (prompts, scenes, visual_prompts, visualPrompts, or storyboard)');
      result.fieldErrors.push({
        field: 'prompts/scenes',
        message: 'No storyboard array field found',
        expectedType: 'array',
        actualType: 'undefined',
        suggestion: 'Add a "prompts" or "scenes" array containing the visual prompts'
      });
      result.suggestions.push('Add a "prompts" array with scene objects containing "text" or "prompt" fields');
      return result;
    }

    // Get the array field value
    const arrayValue = obj[foundArrayField as keyof StoryboardData];

    // Validate it's actually an array
    if (!Array.isArray(arrayValue)) {
      result.isValid = false;
      result.errors.push(`Field "${foundArrayField}" must be an array`);
      result.fieldErrors.push({
        field: foundArrayField,
        message: `Expected array but received ${typeof arrayValue}`,
        expectedType: 'array',
        actualType: typeof arrayValue,
        suggestion: `Convert "${foundArrayField}" to an array of scene objects`
      });
      return result;
    }

    // Validate array is not empty
    if (arrayValue.length === 0) {
      result.isValid = false;
      result.errors.push(`Field "${foundArrayField}" cannot be empty`);
      result.fieldErrors.push({
        field: foundArrayField,
        message: 'Storyboard array is empty',
        expectedType: 'non-empty array',
        actualType: 'empty array',
        suggestion: 'Add at least one scene/prompt to the storyboard array'
      });
      return result;
    }

    // Validate each scene/prompt in the array
    const scenes = arrayValue as StoryboardScene[];
    for (let i = 0; i < scenes.length; i++) {
      const scene = scenes[i];
      const sceneValidation = this.validateScene(scene, i, foundArrayField);

      if (!sceneValidation.isValid) {
        result.isValid = false;
      }

      result.errors.push(...sceneValidation.errors);
      result.warnings.push(...sceneValidation.warnings);
      result.fieldErrors.push(...sceneValidation.fieldErrors);
      result.suggestions.push(...sceneValidation.suggestions);
    }

    // Validate metadata if present
    if (obj.metadata !== undefined) {
      if (typeof obj.metadata !== 'object' || obj.metadata === null) {
        result.warnings.push('Metadata field should be an object');
        result.fieldErrors.push({
          field: 'metadata',
          message: 'Expected object but received ' + (obj.metadata === null ? 'null' : typeof obj.metadata),
          expectedType: 'object',
          actualType: obj.metadata === null ? 'null' : typeof obj.metadata,
          suggestion: 'Convert metadata to an object with key-value pairs'
        });
      }
    }

    // Add general suggestions if there are errors
    if (!result.isValid && result.suggestions.length === 0) {
      result.suggestions.push('Review the storyboard structure and ensure all required fields are present');
    }

    return result;
  }

  /**
   * Validate a single scene/prompt object.
   * 
   * Requirements: 3.1, 3.3, 3.4
   * 
   * @param scene - The scene object to validate
   * @param index - The index of the scene in the array
   * @param arrayField - The name of the parent array field
   * @returns ValidationResult for this scene
   */
  private validateScene(scene: unknown, index: number, arrayField: string): ValidationResult {
    const result: ValidationResult = {
      isValid: true,
      errors: [],
      warnings: [],
      fieldErrors: [],
      suggestions: []
    };

    const fieldPrefix = `${arrayField}[${index}]`;

    // Check if scene is an object
    if (!scene || typeof scene !== 'object') {
      result.isValid = false;
      result.errors.push(`${fieldPrefix}: Scene must be an object`);
      result.fieldErrors.push({
        field: fieldPrefix,
        message: 'Expected object but received ' + (scene === null ? 'null' : typeof scene),
        expectedType: 'object',
        actualType: scene === null ? 'null' : typeof scene,
        suggestion: 'Convert scene to an object with prompt/text fields'
      });
      return result;
    }

    const sceneObj = scene as StoryboardScene;

    // Check for at least one text field
    const foundTextField = JSONExtractor.SCENE_TEXT_FIELDS.find(field => {
      const value = sceneObj[field as keyof StoryboardScene];
      return value !== undefined && value !== null && value !== '';
    });

    if (!foundTextField) {
      result.isValid = false;
      result.errors.push(`${fieldPrefix}: Missing required text field (prompt, text, description, or content)`);
      result.fieldErrors.push({
        field: `${fieldPrefix}.prompt/text`,
        message: 'No text content found in scene',
        expectedType: 'string',
        actualType: 'undefined',
        suggestion: 'Add a "prompt" or "text" field with the visual description'
      });
      return result;
    }

    // Get the text value and validate it
    const textValue = sceneObj[foundTextField as keyof StoryboardScene];

    if (typeof textValue !== 'string') {
      result.isValid = false;
      result.errors.push(`${fieldPrefix}.${foundTextField}: Must be a string`);
      result.fieldErrors.push({
        field: `${fieldPrefix}.${foundTextField}`,
        message: `Expected string but received ${typeof textValue}`,
        expectedType: 'string',
        actualType: typeof textValue,
        suggestion: 'Convert the prompt/text value to a string'
      });
      return result;
    }

    // Check minimum prompt length (quality requirement)
    if (textValue.trim().length < JSONExtractor.MIN_PROMPT_LENGTH) {
      result.warnings.push(`${fieldPrefix}.${foundTextField}: Prompt text is very short (${textValue.trim().length} chars), may not generate quality images`);
      result.fieldErrors.push({
        field: `${fieldPrefix}.${foundTextField}`,
        message: `Prompt text is too short (minimum ${JSONExtractor.MIN_PROMPT_LENGTH} characters recommended)`,
        expectedType: `string (min ${JSONExtractor.MIN_PROMPT_LENGTH} chars)`,
        actualType: `string (${textValue.trim().length} chars)`,
        suggestion: 'Add more descriptive details to the prompt for better image generation'
      });
    }

    // Validate scene number if present
    if (sceneObj.scene !== undefined) {
      if (typeof sceneObj.scene !== 'number') {
        result.warnings.push(`${fieldPrefix}.scene: Should be a number`);
        result.fieldErrors.push({
          field: `${fieldPrefix}.scene`,
          message: `Expected number but received ${typeof sceneObj.scene}`,
          expectedType: 'number',
          actualType: typeof sceneObj.scene,
          suggestion: 'Convert scene number to a numeric value'
        });
      }
    }

    // Validate mood if present
    if (sceneObj.mood !== undefined && typeof sceneObj.mood !== 'string') {
      result.warnings.push(`${fieldPrefix}.mood: Should be a string`);
      result.fieldErrors.push({
        field: `${fieldPrefix}.mood`,
        message: `Expected string but received ${typeof sceneObj.mood}`,
        expectedType: 'string',
        actualType: typeof sceneObj.mood,
        suggestion: 'Convert mood to a string value'
      });
    }

    // Validate timestamp if present
    if (sceneObj.timestamp !== undefined && typeof sceneObj.timestamp !== 'string') {
      result.warnings.push(`${fieldPrefix}.timestamp: Should be a string`);
      result.fieldErrors.push({
        field: `${fieldPrefix}.timestamp`,
        message: `Expected string but received ${typeof sceneObj.timestamp}`,
        expectedType: 'string',
        actualType: typeof sceneObj.timestamp,
        suggestion: 'Convert timestamp to a string format (e.g., "00:01:30")'
      });
    }

    // Validate confidence if present
    if (sceneObj.confidence !== undefined) {
      if (typeof sceneObj.confidence !== 'number') {
        result.warnings.push(`${fieldPrefix}.confidence: Should be a number`);
        result.fieldErrors.push({
          field: `${fieldPrefix}.confidence`,
          message: `Expected number but received ${typeof sceneObj.confidence}`,
          expectedType: 'number',
          actualType: typeof sceneObj.confidence,
          suggestion: 'Convert confidence to a numeric value between 0 and 1'
        });
      } else if (sceneObj.confidence < 0 || sceneObj.confidence > 1) {
        result.warnings.push(`${fieldPrefix}.confidence: Should be between 0 and 1`);
        result.fieldErrors.push({
          field: `${fieldPrefix}.confidence`,
          message: `Confidence value ${sceneObj.confidence} is out of range`,
          expectedType: 'number (0-1)',
          actualType: `number (${sceneObj.confidence})`,
          suggestion: 'Normalize confidence to a value between 0 and 1'
        });
      }
    }

    // Validate source if present
    if (sceneObj.source !== undefined) {
      const validSources = ['llm', 'fallback', 'reconstructed'];
      if (typeof sceneObj.source !== 'string' || !validSources.includes(sceneObj.source)) {
        result.warnings.push(`${fieldPrefix}.source: Should be one of: ${validSources.join(', ')}`);
        result.fieldErrors.push({
          field: `${fieldPrefix}.source`,
          message: `Invalid source value: ${sceneObj.source}`,
          expectedType: `"llm" | "fallback" | "reconstructed"`,
          actualType: typeof sceneObj.source === 'string' ? `"${sceneObj.source}"` : typeof sceneObj.source,
          suggestion: 'Use one of the valid source values: llm, fallback, or reconstructed'
        });
      }
    }

    return result;
  }

  /**
   * Attempt to reconstruct or fix invalid storyboard data.
   * 
   * Requirements: 3.2
   * 
   * @param json - The invalid JSON to attempt to fix
   * @param validationResult - The validation result with errors
   * @returns Updated ValidationResult with fixedData if reconstruction was possible
   */
  attemptReconstruction(json: unknown, validationResult: ValidationResult): ValidationResult {
    const result = { ...validationResult };

    if (!json || typeof json !== 'object') {
      return result;
    }

    const obj = json as Record<string, unknown>;
    const fixedData: StoryboardData = {};
    let wasFixed = false;

    // Try to find and normalize the prompts array
    for (const field of JSONExtractor.STORYBOARD_ARRAY_FIELDS) {
      if (obj[field] !== undefined) {
        const value = obj[field];

        // If it's already an array, use it
        if (Array.isArray(value)) {
          fixedData.prompts = this.normalizeScenes(value);
          wasFixed = true;
          break;
        }

        // If it's a single object, wrap it in an array
        if (typeof value === 'object' && value !== null) {
          fixedData.prompts = this.normalizeScenes([value]);
          wasFixed = true;
          break;
        }
      }
    }

    // If no array field found, check if the object itself looks like a scene
    if (!wasFixed && this.looksLikeScene(obj)) {
      fixedData.prompts = this.normalizeScenes([obj]);
      wasFixed = true;
    }

    // Copy metadata if present
    if (obj.metadata && typeof obj.metadata === 'object') {
      fixedData.metadata = obj.metadata as Record<string, unknown>;
    }

    if (wasFixed && fixedData.prompts && fixedData.prompts.length > 0) {
      result.fixedData = fixedData;
      result.suggestions.push('Data was automatically reconstructed - please verify the results');
    }

    return result;
  }

  /**
   * Check if an object looks like a scene/prompt.
   */
  private looksLikeScene(obj: Record<string, unknown>): boolean {
    return JSONExtractor.SCENE_TEXT_FIELDS.some(field =>
      typeof obj[field] === 'string' && (obj[field] as string).length > 0
    );
  }

  /**
   * Normalize an array of scenes to ensure consistent structure.
   */
  private normalizeScenes(scenes: unknown[]): StoryboardScene[] {
    return scenes
      .filter(scene => scene && typeof scene === 'object')
      .map((scene, index) => {
        const sceneObj = scene as Record<string, unknown>;
        const normalized: StoryboardScene = {
          scene: index + 1
        };

        // Find and copy the text field
        for (const field of JSONExtractor.SCENE_TEXT_FIELDS) {
          if (typeof sceneObj[field] === 'string') {
            normalized.prompt = sceneObj[field] as string;
            break;
          }
        }

        // Copy other fields if present
        if (typeof sceneObj.mood === 'string') {
          normalized.mood = sceneObj.mood;
        }
        if (typeof sceneObj.timestamp === 'string') {
          normalized.timestamp = sceneObj.timestamp;
        }
        if (typeof sceneObj.confidence === 'number') {
          normalized.confidence = sceneObj.confidence;
        }
        if (typeof sceneObj.source === 'string') {
          normalized.source = sceneObj.source as 'llm' | 'fallback' | 'reconstructed';
        }

        return normalized;
      })
      .filter(scene => scene.prompt !== undefined);
  }

  // --- Content Sanitization Methods (Requirements: 3.5) ---

  /**
   * Patterns for potentially harmful content that should be removed or neutralized.
   * Requirements: 3.5
   */
  private static readonly HARMFUL_PATTERNS: { pattern: RegExp; replacement: string; description: string }[] = [
    // Script injection patterns
    { pattern: /<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>/gi, replacement: '', description: 'script tags' },
    { pattern: /javascript:/gi, replacement: '', description: 'javascript: protocol' },
    { pattern: /on\w+\s*=/gi, replacement: '', description: 'event handlers' },

    // HTML injection patterns
    { pattern: /<iframe\b[^<]*(?:(?!<\/iframe>)<[^<]*)*<\/iframe>/gi, replacement: '', description: 'iframe tags' },
    { pattern: /<object\b[^<]*(?:(?!<\/object>)<[^<]*)*<\/object>/gi, replacement: '', description: 'object tags' },
    { pattern: /<embed\b[^>]*>/gi, replacement: '', description: 'embed tags' },
    { pattern: /<link\b[^>]*>/gi, replacement: '', description: 'link tags' },
    { pattern: /<style\b[^<]*(?:(?!<\/style>)<[^<]*)*<\/style>/gi, replacement: '', description: 'style tags' },

    // Data URI patterns (potential for embedded malicious content)
    { pattern: /data:\s*[^,]*;base64,/gi, replacement: '', description: 'base64 data URIs' },

    // SQL injection patterns (basic)
    { pattern: /;\s*DROP\s+TABLE/gi, replacement: '', description: 'SQL DROP statements' },
    { pattern: /;\s*DELETE\s+FROM/gi, replacement: '', description: 'SQL DELETE statements' },
    { pattern: /;\s*INSERT\s+INTO/gi, replacement: '', description: 'SQL INSERT statements' },
    { pattern: /;\s*UPDATE\s+\w+\s+SET/gi, replacement: '', description: 'SQL UPDATE statements' },

    // Command injection patterns
    { pattern: /\$\([^)]+\)/g, replacement: '', description: 'shell command substitution' },
    { pattern: /`[^`]+`/g, replacement: '', description: 'backtick command execution' },
    { pattern: /\|\s*\w+/g, replacement: '', description: 'pipe commands' },
  ];

  /**
   * Fields that should be sanitized in storyboard data.
   * Requirements: 3.5
   */
  private static readonly SANITIZABLE_FIELDS = ['prompt', 'text', 'description', 'content', 'mood', 'style'];

  /**
   * Sanitize extracted JSON to remove potentially harmful content.
   * 
   * Requirements: 3.5
   * 
   * @param json - The JSON data to sanitize
   * @returns Sanitized JSON data
   */
  sanitizeJSON(json: unknown): unknown {
    if (json === null || json === undefined) {
      return json;
    }

    if (typeof json === 'string') {
      return this.sanitizeString(json);
    }

    if (Array.isArray(json)) {
      return json.map(item => this.sanitizeJSON(item));
    }

    if (typeof json === 'object') {
      return this.sanitizeObject(json as Record<string, unknown>);
    }

    // Primitives (number, boolean) pass through unchanged
    return json;
  }

  /**
   * Sanitize a string value by removing harmful patterns.
   * 
   * Requirements: 3.5
   * 
   * @param str - The string to sanitize
   * @returns Sanitized string
   */
  sanitizeString(str: string): string {
    let sanitized = str;

    for (const { pattern, replacement } of JSONExtractor.HARMFUL_PATTERNS) {
      sanitized = sanitized.replace(pattern, replacement);
    }

    // Remove null bytes
    sanitized = sanitized.replace(/\0/g, '');

    // Normalize whitespace (but preserve intentional formatting)
    sanitized = sanitized.replace(/[\r\n]+/g, '\n').trim();

    return sanitized;
  }

  /**
   * Sanitize an object by sanitizing all string fields.
   * 
   * Requirements: 3.5
   * 
   * @param obj - The object to sanitize
   * @returns Sanitized object
   */
  private sanitizeObject(obj: Record<string, unknown>): Record<string, unknown> {
    const sanitized: Record<string, unknown> = {};

    for (const [key, value] of Object.entries(obj)) {
      // Sanitize the key itself
      const sanitizedKey = this.sanitizeString(key);

      // Recursively sanitize the value
      sanitized[sanitizedKey] = this.sanitizeJSON(value);
    }

    return sanitized;
  }

  /**
   * Sanitize storyboard data specifically, with field-aware sanitization.
   * 
   * Requirements: 3.5
   * 
   * @param storyboard - The storyboard data to sanitize
   * @returns Sanitized storyboard data
   */
  sanitizeStoryboard(storyboard: StoryboardData): StoryboardData {
    const sanitized: StoryboardData = {};

    // Sanitize each array field if present
    if (storyboard.prompts && Array.isArray(storyboard.prompts)) {
      sanitized.prompts = storyboard.prompts.map(scene => this.sanitizeScene(scene));
    }
    if (storyboard.scenes && Array.isArray(storyboard.scenes)) {
      sanitized.scenes = storyboard.scenes.map(scene => this.sanitizeScene(scene));
    }
    if (storyboard.visual_prompts && Array.isArray(storyboard.visual_prompts)) {
      sanitized.visual_prompts = storyboard.visual_prompts.map(scene => this.sanitizeScene(scene));
    }
    if (storyboard.visualPrompts && Array.isArray(storyboard.visualPrompts)) {
      sanitized.visualPrompts = storyboard.visualPrompts.map(scene => this.sanitizeScene(scene));
    }
    if (storyboard.storyboard && Array.isArray(storyboard.storyboard)) {
      sanitized.storyboard = storyboard.storyboard.map(scene => this.sanitizeScene(scene));
    }

    // Sanitize metadata if present
    if (storyboard.metadata) {
      sanitized.metadata = this.sanitizeJSON(storyboard.metadata) as Record<string, unknown>;
    }

    return sanitized;
  }

  /**
   * Sanitize a single scene/prompt object.
   * 
   * Requirements: 3.5
   * 
   * @param scene - The scene to sanitize
   * @returns Sanitized scene
   */
  private sanitizeScene(scene: StoryboardScene): StoryboardScene {
    const sanitized: StoryboardScene = {};

    // Copy and sanitize text fields
    for (const field of JSONExtractor.SANITIZABLE_FIELDS) {
      const value = scene[field as keyof StoryboardScene];
      if (typeof value === 'string') {
        (sanitized as Record<string, unknown>)[field] = this.sanitizeString(value);
      }
    }

    // Copy non-string fields directly
    if (typeof scene.scene === 'number') {
      sanitized.scene = scene.scene;
    }
    if (typeof scene.confidence === 'number') {
      sanitized.confidence = scene.confidence;
    }
    if (scene.source && ['llm', 'fallback', 'reconstructed'].includes(scene.source)) {
      sanitized.source = scene.source;
    }
    if (typeof scene.timestamp === 'string') {
      sanitized.timestamp = this.sanitizeString(scene.timestamp);
    }

    return sanitized;
  }

  /**
   * Check if a string contains potentially harmful content.
   * 
   * Requirements: 3.5
   * 
   * @param str - The string to check
   * @returns Object with isHarmful flag and detected patterns
   */
  detectHarmfulContent(str: string): { isHarmful: boolean; detectedPatterns: string[] } {
    const detectedPatterns: string[] = [];

    for (const { pattern, description } of JSONExtractor.HARMFUL_PATTERNS) {
      if (pattern.test(str)) {
        detectedPatterns.push(description);
        // Reset regex lastIndex for global patterns
        pattern.lastIndex = 0;
      }
    }

    return {
      isHarmful: detectedPatterns.length > 0,
      detectedPatterns
    };
  }

  /**
   * Sanitize and validate JSON in one operation.
   * 
   * Requirements: 3.1, 3.5
   * 
   * @param json - The JSON to sanitize and validate
   * @returns Object with sanitized data and validation result
   */
  sanitizeAndValidate(json: unknown): { sanitized: unknown; validation: ValidationResult } {
    const sanitized = this.sanitizeJSON(json);
    const validation = this.validateStoryboard(sanitized);

    return { sanitized, validation };
  }

  // --- Validation Error Reporting Methods (Requirements: 3.4) ---

  /**
   * Generate a human-readable validation error report.
   * 
   * Requirements: 3.4
   * 
   * @param validation - The validation result to format
   * @returns Formatted error report string
   */
  formatValidationReport(validation: ValidationResult): string {
    const lines: string[] = [];

    lines.push('=== Validation Report ===');
    lines.push(`Status: ${validation.isValid ? 'VALID' : 'INVALID'}`);
    lines.push('');

    if (validation.errors.length > 0) {
      lines.push('Errors:');
      for (const error of validation.errors) {
        lines.push(`  ❌ ${error}`);
      }
      lines.push('');
    }

    if (validation.warnings.length > 0) {
      lines.push('Warnings:');
      for (const warning of validation.warnings) {
        lines.push(`  ⚠️ ${warning}`);
      }
      lines.push('');
    }

    if (validation.fieldErrors.length > 0) {
      lines.push('Field Details:');
      for (const fieldError of validation.fieldErrors) {
        lines.push(`  Field: ${fieldError.field}`);
        lines.push(`    Message: ${fieldError.message}`);
        if (fieldError.expectedType) {
          lines.push(`    Expected: ${fieldError.expectedType}`);
        }
        if (fieldError.actualType) {
          lines.push(`    Actual: ${fieldError.actualType}`);
        }
        if (fieldError.suggestion) {
          lines.push(`    Suggestion: ${fieldError.suggestion}`);
        }
        lines.push('');
      }
    }

    if (validation.suggestions.length > 0) {
      lines.push('Suggestions:');
      for (const suggestion of validation.suggestions) {
        lines.push(`  💡 ${suggestion}`);
      }
      lines.push('');
    }

    if (validation.fixedData) {
      lines.push('Note: Automatic reconstruction was attempted. Please verify the results.');
    }

    return lines.join('\n');
  }

  /**
   * Create a structured validation error for API responses.
   * 
   * Requirements: 3.4
   * 
   * @param validation - The validation result
   * @returns Structured error object suitable for API responses
   */
  createValidationError(validation: ValidationResult): ParseError {
    const primaryError = validation.errors[0] || 'Validation failed';
    const allSuggestions = [
      ...validation.suggestions,
      ...validation.fieldErrors
        .filter(fe => fe.suggestion)
        .map(fe => fe.suggestion as string)
    ];

    return {
      type: 'VALIDATION_ERROR',
      message: primaryError,
      originalContent: '',
      attemptedMethods: [],
      suggestions: [...new Set(allSuggestions)], // Deduplicate
      timestamp: new Date().toISOString(),
      contentLength: 0,
      failureReasons: validation.fieldErrors.map(fe => ({
        method: ExtractionMethod.MARKDOWN_BLOCKS, // Placeholder
        error: `${fe.field}: ${fe.message}`,
        attemptedAt: new Date().toISOString()
      }))
    };
  }

  /**
   * Get a summary of validation issues for quick display.
   * 
   * Requirements: 3.4
   * 
   * @param validation - The validation result
   * @returns Summary object with counts and primary issues
   */
  getValidationSummary(validation: ValidationResult): {
    isValid: boolean;
    errorCount: number;
    warningCount: number;
    primaryError: string | null;
    primarySuggestion: string | null;
  } {
    return {
      isValid: validation.isValid,
      errorCount: validation.errors.length,
      warningCount: validation.warnings.length,
      primaryError: validation.errors[0] || null,
      primarySuggestion: validation.suggestions[0] ||
        validation.fieldErrors.find(fe => fe.suggestion)?.suggestion || null
    };
  }

  /**
   * Check if validation result has specific field errors.
   * 
   * Requirements: 3.4
   * 
   * @param validation - The validation result
   * @param fieldPattern - Regex pattern to match field names
   * @returns Array of matching field errors
   */
  getFieldErrors(validation: ValidationResult, fieldPattern: RegExp): FieldValidationError[] {
    return validation.fieldErrors.filter(fe => fieldPattern.test(fe.field));
  }

  /**
   * Merge multiple validation results into one.
   * Useful when validating multiple parts of a complex structure.
   * 
   * Requirements: 3.4
   * 
   * @param results - Array of validation results to merge
   * @returns Merged validation result
   */
  mergeValidationResults(...results: ValidationResult[]): ValidationResult {
    const merged: ValidationResult = {
      isValid: true,
      errors: [],
      warnings: [],
      fieldErrors: [],
      suggestions: []
    };

    for (const result of results) {
      if (!result.isValid) {
        merged.isValid = false;
      }
      merged.errors.push(...result.errors);
      merged.warnings.push(...result.warnings);
      merged.fieldErrors.push(...result.fieldErrors);
      merged.suggestions.push(...result.suggestions);

      if (result.fixedData && !merged.fixedData) {
        merged.fixedData = result.fixedData;
      }
    }

    // Deduplicate suggestions
    merged.suggestions = [...new Set(merged.suggestions)];

    return merged;
  }

}

// --- Fallback Processing System (Requirements: 4.1, 4.2, 4.3, 4.4, 4.5) ---

/**
 * Fallback notification callback type.
 * Requirements: 4.4
 */
export type FallbackNotificationCallback = (notification: FallbackNotification) => void;

/**
 * Fallback notification structure.
 * Requirements: 4.4
 */
export interface FallbackNotification {
  type: 'fallback_used';
  message: string;
  originalContentPreview: string;
  extractedPromptCount: number;
  timestamp: string;
  reducedFunctionality: string[];
}

/**
 * Fallback usage metrics.
 * Requirements: 4.5
 */
export interface FallbackMetrics {
  totalFallbackUsages: number;
  successfulFallbacks: number;
  failedFallbacks: number;
  averagePromptsExtracted: number;
  lastFallbackTimestamp: string | null;
  fallbackReasons: Map<string, number>;
}

/**
 * Basic storyboard structure generated by fallback processing.
 * Requirements: 4.2
 */
export interface BasicStoryboard {
  prompts: StoryboardScene[];
  metadata: {
    source: 'fallback';
    extractionMethod: 'text_based';
    confidence: number;
    originalContentLength: number;
    processingTimestamp: string;
    warnings: string[];
  };
}

/**
 * Result of text-based prompt extraction.
 * Requirements: 4.1
 */
export interface TextExtractionResult {
  prompts: string[];
  emotionalContent: string[];
  sceneDescriptions: string[];
  confidence: number;
}

/**
 * FallbackProcessor provides alternative processing when primary JSON extraction fails.
 * 
 * It implements:
 * 1. Text-based prompt extraction from unstructured content
 * 2. Basic storyboard generation from extracted prompts
 * 3. Semantic information preservation
 * 4. User notification system
 * 5. Metrics tracking
 * 
 * Requirements: 4.1, 4.2, 4.3, 4.4, 4.5
 */
export class FallbackProcessor {
  private metrics: FallbackMetrics;
  private notificationCallbacks: FallbackNotificationCallback[] = [];
  private logger: ExtractorLogger;

  constructor(logger?: ExtractorLogger) {
    this.logger = logger || defaultLogger;
    this.metrics = this.initializeMetrics();
  }

  /**
   * Initialize fallback metrics.
   * Requirements: 4.5
   */
  private initializeMetrics(): FallbackMetrics {
    return {
      totalFallbackUsages: 0,
      successfulFallbacks: 0,
      failedFallbacks: 0,
      averagePromptsExtracted: 0,
      lastFallbackTimestamp: null,
      fallbackReasons: new Map()
    };
  }

  /**
   * Register a callback for fallback notifications.
   * Requirements: 4.4
   */
  registerNotificationCallback(callback: FallbackNotificationCallback): void {
    this.notificationCallbacks.push(callback);
  }

  /**
   * Unregister a notification callback.
   * Requirements: 4.4
   */
  unregisterNotificationCallback(callback: FallbackNotificationCallback): void {
    const index = this.notificationCallbacks.indexOf(callback);
    if (index > -1) {
      this.notificationCallbacks.splice(index, 1);
    }
  }

  /**
   * Notify all registered callbacks about fallback usage.
   * Requirements: 4.4
   */
  private notifyFallbackUsed(notification: FallbackNotification): void {
    for (const callback of this.notificationCallbacks) {
      try {
        callback(notification);
      } catch (error) {
        this.logger.error('Notification callback error', {
          error: error instanceof Error ? error.message : String(error)
        });
      }
    }
  }

  /**
   * Extract visual prompts from unstructured text content.
   * Identifies scene descriptions, emotional content, and visual elements.
   * 
   * Requirements: 4.1, 4.2
   * 
   * @param content - The unstructured text content to process
   * @returns TextExtractionResult with extracted prompts and metadata
   */
  extractPromptsFromText(content: string): TextExtractionResult {
    const result: TextExtractionResult = {
      prompts: [],
      emotionalContent: [],
      sceneDescriptions: [],
      confidence: 0
    };

    if (!content || typeof content !== 'string' || content.trim().length === 0) {
      return result;
    }

    // Patterns for identifying visual/scene descriptions
    const visualPatterns = [
      // Scene descriptions with visual keywords
      /(?:scene|visual|image|picture|shot|frame|view|setting)[\s:]+([^.!?\n]{15,200})/gi,
      // Descriptions starting with visual verbs
      /(?:show|display|depict|illustrate|portray|capture|reveal)s?\s+([^.!?\n]{15,200})/gi,
      // "A/An/The [adjective] [noun]" patterns (common in prompts)
      /(?:^|\.\s+)((?:a|an|the)\s+(?:\w+\s+){1,5}(?:scene|landscape|portrait|view|moment|setting)[^.!?\n]{10,150})/gi,
      // Descriptive sentences with visual adjectives
      /([^.!?\n]*(?:beautiful|stunning|dramatic|serene|vibrant|dark|bright|colorful|moody|atmospheric)[^.!?\n]{15,150})/gi,
    ];

    // Patterns for emotional content
    const emotionalPatterns = [
      /(?:mood|emotion|feeling|atmosphere|tone)[\s:]+([^.!?\n]{5,100})/gi,
      /(?:evokes?|conveys?|expresses?|captures?)\s+(?:a\s+)?(?:sense\s+of\s+)?([^.!?\n]{5,100})/gi,
      /(?:melancholic|hopeful|intense|peaceful|dramatic|serene|joyful|somber|nostalgic|ethereal)[^.!?\n]{0,50}/gi,
    ];

    // Extract visual prompts
    const extractedPrompts = new Set<string>();
    for (const pattern of visualPatterns) {
      const matches = content.matchAll(pattern);
      for (const match of matches) {
        const prompt = (match[1] || match[0]).trim();
        if (prompt.length >= 15 && prompt.length <= 300) {
          extractedPrompts.add(this.cleanPromptText(prompt));
        }
      }
    }

    // Extract emotional content
    const extractedEmotions = new Set<string>();
    for (const pattern of emotionalPatterns) {
      const matches = content.matchAll(pattern);
      for (const match of matches) {
        const emotion = (match[1] || match[0]).trim();
        if (emotion.length >= 3 && emotion.length <= 100) {
          extractedEmotions.add(emotion.toLowerCase());
        }
      }
    }

    // If no patterns matched, try sentence-based extraction
    if (extractedPrompts.size === 0) {
      const sentences = this.extractSentences(content);
      for (const sentence of sentences) {
        if (this.looksLikeVisualDescription(sentence)) {
          extractedPrompts.add(this.cleanPromptText(sentence));
        }
      }
    }

    // Extract scene descriptions (numbered or labeled sections)
    const scenePatterns = [
      /(?:scene\s*\d+|part\s*\d+|section\s*\d+)[\s:]+([^.!?\n]{15,200})/gi,
      /(?:\d+\.\s*)([^.!?\n]{15,200})/gi,
      /(?:intro|verse|chorus|bridge|outro)[\s:]+([^.!?\n]{15,200})/gi,
    ];

    for (const pattern of scenePatterns) {
      const matches = content.matchAll(pattern);
      for (const match of matches) {
        const scene = (match[1] || match[0]).trim();
        if (scene.length >= 15) {
          result.sceneDescriptions.push(this.cleanPromptText(scene));
        }
      }
    }

    result.prompts = Array.from(extractedPrompts);
    result.emotionalContent = Array.from(extractedEmotions);

    // Calculate confidence based on extraction quality
    result.confidence = this.calculateExtractionConfidence(result, content);

    return result;
  }

  /**
   * Clean and normalize prompt text.
   */
  private cleanPromptText(text: string): string {
    return text
      .replace(/\s+/g, ' ')
      .replace(/^[\s,.:;-]+|[\s,.:;-]+$/g, '')
      .trim();
  }

  /**
   * Extract sentences from content.
   */
  private extractSentences(content: string): string[] {
    // Split on sentence boundaries
    const sentences = content.split(/(?<=[.!?])\s+/);
    return sentences
      .map(s => s.trim())
      .filter(s => s.length >= 15 && s.length <= 300);
  }

  /**
   * Check if a sentence looks like a visual description.
   */
  private looksLikeVisualDescription(sentence: string): boolean {
    const visualKeywords = [
      'scene', 'visual', 'image', 'picture', 'view', 'landscape',
      'portrait', 'setting', 'background', 'foreground', 'color',
      'light', 'shadow', 'sky', 'ocean', 'mountain', 'forest',
      'city', 'room', 'person', 'figure', 'silhouette'
    ];

    const visualAdjectives = [
      'beautiful', 'stunning', 'dramatic', 'serene', 'vibrant',
      'dark', 'bright', 'colorful', 'moody', 'atmospheric',
      'ethereal', 'mystical', 'peaceful', 'intense', 'soft'
    ];

    const lowerSentence = sentence.toLowerCase();

    const hasVisualKeyword = visualKeywords.some(kw => lowerSentence.includes(kw));
    const hasVisualAdjective = visualAdjectives.some(adj => lowerSentence.includes(adj));

    return hasVisualKeyword || hasVisualAdjective;
  }

  /**
   * Calculate confidence score for extraction result.
   */
  private calculateExtractionConfidence(result: TextExtractionResult, originalContent: string): number {
    let confidence = 0;

    // Base confidence from prompt count
    if (result.prompts.length > 0) {
      confidence += Math.min(0.3, result.prompts.length * 0.1);
    }

    // Bonus for emotional content
    if (result.emotionalContent.length > 0) {
      confidence += Math.min(0.2, result.emotionalContent.length * 0.05);
    }

    // Bonus for scene descriptions
    if (result.sceneDescriptions.length > 0) {
      confidence += Math.min(0.2, result.sceneDescriptions.length * 0.05);
    }

    // Bonus for content coverage
    const totalExtractedLength = result.prompts.join(' ').length +
      result.sceneDescriptions.join(' ').length;
    const coverageRatio = totalExtractedLength / originalContent.length;
    confidence += Math.min(0.3, coverageRatio);

    return Math.min(1, Math.max(0, confidence));
  }

  /**
   * Generate a basic storyboard structure from text prompts.
   * Preserves semantic information during conversion.
   * 
   * Requirements: 4.2, 4.3
   * 
   * @param content - The original text content
   * @param reason - The reason fallback was triggered
   * @returns BasicStoryboard with generated scenes
   */
  generateBasicStoryboard(content: string, reason: string = 'json_extraction_failed'): BasicStoryboard {
    const startTime = Date.now();
    const extraction = this.extractPromptsFromText(content);

    const storyboard: BasicStoryboard = {
      prompts: [],
      metadata: {
        source: 'fallback',
        extractionMethod: 'text_based',
        confidence: extraction.confidence,
        originalContentLength: content.length,
        processingTimestamp: new Date().toISOString(),
        warnings: []
      }
    };

    // Combine prompts and scene descriptions
    const allPrompts = [...extraction.prompts, ...extraction.sceneDescriptions];

    // Remove duplicates while preserving order
    const uniquePrompts = [...new Set(allPrompts)];

    // Generate scenes from extracted prompts
    for (let i = 0; i < uniquePrompts.length; i++) {
      const prompt = uniquePrompts[i];
      const scene: StoryboardScene = {
        scene: i + 1,
        prompt: prompt,
        source: 'fallback',
        confidence: extraction.confidence
      };

      // Try to associate emotional content with scenes
      if (extraction.emotionalContent.length > 0) {
        const moodIndex = i % extraction.emotionalContent.length;
        scene.mood = extraction.emotionalContent[moodIndex];
      }

      storyboard.prompts.push(scene);
    }

    // If no prompts were extracted, create a minimal storyboard from content
    if (storyboard.prompts.length === 0) {
      storyboard.metadata.warnings.push('No visual prompts could be extracted from content');

      // Try to create at least one scene from the content
      const trimmedContent = content.trim();
      if (trimmedContent.length >= 15) {
        const fallbackPrompt = trimmedContent.length > 200
          ? trimmedContent.substring(0, 200) + '...'
          : trimmedContent;

        storyboard.prompts.push({
          scene: 1,
          prompt: fallbackPrompt,
          source: 'fallback',
          confidence: 0.1
        });
        storyboard.metadata.warnings.push('Created minimal scene from raw content');
      }
    }

    // Update metrics
    this.updateMetrics(storyboard.prompts.length, reason, storyboard.prompts.length > 0);

    // Send notification
    this.notifyFallbackUsed({
      type: 'fallback_used',
      message: `Fallback processing was used because: ${reason}. ${storyboard.prompts.length} prompts were extracted.`,
      originalContentPreview: content.substring(0, 200) + (content.length > 200 ? '...' : ''),
      extractedPromptCount: storyboard.prompts.length,
      timestamp: new Date().toISOString(),
      reducedFunctionality: this.getReducedFunctionalityList(storyboard)
    });

    this.logger.info('Fallback storyboard generated', {
      promptCount: storyboard.prompts.length,
      confidence: storyboard.metadata.confidence,
      processingTimeMs: Date.now() - startTime,
      reason
    });

    return storyboard;
  }

  /**
   * Get list of reduced functionality when using fallback.
   * Requirements: 4.4
   */
  private getReducedFunctionalityList(storyboard: BasicStoryboard): string[] {
    const reduced: string[] = [];

    if (storyboard.metadata.confidence < 0.5) {
      reduced.push('Low confidence in extracted prompts - results may not match original intent');
    }

    if (storyboard.prompts.length === 0) {
      reduced.push('No prompts could be extracted - storyboard is empty');
    } else if (storyboard.prompts.length < 3) {
      reduced.push('Limited number of prompts extracted - storyboard may be incomplete');
    }

    if (!storyboard.prompts.some(p => p.mood)) {
      reduced.push('Emotional/mood information could not be extracted');
    }

    if (storyboard.metadata.warnings.length > 0) {
      reduced.push('Processing warnings occurred - review storyboard carefully');
    }

    return reduced;
  }

  /**
   * Update fallback metrics.
   * Requirements: 4.5
   */
  private updateMetrics(promptCount: number, reason: string, success: boolean): void {
    this.metrics.totalFallbackUsages++;
    this.metrics.lastFallbackTimestamp = new Date().toISOString();

    if (success) {
      this.metrics.successfulFallbacks++;
      // Update average prompts extracted
      const totalPrompts = this.metrics.averagePromptsExtracted * (this.metrics.successfulFallbacks - 1) + promptCount;
      this.metrics.averagePromptsExtracted = totalPrompts / this.metrics.successfulFallbacks;
    } else {
      this.metrics.failedFallbacks++;
    }

    // Track fallback reasons
    const currentCount = this.metrics.fallbackReasons.get(reason) || 0;
    this.metrics.fallbackReasons.set(reason, currentCount + 1);
  }

  /**
   * Get current fallback metrics.
   * Requirements: 4.5
   */
  getMetrics(): FallbackMetrics {
    return {
      ...this.metrics,
      fallbackReasons: new Map(this.metrics.fallbackReasons)
    };
  }

  /**
   * Reset fallback metrics.
   * Requirements: 4.5
   */
  resetMetrics(): void {
    this.metrics = this.initializeMetrics();
  }

  /**
   * Get a summary of fallback usage for monitoring.
   * Requirements: 4.5
   */
  getMetricsSummary(): {
    totalUsages: number;
    successRate: number;
    averagePrompts: number;
    topReasons: { reason: string; count: number }[];
  } {
    const topReasons = Array.from(this.metrics.fallbackReasons.entries())
      .map(([reason, count]) => ({ reason, count }))
      .sort((a, b) => b.count - a.count)
      .slice(0, 5);

    return {
      totalUsages: this.metrics.totalFallbackUsages,
      successRate: this.metrics.totalFallbackUsages > 0
        ? this.metrics.successfulFallbacks / this.metrics.totalFallbackUsages
        : 0,
      averagePrompts: this.metrics.averagePromptsExtracted,
      topReasons
    };
  }

  /**
   * Process content with fallback when JSON extraction fails.
   * This is the main entry point for fallback processing.
   * 
   * Requirements: 4.1, 4.2, 4.3, 4.4
   * 
   * @param content - The content that failed JSON extraction
   * @param extractionError - The error from JSON extraction
   * @returns BasicStoryboard or null if fallback also fails
   */
  processWithFallback(content: string, extractionError: string): BasicStoryboard | null {
    this.logger.info('Starting fallback processing', {
      contentLength: content.length,
      extractionError
    });

    try {
      const storyboard = this.generateBasicStoryboard(content, extractionError);

      if (storyboard.prompts.length === 0) {
        this.logger.warn('Fallback processing produced no prompts');
        return null;
      }

      return storyboard;
    } catch (error) {
      this.logger.error('Fallback processing failed', {
        error: error instanceof Error ? error.message : String(error)
      });
      this.updateMetrics(0, 'fallback_error', false);
      return null;
    }
  }
}

// Export singleton instances for convenience
export const jsonExtractor = new JSONExtractor();
export const fallbackProcessor = new FallbackProcessor();
````

## File: packages/shared/src/services/languageDetector.ts
````typescript
/**
 * Language Detection Service
 *
 * Centralized language detection extracted from scattered inline checks.
 * Supports Arabic and English with confidence scoring.
 *
 * Replaces ad-hoc `/[\u0600-\u06FF]/.test()` checks throughout the codebase
 * with a consistent, testable interface.
 *
 * Requirement: 19.1
 */

/**
 * Language detection result with confidence scores
 */
export interface LanguageDetectionResult {
  /** Primary detected language code */
  language: 'ar' | 'en';
  /** Confidence score 0-1 for the primary language */
  confidence: number;
  /** Per-language character counts */
  scores: {
    ar: number;
    en: number;
  };
}

/**
 * Language Detector class
 *
 * Analyzes text using Unicode character ranges to determine the primary language.
 * Focused on Arabic vs English detection since those are the supported languages.
 */
export class LanguageDetector {
  /**
   * Detect the primary language of the given text.
   *
   * @param text Text to analyze
   * @returns Detection result with language, confidence, and per-language scores
   */
  detect(text: string): LanguageDetectionResult {
    if (!text || text.trim().length === 0) {
      return { language: 'en', confidence: 1, scores: { ar: 0, en: 0 } };
    }

    let arabicCount = 0;
    let latinCount = 0;

    for (const char of text) {
      const code = char.charCodeAt(0);
      // Arabic: U+0600–U+06FF, U+0750–U+077F (Arabic Supplement)
      if ((code >= 0x0600 && code <= 0x06FF) || (code >= 0x0750 && code <= 0x077F)) {
        arabicCount++;
      }
      // Latin (A-Z, a-z, extended Latin)
      else if ((code >= 0x0041 && code <= 0x007A) || (code >= 0x00C0 && code <= 0x024F)) {
        latinCount++;
      }
    }

    const totalAlpha = arabicCount + latinCount;
    if (totalAlpha === 0) {
      return { language: 'en', confidence: 0.5, scores: { ar: 0, en: 0 } };
    }

    const arabicRatio = arabicCount / totalAlpha;
    const isArabic = arabicRatio > 0.3;

    return {
      language: isArabic ? 'ar' : 'en',
      confidence: isArabic ? arabicRatio : 1 - arabicRatio,
      scores: { ar: arabicCount, en: latinCount },
    };
  }

  /**
   * Quick boolean check: is this text primarily Arabic?
   */
  isArabic(text: string): boolean {
    return this.detect(text).language === 'ar';
  }

  /**
   * Quick boolean check: is this text primarily English?
   */
  isEnglish(text: string): boolean {
    return this.detect(text).language === 'en';
  }
}

/** Singleton instance for convenience */
export const languageDetector = new LanguageDetector();

/**
 * Standalone function matching the existing `detectLanguageFromText` signature
 * for drop-in replacement across the codebase.
 *
 * Returns 'ar' | 'en' (narrower than the original which could return other codes).
 * The original `detectLanguageFromText` in production/utils.ts supports more languages —
 * this function focuses on the two supported pipeline languages.
 */
export function detectLanguage(text: string): 'ar' | 'en' {
  return languageDetector.detect(text).language;
}
````

## File: packages/shared/src/services/logger.ts
````typescript
/**
 * Logging Service
 *
 * Centralized logging with:
 * - Log levels (debug, info, warn, error)
 * - Contextual prefixes
 * - Environment-aware output (verbose in dev, minimal in prod)
 * - Structured log format
 */

export enum LogLevel {
  DEBUG = 0,
  INFO = 1,
  WARN = 2,
  ERROR = 3,
  SILENT = 4,
}

interface LogEntry {
  level: LogLevel;
  timestamp: string;
  context: string;
  message: string;
  data?: unknown;
}

type LogCallback = (entry: LogEntry) => void;

class Logger {
  private level: LogLevel;
  private context: string;
  private callbacks: LogCallback[] = [];

  constructor(context: string = 'App', level?: LogLevel) {
    this.context = context;
    this.level = level ?? this.getDefaultLevel();
  }

  private getDefaultLevel(): LogLevel {
    // In production, only show warnings and errors
    // In development, show everything
    if (typeof window !== 'undefined') {
      return import.meta.env?.PROD ? LogLevel.WARN : LogLevel.DEBUG;
    }
    return process.env.NODE_ENV === 'production' ? LogLevel.WARN : LogLevel.DEBUG;
  }

  private formatTimestamp(): string {
    return new Date().toISOString();
  }

  private log(level: LogLevel, message: string, data?: unknown): void {
    if (level < this.level) return;

    const entry: LogEntry = {
      level,
      timestamp: this.formatTimestamp(),
      context: this.context,
      message,
      data,
    };

    // Notify callbacks (for external logging services)
    this.callbacks.forEach(cb => cb(entry));

    // Console output
    const prefix = `[${this.context}]`;
    const args = data !== undefined ? [prefix, message, data] : [prefix, message];

    switch (level) {
      case LogLevel.DEBUG:
        console.debug(...args);
        break;
      case LogLevel.INFO:
        console.info(...args);
        break;
      case LogLevel.WARN:
        console.warn(...args);
        break;
      case LogLevel.ERROR:
        console.error(...args);
        break;
    }
  }

  debug(message: string, data?: unknown): void {
    this.log(LogLevel.DEBUG, message, data);
  }

  info(message: string, data?: unknown): void {
    this.log(LogLevel.INFO, message, data);
  }

  warn(message: string, data?: unknown): void {
    this.log(LogLevel.WARN, message, data);
  }

  error(message: string, data?: unknown): void {
    this.log(LogLevel.ERROR, message, data);
  }

  /** Create a child logger with a sub-context */
  child(subContext: string): Logger {
    return new Logger(`${this.context}:${subContext}`, this.level);
  }

  /** Set the minimum log level */
  setLevel(level: LogLevel): void {
    this.level = level;
  }

  /** Add a callback for external logging services */
  addCallback(callback: LogCallback): void {
    this.callbacks.push(callback);
  }

  /** Remove a callback */
  removeCallback(callback: LogCallback): void {
    const index = this.callbacks.indexOf(callback);
    if (index > -1) {
      this.callbacks.splice(index, 1);
    }
  }
}

// Factory function to create loggers with different contexts
export function createLogger(context: string): Logger {
  return new Logger(context);
}

// Default application logger
export const logger = new Logger('App');

// Pre-configured loggers for common contexts
export const agentLogger = new Logger('Agent');
export const serverLogger = new Logger('Server');
export const exportLogger = new Logger('Export');
export const ffmpegLogger = new Logger('FFmpeg');
export const sunoLogger = new Logger('Suno');
export const geminiLogger = new Logger('Gemini');

export default logger;
````

## File: packages/shared/src/services/musicProducerAgentV2.ts
````typescript
/**
 * Music Producer Agent V2
 * 
 * Enhanced agent with tools to directly call Suno API.
 * Uses ChatGoogleGenerativeAI with bindTools for browser compatibility.
 * Features:
 * - Conversational music production assistant
 * - Direct Suno API integration via tool calling
 * - Best practices prompt engineering for Suno
 * - Arabic/Khaliji music expertise
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { tool } from "@langchain/core/tools";
import { HumanMessage, AIMessage, SystemMessage, ToolMessage, type BaseMessage } from "@langchain/core/messages";
import { z } from "zod";
import { API_KEY, MODELS } from "./shared/apiClient";
import {
  generateMusic,
  getTaskStatus,
  waitForCompletion,
  getCredits,
  isSunoConfigured,
  type SunoGeneratedTrack,
  type SunoGenerationConfig,
} from "./sunoService";

// --- Best Practices Prompt System ---

const SUNO_BEST_PRACTICES = `
## SUNO API BEST PRACTICES

### Prompt Structure (CRITICAL)
Always structure prompts with section tags. This dramatically improves output quality:

[Intro]
(Describe instrumental opening, mood setting)

[Verse 1]
(Vocal delivery notes: soft, powerful, whispered, etc.)
Actual lyrics line 1
Actual lyrics line 2
Actual lyrics line 3
Actual lyrics line 4

[Pre-Chorus]
(Build tension, transition notes)
Lyrics building to chorus

[Chorus]
(Full energy, memorable hook)
Main hook lyrics
Catchy repeated phrase

[Verse 2]
(Continue story, vary delivery)
More lyrics...

[Bridge]
(Contrast section, emotional peak or quiet moment)
Bridge lyrics...

[Outro]
(Resolution, fade out notes)
Final lyrics or instrumental fade

### Style Description Best Practices
Be VERY detailed in the style field. Include:
- Primary genre + subgenre (e.g., "Modern Khaliji Pop with Gulf influences")
- Vocal characteristics (e.g., "warm female vocal with gentle vibrato")
- Instrumentation (e.g., "Oud, Darbuka, subtle synth pads, string section")
- Production style (e.g., "polished modern production with traditional elements")
- Mood/energy (e.g., "romantic, nostalgic, medium tempo")
- Reference artists if helpful (e.g., "in the style of Balqees, Hussain Al Jassmi")

### Arabic Music Specifics
For Arabic/Khaliji music, specify:
- Maqam (scale): Hijaz for emotional/dramatic, Bayati for melancholic, Rast for joyful
- Rhythm: Malfuf (2/4 fast), Saidi (4/4 energetic), Maqsum (4/4 standard), Wahda (4/4 slow)
- Dialect in lyrics: Gulf (Khaliji), Egyptian, Levantine, etc.
- Traditional vs modern balance

### Model Selection Guide
- V5: Latest, best overall quality - USE THIS BY DEFAULT
- V4_5ALL: Better song structure, good for complex arrangements
- V4_5PLUS: Richer tones, enhanced variation
- V4_5: Smart prompts, faster generation
- V4: Legacy, improved vocals

### Parameter Guidelines
- styleWeight: 0.6-0.7 for balanced, 0.8+ for strict style adherence
- weirdnessConstraint: 0.3-0.5 for conventional, 0.6-0.8 for experimental
- negativeTags: Always exclude unwanted elements (e.g., "screaming, heavy distortion, off-key")
`;

const SYSTEM_PROMPT = `You are an expert AI music producer assistant specializing in creating professional-quality songs using Suno AI.

${SUNO_BEST_PRACTICES}

## YOUR WORKFLOW

1. GATHER INFORMATION (1-3 messages):
   - Ask about genre/style preferences
   - Understand the mood and theme
   - Get vocal preferences (gender, style, language)
   - Collect any specific lyrics or themes
   - For Arabic music: ask about dialect, maqam preference, traditional vs modern

2. CRAFT THE PERFECT REQUEST:
   - Build a detailed structured prompt with section tags
   - Write or refine lyrics in the requested language
   - Create a comprehensive style description
   - Set appropriate parameters

3. GENERATE MUSIC:
   - Use the generate_music tool to create the song
   - The tool will return track details when complete

## CONVERSATION STYLE
- Be enthusiastic and knowledgeable about music
- Ask ONE focused question at a time
- Offer creative suggestions and examples
- Understand both English and Arabic
- Be concise but thorough

## ARABIC MUSIC EXPERTISE
You are highly knowledgeable about:
- Khaliji (Gulf) pop: Balqees, Hussain Al Jassmi, Ahlam style
- Egyptian pop: Amr Diab, Sherine style
- Lebanese pop: Nancy Ajram, Elissa style
- Traditional: Tarab, Muwashahat
- Maqamat: Hijaz, Bayati, Rast, Nahawand, Saba, Kurd, Ajam
- Rhythms: Malfuf, Saidi, Maqsum, Baladi, Wahda, Ayoub

When the user is ready or after gathering enough information, use the generate_music tool to create their song.`;

// --- Tool Definitions ---

const generateMusicTool = tool(
  async (input) => {
    try {
      console.log("[MusicProducerV2] Generating music with:", {
        title: input.title,
        style: input.style.substring(0, 50) + "...",
        model: input.model,
      });

      const config: SunoGenerationConfig = {
        prompt: input.prompt,
        title: input.title,
        style: input.style,
        instrumental: input.instrumental,
        model: input.model,
        vocalGender: input.vocalGender,
        negativeTags: input.negativeTags,
        styleWeight: input.styleWeight,
        weirdnessConstraint: input.weirdnessConstraint,
      };

      const taskId = await generateMusic(config);
      
      return JSON.stringify({
        success: true,
        taskId,
        message: `Music generation started! Task ID: ${taskId}. The song is being created and will be ready in a few minutes.`,
      });
    } catch (error) {
      return JSON.stringify({
        success: false,
        error: error instanceof Error ? error.message : String(error),
      });
    }
  },
  {
    name: "generate_music",
    description: `Generate a song using Suno AI. Use this when you have gathered enough information about the user's music preferences. 
  
IMPORTANT: The prompt should be a FULL structured song with section tags like [Intro], [Verse], [Chorus], etc.
The style should be a DETAILED description including genre, mood, instruments, and vocal characteristics.`,
    schema: z.object({
      prompt: z.string().describe("Full structured song prompt with [Intro], [Verse], [Chorus], [Bridge], [Outro] tags and complete lyrics"),
      title: z.string().describe("Song title"),
      style: z.string().describe("Detailed style: genre, subgenre, vocal style, instruments, mood, production style"),
      instrumental: z.boolean().describe("True for instrumental only, false for vocal track"),
      vocalGender: z.enum(["m", "f"]).optional().describe("Vocal gender: 'm' for male, 'f' for female"),
      negativeTags: z.string().optional().describe("Comma-separated styles to avoid"),
      model: z.enum(["V4", "V4_5", "V4_5PLUS", "V4_5ALL", "V5"]).default("V5").describe("Suno model version"),
      styleWeight: z.number().min(0).max(1).default(0.65).describe("Style influence strength 0-1"),
      weirdnessConstraint: z.number().min(0).max(1).default(0.5).describe("Creativity level 0-1"),
    }),
  }
);

const checkCreditsTool = tool(
  async () => {
    try {
      const result = await getCredits();
      return JSON.stringify({
        success: true,
        credits: result.credits,
        message: result.credits >= 0 
          ? `You have ${result.credits} credits remaining.`
          : "Could not fetch credit balance.",
      });
    } catch (error) {
      return JSON.stringify({
        success: false,
        error: error instanceof Error ? error.message : String(error),
      });
    }
  },
  {
    name: "check_credits",
    description: "Check the remaining Suno API credits before generating music",
    schema: z.object({}),
  }
);

const checkTaskStatusTool = tool(
  async ({ taskId }) => {
    try {
      const result = await getTaskStatus(taskId);
      return JSON.stringify({
        success: true,
        status: result.status,
        tracks: result.tracks,
        errorMessage: result.errorMessage,
      });
    } catch (error) {
      return JSON.stringify({
        success: false,
        error: error instanceof Error ? error.message : String(error),
      });
    }
  },
  {
    name: "check_task_status",
    description: "Check the status of a music generation task",
    schema: z.object({
      taskId: z.string().describe("The task ID returned from generate_music"),
    }),
  }
);

// Tool registry for execution
async function executeTool(toolName: string, args: Record<string, unknown>): Promise<string> {
  let result: string | unknown;
  
  switch (toolName) {
    case "generate_music":
      result = await generateMusicTool.invoke(args as Parameters<typeof generateMusicTool.invoke>[0]);
      break;
    case "check_credits":
      result = await checkCreditsTool.invoke({});
      break;
    case "check_task_status":
      result = await checkTaskStatusTool.invoke(args as { taskId: string });
      break;
    default:
      return JSON.stringify({ error: `Unknown tool: ${toolName}` });
  }
  
  // Ensure we return a string
  if (typeof result === "string") {
    return result;
  }
  return JSON.stringify(result);
}

const tools = [generateMusicTool, checkCreditsTool, checkTaskStatusTool];

// --- Agent Configuration ---

export interface MusicProducerV2Config {
  model?: string;
  temperature?: number;
  maxIterations?: number;
  onTaskStarted?: (taskId: string) => void;
  onStatusUpdate?: (status: string) => void;
}

const DEFAULT_CONFIG: Required<Omit<MusicProducerV2Config, 'onTaskStarted' | 'onStatusUpdate'>> = {
  model: MODELS.TEXT,
  temperature: 0.8,
  maxIterations: 10,
};

// --- Agent Response Types ---

/**
 * Pending tool call that requires user confirmation before execution.
 * Used for human-in-the-loop confirmation flow.
 */
export interface PendingToolCall {
  id: string;
  name: string;
  args: Record<string, unknown>;
  /** Human-readable summary of what will happen */
  summary: string;
}

export interface AgentV2Response {
  type: "message" | "confirmation_required" | "generating" | "complete" | "error";
  message: string;
  taskId?: string;
  tracks?: SunoGeneratedTrack[];
  error?: string;
  /** Present when type is "confirmation_required" */
  pendingAction?: PendingToolCall;
}

export interface ConversationMessage {
  role: "user" | "assistant";
  content: string;
}


// --- Agent Class ---

export class MusicProducerAgentV2 {
  private model: ChatGoogleGenerativeAI | null = null;
  private modelWithTools: ReturnType<ChatGoogleGenerativeAI["bindTools"]> | null = null;
  private config: Required<Omit<MusicProducerV2Config, 'onTaskStarted' | 'onStatusUpdate'>>;
  private callbacks: Pick<MusicProducerV2Config, 'onTaskStarted' | 'onStatusUpdate'>;
  private conversationHistory: BaseMessage[] = [];
  private currentTaskId: string | null = null;
  /** Pending tool call awaiting user confirmation (human-in-the-loop) */
  private pendingToolCall: PendingToolCall | null = null;
  /** The AI response that contained the pending tool call */
  private pendingAIResponse: AIMessage | null = null;

  constructor(config: MusicProducerV2Config = {}) {
    this.config = { ...DEFAULT_CONFIG, ...config };
    this.callbacks = {
      onTaskStarted: config.onTaskStarted,
      onStatusUpdate: config.onStatusUpdate,
    };
  }

  private initialize(): ReturnType<ChatGoogleGenerativeAI["bindTools"]> {
    if (this.modelWithTools) return this.modelWithTools;

    if (!API_KEY) {
      throw new Error("Gemini API key is not configured");
    }

    if (!isSunoConfigured()) {
      throw new Error("Suno API key is not configured");
    }

    this.model = new ChatGoogleGenerativeAI({
      apiKey: API_KEY,
      model: this.config.model,
      temperature: this.config.temperature,
    });

    this.modelWithTools = this.model.bindTools(tools);
    return this.modelWithTools;
  }

  /**
   * Reset the conversation
   */
  reset(): void {
    this.conversationHistory = [];
    this.currentTaskId = null;
    this.pendingToolCall = null;
    this.pendingAIResponse = null;
  }

  /**
   * Check if there's a pending action awaiting confirmation
   */
  hasPendingAction(): boolean {
    return this.pendingToolCall !== null;
  }

  /**
   * Get the pending action details
   */
  getPendingAction(): PendingToolCall | null {
    return this.pendingToolCall;
  }

  /**
   * Cancel the pending action and continue chatting
   */
  cancelPendingAction(): void {
    if (this.pendingToolCall && this.pendingAIResponse) {
      // Add a tool message indicating cancellation
      this.conversationHistory.push(this.pendingAIResponse);
      this.conversationHistory.push(new ToolMessage({
        tool_call_id: this.pendingToolCall.id,
        content: JSON.stringify({
          cancelled: true,
          message: "User cancelled the action. Ask if they want to modify anything."
        }),
      }));
    }
    this.pendingToolCall = null;
    this.pendingAIResponse = null;
  }

  /**
   * Get conversation history
   */
  getHistory(): ConversationMessage[] {
    return this.conversationHistory
      .filter(msg => msg instanceof HumanMessage || msg instanceof AIMessage)
      .map(msg => ({
        role: (msg instanceof HumanMessage ? "user" : "assistant") as "user" | "assistant",
        content: typeof msg.content === "string" ? msg.content : JSON.stringify(msg.content),
      }));
  }

  /**
   * Get current task ID if generation is in progress
   */
  getCurrentTaskId(): string | null {
    return this.currentTaskId;
  }

  /**
   * Send a message to the agent with automatic tool execution loop.
   * When generate_music is called, returns confirmation_required instead of executing.
   */
  async chat(userMessage: string): Promise<AgentV2Response> {
    if (!userMessage?.trim()) {
      return { type: "error", message: "Message is required", error: "Empty message" };
    }

    try {
      const modelWithTools = this.initialize();

      // Add system message if this is the first message
      if (this.conversationHistory.length === 0) {
        this.conversationHistory.push(new SystemMessage(SYSTEM_PROMPT));
      }

      // Add user message
      this.conversationHistory.push(new HumanMessage(userMessage));

      let iterations = 0;
      let finalOutput = "";

      // Tool execution loop
      while (iterations < this.config.maxIterations) {
        iterations++;

        const response = await modelWithTools.invoke(this.conversationHistory);

        // Check if there are tool calls
        const toolCalls = response.tool_calls;
        console.log("[MusicProducerV2] Tool calls in response:", toolCalls?.map(tc => tc.name) || "none");

        if (!toolCalls || toolCalls.length === 0) {
          // No tool calls - this is the final response
          this.conversationHistory.push(response);
          finalOutput = typeof response.content === "string"
            ? response.content
            : JSON.stringify(response.content);
          break;
        }

        // Check if generate_music is being called - requires confirmation
        const generateMusicCall = toolCalls.find(tc => tc.name === "generate_music");
        if (generateMusicCall) {
          // Store pending action for confirmation (don't add response to history yet)
          const args = generateMusicCall.args as Record<string, unknown>;
          this.pendingAIResponse = response;
          this.pendingToolCall = {
            id: generateMusicCall.id || "generate_music",
            name: "generate_music",
            args,
            summary: this.createConfirmationSummary(args),
          };

          // Extract message content for the confirmation prompt
          const messageContent = typeof response.content === "string"
            ? response.content
            : "";

          console.log("[MusicProducerV2] Returning confirmation_required with pendingAction:", {
            id: this.pendingToolCall.id,
            name: this.pendingToolCall.name,
            summary: this.pendingToolCall.summary,
          });

          return {
            type: "confirmation_required",
            message: messageContent || "I'm ready to generate your song! Please confirm the details below.",
            pendingAction: this.pendingToolCall,
          };
        }

        // For other tools, execute normally
        this.conversationHistory.push(response);

        for (const toolCall of toolCalls) {
          const toolName = toolCall.name;

          try {
            const toolResult = await executeTool(toolName, toolCall.args as Record<string, unknown>);
            this.conversationHistory.push(new ToolMessage({
              tool_call_id: toolCall.id || toolName,
              content: toolResult,
            }));
          } catch (error) {
            this.conversationHistory.push(new ToolMessage({
              tool_call_id: toolCall.id || toolName,
              content: JSON.stringify({
                error: error instanceof Error ? error.message : String(error)
              }),
            }));
          }
        }
      }

      // Check if a task was started
      if (this.currentTaskId) {
        return {
          type: "generating",
          message: finalOutput || `Music generation started! Task ID: ${this.currentTaskId}`,
          taskId: this.currentTaskId,
        };
      }

      return {
        type: "message",
        message: finalOutput,
      };

    } catch (error) {
      console.error("[MusicProducerV2] Chat error:", error);
      return {
        type: "error",
        message: "Sorry, something went wrong. Please try again.",
        error: error instanceof Error ? error.message : String(error),
      };
    }
  }

  /**
   * Create a human-readable summary of the pending generation action
   */
  private createConfirmationSummary(args: Record<string, unknown>): string {
    const title = args.title as string || "Untitled";
    const style = args.style as string || "";
    const instrumental = args.instrumental as boolean;
    const vocalGender = args.vocalGender as string;
    const model = args.model as string || "V5";

    let summary = `🎵 **${title}**\n`;
    summary += `📀 Style: ${style.substring(0, 100)}${style.length > 100 ? "..." : ""}\n`;
    summary += `🎤 ${instrumental ? "Instrumental only" : `Vocals: ${vocalGender === "f" ? "Female" : vocalGender === "m" ? "Male" : "Auto"}`}\n`;
    summary += `⚙️ Model: Suno ${model}`;

    return summary;
  }

  /**
   * Confirm and execute the pending action.
   * Call this after the user clicks the confirm button.
   */
  async confirmAndExecute(): Promise<AgentV2Response> {
    if (!this.pendingToolCall || !this.pendingAIResponse) {
      return {
        type: "error",
        message: "No pending action to confirm.",
        error: "No pending action",
      };
    }

    try {
      // Add the AI response to history now that it's confirmed
      this.conversationHistory.push(this.pendingAIResponse);

      // Execute the pending tool
      const toolResult = await executeTool(
        this.pendingToolCall.name,
        this.pendingToolCall.args
      );

      // Add tool result to history
      this.conversationHistory.push(new ToolMessage({
        tool_call_id: this.pendingToolCall.id,
        content: toolResult,
      }));

      // Check if generation started
      try {
        const parsed = JSON.parse(toolResult);
        if (parsed.success && parsed.taskId) {
          this.currentTaskId = parsed.taskId;
          if (this.currentTaskId) {
            this.callbacks.onTaskStarted?.(this.currentTaskId);
          }

          // Get title before clearing pending state
          const title = this.pendingToolCall?.args.title || "Untitled";

          // Clear pending state
          this.pendingToolCall = null;
          this.pendingAIResponse = null;

          return {
            type: "generating",
            message: `🎶 Music generation started! Your song "${title}" is being created...`,
            taskId: this.currentTaskId ?? undefined,
          };
        } else if (parsed.error) {
          throw new Error(parsed.error);
        }
      } catch (parseError) {
        // If not JSON, treat as error
        if (parseError instanceof SyntaxError) {
          throw new Error(toolResult);
        }
        throw parseError;
      }

      // Unexpected state
      return {
        type: "error",
        message: "Generation may have started but couldn't confirm the task ID.",
        error: "Unexpected response format",
      };

    } catch (error) {
      console.error("[MusicProducerV2] Confirm error:", error);

      // Clear pending state on error
      this.pendingToolCall = null;
      this.pendingAIResponse = null;

      return {
        type: "error",
        message: `Failed to start generation: ${error instanceof Error ? error.message : String(error)}`,
        error: error instanceof Error ? error.message : String(error),
      };
    }
  }

  /**
   * Poll for task completion
   */
  async pollForCompletion(taskId: string, maxWaitMs: number = 10 * 60 * 1000): Promise<SunoGeneratedTrack[]> {
    return waitForCompletion(taskId, maxWaitMs);
  }
}

// --- Factory Function ---

export function createMusicProducerAgentV2(config?: MusicProducerV2Config): MusicProducerAgentV2 {
  return new MusicProducerAgentV2(config);
}
````

## File: packages/shared/src/services/narratorService.ts
````typescript
/**
 * Narrator Service
 * 
 * Text-to-Speech generation for video narration using Gemini TTS.
 * Uses the gemini-2.5-flash-preview-tts model with audio response modality.
 * 
 * Responsibilities:
 * - Convert narration scripts to speech audio
 * - Match voice style to emotional tone
 * - Generate audio segments for each scene
 */

import { ai, API_KEY, MODELS, withRetry } from "./shared/apiClient";
import { Scene, NarrationSegment, EmotionalTone, InstructionTriplet, VideoFormat, ShotlistEntry, ScreenplayScene } from "../types";
import { ParallelExecutionEngine } from "./parallelExecutionEngine";
import { cleanForTTS } from "./textSanitizer";
import { VideoPurpose, type LanguageCode } from "../constants";
import { traceAsync } from "./tracing";
import { cloudAutosave } from "./cloudStorageService";
import { logAICall } from "./aiLogService";
import { getEffectiveTriplet, getEffectiveLegacyTone } from "./tripletUtils";
import { tripletToPromptFragments } from "./prompt/vibeLibrary";
import { convertMarkersToDirectorNote } from "./tts/deliveryMarkers";

// --- TTS Throttling (mutex-safe for parallel callers) ---
// Gemini TTS has rate limits; enforce minimum delay between calls via a promise-chain mutex.
// The old TOCTOU pattern (read lastTtsCallTime → check → set) allowed concurrent callers to
// both see "enough time has passed" and proceed simultaneously. The gate below serializes all
// TTS callers globally so only one call is in-flight at a time with a mandatory 2s cooldown.

const TTS_INTER_CALL_DELAY_MS = 2000;

// A promise chain that gates TTS calls sequentially with enforced spacing.
let _ttsGate: Promise<void> = Promise.resolve();

/**
 * Acquire a TTS slot — returns a release function that the caller MUST invoke
 * (in a finally block) after the API call completes.
 * The release function starts the 2-second cooldown timer for the next caller.
 */
async function acquireTtsSlot(): Promise<() => void> {
    let releaseCallback!: () => void;
    const thisSlot = new Promise<void>(resolve => { releaseCallback = resolve; });

    // Chain onto the existing gate so callers queue up in arrival order.
    const prevGate = _ttsGate;
    _ttsGate = prevGate.then(() => thisSlot);

    // Wait until all previous callers have finished AND their cooldown has elapsed.
    await prevGate;

    // Our turn. Return the release function — caller must invoke it in finally{}.
    // It starts TTS_INTER_CALL_DELAY_MS timer then unblocks the next caller.
    return () => {
        setTimeout(releaseCallback, TTS_INTER_CALL_DELAY_MS);
    };
}

// --- Voice Configuration ---

/**
 * Available Gemini TTS voices
 * See: https://cloud.google.com/text-to-speech/docs/voices
 */
export const TTS_VOICES = {
    // English voices with different characteristics
    KORE: "Kore",      // Warm, friendly female voice
    CHARON: "Charon",  // Deep, authoritative male voice  
    PUCK: "Puck",      // Energetic, youthful voice
    FENRIR: "Fenrir",  // Strong, dramatic voice
    AOEDE: "Aoede",    // Calm, soothing female voice
    LEDA: "Leda",      // Professional, clear female voice
    ORUS: "Orus",      // Balanced, neutral male voice
    ZEPHYR: "Zephyr",  // Light, airy voice
} as const;

export type TTSVoice = typeof TTS_VOICES[keyof typeof TTS_VOICES];

/**
 * Voice configuration for TTS
 */
export interface VoiceConfig {
    voiceName: TTSVoice;
    pitch?: number;      // -20.0 to 20.0, 0 is normal
    speakingRate?: number; // 0.25 to 4.0, 1.0 is normal
}

/**
 * Style prompt configuration for Gemini 2.5 TTS "Director's Notes"
 * These natural language instructions steer the voice performance.
 */
export interface StylePrompt {
    /** Character persona (e.g., "A wise old storyteller", "An excited sports commentator") */
    persona?: string;
    /** Emotional delivery (e.g., "warm and reassuring", "tense and suspenseful") */
    emotion?: string;
    /** Speaking pace (e.g., "slow and deliberate", "fast-paced and energetic") */
    pacing?: string;
    /** Accent or style (e.g., "British narrator", "casual conversational") */
    accent?: string;
    /** Custom director's note (overrides other style options if provided) */
    customDirectorNote?: string;
}

/**
 * Extended voice configuration with style prompts
 */
export interface ExtendedVoiceConfig extends VoiceConfig {
    stylePrompt?: StylePrompt;
}

/**
 * Maps emotional tone to recommended voice settings with style prompts
 */
const TONE_VOICE_MAP: Record<EmotionalTone, ExtendedVoiceConfig> = {
    professional: {
        voiceName: TTS_VOICES.LEDA,
        pitch: 0,
        speakingRate: 1.1,
        stylePrompt: {
            persona: "A polished corporate presenter delivering a keynote",
            emotion: "clear, authoritative, confident, and composed",
            pacing: "measured and articulate with crisp enunciation"
        }
    },
    dramatic: {
        voiceName: TTS_VOICES.FENRIR,
        pitch: -2,
        speakingRate: 0.95,
        stylePrompt: {
            persona: "A legendary storyteller narrating an epic tale by firelight",
            emotion: "intense, powerful, gripping, with gravitas",
            pacing: "deliberate with dramatic pauses before key revelations"
        }
    },
    friendly: {
        voiceName: TTS_VOICES.PUCK,
        pitch: 2,
        speakingRate: 1.2,
        stylePrompt: {
            persona: "An enthusiastic best friend sharing an exciting story",
            emotion: "warm, cheerful, genuine, and inviting",
            pacing: "natural and conversational with energetic emphasis"
        }
    },
    urgent: {
        voiceName: TTS_VOICES.CHARON,
        pitch: -1,
        speakingRate: 1.3,
        stylePrompt: {
            persona: "A field correspondent reporting live from a crisis zone",
            emotion: "alert, compelling, serious, with controlled intensity",
            pacing: "rapid and purposeful, each word hitting with weight"
        }
    },
    calm: {
        voiceName: TTS_VOICES.AOEDE,
        pitch: -3,
        speakingRate: 0.9,
        stylePrompt: {
            persona: "A gentle guide leading a moonlit meditation",
            emotion: "serene, soothing, peaceful, like a warm breeze",
            pacing: "slow and flowing with long restful pauses between phrases"
        }
    },
};

// --- Configuration ---

export interface NarratorConfig {
    model?: string;
    defaultVoice?: TTSVoice;
    /** Video purpose for auto-selecting appropriate style prompts */
    videoPurpose?: VideoPurpose;
    /** Override style prompt (takes precedence over auto-selection) */
    styleOverride?: StylePrompt;
    /** Content language - affects voice selection for multilingual support */
    language?: LanguageCode;
}

/**
 * Language-to-voice mapping for multilingual TTS support.
 * Some voices work better for certain languages.
 */
const LANGUAGE_VOICE_MAP: Partial<Record<string, TTSVoice>> = {
    'ar': TTS_VOICES.AOEDE,   // Arabic - Aoede has better multilingual support
    'en': TTS_VOICES.KORE,    // English - Kore is the default English voice
    'es': TTS_VOICES.AOEDE,   // Spanish
    'fr': TTS_VOICES.AOEDE,   // French
    'de': TTS_VOICES.ORUS,    // German
    'ru': TTS_VOICES.CHARON,  // Russian
    'zh': TTS_VOICES.AOEDE,   // Chinese
    'ja': TTS_VOICES.AOEDE,   // Japanese
    'ko': TTS_VOICES.AOEDE,   // Korean
    'hi': TTS_VOICES.AOEDE,   // Hindi
    'tr': TTS_VOICES.AOEDE,   // Turkish
    'fa': TTS_VOICES.AOEDE,   // Persian
    'ur': TTS_VOICES.AOEDE,   // Urdu
    'he': TTS_VOICES.AOEDE,   // Hebrew
};

const DEFAULT_CONFIG: Required<Omit<NarratorConfig, 'styleOverride' | 'language'>> & { styleOverride?: StylePrompt; language?: LanguageCode } = {
    model: MODELS.TTS,
    defaultVoice: TTS_VOICES.KORE,
    videoPurpose: "documentary",
};

// --- Multi-Voice Dialogue Support ---

/**
 * Character voice mapping for dialogue scenes.
 * Different voices for narrator, male, and female characters.
 */
export const CHARACTER_VOICE_MAP = {
    narrator: TTS_VOICES.KORE,       // Default narrator voice
    male: TTS_VOICES.CHARON,         // Deep male voice for male characters
    female: TTS_VOICES.AOEDE,        // Soft female voice for female characters
    elder: TTS_VOICES.ORUS,          // Wise elder voice
    youth: TTS_VOICES.PUCK,          // Energetic young voice
    mysterious: TTS_VOICES.FENRIR,   // Dramatic mysterious voice
} as const;

/**
 * Represents a segment of dialogue with speaker identification
 */
export interface DialogueSegment {
    /** Type of speaker */
    speaker: 'narrator' | 'male' | 'female' | 'elder' | 'youth' | 'mysterious';
    /** The text spoken by this speaker */
    text: string;
    /** Whether this is quoted dialogue */
    isDialogue: boolean;
}

/**
 * Detects dialogue patterns in narration scripts and splits into segments.
 * Supports patterns like:
 * - "Hello," said John.
 * - John said, "Hello."
 * - "Hello!" (standalone dialogue)
 * 
 * @param script - The narration script to analyze
 * @returns Array of dialogue segments with speaker identification
 */
export function detectDialogue(script: string): DialogueSegment[] {
    const segments: DialogueSegment[] = [];

    // Pattern to match quoted dialogue with optional speaker tags
    // Matches: "text" [said/asked/replied etc] [name], or [name] [said etc] "text"
    const dialoguePattern = /"([^"]+)"/g;
    const speakerHintPattern = /\b(he|she|the man|the woman|the old man|the elder|the boy|the girl|grandfather|grandmother)\b/i;
    const maleSpeakerPattern = /\b(he|man|boy|father|grandfather|king|lord|sir|mr|uncle|brother|son)\b/i;
    const femaleSpeakerPattern = /\b(she|woman|girl|mother|grandmother|queen|lady|mrs|miss|aunt|sister|daughter)\b/i;
    const elderPattern = /\b(old|elder|grandfather|grandmother|ancient|wise|sage)\b/i;
    const youthPattern = /\b(young|boy|girl|child|kid|youth|teen)\b/i;

    let lastIndex = 0;
    let match;

    while ((match = dialoguePattern.exec(script)) !== null) {
        const dialogueText = match[1];
        const matchStart = match.index;
        const matchEnd = matchStart + match[0].length;

        // Add narration before dialogue
        if (matchStart > lastIndex) {
            const narrationText = script.slice(lastIndex, matchStart).trim();
            if (narrationText) {
                segments.push({
                    speaker: 'narrator',
                    text: narrationText,
                    isDialogue: false,
                });
            }
        }

        // Determine speaker from context (look at surrounding text)
        const contextStart = Math.max(0, matchStart - 50);
        const contextEnd = Math.min(script.length, matchEnd + 50);
        const context = script.slice(contextStart, contextEnd);

        let speaker: DialogueSegment['speaker'] = 'narrator';

        if (elderPattern.test(context)) {
            speaker = 'elder';
        } else if (youthPattern.test(context)) {
            speaker = 'youth';
        } else if (femaleSpeakerPattern.test(context)) {
            speaker = 'female';
        } else if (maleSpeakerPattern.test(context)) {
            speaker = 'male';
        }

        segments.push({
            speaker,
            text: dialogueText || "",
            isDialogue: true,
        });

        lastIndex = matchEnd;
    }

    // Add remaining narration
    if (lastIndex < script.length) {
        const remainingText = script.slice(lastIndex).trim();
        if (remainingText) {
            segments.push({
                speaker: 'narrator',
                text: remainingText,
                isDialogue: false,
            });
        }
    }

    // If no dialogue found, return entire script as narrator
    if (segments.length === 0) {
        segments.push({
            speaker: 'narrator',
            text: script,
            isDialogue: false,
        });
    }

    return segments;
}

/**
 * Check if a script contains dialogue that would benefit from multi-voice
 */
export function hasDialogue(script: string): boolean {
    const dialoguePattern = /"[^"]+"/;
    return dialoguePattern.test(script);
}

/**
 * Maps video purpose to recommended style prompts.
 * These enhance the base emotional tone with purpose-specific delivery.
 */
const PURPOSE_STYLE_MAP: Record<VideoPurpose, StylePrompt> = {
    music_video: {
        persona: "A cinematic music video narrator",
        emotion: "emotional, evocative, and artistic",
        pacing: "rhythmic, matching the musical flow"
    },
    social_short: {
        persona: "A trendy social media content creator",
        emotion: "energetic, punchy, and attention-grabbing",
        pacing: "fast and dynamic with quick delivery"
    },
    documentary: {
        persona: "A professional documentary narrator",
        emotion: "informative, engaging, and authoritative",
        pacing: "measured and thoughtful"
    },
    commercial: {
        persona: "A persuasive commercial voice-over artist",
        emotion: "confident, trustworthy, and compelling",
        pacing: "polished and well-articulated"
    },
    podcast_visual: {
        persona: "A conversational podcast host",
        emotion: "warm, relatable, and authentic",
        pacing: "natural and conversational"
    },
    lyric_video: {
        persona: "A poetic lyric narrator",
        emotion: "expressive, melodic, and heartfelt",
        pacing: "flowing and synchronized with the music"
    },
    storytelling: {
        persona: "A master storyteller weaving an epic tale",
        emotion: "dramatic, immersive, and captivating",
        pacing: "natural with occasional dramatic pauses"
    },
    educational: {
        persona: "A friendly and knowledgeable teacher",
        emotion: "clear, encouraging, and patient",
        pacing: "steady and easy to follow"
    },
    horror_mystery: {
        persona: "A mysterious narrator telling a chilling tale",
        emotion: "eerie, suspenseful, and unsettling",
        pacing: "steady with subtle tension, not too slow"
    },
    travel: {
        persona: "An adventurous travel documentary host",
        emotion: "wonder-filled, inspiring, and enthusiastic",
        pacing: "flowing and descriptive"
    },
    motivational: {
        persona: "An inspiring motivational speaker",
        emotion: "powerful, uplifting, and empowering",
        pacing: "building energy with impactful delivery"
    },
    news_report: {
        persona: "A professional news anchor",
        emotion: "objective, clear, and authoritative",
        pacing: "crisp and well-articulated"
    },
    story_drama: {
        persona: "A dramatic storyteller with emotional depth",
        emotion: "intense, moving, and deeply felt",
        pacing: "measured with emotional crescendos"
    },
    story_comedy: {
        persona: "A witty comedic narrator",
        emotion: "light-hearted, playful, and humorous",
        pacing: "upbeat with well-timed comedic beats"
    },
    story_thriller: {
        persona: "A tense thriller narrator",
        emotion: "suspenseful, gripping, and urgent",
        pacing: "tight and relentless with sudden shifts"
    },
    story_scifi: {
        persona: "A futuristic sci-fi narrator",
        emotion: "awe-inspiring, cerebral, and visionary",
        pacing: "steady with moments of wonder"
    },
    story_action: {
        persona: "A high-energy action narrator",
        emotion: "thrilling, explosive, and adrenaline-fueled",
        pacing: "fast and intense with punchy delivery"
    },
    story_fantasy: {
        persona: "An enchanting fantasy storyteller",
        emotion: "magical, wondrous, and mythical",
        pacing: "flowing and grand with epic moments"
    },
    story_romance: {
        persona: "A tender romantic narrator",
        emotion: "warm, passionate, and intimate",
        pacing: "gentle and heartfelt"
    },
    story_historical: {
        persona: "A distinguished historical narrator",
        emotion: "reverent, authoritative, and evocative",
        pacing: "stately and deliberate"
    },
    story_animation: {
        persona: "A lively animated story narrator",
        emotion: "colorful, expressive, and fun",
        pacing: "dynamic and energetic"
    },
};

// --- Error Types ---

export class NarratorError extends Error {
    constructor(
        message: string,
        public readonly code: "API_FAILURE" | "INVALID_INPUT" | "AUDIO_ERROR" | "NOT_CONFIGURED",
        public readonly originalError?: Error
    ) {
        super(message);
        this.name = "NarratorError";
    }
}

// --- Main TTS Functions ---

/**
 * Build a natural language style prompt (Director's Note) for Gemini TTS.
 * This prepends instructions to the text to steer voice performance.
 * 
 * @param stylePrompt - Style configuration
 * @returns Formatted director's note string
 */
function buildDirectorNote(stylePrompt?: StylePrompt): string {
    if (!stylePrompt) return "";

    // If custom note provided, use it directly
    if (stylePrompt.customDirectorNote) {
        return stylePrompt.customDirectorNote;
    }

    const parts: string[] = [];

    if (stylePrompt.persona) {
        parts.push(`Speak as ${stylePrompt.persona}`);
    }

    if (stylePrompt.emotion) {
        parts.push(`with a ${stylePrompt.emotion} tone`);
    }

    if (stylePrompt.pacing) {
        parts.push(`at a ${stylePrompt.pacing} pace`);
    }

    if (stylePrompt.accent) {
        parts.push(`in a ${stylePrompt.accent} style`);
    }

    return parts.length > 0 ? parts.join(", ") : "";
}

/**
 * Merge multiple style prompts, with later ones taking precedence.
 * This allows layering: base tone style + purpose style + custom override.
 */
function mergeStylePrompts(...prompts: (StylePrompt | undefined)[]): StylePrompt {
    const merged: StylePrompt = {};

    for (const prompt of prompts) {
        if (!prompt) continue;

        // Custom director note overrides everything
        if (prompt.customDirectorNote) {
            return { customDirectorNote: prompt.customDirectorNote };
        }

        if (prompt.persona) merged.persona = prompt.persona;
        if (prompt.emotion) merged.emotion = prompt.emotion;
        if (prompt.pacing) merged.pacing = prompt.pacing;
        if (prompt.accent) merged.accent = prompt.accent;
    }

    return merged;
}

/**
 * Get the best style prompt for a given context.
 * Combines emotional tone, video purpose, and any custom overrides.
 * 
 * @param emotionalTone - The scene's emotional tone
 * @param videoPurpose - The video's purpose (optional)
 * @param styleOverride - Custom style override (optional)
 * @returns Merged style prompt
 */
export function getAutoStylePrompt(
    emotionalTone: EmotionalTone,
    videoPurpose?: VideoPurpose,
    styleOverride?: StylePrompt
): StylePrompt {
    const toneStyle = TONE_VOICE_MAP[emotionalTone]?.stylePrompt;
    const purposeStyle = videoPurpose ? PURPOSE_STYLE_MAP[videoPurpose] : undefined;

    // Layer: tone style (base) -> purpose style (context) -> override (custom)
    return mergeStylePrompts(toneStyle, purposeStyle, styleOverride);
}

/**
 * Build a Director's Note from an InstructionTriplet.
 * Looks up vibe term prompt fragments and assembles a rich voice direction.
 */
export function buildTripletDirectorNote(triplet: InstructionTriplet): string {
    const { emotionFragment, cinematicFragment, atmosphereFragment } = tripletToPromptFragments(triplet);

    return `Deliver this with ${emotionFragment}, matching the visual intensity of ${cinematicFragment}, as if speaking within ${atmosphereFragment}`;
}

/**
 * Format text with director's note for Gemini TTS.
 * Uses the natural language prompt format: "[Director's Note]: [Text]"
 * 
 * @param text - The text to speak
 * @param stylePrompt - Optional style configuration
 * @returns Formatted text with director's note
 */
function formatTextWithStyle(text: string, stylePrompt?: StylePrompt): string {
    // First, extract delivery markers from the text
    const { directorInstructions: markerInstructions, cleanText } = convertMarkersToDirectorNote(text);

    // Build the base director note from style prompt
    const baseNote = buildDirectorNote(stylePrompt);

    // Combine: base style note + marker-derived instructions
    const parts = [baseNote, markerInstructions].filter(Boolean);
    const combinedNote = parts.join(". ");

    if (!combinedNote) {
        return cleanText;
    }

    // Gemini 2.5 TTS format: prepend style instruction
    return `${combinedNote}: "${cleanText}"`;
}

/**
 * Create a WAV header for PCM audio data.
 * Gemini TTS returns raw PCM (L16) at 24kHz, 16-bit mono.
 * We need to add WAV headers for browser playback.
 * 
 * @param pcmDataLength - Length of PCM data in bytes
 * @param sampleRate - Sample rate (default 24000 for Gemini TTS)
 * @param numChannels - Number of channels (default 1 for mono)
 * @param bitsPerSample - Bits per sample (default 16)
 * @returns WAV header as Uint8Array
 */
function createWavHeader(
    pcmDataLength: number,
    sampleRate: number = 24000,
    numChannels: number = 1,
    bitsPerSample: number = 16
): Uint8Array {
    const byteRate = sampleRate * numChannels * (bitsPerSample / 8);
    const blockAlign = numChannels * (bitsPerSample / 8);
    const dataSize = pcmDataLength;
    const fileSize = 36 + dataSize;

    const header = new ArrayBuffer(44);
    const view = new DataView(header);

    // RIFF chunk descriptor
    writeString(view, 0, 'RIFF');
    view.setUint32(4, fileSize, true);
    writeString(view, 8, 'WAVE');

    // fmt sub-chunk
    writeString(view, 12, 'fmt ');
    view.setUint32(16, 16, true); // Subchunk1Size (16 for PCM)
    view.setUint16(20, 1, true); // AudioFormat (1 for PCM)
    view.setUint16(22, numChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, byteRate, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bitsPerSample, true);

    // data sub-chunk
    writeString(view, 36, 'data');
    view.setUint32(40, dataSize, true);

    return new Uint8Array(header);
}

/**
 * Helper to write ASCII string to DataView
 */
function writeString(view: DataView, offset: number, str: string): void {
    for (let i = 0; i < str.length; i++) {
        view.setUint8(offset + i, str.charCodeAt(i));
    }
}

/**
 * Convert raw PCM data to WAV format for browser playback.
 * 
 * @param pcmData - Raw PCM audio data (L16, 24kHz, mono)
 * @returns WAV blob that can be played in browser
 */
function pcmToWav(pcmData: Uint8Array): Blob {
    const wavHeader = createWavHeader(pcmData.length);
    const wavData = new Uint8Array(wavHeader.length + pcmData.length);
    wavData.set(wavHeader, 0);
    wavData.set(pcmData, wavHeader.length);
    return new Blob([wavData], { type: 'audio/wav' });
}

/**
 * Generate speech audio from text using Gemini TTS.
 * Supports Gemini 2.5 TTS "Director's Notes" for voice steering.
 * 
 * @param text - The text to convert to speech
 * @param voiceConfig - Voice configuration (or emotional tone)
 * @param config - Optional narrator config
 * @returns Audio blob as WAV (playable in browser)
 */
export const synthesizeSpeech = traceAsync(
    async function synthesizeSpeechImpl(
        text: string,
        voiceConfig: ExtendedVoiceConfig | EmotionalTone = "friendly",
        config?: NarratorConfig
    ): Promise<Blob> {
        if (!API_KEY) {
            throw new NarratorError(
                "Gemini API key is not configured",
                "NOT_CONFIGURED"
            );
        }

        if (!text?.trim()) {
            throw new NarratorError(
                "Text is required for speech synthesis",
                "INVALID_INPUT"
            );
        }

        // Resolve voice config from emotional tone if needed
        const resolvedConfig: ExtendedVoiceConfig = typeof voiceConfig === "string"
            ? TONE_VOICE_MAP[voiceConfig]
            : voiceConfig;

        const mergedConfig = { ...DEFAULT_CONFIG, ...config };

        // Format text with style prompt (Director's Note) for Gemini 2.5 TTS
        const styledText = formatTextWithStyle(text, resolvedConfig.stylePrompt);

        console.log(`[Narrator] Synthesizing speech: "${text.substring(0, 50)}..." with voice ${resolvedConfig.voiceName}`);
        if (resolvedConfig.stylePrompt) {
            console.log(`[Narrator] Using style prompt: ${buildDirectorNote(resolvedConfig.stylePrompt)}`);
        }

        // Acquire TTS slot (mutex) to prevent rate limiting.
        // MUST release in finally{} so the next caller can proceed after 2s cooldown.
        const releaseSlot = await acquireTtsSlot();

        // TTS is prone to transient 500 errors - use more aggressive retry settings
        // NarratorError wrapping is OUTSIDE withRetry so the raw API error (with .status)
        // reaches withRetry's isRetryable check instead of a wrapped NarratorError.
        try {
            return await withRetry(async () => {
                // Use the Gemini TTS model with audio response modality
                const response = await ai.models.generateContent({
                    model: mergedConfig.model,
                    contents: [
                        {
                            role: "user",
                            parts: [{ text: styledText }],
                        },
                    ],
                    config: {
                        responseModalities: ["AUDIO"],
                        speechConfig: {
                            voiceConfig: {
                                prebuiltVoiceConfig: {
                                    voiceName: resolvedConfig.voiceName,
                                },
                            },
                        },
                    },
                });

                // Extract audio data from response
                const audioData = response.candidates?.[0]?.content?.parts?.[0]?.inlineData;

                if (!audioData?.data || !audioData?.mimeType) {
                    throw new NarratorError(
                        "No audio data in response",
                        "API_FAILURE"
                    );
                }

                // Convert base64 to Uint8Array
                const binaryString = atob(audioData.data);
                const pcmData = new Uint8Array(binaryString.length);
                for (let i = 0; i < binaryString.length; i++) {
                    pcmData[i] = binaryString.charCodeAt(i);
                }

                // Gemini TTS returns raw PCM (L16) at 24kHz, 16-bit mono
                // Convert to WAV format for browser playback
                console.log(`[Narrator] Received ${pcmData.length} bytes of PCM audio, converting to WAV`);
                return pcmToWav(pcmData);
            }, 5, 2000, 2); // 5 retries, starting at 2s delay, doubling each time (2s, 4s, 8s, 16s, 30s cap)
        } catch (error) {
            console.error("[Narrator] TTS synthesis failed:", error);
            throw new NarratorError(
                `Speech synthesis failed: ${error instanceof Error ? error.message : String(error)}`,
                "API_FAILURE",
                error instanceof Error ? error : undefined
            );
        } finally {
            releaseSlot(); // Starts 2s cooldown; next caller proceeds after it elapses.
        }
    },
    "synthesizeSpeech",
    {
        runType: "tool",
        metadata: { service: "narrator", operation: "tts" },
        tags: ["tts", "gemini", "audio"],
    }
);

/**
 * Calculate actual audio duration from WAV blob.
 * Since we convert to WAV with known parameters (24kHz, 16-bit, mono),
 * we can calculate duration precisely.
 * 
 * @param audioBlob - The WAV audio blob
 * @returns Duration in seconds
 */
export function calculateAudioDuration(audioBlob: Blob): number {
    // WAV header is 44 bytes, rest is PCM data
    // PCM at 24kHz, 16-bit (2 bytes), mono = 48000 bytes per second
    const WAV_HEADER_SIZE = 44;
    const BYTES_PER_SECOND = 24000 * 2 * 1; // sampleRate * bytesPerSample * channels

    const pcmDataSize = audioBlob.size - WAV_HEADER_SIZE;
    return pcmDataSize / BYTES_PER_SECOND;
}

/**
 * Generate narration for a single scene.
 * Automatically applies style prompts based on emotional tone and video purpose.
 *
 * @param scene - The scene to narrate
 * @param config - Optional narrator config (includes videoPurpose for auto-styling)
 * @param sessionId - Optional session ID for cloud autosave
 * @returns Narration segment with audio blob
 */
export async function narrateScene(
    scene: Scene,
    config?: NarratorConfig,
    sessionId?: string
): Promise<NarrationSegment> {
    console.log(`[Narrator] Narrating scene: ${scene.name}`);

    // Resolve tone via triplet bridge (backward compatible)
    const effectiveTone = getEffectiveLegacyTone(scene);

    // Get base voice config from effective emotional tone
    const baseVoiceConfig = TONE_VOICE_MAP[effectiveTone];

    // Check if language-specific voice should be used
    const languageVoice = config?.language && config.language !== 'auto'
        ? LANGUAGE_VOICE_MAP[config.language]
        : undefined;

    // Build style prompt: triplet-based if available, else legacy auto-style
    let stylePrompt: StylePrompt;

    if (scene.instructionTriplet) {
        // New system: rich Director's Note from InstructionTriplet
        const tripletNote = buildTripletDirectorNote(scene.instructionTriplet);
        stylePrompt = { customDirectorNote: tripletNote };
        console.log(`[Narrator] Using triplet-based director note for "${scene.name}"`);
    } else {
        // Legacy: auto-generate from tone + purpose + override
        stylePrompt = getAutoStylePrompt(
            effectiveTone,
            config?.videoPurpose,
            config?.styleOverride
        );
    }

    // Create enhanced voice config with auto-generated style
    // Language-specific voice takes precedence if set
    const enhancedVoiceConfig: ExtendedVoiceConfig = {
        ...baseVoiceConfig,
        ...(languageVoice && { voiceName: languageVoice }),
        stylePrompt,
    };

    if (languageVoice) {
        console.log(`[Narrator] Using language-specific voice "${languageVoice}" for ${config?.language}`);
    }

    const ttsStart = Date.now();
    let audioBlob: Blob;
    try {
        audioBlob = await synthesizeSpeech(
            scene.narrationScript,
            enhancedVoiceConfig,
            config
        );
    } catch (err) {
        // Log the failed TTS call
        if (sessionId) {
            logAICall({
                sessionId,
                step: 'tts',
                model: MODELS.TTS,
                input: scene.narrationScript,
                output: '',
                durationMs: Date.now() - ttsStart,
                status: 'error',
                error: err instanceof Error ? err.message : String(err),
                metadata: { sceneName: scene.name, voice: enhancedVoiceConfig.voiceName },
            });
        }
        throw err;
    }

    // Calculate duration from WAV blob (precise since we know the format)
    const audioDuration = calculateAudioDuration(audioBlob);

    // Log the successful TTS call (fire-and-forget)
    if (sessionId) {
        logAICall({
            sessionId,
            step: 'tts',
            model: MODELS.TTS,
            input: scene.narrationScript,
            output: `audio: ${audioDuration.toFixed(1)}s, ${audioBlob.size} bytes`,
            durationMs: Date.now() - ttsStart,
            status: 'success',
            metadata: {
                sceneName: scene.name,
                voice: enhancedVoiceConfig.voiceName,
                audioDurationSec: audioDuration,
                audioSizeBytes: audioBlob.size,
            },
        });
    }

    const wordCount = scene.narrationScript.split(/\s+/).filter(w => w.length > 0).length;
    console.log(`[Narrator] Scene "${scene.name}" audio: ${audioDuration.toFixed(1)}s (${wordCount} words, ${audioBlob.size} bytes)`);

    // Cloud autosave trigger (fire-and-forget, non-blocking)
    if (sessionId) {
        cloudAutosave.saveNarration(sessionId, audioBlob, scene.id).catch(err => {
            console.warn('[Narrator] Cloud autosave failed (non-fatal):', err);
        });
    }

    return {
        sceneId: scene.id,
        audioBlob,
        audioDuration,
        transcript: scene.narrationScript,
    };
}

/**
 * Generate narration for all scenes in a content plan.
 * 
 * @param scenes - Array of scenes to narrate
 * @param config - Optional narrator config
 * @param onProgress - Progress callback (sceneIndex, totalScenes)
 * @returns Array of narration segments
 */
export const narrateAllScenes = traceAsync(
    async function narrateAllScenesImpl(
        scenes: Scene[],
        config?: NarratorConfig,
        onProgress?: (sceneIndex: number, totalScenes: number) => void,
        sessionId?: string
    ): Promise<NarrationSegment[]> {
        console.log(`[Narrator] Starting narration for ${scenes.length} scenes`);

        const segments: NarrationSegment[] = [];

        for (let i = 0; i < scenes.length; i++) {
            onProgress?.(i, scenes.length);

            const scene = scenes[i];
            if (!scene) {
                console.warn(`[Narrator] Scene at index ${i} is undefined, skipping`);
                continue;
            }

            try {
                const segment = await narrateScene(scene, config, sessionId);
                segments.push(segment);
                console.log(`[Narrator] Completed scene ${i + 1}/${scenes.length}`);
            } catch (error) {
                console.error(`[Narrator] Failed to narrate scene ${scene.id}:`, error);
                throw error;
            }
        }

        onProgress?.(scenes.length, scenes.length);
        console.log(`[Narrator] All ${segments.length} scenes narrated`);

        return segments;
    },
    "narrateAllScenes",
    {
        runType: "chain",
        metadata: { service: "narrator" },
        tags: ["tts", "narration"],
    }
);

/**
 * Generate narration for all shots using per-shot scriptSegment text.
 * Falls back to scene action text when scriptSegment is absent.
 * Uses ParallelExecutionEngine with concurrency 2 (effectively serialized by acquireTtsSlot).
 *
 * @param shots - ShotlistEntry array with optional scriptSegment for per-shot narration text
 * @param screenplayScenes - ScreenplayScene[] for fallback action text
 * @param config - NarratorConfig (voice, language, etc.)
 * @param onProgress - Progress callback (completedCount, totalCount)
 * @param sessionId - For cloud autosave
 * @param existingStatus - Per-shot narration status map (skip shots already marked 'success')
 * @param existingShotNarrations - Already narrated shots (to skip on resume)
 * @returns Array of per-shot narration results (only successful ones)
 */
export async function narrateAllShots(
    shots: ShotlistEntry[],
    screenplayScenes: ScreenplayScene[],
    config: NarratorConfig | undefined,
    onProgress: ((completed: number, total: number) => void) | undefined,
    sessionId: string | undefined,
    existingStatus?: Record<string, 'pending' | 'success' | 'failed'>,
    existingShotNarrations?: Array<{ shotId: string; sceneId: string; audioUrl: string; duration: number; text: string }>,
): Promise<Array<{ shotId: string; sceneId: string; audioBlob: Blob; duration: number; text: string }>> {
    // Build scene action map for fallback text
    const sceneActionMap = new Map<string, string>();
    for (const scene of screenplayScenes) {
        sceneActionMap.set(scene.id, scene.action);
    }

    // Set of shot IDs already successfully narrated (for resume)
    const existingNarrationMap = new Map<string, NonNullable<typeof existingShotNarrations>[number]>(
        (existingShotNarrations || []).filter(n => n.audioUrl).map(n => [n.shotId, n])
    );

    // Filter to only shots that still need narration
    const shotsToProcess = shots.filter(shot => {
        if (existingStatus?.[shot.id] === 'success' && existingNarrationMap.has(shot.id)) {
            return false; // Already done
        }
        return true;
    });

    if (shotsToProcess.length === 0) {
        console.log('[Narrator] narrateAllShots: all shots already narrated, skipping');
        return [];
    }

    console.log(`[Narrator] narrateAllShots: narrating ${shotsToProcess.length}/${shots.length} shots`);

    // Default voice for shot-level narration (uses dramatic/storytelling profile)
    const defaultVoiceConfig: ExtendedVoiceConfig = TONE_VOICE_MAP['dramatic'];

    // Build tasks for the ParallelExecutionEngine
    const tasks = shotsToProcess
        .map(shot => {
            // Build narration text with fallback chain
            const rawText = shot.scriptSegment
                ?? sceneActionMap.get(shot.sceneId)
                ?? shot.description;
            const narrationText = cleanForTTS(rawText || '');

            // Skip shots with empty narration text (synthesizeSpeech throws on empty input)
            if (!narrationText.trim()) {
                console.warn(`[Narrator] Skipping shot ${shot.id}: empty narration text after cleaning`);
                return null;
            }

            return {
                id: shot.id,
                type: 'audio' as const,
                priority: shot.shotNumber,
                retryable: true,
                timeout: 45_000,
                execute: async (): Promise<{ shotId: string; sceneId: string; audioBlob: Blob; duration: number; text: string }> => {
                    console.log(`[Narrator] Narrating shot ${shot.shotNumber} (${shot.id}): "${narrationText.substring(0, 50)}..."`);
                    const audioBlob = await synthesizeSpeech(narrationText, defaultVoiceConfig, config);
                    const duration = calculateAudioDuration(audioBlob);
                    return { shotId: shot.id, sceneId: shot.sceneId, audioBlob, duration, text: narrationText };
                },
            };
        })
        .filter((t): t is NonNullable<typeof t> => t !== null);

    if (tasks.length === 0) {
        console.warn('[Narrator] narrateAllShots: no tasks after filtering empty text');
        return [];
    }

    const engine = new ParallelExecutionEngine();
    const taskResults = await engine.execute(tasks, {
        concurrencyLimit: 2, // 2 slots — effectively 1 due to acquireTtsSlot gate
        retryAttempts: 3,
        retryDelay: 3000,
        exponentialBackoff: true,
        onProgress: (p) => onProgress?.(p.completedTasks, p.totalTasks),
        onTaskFail: (taskId, error) => {
            console.error(`[Narrator] Shot narration failed for ${taskId}:`, error.message);
        },
    });

    // Collect only successful results
    const results: Array<{ shotId: string; sceneId: string; audioBlob: Blob; duration: number; text: string }> = [];
    for (const result of taskResults) {
        if (result.success && result.data) {
            results.push(result.data);
        }
    }

    const failedCount = taskResults.filter(r => !r.success).length;
    if (failedCount > 0) {
        console.warn(`[Narrator] narrateAllShots: ${failedCount} shots failed (non-fatal)`);
    }

    console.log(`[Narrator] narrateAllShots: completed ${results.length}/${tasks.length} shots`);
    return results;
}

/**
 * Get voice config for an emotional tone.
 */
export function getVoiceForTone(tone: EmotionalTone): ExtendedVoiceConfig {
    return TONE_VOICE_MAP[tone];
}

/**
 * Get list of available voices.
 */
export function getAvailableVoices(): TTSVoice[] {
    return Object.values(TTS_VOICES);
}

/**
 * Estimate narration duration from text.
 * Based on average speaking rate of 150 words per minute.
 * 
 * @param text - The narration text
 * @param speakingRate - Speaking rate multiplier (1.0 = normal)
 * @returns Estimated duration in seconds
 */
export function estimateNarrationDuration(
    text: string,
    speakingRate: number = 1.0
): number {
    const wordCount = text.split(/\s+/).filter(w => w.length > 0).length;
    const wordsPerSecond = (150 / 60) * speakingRate; // 2.5 words/sec at normal speed
    return Math.ceil(wordCount / wordsPerSecond);
}

/**
 * Create audio URL from narration segment for playback.
 */
export function createAudioUrl(segment: NarrationSegment): string {
    return URL.createObjectURL(segment.audioBlob);
}

/**
 * Revoke audio URL to free memory.
 */
export function revokeAudioUrl(url: string): void {
    URL.revokeObjectURL(url);
}

/**
 * Create a custom voice configuration with style prompt.
 * Use this for advanced voice steering beyond the preset emotional tones.
 * 
 * @example
 * // Create a spooky narrator voice
 * const spookyVoice = createCustomVoice({
 *     voiceName: "Charon",
 *     stylePrompt: {
 *         persona: "A mysterious storyteller from a haunted mansion",
 *         emotion: "eerie, whispering, and unsettling",
 *         pacing: "slow with long, suspenseful pauses"
 *     }
 * });
 * 
 * @example
 * // Create an excited sports commentator
 * const sportsVoice = createCustomVoice({
 *     voiceName: "Puck",
 *     stylePrompt: {
 *         customDirectorNote: "Speak like an excited sports commentator calling a winning goal, with rising energy and enthusiasm"
 *     }
 * });
 */
export function createCustomVoice(config: {
    voiceName: TTSVoice;
    stylePrompt?: StylePrompt;
    pitch?: number;
    speakingRate?: number;
}): ExtendedVoiceConfig {
    return {
        voiceName: config.voiceName,
        pitch: config.pitch ?? 0,
        speakingRate: config.speakingRate ?? 1.0,
        stylePrompt: config.stylePrompt,
    };
}

/**
 * Preset style prompts for common use cases.
 * Combine with any voice for quick styling.
 */
export const STYLE_PRESETS: Record<string, StylePrompt> = {
    // Narration styles
    DOCUMENTARY: {
        persona: "A documentary narrator",
        emotion: "informative, engaging, and thoughtful",
        pacing: "measured with natural pauses"
    },
    MOVIE_TRAILER: {
        persona: "An epic movie trailer voice",
        emotion: "dramatic, powerful, and intense",
        pacing: "slow and deliberate with dramatic pauses"
    },
    AUDIOBOOK: {
        persona: "A skilled audiobook narrator",
        emotion: "expressive, immersive, and captivating",
        pacing: "varied to match the story's rhythm"
    },

    // Character styles
    WISE_MENTOR: {
        persona: "A wise old mentor sharing ancient wisdom",
        emotion: "warm, knowing, and gentle",
        pacing: "slow and contemplative"
    },
    EXCITED_HOST: {
        persona: "An enthusiastic TV show host",
        emotion: "energetic, upbeat, and engaging",
        pacing: "fast and lively"
    },
    MYSTERIOUS: {
        persona: "A mysterious figure revealing secrets",
        emotion: "intriguing, hushed, and suspenseful",
        pacing: "slow with pregnant pauses"
    },

    // Functional styles
    TUTORIAL: {
        persona: "A patient teacher",
        emotion: "clear, helpful, and encouraging",
        pacing: "steady and easy to follow"
    },
    ANNOUNCEMENT: {
        persona: "A professional announcer",
        emotion: "clear, confident, and attention-grabbing",
        pacing: "crisp and well-articulated"
    },
    MEDITATION: {
        persona: "A calming meditation guide",
        emotion: "peaceful, soothing, and tranquil",
        pacing: "very slow with long, restful pauses"
    },
};

// --- Format-Specific Voice Profiles (Task 9.1) ---

/**
 * Voice profile configuration for a specific video format.
 * Combines a base voice, style prompt, and optional speaking rate adjustments.
 *
 * Requirements: 3.4, 4.4, 9.6, 14.1
 */
export interface FormatVoiceProfile {
    /** Human-readable profile label (e.g., "Conversational", "Energetic") */
    label: string;
    /** Base voice configuration */
    voice: ExtendedVoiceConfig;
    /** Corresponding VideoPurpose for legacy integration */
    videoPurpose: VideoPurpose;
}

/**
 * Maps each VideoFormat to a recommended voice profile.
 *
 * Requirements: 3.4 (YouTube conversational), 4.4 (Ad energetic),
 * 9.6 (News neutral), 14.1 (format-specific voice profiles)
 */
export const FORMAT_VOICE_PROFILE_MAP: Record<VideoFormat, FormatVoiceProfile> = {
    'youtube-narrator': {
        label: 'Conversational',
        voice: {
            voiceName: TTS_VOICES.KORE,
            pitch: 1,
            speakingRate: 1.1,
            stylePrompt: {
                persona: "A popular YouTube host sharing fascinating insights",
                emotion: "warm, engaging, and conversational",
                pacing: "natural and flowing with well-placed pauses for emphasis",
            },
        },
        videoPurpose: 'documentary',
    },
    'advertisement': {
        label: 'Energetic',
        voice: {
            voiceName: TTS_VOICES.PUCK,
            pitch: 2,
            speakingRate: 1.25,
            stylePrompt: {
                persona: "A high-energy commercial voice-over artist",
                emotion: "confident, persuasive, and attention-grabbing",
                pacing: "punchy and dynamic with crisp delivery",
            },
        },
        videoPurpose: 'commercial',
    },
    'movie-animation': {
        label: 'Dramatic',
        voice: {
            voiceName: TTS_VOICES.FENRIR,
            pitch: -2,
            speakingRate: 0.95,
            stylePrompt: {
                persona: "A legendary storyteller narrating an epic tale",
                emotion: "dramatic, immersive, and emotionally charged",
                pacing: "deliberate with dramatic pauses at key revelations",
            },
        },
        videoPurpose: 'storytelling',
    },
    'educational': {
        label: 'Professional',
        voice: {
            voiceName: TTS_VOICES.LEDA,
            pitch: 0,
            speakingRate: 1.0,
            stylePrompt: {
                persona: "A friendly and knowledgeable teacher",
                emotion: "clear, encouraging, patient, and authoritative",
                pacing: "steady and easy to follow with pauses between concepts",
            },
        },
        videoPurpose: 'educational',
    },
    'shorts': {
        label: 'Energetic',
        voice: {
            voiceName: TTS_VOICES.PUCK,
            pitch: 3,
            speakingRate: 1.3,
            stylePrompt: {
                persona: "A trendy social media content creator",
                emotion: "energetic, punchy, and scroll-stopping",
                pacing: "fast and dynamic with rapid-fire delivery",
            },
        },
        videoPurpose: 'social_short',
    },
    'documentary': {
        label: 'Professional',
        voice: {
            voiceName: TTS_VOICES.CHARON,
            pitch: -1,
            speakingRate: 0.95,
            stylePrompt: {
                persona: "A distinguished documentary narrator",
                emotion: "informative, measured, and authoritative",
                pacing: "thoughtful and deliberate with gravitas",
            },
        },
        videoPurpose: 'documentary',
    },
    'music-video': {
        label: 'Dramatic',
        voice: {
            voiceName: TTS_VOICES.AOEDE,
            pitch: -1,
            speakingRate: 0.9,
            stylePrompt: {
                persona: "A cinematic music video narrator",
                emotion: "evocative, artistic, and emotionally rich",
                pacing: "rhythmic and flowing, matching musical energy",
            },
        },
        videoPurpose: 'music_video',
    },
    'news-politics': {
        label: 'Neutral',
        voice: {
            voiceName: TTS_VOICES.ORUS,
            pitch: 0,
            speakingRate: 1.1,
            stylePrompt: {
                persona: "A professional news anchor delivering a report",
                emotion: "objective, clear, balanced, and authoritative",
                pacing: "crisp and well-articulated with neutral delivery",
            },
        },
        videoPurpose: 'news_report',
    },
};

/**
 * Get the recommended voice profile for a video format.
 * Falls back to 'movie-animation' profile for unknown formats.
 *
 * @param formatId - Video format identifier
 * @returns Format-specific voice profile
 */
export function getVoiceProfileForFormat(formatId: VideoFormat): FormatVoiceProfile {
    return FORMAT_VOICE_PROFILE_MAP[formatId] ?? FORMAT_VOICE_PROFILE_MAP['movie-animation'];
}

/**
 * Get language-aware voice config for a format.
 * Applies the format's voice profile, then overrides with a language-specific
 * voice if the content language differs from English.
 *
 * Requirements: 14.3, 19.3
 *
 * @param formatId - Video format identifier
 * @param language - Content language code
 * @returns Extended voice config with language-appropriate voice
 */
export function getFormatVoiceForLanguage(
    formatId: VideoFormat,
    language: LanguageCode | 'ar' | 'en'
): ExtendedVoiceConfig {
    const profile = getVoiceProfileForFormat(formatId);
    const langVoice = language !== 'en' && language !== 'auto'
        ? LANGUAGE_VOICE_MAP[language]
        : undefined;

    return {
        ...profile.voice,
        ...(langVoice && { voiceName: langVoice }),
    };
}
````

## File: packages/shared/src/services/parallelExecutionEngine.ts
````typescript
/**
 * Parallel Execution Engine
 * 
 * Executes multiple AI tasks concurrently with controlled concurrency,
 * retry logic, progress tracking, and cancellation support.
 * 
 * Features:
 * - Task queue with priority ordering
 * - Worker pool with configurable concurrency (3-5 workers)
 * - Task state tracking (queued, in-progress, completed, failed)
 * - Progress calculation and event emission
 * - Retry logic with exponential backoff
 * - Cancellation support with resource cleanup
 */

// Use built-in crypto.randomUUID() for ID generation

// ============================================================================
// Types and Interfaces
// ============================================================================

export type TaskType = 'research' | 'script' | 'visual' | 'audio' | 'assembly';
export type TaskState = 'queued' | 'in-progress' | 'completed' | 'failed' | 'cancelled';

export interface Task<T = any> {
  id: string;
  type: TaskType;
  execute: () => Promise<T>;
  priority: number;
  retryable: boolean;
  timeout: number; // milliseconds
}

export interface ExecutionOptions {
  concurrencyLimit: number;
  retryAttempts: number;
  retryDelay: number; // milliseconds (base delay)
  exponentialBackoff: boolean;
  onProgress?: (progress: ExecutionProgress) => void;
  onTaskComplete?: (taskId: string, result: any) => void;
  onTaskFail?: (taskId: string, error: Error) => void;
}

export interface TaskResult<T = any> {
  taskId: string;
  success: boolean;
  data?: T;
  error?: Error;
  attempts: number;
  duration: number; // milliseconds
}

export interface ExecutionProgress {
  executionId: string;
  totalTasks: number;
  completedTasks: number;
  failedTasks: number;
  inProgressTasks: number;
  queuedTasks: number;
  estimatedTimeRemaining: number; // milliseconds
}

interface TaskMetadata<T = any> {
  task: Task<T>;
  state: TaskState;
  attempts: number;
  rateLimitRetries?: number;
  startTime?: number;
  endTime?: number;
  result?: T;
  error?: Error;
  abortController?: AbortController;
}

// ============================================================================
// Parallel Execution Engine
// ============================================================================

export class ParallelExecutionEngine {
  private executionId: string = '';
  private taskQueue: TaskMetadata[] = [];
  private inProgressTasks: Map<string, TaskMetadata> = new Map();
  private completedTasks: Map<string, TaskMetadata> = new Map();
  private failedTasks: Map<string, TaskMetadata> = new Map();
  private cancelledTasks: Set<string> = new Set();
  
  private options: ExecutionOptions = {
    concurrencyLimit: 3,
    retryAttempts: 3,
    retryDelay: 1000,
    exponentialBackoff: true,
  };
  
  private isCancelled: boolean = false;
  private isExecuting: boolean = false;
  private cancellationPromise: Promise<void> | null = null;
  
  /**
   * Execute multiple tasks in parallel with controlled concurrency
   */
  async execute<T>(tasks: Task<T>[], options: Partial<ExecutionOptions> = {}): Promise<TaskResult<T>[]> {
    // Merge options
    this.options = { ...this.options, ...options };
    
    // Initialize execution
    this.executionId = crypto.randomUUID();
    this.isCancelled = false;
    this.isExecuting = true;
    this.taskQueue = [];
    this.inProgressTasks.clear();
    this.completedTasks.clear();
    this.failedTasks.clear();
    this.cancelledTasks.clear();
    
    // Sort tasks by priority (higher priority first)
    const sortedTasks = [...tasks].sort((a, b) => b.priority - a.priority);
    
    // Initialize task metadata
    this.taskQueue = sortedTasks.map(task => ({
      task,
      state: 'queued' as TaskState,
      attempts: 0,
    }));
    
    // Emit initial progress
    this.emitProgress();
    
    // Start worker pool
    const workers: Promise<void>[] = [];
    for (let i = 0; i < this.options.concurrencyLimit; i++) {
      workers.push(this.worker());
    }
    
    // Wait for all workers to complete
    await Promise.all(workers);
    
    this.isExecuting = false;
    
    // Build results
    const results: TaskResult<T>[] = [];
    
    // Add completed tasks
    this.completedTasks.forEach((metadata) => {
      results.push({
        taskId: metadata.task.id,
        success: true,
        data: metadata.result,
        attempts: metadata.attempts,
        duration: (metadata.endTime || 0) - (metadata.startTime || 0),
      });
    });
    
    // Add failed tasks
    this.failedTasks.forEach((metadata) => {
      results.push({
        taskId: metadata.task.id,
        success: false,
        error: metadata.error,
        attempts: metadata.attempts,
        duration: (metadata.endTime || 0) - (metadata.startTime || 0),
      });
    });
    
    // Add cancelled tasks
    this.cancelledTasks.forEach((taskId) => {
      results.push({
        taskId,
        success: false,
        error: new Error('Task cancelled'),
        attempts: 0,
        duration: 0,
      });
    });
    
    return results;
  }
  
  /**
   * Cancel all pending and in-flight tasks
   */
  async cancel(executionId: string): Promise<void> {
    if (executionId !== this.executionId) {
      throw new Error('Invalid execution ID');
    }
    
    if (this.isCancelled) {
      return this.cancellationPromise || Promise.resolve();
    }
    
    this.isCancelled = true;
    
    // Create cancellation promise
    this.cancellationPromise = new Promise<void>((resolve) => {
      const startTime = Date.now();
      const timeout = 5000; // 5 seconds
      
      const checkCompletion = () => {
        const elapsed = Date.now() - startTime;
        
        // Check if all tasks are done
        if (this.inProgressTasks.size === 0 && this.taskQueue.length === 0) {
          resolve();
          return;
        }
        
        // Check timeout
        if (elapsed >= timeout) {
          // Force abort all in-progress tasks
          this.inProgressTasks.forEach((metadata) => {
            if (metadata.abortController) {
              metadata.abortController.abort();
            }
          });
          resolve();
          return;
        }
        
        // Check again in 100ms
        setTimeout(checkCompletion, 100);
      };
      
      checkCompletion();
    });
    
    // Abort all in-progress tasks
    this.inProgressTasks.forEach((metadata) => {
      if (metadata.abortController) {
        metadata.abortController.abort();
      }
    });
    
    // Mark queued tasks as cancelled
    this.taskQueue.forEach((metadata) => {
      this.cancelledTasks.add(metadata.task.id);
    });
    this.taskQueue = [];
    
    // Emit progress
    this.emitProgress();
    
    return this.cancellationPromise;
  }
  
  /**
   * Get current execution progress
   */
  getProgress(executionId: string): ExecutionProgress {
    if (executionId !== this.executionId) {
      throw new Error('Invalid execution ID');
    }
    
    return this.calculateProgress();
  }
  
  // ==========================================================================
  // Private Methods
  // ==========================================================================
  
  /**
   * Worker that processes tasks from the queue
   */
  private async worker(): Promise<void> {
    while (!this.isCancelled) {
      // Get next task from queue
      const metadata = this.taskQueue.shift();
      
      if (!metadata) {
        // No more tasks, exit worker
        break;
      }
      
      // Execute task
      await this.executeTask(metadata);
    }
  }
  
  /**
   * Execute a single task with retry logic
   */
  private async executeTask<T>(metadata: TaskMetadata<T>): Promise<void> {
    const { task } = metadata;
    
    // Check if cancelled
    if (this.isCancelled) {
      this.cancelledTasks.add(task.id);
      return;
    }
    
    // Mark as in-progress
    metadata.state = 'in-progress';
    metadata.startTime = Date.now();
    metadata.abortController = new AbortController();
    this.inProgressTasks.set(task.id, metadata);
    this.emitProgress();
    
    try {
      // Execute task with timeout
      const result = await this.executeWithTimeout(
        task.execute(),
        task.timeout,
        metadata.abortController.signal
      );
      
      // Task succeeded
      metadata.state = 'completed';
      metadata.endTime = Date.now();
      metadata.result = result;
      this.inProgressTasks.delete(task.id);
      this.completedTasks.set(task.id, metadata);
      
      // Emit progress and callback
      this.emitProgress();
      if (this.options.onTaskComplete) {
        this.options.onTaskComplete(task.id, result);
      }
      
    } catch (error) {
      // Task failed
      metadata.error = error as Error;

      // Rate limit errors get special handling: re-queue with longer delay
      // and do NOT count against retry attempts (Requirement 20.5, Property 42)
      if (this.isRateLimitError(error) && !this.isCancelled) {
        const resetDelay = this.getRateLimitResetDelay(error);
        metadata.rateLimitRetries = (metadata.rateLimitRetries ?? 0) + 1;

        await this.sleep(resetDelay);

        if (!this.isCancelled) {
          metadata.state = 'queued';
          this.inProgressTasks.delete(task.id);
          this.taskQueue.push(metadata);
          this.emitProgress();
        } else {
          this.cancelledTasks.add(task.id);
          this.inProgressTasks.delete(task.id);
        }
        return;
      }

      // Normal failure: count attempt
      metadata.attempts++;

      // Check if retryable and attempts remaining
      const shouldRetry =
        task.retryable &&
        metadata.attempts < this.options.retryAttempts &&
        !this.isCancelled;

      if (shouldRetry) {
        // Calculate retry delay with exponential backoff
        const delay = this.calculateRetryDelay(metadata.attempts);

        // Wait before retry
        await this.sleep(delay);

        // Re-queue task if not cancelled
        if (!this.isCancelled) {
          metadata.state = 'queued';
          this.inProgressTasks.delete(task.id);
          this.taskQueue.push(metadata);
          this.emitProgress();
        } else {
          this.cancelledTasks.add(task.id);
          this.inProgressTasks.delete(task.id);
        }
      } else {
        // Task failed permanently
        metadata.state = 'failed';
        metadata.endTime = Date.now();
        this.inProgressTasks.delete(task.id);
        this.failedTasks.set(task.id, metadata);

        // Emit progress and callback
        this.emitProgress();
        if (this.options.onTaskFail) {
          this.options.onTaskFail(task.id, error as Error);
        }
      }
    }
  }
  
  /**
   * Execute a promise with timeout and abort signal
   */
  private async executeWithTimeout<T>(
    promise: Promise<T>,
    timeout: number,
    signal: AbortSignal
  ): Promise<T> {
    return new Promise<T>((resolve, reject) => {
      const timeoutId = setTimeout(() => {
        reject(new Error(`Task timeout after ${timeout}ms`));
      }, timeout);
      
      const abortHandler = () => {
        clearTimeout(timeoutId);
        reject(new Error('Task aborted'));
      };
      
      signal.addEventListener('abort', abortHandler);
      
      promise
        .then((result) => {
          clearTimeout(timeoutId);
          signal.removeEventListener('abort', abortHandler);
          resolve(result);
        })
        .catch((error) => {
          clearTimeout(timeoutId);
          signal.removeEventListener('abort', abortHandler);
          reject(error);
        });
    });
  }
  
  /**
   * Calculate retry delay with exponential backoff
   */
  private calculateRetryDelay(attempt: number): number {
    const { retryDelay, exponentialBackoff } = this.options;
    
    if (!exponentialBackoff) {
      return retryDelay;
    }
    
    // Exponential backoff: delay = baseDelay * (2 ^ attempt) + jitter
    const exponentialDelay = retryDelay * Math.pow(2, attempt);
    const jitter = Math.random() * retryDelay;
    
    return exponentialDelay + jitter;
  }
  
  /**
   * Check if error is a rate limit error
   */
  private isRateLimitError(error: any): boolean {
    const message = error?.message?.toLowerCase() || '';
    return (
      message.includes('rate limit') ||
      message.includes('too many requests') ||
      message.includes('429') ||
      message.includes('quota exceeded')
    );
  }

  /**
   * Parse rate limit reset delay from error, or use default (60s)
   */
  private getRateLimitResetDelay(error: any): number {
    // Try to extract retry-after from error details
    const retryAfter = error?.details?.retryAfter
      ?? error?.response?.headers?.['retry-after']
      ?? error?.retryAfter;

    if (typeof retryAfter === 'number' && retryAfter > 0) {
      // If value looks like seconds (< 1000), convert to ms
      return retryAfter < 1000 ? retryAfter * 1000 : retryAfter;
    }
    if (typeof retryAfter === 'string') {
      const parsed = parseInt(retryAfter, 10);
      if (!isNaN(parsed) && parsed > 0) {
        return parsed < 1000 ? parsed * 1000 : parsed;
      }
    }

    // Default: 60 seconds
    return 60_000;
  }
  
  /**
   * Calculate current execution progress
   */
  private calculateProgress(): ExecutionProgress {
    const totalTasks = 
      this.completedTasks.size +
      this.failedTasks.size +
      this.inProgressTasks.size +
      this.taskQueue.length +
      this.cancelledTasks.size;
    
    const completedTasks = this.completedTasks.size;
    const failedTasks = this.failedTasks.size;
    const inProgressTasks = this.inProgressTasks.size;
    const queuedTasks = this.taskQueue.length;
    
    // Calculate estimated time remaining
    let estimatedTimeRemaining = 0;
    if (completedTasks > 0) {
      const completedMetadata = Array.from(this.completedTasks.values());
      const avgDuration = completedMetadata.reduce((sum, m) => {
        return sum + ((m.endTime || 0) - (m.startTime || 0));
      }, 0) / completedTasks;
      
      const remainingTasks = inProgressTasks + queuedTasks;
      estimatedTimeRemaining = Math.ceil(avgDuration * remainingTasks / this.options.concurrencyLimit);
    }
    
    return {
      executionId: this.executionId,
      totalTasks,
      completedTasks,
      failedTasks,
      inProgressTasks,
      queuedTasks,
      estimatedTimeRemaining,
    };
  }
  
  /**
   * Emit progress event
   */
  private emitProgress(): void {
    if (this.options.onProgress) {
      const progress = this.calculateProgress();
      this.options.onProgress(progress);
    }
  }
  
  /**
   * Sleep for specified milliseconds
   */
  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}
````

## File: packages/shared/src/services/pipelines/advertisement.ts
````typescript
/**
 * Advertisement Pipeline
 *
 * Produces short, high-impact promotional videos with clear CTAs.
 * Phases: Script → Visuals → Audio → Assembly.
 *
 * Requirements: 4.1–4.6
 */

import type { FormatMetadata, VideoFormat, Scene, NarrationSegment, ScreenplayScene } from '../../types';
import type { FormatPipeline, PipelineRequest, PipelineResult, PipelineCallbacks } from '../formatRouter';
import { formatRegistry } from '../formatRegistry';
import {
  buildBreakdownPrompt,
  buildScreenplayPrompt,
  countScriptWords,
  validateDurationConstraint,
  type FormatAwareGenerationOptions,
} from '../ai/storyPipeline';
import { ParallelExecutionEngine, type Task } from '../parallelExecutionEngine';
import { CheckpointSystem } from '../checkpointSystem';
import { narrateScene, getFormatVoiceForLanguage, type NarratorConfig } from '../narratorService';
import { generateImageFromPrompt } from '../imageService';
import { buildImageStyleGuide } from '../prompt/imageStyleGuide';
import { buildAssemblyRules, buildCTAMarker, validateCTAPosition } from '../ffmpeg/formatAssembly';
import { detectLanguage } from '../languageDetector';
import { storyModeStore } from '../ai/production/store';
import type { StoryModeState } from '../ai/production/types';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { GEMINI_API_KEY, MODELS } from '../shared/apiClient';
import { z } from 'zod';
import { agentLogger } from '../logger';

const FORMAT_ID: VideoFormat = 'advertisement';
const log = agentLogger.child('AdvertisementPipeline');

// ============================================================================
// Schemas
// ============================================================================

const BreakdownSchema = z.object({
  acts: z.array(z.object({
    title: z.string(),
    emotionalHook: z.string(),
    narrativeBeat: z.string(),
  })).min(2).max(4),
});

const ScreenplaySchema = z.object({
  scenes: z.array(z.object({
    heading: z.string(),
    action: z.string(),
    dialogue: z.array(z.object({
      speaker: z.string().max(30),
      text: z.string().min(1),
    })),
  })).min(2).max(5),
});

// ============================================================================
// Pipeline Implementation
// ============================================================================

export class AdvertisementPipeline implements FormatPipeline {
  private parallelEngine: ParallelExecutionEngine;

  constructor(parallelEngine?: ParallelExecutionEngine) {
    this.parallelEngine = parallelEngine ?? new ParallelExecutionEngine();
  }

  getMetadata(): FormatMetadata {
    return formatRegistry.getFormat(FORMAT_ID)!;
  }

  async validate(request: PipelineRequest): Promise<boolean> {
    return !!request.idea && request.idea.trim().length > 0;
  }

  async execute(request: PipelineRequest, callbacks?: PipelineCallbacks): Promise<PipelineResult> {
    const sessionId = `ad_${Date.now()}_${Math.random().toString(36).slice(2, 8)}`;
    const language = request.language ?? detectLanguage(request.idea);
    const metadata = this.getMetadata();

    let cancelled = false;
    const checkpoints = new CheckpointSystem({
      maxCheckpoints: metadata.checkpointCount,
      onCheckpointCreated: callbacks?.onCheckpointCreated,
    });
    callbacks?.onCheckpointSystemCreated?.(checkpoints);
    callbacks?.onCancelRequested?.(() => { cancelled = true; checkpoints.dispose(); });

    log.info(`Starting Advertisement pipeline: "${request.idea.slice(0, 60)}..." [${language}]`);

    try {
      // ----------------------------------------------------------------
      // Phase 1: Script generation with CTA — Requirements 4.1, 4.2
      // ----------------------------------------------------------------
      log.info('Phase 1: Script generation');

      const formatOptions: FormatAwareGenerationOptions = {
        formatId: FORMAT_ID,
        genre: request.genre,
        language,
      };

      const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey: GEMINI_API_KEY,
        temperature: 0.8,
      });

      // Step 1a: Breakdown
      const breakdownPrompt = buildBreakdownPrompt(request.idea, formatOptions);
      const breakdownResult = await model.withStructuredOutput(BreakdownSchema).invoke(breakdownPrompt);

      // Step 1b: Screenplay
      const screenplayPrompt = buildScreenplayPrompt(breakdownResult.acts, formatOptions);
      const screenplayResult = await model.withStructuredOutput(ScreenplaySchema).invoke(screenplayPrompt);

      const screenplay: ScreenplayScene[] = screenplayResult.scenes.map((s, i) => ({
        id: `scene_${i}`,
        sceneNumber: i + 1,
        heading: s.heading,
        action: s.action,
        dialogue: s.dialogue.filter(d => d.text.trim().length > 0),
        charactersPresent: [],
      }));

      // Duration validation
      const wordCount = countScriptWords(screenplay);
      const durationCheck = validateDurationConstraint(wordCount, metadata);
      if (!durationCheck.valid) {
        log.warn(`Duration constraint: ${durationCheck.message}`);
      }

      // Extract CTA from final scene dialogue
      const finalScene = screenplay[screenplay.length - 1];
      const ctaText = finalScene?.dialogue[finalScene.dialogue.length - 1]?.text ?? 'Learn More';

      // Persist state
      const state: StoryModeState = {
        id: sessionId,
        topic: request.idea,
        breakdown: breakdownResult.acts.map(a => `${a.title}: ${a.narrativeBeat}`).join('\n'),
        screenplay,
        characters: [],
        shotlist: [],
        currentStep: 'screenplay',
        updatedAt: Date.now(),
        formatId: FORMAT_ID,
        language,
      };
      storyModeStore.set(sessionId, state);

      // Checkpoint 1: Script with CTA — Requirement 4.5
      const scriptApproval = await checkpoints.createCheckpoint('script-with-cta', {
        sceneCount: screenplay.length,
        scenes: screenplay.map(s => ({
          heading: s.heading,
          action: s.action.slice(0, 200),
        })),
        ctaText,
        wordCount,
        estimatedDuration: durationCheck.estimatedSeconds,
        durationValid: durationCheck.valid,
      });
      if (!scriptApproval.approved) {
        log.info('Script rejected by user');
        checkpoints.dispose();
        return { success: false, error: 'Script rejected by user', partialResults: { screenplay } };
      }

      // ----------------------------------------------------------------
      // Phase 2: Visual generation (high-impact, 16:9) — Requirements 4.3
      // ----------------------------------------------------------------
      log.info('Phase 2: Visual generation');

      const visualTasks: Task<{ sceneId: string; imageUrl: string }>[] = screenplay.map((scene, i) => ({
        id: `visual_${i}`,
        type: 'visual' as const,
        priority: 1,
        retryable: true,
        timeout: 60_000,
        execute: async () => {
          const guide = buildImageStyleGuide({
            scene: scene.action,
            style: 'High-Impact Commercial',
            background: scene.heading,
            mood: breakdownResult.acts[i]?.emotionalHook ?? 'energetic',
          });

          const imageUrl = await generateImageFromPrompt(
            scene.action,
            'High-Impact Commercial',
            '',
            '16:9',
            true,
            undefined,
            sessionId,
            i,
            guide,
          );
          return { sceneId: scene.id, imageUrl };
        },
      }));

      const visualResults = await this.parallelEngine.execute(visualTasks, {
        concurrencyLimit: metadata.concurrencyLimit,
        retryAttempts: 2,
        retryDelay: 1000,
        exponentialBackoff: true,
      });

      const visuals = visualResults
        .filter(r => r.success && r.data)
        .map(r => r.data!);

      log.info(`Visuals generated: ${visuals.length}/${screenplay.length}`);

      // Update state
      state.shotlist = visuals.map((v, i) => ({
        id: `shot_${i}`,
        sceneId: v.sceneId,
        shotNumber: i + 1,
        description: screenplay[i]?.action ?? '',
        cameraAngle: 'Dynamic',
        movement: 'Fast',
        lighting: 'High-Key',
        dialogue: screenplay[i]?.dialogue[0]?.text ?? '',
        imageUrl: v.imageUrl,
      }));
      state.currentStep = 'shotlist';
      state.updatedAt = Date.now();
      storyModeStore.set(sessionId, state);

      // ----------------------------------------------------------------
      // Phase 3: Audio generation (energetic voice) — Requirement 4.4
      // ----------------------------------------------------------------
      log.info('Phase 3: Audio generation');

      const scenes: Scene[] = screenplay.map((s, i) => ({
        id: s.id,
        name: s.heading,
        duration: durationCheck.estimatedSeconds / screenplay.length,
        visualDescription: s.action,
        narrationScript: s.dialogue.map(d => d.text).join(' '),
        emotionalTone: 'urgent' as const,
      }));

      const voiceConfig = getFormatVoiceForLanguage(FORMAT_ID, language);
      const narratorConfig: NarratorConfig = {
        defaultVoice: voiceConfig.voiceName,
        videoPurpose: 'commercial',
        language: language as any,
        styleOverride: voiceConfig.stylePrompt,
      };

      const narrationSegments: NarrationSegment[] = [];
      for (const scene of scenes) {
        try {
          const segment = await narrateScene(scene, narratorConfig, sessionId);
          narrationSegments.push(segment);
        } catch (err) {
          log.warn(`Narration failed for scene ${scene.id}:`, err);
        }
      }

      log.info(`Narration complete: ${narrationSegments.length}/${scenes.length} segments`);

      // ----------------------------------------------------------------
      // Phase 4: Assembly with CTA emphasis — Requirement 4.6
      // ----------------------------------------------------------------
      log.info('Phase 4: Assembly');

      const totalDuration = narrationSegments.reduce((sum, s) => sum + s.audioDuration, 0);

      // Build CTA marker for final 5 seconds
      const ctaMarker = buildCTAMarker(ctaText, totalDuration);
      const ctaValid = validateCTAPosition(ctaMarker, totalDuration);
      if (!ctaValid) {
        log.warn('CTA position validation failed — adjusting');
      }

      const assemblyRules = buildAssemblyRules(FORMAT_ID, {
        totalDuration,
        ctaText,
      });

      // Checkpoint 2: Final Preview — Requirement 4.5
      const finalApproval = await checkpoints.createCheckpoint('final-preview', {
        sceneCount: screenplay.length,
        visualCount: visuals.length,
        narrationCount: narrationSegments.length,
        totalDuration,
        ctaText,
        ctaPositionValid: ctaValid,
      });
      if (!finalApproval.approved) {
        log.info('Final preview rejected by user');
        checkpoints.dispose();
        return {
          success: false,
          error: 'Final preview rejected by user',
          partialResults: { screenplay, visuals, narrationSegments },
        };
      }

      state.currentStep = 'production';
      state.updatedAt = Date.now();
      state.checkpoints = checkpoints.getAllCheckpoints();
      storyModeStore.set(sessionId, state);

      checkpoints.dispose();

      log.info(`Advertisement pipeline complete: ${screenplay.length} scenes, ${visuals.length} visuals, CTA="${ctaText}"`);

      return {
        success: true,
        partialResults: {
          sessionId,
          screenplay,
          visuals,
          narrationSegments,
          assemblyRules,
          ctaMarker,
          totalDuration,
        },
      };

    } catch (error) {
      checkpoints.dispose();
      const msg = error instanceof Error ? error.message : String(error);
      log.error('Advertisement pipeline failed:', msg);
      return { success: false, error: msg };
    }
  }
}
````

## File: packages/shared/src/services/pipelines/documentary.ts
````typescript
/**
 * Documentary Pipeline
 *
 * Produces deeply researched long-form videos with chapter structure, citations,
 * and archival visuals. Phases: Research → Script → Visuals → Audio → Assembly.
 *
 * Requirements: 7.1–7.6
 */

import type { FormatMetadata, VideoFormat, Scene, NarrationSegment, ScreenplayScene } from '../../types';
import type { FormatPipeline, PipelineRequest, PipelineResult, PipelineCallbacks } from '../formatRouter';
import { formatRegistry } from '../formatRegistry';
import { ResearchService, type ResearchResult } from '../researchService';
import {
  buildBreakdownPrompt,
  buildScreenplayPrompt,
  countScriptWords,
  validateDurationConstraint,
  type FormatAwareGenerationOptions,
} from '../ai/storyPipeline';
import { ParallelExecutionEngine, type Task } from '../parallelExecutionEngine';
import { CheckpointSystem } from '../checkpointSystem';
import { narrateScene, getFormatVoiceForLanguage, type NarratorConfig } from '../narratorService';
import { generateImageFromPrompt } from '../imageService';
import { buildImageStyleGuide } from '../prompt/imageStyleGuide';
import {
  buildAssemblyRules,
  buildChapterMarkers,
  validateChapterSequence,
} from '../ffmpeg/formatAssembly';
import { detectLanguage } from '../languageDetector';
import { storyModeStore } from '../ai/production/store';
import type { StoryModeState } from '../ai/production/types';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { GEMINI_API_KEY, MODELS } from '../shared/apiClient';
import { z } from 'zod';
import { agentLogger } from '../logger';

const FORMAT_ID: VideoFormat = 'documentary';
const log = agentLogger.child('DocumentaryPipeline');

// ============================================================================
// Schemas
// ============================================================================

const BreakdownSchema = z.object({
  acts: z.array(z.object({
    title: z.string(),
    emotionalHook: z.string(),
    narrativeBeat: z.string(),
    chapterTitle: z.string(),
  })).min(4).max(8),
});

const ScreenplaySchema = z.object({
  scenes: z.array(z.object({
    heading: z.string(),
    action: z.string(),
    dialogue: z.array(z.object({
      speaker: z.string().max(30),
      text: z.string().min(1),
    })),
  })).min(4).max(10),
});

// ============================================================================
// Pipeline Implementation
// ============================================================================

export class DocumentaryPipeline implements FormatPipeline {
  private researchService: ResearchService;
  private parallelEngine: ParallelExecutionEngine;

  constructor(
    researchService?: ResearchService,
    parallelEngine?: ParallelExecutionEngine,
  ) {
    this.researchService = researchService ?? new ResearchService();
    this.parallelEngine = parallelEngine ?? new ParallelExecutionEngine();
  }

  getMetadata(): FormatMetadata {
    return formatRegistry.getFormat(FORMAT_ID)!;
  }

  async validate(request: PipelineRequest): Promise<boolean> {
    return !!request.idea && request.idea.trim().length > 0;
  }

  async execute(request: PipelineRequest, callbacks?: PipelineCallbacks): Promise<PipelineResult> {
    const sessionId = `doc_${Date.now()}_${Math.random().toString(36).slice(2, 8)}`;
    const language = request.language ?? detectLanguage(request.idea);
    const metadata = this.getMetadata();

    let cancelled = false;
    const checkpoints = new CheckpointSystem({
      maxCheckpoints: metadata.checkpointCount,
      onCheckpointCreated: callbacks?.onCheckpointCreated,
    });
    callbacks?.onCheckpointSystemCreated?.(checkpoints);
    callbacks?.onCancelRequested?.(() => { cancelled = true; checkpoints.dispose(); });

    log.info(`Starting Documentary pipeline: "${request.idea.slice(0, 60)}..." [${language}]`);

    try {
      // ----------------------------------------------------------------
      // Phase 1: Extensive research with multiple sources — Requirements 7.2, 7.6
      // ----------------------------------------------------------------
      log.info('Phase 1: Deep research across multiple sources');

      const researchResult: ResearchResult = await this.researchService.research({
        topic: request.idea,
        language,
        depth: 'deep',
        sources: ['web', 'knowledge-base', ...(request.referenceDocuments?.length ? ['references' as const] : [])],
        maxResults: 20,
        referenceDocuments: request.referenceDocuments,
      });

      log.info(`Research complete: ${researchResult.sources.length} sources, confidence=${researchResult.confidence.toFixed(2)}`);

      // Checkpoint 1: Research Summary — Requirement 7.5
      const researchApproval = await checkpoints.createCheckpoint('research-summary', {
        sourceCount: researchResult.sources.length,
        confidence: researchResult.confidence,
        keyTopics: researchResult.summary.slice(0, 200),
        citationCount: researchResult.citations.length,
      });
      if (!researchApproval.approved) {
        log.info('Research rejected by user');
        checkpoints.dispose();
        return { success: false, error: 'Research rejected by user', partialResults: { research: researchResult } };
      }

      // ----------------------------------------------------------------
      // Phase 2: Chapter-structured script with citations — Requirement 7.3
      // ----------------------------------------------------------------
      log.info('Phase 2: Chapter-structured script generation');

      const formatOptions: FormatAwareGenerationOptions = {
        formatId: FORMAT_ID,
        genre: request.genre,
        language,
        researchSummary: researchResult.summary,
        researchCitations: researchResult.citations.map(c => c.text).join('; '),
      };

      const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey: GEMINI_API_KEY,
        temperature: 0.6,
      });

      const breakdownPrompt = buildBreakdownPrompt(request.idea, formatOptions);
      const breakdownResult = await model.withStructuredOutput(BreakdownSchema).invoke(breakdownPrompt);

      const screenplayPrompt = buildScreenplayPrompt(breakdownResult.acts, formatOptions);
      const screenplayResult = await model.withStructuredOutput(ScreenplaySchema).invoke(screenplayPrompt);

      const screenplay: ScreenplayScene[] = screenplayResult.scenes.map((s, i) => ({
        id: `scene_${i}`,
        sceneNumber: i + 1,
        heading: s.heading,
        action: s.action,
        dialogue: s.dialogue.filter(d => d.text.trim().length > 0),
        charactersPresent: [],
      }));

      const wordCount = countScriptWords(screenplay);
      const durationCheck = validateDurationConstraint(wordCount, metadata);
      if (!durationCheck.valid) {
        log.warn(`Duration constraint: ${durationCheck.message}`);
      }

      // Persist state
      const state: StoryModeState = {
        id: sessionId,
        topic: request.idea,
        breakdown: breakdownResult.acts.map(a => `${a.chapterTitle}: ${a.narrativeBeat}`).join('\n'),
        screenplay,
        characters: [],
        shotlist: [],
        currentStep: 'screenplay',
        updatedAt: Date.now(),
        formatId: FORMAT_ID,
        language,
      };
      storyModeStore.set(sessionId, state);

      // Checkpoint 2: Chapter Structure — Requirement 7.5
      const chapterApproval = await checkpoints.createCheckpoint('chapter-structure', {
        sceneCount: screenplay.length,
        scenes: screenplay.map(s => ({
          heading: s.heading,
          action: s.action.slice(0, 120),
        })),
      });
      if (!chapterApproval.approved) {
        log.info('Chapter structure rejected by user');
        checkpoints.dispose();
        return { success: false, error: 'Chapter structure rejected by user', partialResults: { screenplay, research: researchResult } };
      }

      // ----------------------------------------------------------------
      // Phase 3: Archival visuals and data visualizations — Requirement 7.4
      // ----------------------------------------------------------------
      log.info('Phase 3: Archival visual generation');

      const visualTasks: Task<{ sceneId: string; imageUrl: string }>[] = screenplay.map((scene, i) => ({
        id: `visual_${i}`,
        type: 'visual' as const,
        priority: 1,
        retryable: true,
        timeout: 60_000,
        execute: async () => {
          const guide = buildImageStyleGuide({
            scene: scene.action,
            style: 'Archival Documentary',
            background: scene.heading,
            mood: breakdownResult.acts[i]?.emotionalHook ?? 'solemn',
          });

          const imageUrl = await generateImageFromPrompt(
            scene.action,
            'Archival Documentary',
            '',
            '16:9',
            true,
            undefined,
            sessionId,
            i,
            guide,
          );
          return { sceneId: scene.id, imageUrl };
        },
      }));

      const visualResults = await this.parallelEngine.execute(visualTasks, {
        concurrencyLimit: metadata.concurrencyLimit,
        retryAttempts: 2,
        retryDelay: 1500,
        exponentialBackoff: true,
      });

      const visuals = visualResults
        .filter(r => r.success && r.data)
        .map(r => r.data!);

      log.info(`Visuals generated: ${visuals.length}/${screenplay.length}`);

      // Update state
      state.shotlist = visuals.map((v, i) => ({
        id: `shot_${i}`,
        sceneId: v.sceneId,
        shotNumber: i + 1,
        description: screenplay[i]?.action ?? '',
        cameraAngle: 'Wide',
        movement: 'Slow Pan',
        lighting: 'Natural',
        dialogue: screenplay[i]?.dialogue[0]?.text ?? '',
        imageUrl: v.imageUrl,
      }));
      state.currentStep = 'shotlist';
      state.updatedAt = Date.now();
      storyModeStore.set(sessionId, state);

      // Checkpoint 3: Visual Preview — Requirement 7.5
      const visualApproval = await checkpoints.createCheckpoint('visual-preview', {
        visualCount: visuals.length,
        totalScenes: screenplay.length,
        visuals: visuals.map(v => ({
          sceneId: v.sceneId,
          imageUrl: v.imageUrl,
        })),
      });
      if (!visualApproval.approved) {
        log.info('Visuals rejected by user');
        checkpoints.dispose();
        return {
          success: false,
          error: 'Visuals rejected by user',
          partialResults: { screenplay, visuals, research: researchResult },
        };
      }

      // ----------------------------------------------------------------
      // Phase 4: Audio generation (neutral voice) — Requirements 7.1
      // ----------------------------------------------------------------
      log.info('Phase 4: Audio generation');

      const scenes: Scene[] = screenplay.map((s, i) => ({
        id: s.id,
        name: s.heading,
        duration: durationCheck.estimatedSeconds / screenplay.length,
        visualDescription: s.action,
        narrationScript: s.dialogue.map(d => d.text).join(' '),
        emotionalTone: 'dramatic',
      }));

      const voiceConfig = getFormatVoiceForLanguage(FORMAT_ID, language);
      const narratorConfig: NarratorConfig = {
        defaultVoice: voiceConfig.voiceName,
        videoPurpose: 'documentary',
        language: language as any,
        styleOverride: voiceConfig.stylePrompt,
      };

      const narrationSegments: NarrationSegment[] = [];
      for (const scene of scenes) {
        try {
          const segment = await narrateScene(scene, narratorConfig, sessionId);
          narrationSegments.push(segment);
        } catch (err) {
          log.warn(`Narration failed for scene ${scene.id}:`, err);
        }
      }

      log.info(`Narration complete: ${narrationSegments.length}/${scenes.length} segments`);

      // ----------------------------------------------------------------
      // Phase 5: Assembly with chapter markers — Requirement 7.5
      // ----------------------------------------------------------------
      log.info('Phase 5: Assembly with chapter markers');

      const sceneDurations = narrationSegments.map(s => s.audioDuration);
      const totalDuration = sceneDurations.reduce((sum, d) => sum + d, 0);

      const chapters = buildChapterMarkers(screenplay, sceneDurations);
      const chaptersValid = validateChapterSequence(chapters);
      if (!chaptersValid) {
        log.warn('Chapter sequence validation failed — some chapters may overlap');
      }

      const assemblyRules = buildAssemblyRules(FORMAT_ID, {
        totalDuration,
        scenes: screenplay,
        sceneDurations,
      });

      // Checkpoint 4: Final Assembly — Requirement 7.5
      const assemblyApproval = await checkpoints.createCheckpoint('final-assembly', {
        sceneCount: screenplay.length,
        visualCount: visuals.length,
        narrationCount: narrationSegments.length,
        totalDuration,
      });
      if (!assemblyApproval.approved) {
        log.info('Assembly rejected by user');
        checkpoints.dispose();
        return {
          success: false,
          error: 'Assembly rejected by user',
          partialResults: { screenplay, visuals, narrationSegments, chapters },
        };
      }

      state.currentStep = 'production';
      state.updatedAt = Date.now();
      state.checkpoints = checkpoints.getAllCheckpoints();
      storyModeStore.set(sessionId, state);

      checkpoints.dispose();

      log.info(`Documentary pipeline complete: ${screenplay.length} scenes, ${chapters.length} chapters, ${researchResult.citations.length} citations`);

      return {
        success: true,
        partialResults: {
          sessionId,
          screenplay,
          visuals,
          narrationSegments,
          assemblyRules,
          chapters,
          research: researchResult,
          totalDuration,
        },
      };

    } catch (error) {
      checkpoints.dispose();
      const msg = error instanceof Error ? error.message : String(error);
      log.error('Documentary pipeline failed:', msg);
      return { success: false, error: msg };
    }
  }
}
````

## File: packages/shared/src/services/pipelines/educational.ts
````typescript
/**
 * Educational Pipeline
 *
 * Produces structured educational videos with visual aids, diagrams, and
 * clear learning objectives. Phases: Script → Visuals with Overlays → Audio → Assembly.
 *
 * Requirements: 5.1–5.6
 */

import type { FormatMetadata, VideoFormat, Scene, NarrationSegment, ScreenplayScene } from '../../types';
import type { FormatPipeline, PipelineRequest, PipelineResult, PipelineCallbacks } from '../formatRouter';
import { formatRegistry } from '../formatRegistry';
import { ResearchService, type ResearchResult } from '../researchService';
import {
  buildBreakdownPrompt,
  buildScreenplayPrompt,
  countScriptWords,
  validateDurationConstraint,
  type FormatAwareGenerationOptions,
} from '../ai/storyPipeline';
import { ParallelExecutionEngine, type Task } from '../parallelExecutionEngine';
import { CheckpointSystem } from '../checkpointSystem';
import { narrateScene, getFormatVoiceForLanguage, type NarratorConfig } from '../narratorService';
import { generateImageFromPrompt } from '../imageService';
import { buildImageStyleGuide } from '../prompt/imageStyleGuide';
import {
  buildAssemblyRules,
  buildChapterMarkers,
} from '../ffmpeg/formatAssembly';
import { detectLanguage } from '../languageDetector';
import { storyModeStore } from '../ai/production/store';
import type { StoryModeState } from '../ai/production/types';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { GEMINI_API_KEY, MODELS } from '../shared/apiClient';
import { z } from 'zod';
import { agentLogger } from '../logger';

const FORMAT_ID: VideoFormat = 'educational';
const log = agentLogger.child('EducationalPipeline');

// ============================================================================
// Schemas
// ============================================================================

const BreakdownSchema = z.object({
  acts: z.array(z.object({
    title: z.string(),
    emotionalHook: z.string(),
    narrativeBeat: z.string(),
    learningObjective: z.string(),
  })).min(3).max(6),
});

const ScreenplaySchema = z.object({
  scenes: z.array(z.object({
    heading: z.string(),
    action: z.string(),
    dialogue: z.array(z.object({
      speaker: z.string().max(30),
      text: z.string().min(1),
    })),
  })).min(3).max(8),
});

// ============================================================================
// Pipeline Implementation
// ============================================================================

export class EducationalPipeline implements FormatPipeline {
  private researchService: ResearchService;
  private parallelEngine: ParallelExecutionEngine;

  constructor(
    researchService?: ResearchService,
    parallelEngine?: ParallelExecutionEngine,
  ) {
    this.researchService = researchService ?? new ResearchService();
    this.parallelEngine = parallelEngine ?? new ParallelExecutionEngine();
  }

  getMetadata(): FormatMetadata {
    return formatRegistry.getFormat(FORMAT_ID)!;
  }

  async validate(request: PipelineRequest): Promise<boolean> {
    return !!request.idea && request.idea.trim().length > 0;
  }

  async execute(request: PipelineRequest, callbacks?: PipelineCallbacks): Promise<PipelineResult> {
    const sessionId = `edu_${Date.now()}_${Math.random().toString(36).slice(2, 8)}`;
    const language = request.language ?? detectLanguage(request.idea);
    const metadata = this.getMetadata();

    let cancelled = false;
    const checkpoints = new CheckpointSystem({
      maxCheckpoints: metadata.checkpointCount,
      onCheckpointCreated: callbacks?.onCheckpointCreated,
    });
    callbacks?.onCheckpointSystemCreated?.(checkpoints);
    callbacks?.onCancelRequested?.(() => { cancelled = true; checkpoints.dispose(); });

    log.info(`Starting Educational pipeline: "${request.idea.slice(0, 60)}..." [${language}]`);

    try {
      // ----------------------------------------------------------------
      // Phase 1: Research (parallel queries) — Requirements 5.2
      // ----------------------------------------------------------------
      log.info('Phase 1: Research');

      let researchResult: ResearchResult | undefined;

      researchResult = await this.researchService.research({
        topic: request.idea,
        language,
        depth: 'medium',
        sources: ['web', 'knowledge-base', ...(request.referenceDocuments?.length ? ['references' as const] : [])],
        maxResults: 8,
        referenceDocuments: request.referenceDocuments,
      });

      log.info(`Research complete: ${researchResult.sources.length} sources, confidence=${researchResult.confidence.toFixed(2)}`);

      // ----------------------------------------------------------------
      // Phase 2: Script with learning objectives — Requirements 5.1, 5.2
      // ----------------------------------------------------------------
      log.info('Phase 2: Script generation');

      const formatOptions: FormatAwareGenerationOptions = {
        formatId: FORMAT_ID,
        genre: request.genre,
        language,
        researchSummary: researchResult.summary,
        researchCitations: researchResult.citations.map(c => c.text).join('; '),
      };

      const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey: GEMINI_API_KEY,
        temperature: 0.6,
      });

      // Step 2a: Breakdown with learning objectives
      const breakdownPrompt = buildBreakdownPrompt(request.idea, formatOptions);
      const breakdownResult = await model.withStructuredOutput(BreakdownSchema).invoke(breakdownPrompt);

      // Step 2b: Screenplay
      const screenplayPrompt = buildScreenplayPrompt(breakdownResult.acts, formatOptions);
      const screenplayResult = await model.withStructuredOutput(ScreenplaySchema).invoke(screenplayPrompt);

      const screenplay: ScreenplayScene[] = screenplayResult.scenes.map((s, i) => ({
        id: `scene_${i}`,
        sceneNumber: i + 1,
        heading: s.heading,
        action: s.action,
        dialogue: s.dialogue.filter(d => d.text.trim().length > 0),
        charactersPresent: [],
      }));

      const wordCount = countScriptWords(screenplay);
      const durationCheck = validateDurationConstraint(wordCount, metadata);
      if (!durationCheck.valid) {
        log.warn(`Duration constraint: ${durationCheck.message}`);
      }

      const learningObjectives = breakdownResult.acts.map(a => a.learningObjective);

      // Persist state
      const state: StoryModeState = {
        id: sessionId,
        topic: request.idea,
        breakdown: breakdownResult.acts.map(a => `${a.title}: ${a.narrativeBeat}`).join('\n'),
        screenplay,
        characters: [],
        shotlist: [],
        currentStep: 'screenplay',
        updatedAt: Date.now(),
        formatId: FORMAT_ID,
        language,
      };
      storyModeStore.set(sessionId, state);

      // Checkpoint 1: Learning Structure — Requirement 5.5
      const structureApproval = await checkpoints.createCheckpoint('learning-structure', {
        scenes: screenplay.map(s => ({
          heading: s.heading,
          action: s.action.length > 200 ? s.action.slice(0, 200) + '...' : s.action,
        })),
        sceneCount: screenplay.length,
        learningObjectives,
        acts: breakdownResult.acts.map(a => ({ title: a.title, learningObjective: a.learningObjective })),
      });
      if (!structureApproval.approved) {
        log.info('Learning structure rejected by user');
        checkpoints.dispose();
        return { success: false, error: 'Learning structure rejected by user', partialResults: { screenplay, learningObjectives } };
      }

      // ----------------------------------------------------------------
      // Phase 3: Visuals with text overlays — Requirements 5.3, 5.4
      // ----------------------------------------------------------------
      log.info('Phase 3: Visual generation with text overlays');

      const visualTasks: Task<{ sceneId: string; imageUrl: string; overlay: string }>[] = screenplay.map((scene, i) => ({
        id: `visual_${i}`,
        type: 'visual' as const,
        priority: 1,
        retryable: true,
        timeout: 60_000,
        execute: async () => {
          const keyConceptOverlay = learningObjectives[i] ?? scene.heading;

          const guide = buildImageStyleGuide({
            scene: `Educational diagram: ${scene.action}`,
            style: 'Educational Diagram',
            background: scene.heading,
            mood: 'informative and clear',
          });

          const imageUrl = await generateImageFromPrompt(
            `Educational visual: ${scene.action}`,
            'Educational Diagram',
            '',
            '16:9',
            true,
            undefined,
            sessionId,
            i,
            guide,
          );

          return { sceneId: scene.id, imageUrl, overlay: keyConceptOverlay };
        },
      }));

      const visualResults = await this.parallelEngine.execute(visualTasks, {
        concurrencyLimit: metadata.concurrencyLimit,
        retryAttempts: 2,
        retryDelay: 1000,
        exponentialBackoff: true,
      });

      const visuals = visualResults
        .filter(r => r.success && r.data)
        .map(r => r.data!);

      log.info(`Visuals generated: ${visuals.length}/${screenplay.length}`);

      // Update state with visuals
      state.shotlist = visuals.map((v, i) => ({
        id: `shot_${i}`,
        sceneId: v.sceneId,
        shotNumber: i + 1,
        description: screenplay[i]?.action ?? '',
        cameraAngle: 'Static',
        movement: 'None',
        lighting: 'Bright',
        dialogue: screenplay[i]?.dialogue[0]?.text ?? '',
        imageUrl: v.imageUrl,
      }));
      state.currentStep = 'shotlist';
      state.updatedAt = Date.now();
      storyModeStore.set(sessionId, state);

      // Checkpoint 2: Visual Aids — Requirement 5.5
      const visualApproval = await checkpoints.createCheckpoint('visual-aids', {
        visuals: visuals.map(v => ({ imageUrl: v.imageUrl, sceneId: v.sceneId })),
        visualCount: visuals.length,
        totalScenes: screenplay.length,
      });
      if (!visualApproval.approved) {
        log.info('Visual aids rejected by user');
        checkpoints.dispose();
        return { success: false, error: 'Visual aids rejected by user', partialResults: { screenplay, visuals, learningObjectives } };
      }

      // ----------------------------------------------------------------
      // Phase 4: Audio generation (professional voice) — Requirements 5.1
      // ----------------------------------------------------------------
      log.info('Phase 4: Audio generation');

      const scenes: Scene[] = screenplay.map((s, i) => ({
        id: s.id,
        name: s.heading,
        duration: durationCheck.estimatedSeconds / screenplay.length,
        visualDescription: s.action,
        narrationScript: s.dialogue.map(d => d.text).join(' '),
        emotionalTone: 'friendly' as const,
      }));

      const voiceConfig = getFormatVoiceForLanguage(FORMAT_ID, language);
      const narratorConfig: NarratorConfig = {
        defaultVoice: voiceConfig.voiceName,
        videoPurpose: 'educational',
        language: language as any,
        styleOverride: voiceConfig.stylePrompt,
      };

      const narrationSegments: NarrationSegment[] = [];
      for (const scene of scenes) {
        try {
          const segment = await narrateScene(scene, narratorConfig, sessionId);
          narrationSegments.push(segment);
        } catch (err) {
          log.warn(`Narration failed for scene ${scene.id}:`, err);
        }
      }

      log.info(`Narration complete: ${narrationSegments.length}/${scenes.length} segments`);

      // ----------------------------------------------------------------
      // Phase 5: Assembly with chapter organization — Requirement 5.6
      // ----------------------------------------------------------------
      log.info('Phase 5: Assembly with chapter organization');

      const sceneDurations = narrationSegments.map(s => s.audioDuration);
      const totalDuration = sceneDurations.reduce((sum, d) => sum + d, 0);

      const chapters = buildChapterMarkers(screenplay, sceneDurations);
      const assemblyRules = buildAssemblyRules(FORMAT_ID, {
        totalDuration,
        scenes: screenplay,
        sceneDurations,
      });

      // Checkpoint 3: Final Assembly — Requirement 5.5
      const assemblyApproval = await checkpoints.createCheckpoint('final-assembly', {
        sceneCount: screenplay.length,
        visualCount: visuals.length,
        narrationCount: narrationSegments.length,
        totalDuration,
      });
      if (!assemblyApproval.approved) {
        log.info('Assembly rejected by user');
        checkpoints.dispose();
        return {
          success: false,
          error: 'Assembly rejected by user',
          partialResults: { screenplay, visuals, narrationSegments, chapters },
        };
      }

      state.currentStep = 'production';
      state.updatedAt = Date.now();
      state.checkpoints = checkpoints.getAllCheckpoints();
      storyModeStore.set(sessionId, state);

      checkpoints.dispose();

      log.info(`Educational pipeline complete: ${screenplay.length} scenes, ${visuals.length} visuals, ${chapters.length} chapters`);

      return {
        success: true,
        partialResults: {
          sessionId,
          screenplay,
          visuals,
          narrationSegments,
          assemblyRules,
          chapters,
          learningObjectives,
          research: researchResult,
          totalDuration,
        },
      };

    } catch (error) {
      checkpoints.dispose();
      const msg = error instanceof Error ? error.message : String(error);
      log.error('Educational pipeline failed:', msg);
      return { success: false, error: msg };
    }
  }
}
````

## File: packages/shared/src/services/pipelines/movieAnimation.ts
````typescript
/**
 * Movie/Animation Pipeline
 *
 * Thin wrapper around the existing story pipeline (services/ai/storyPipeline.ts)
 * that implements the FormatPipeline interface for the format router.
 *
 * Design:
 * - Keeps existing 2-layer architecture intact:
 *   - services/ai/storyPipeline.ts (core functions)
 *   - services/ai/production/tools/storyTools.ts (LangChain tool wrappers)
 * - Adds format metadata and FormatPipeline interface compliance
 * - Integrates with CheckpointSystem and ParallelExecutionEngine where possible
 *
 * Requirements: 24.1, 24.2, 24.3
 */

import type { FormatMetadata, VideoFormat } from '../../types';
import type { FormatPipeline, PipelineRequest, PipelineResult } from '../formatRouter';
import { formatRegistry } from '../formatRegistry';
import { runStoryPipeline, type StoryPipelineOptions, type StoryPipelineResult } from '../ai/storyPipeline';
import { CheckpointSystem } from '../checkpointSystem';
import { detectLanguage } from '../languageDetector';
import { storyModeStore } from '../ai/production/store';
import { agentLogger } from '../logger';

const FORMAT_ID: VideoFormat = 'movie-animation';
const log = agentLogger.child('MovieAnimationPipeline');

// ============================================================================
// Pipeline Implementation
// ============================================================================

export class MovieAnimationPipeline implements FormatPipeline {
  getMetadata(): FormatMetadata {
    return formatRegistry.getFormat(FORMAT_ID)!;
  }

  async validate(request: PipelineRequest): Promise<boolean> {
    return !!request.idea && request.idea.trim().length > 0;
  }

  /**
   * Execute the Movie/Animation pipeline by delegating to the existing
   * runStoryPipeline function. This ensures backward compatibility while
   * providing a consistent FormatPipeline interface.
   *
   * Requirements: 24.1 (use existing pipeline), 24.3 (format integration)
   */
  async execute(request: PipelineRequest): Promise<PipelineResult> {
    const language = request.language ?? detectLanguage(request.idea);
    const metadata = this.getMetadata();

    const checkpoints = new CheckpointSystem({ maxCheckpoints: metadata.checkpointCount });

    log.info(`Starting Movie/Animation pipeline: "${request.idea.slice(0, 60)}..." [${language}]`);

    try {
      // Build options compatible with the existing pipeline
      const pipelineOptions: StoryPipelineOptions = {
        topic: request.idea,
        generateCharacterRefs: true,
        generateVisuals: true,
        visualStyle: 'Cinematic',
        formatId: FORMAT_ID,
        genre: request.genre,
        language,
        onProgress: (progress) => {
          log.info(`[${progress.stage}] ${progress.message} (${progress.progress ?? 0}%)`);
        },
      };

      // Delegate to existing pipeline
      const result: StoryPipelineResult = await runStoryPipeline(pipelineOptions);

      if (!result.success) {
        checkpoints.dispose();
        return { success: false, error: result.error };
      }

      // Retrieve the session state populated by the story pipeline
      const state = storyModeStore.get(result.sessionId);

      // Ensure format metadata is present in state (Requirement 24.2)
      if (state && !state.formatId) {
        state.formatId = FORMAT_ID;
        state.language = language;
        state.updatedAt = Date.now();
        storyModeStore.set(result.sessionId, state);
      }

      checkpoints.dispose();

      log.info(`Movie/Animation pipeline complete: ${result.actCount} acts, ${result.sceneCount} scenes, ${result.characterCount} characters, ${result.visualCount} visuals`);

      return {
        success: true,
        partialResults: {
          sessionId: result.sessionId,
          actCount: result.actCount,
          sceneCount: result.sceneCount,
          characterCount: result.characterCount,
          visualCount: result.visualCount,
          state,
        },
      };

    } catch (error) {
      checkpoints.dispose();
      const msg = error instanceof Error ? error.message : String(error);
      log.error('Movie/Animation pipeline failed:', msg);
      return { success: false, error: msg };
    }
  }
}
````

## File: packages/shared/src/services/pipelines/musicVideo.ts
````typescript
/**
 * Music Video Pipeline
 *
 * Produces AI-generated music videos with lyrics, music composition, and
 * beat-synchronized visuals.
 * Phases: Lyrics → Music → Visuals → Assembly.
 *
 * Requirements: 8.1–8.6
 */

import type { FormatMetadata, VideoFormat, Scene, NarrationSegment, ScreenplayScene } from '../../types';
import type { FormatPipeline, PipelineRequest, PipelineResult, PipelineCallbacks } from '../formatRouter';
import { formatRegistry } from '../formatRegistry';
import {
  buildBreakdownPrompt,
  buildScreenplayPrompt,
  countScriptWords,
  validateDurationConstraint,
  type FormatAwareGenerationOptions,
} from '../ai/storyPipeline';
import { ParallelExecutionEngine, type Task } from '../parallelExecutionEngine';
import { CheckpointSystem } from '../checkpointSystem';
import { narrateScene, getFormatVoiceForLanguage, type NarratorConfig } from '../narratorService';
import { generateImageFromPrompt } from '../imageService';
import { buildImageStyleGuide } from '../prompt/imageStyleGuide';
import {
  buildAssemblyRules,
  generateBeatMetadata,
  alignTransitionsToBeat,
} from '../ffmpeg/formatAssembly';
import { detectLanguage } from '../languageDetector';
import { storyModeStore } from '../ai/production/store';
import type { StoryModeState } from '../ai/production/types';
import type { BeatMetadata } from '../../types';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { GEMINI_API_KEY, MODELS } from '../shared/apiClient';
import { z } from 'zod';
import { agentLogger } from '../logger';

const FORMAT_ID: VideoFormat = 'music-video';
const log = agentLogger.child('MusicVideoPipeline');

// Default BPM map per genre for beat metadata generation
const GENRE_BPM: Record<string, number> = {
  'Pop': 120,
  'Rock': 130,
  'Hip Hop': 95,
  'Electronic': 128,
  'Jazz': 80,
  'Classical': 70,
  'R&B': 90,
  'Country': 100,
  'Indie': 110,
  'Ambient': 60,
};

// ============================================================================
// Schemas
// ============================================================================

const LyricsBreakdownSchema = z.object({
  acts: z.array(z.object({
    title: z.string(),      // e.g., "Verse 1", "Chorus", "Bridge"
    emotionalHook: z.string(),
    narrativeBeat: z.string(),
  })).min(2).max(5),
});

const LyricsScreenplaySchema = z.object({
  scenes: z.array(z.object({
    heading: z.string(),   // e.g., "VERSE 1 - CITY STREETS"
    action: z.string(),    // visual description for this lyric section
    dialogue: z.array(z.object({
      speaker: z.string().max(30),
      text: z.string().min(1),  // lyric lines
    })),
  })).min(2).max(5),
});

// ============================================================================
// Pipeline Implementation
// ============================================================================

export class MusicVideoPipeline implements FormatPipeline {
  private parallelEngine: ParallelExecutionEngine;

  constructor(parallelEngine?: ParallelExecutionEngine) {
    this.parallelEngine = parallelEngine ?? new ParallelExecutionEngine();
  }

  getMetadata(): FormatMetadata {
    return formatRegistry.getFormat(FORMAT_ID)!;
  }

  async validate(request: PipelineRequest): Promise<boolean> {
    return !!request.idea && request.idea.trim().length > 0;
  }

  async execute(request: PipelineRequest, callbacks?: PipelineCallbacks): Promise<PipelineResult> {
    const sessionId = `mv_${Date.now()}_${Math.random().toString(36).slice(2, 8)}`;
    const language = request.language ?? detectLanguage(request.idea);
    const metadata = this.getMetadata();

    let cancelled = false;
    const checkpoints = new CheckpointSystem({
      maxCheckpoints: metadata.checkpointCount,
      onCheckpointCreated: callbacks?.onCheckpointCreated,
    });
    callbacks?.onCheckpointSystemCreated?.(checkpoints);
    callbacks?.onCancelRequested?.(() => { cancelled = true; checkpoints.dispose(); });

    log.info(`Starting Music Video pipeline: "${request.idea.slice(0, 60)}..." genre=${request.genre ?? 'Pop'} [${language}]`);

    try {
      // ----------------------------------------------------------------
      // Phase 1: Lyrics generation — Requirement 8.2
      // ----------------------------------------------------------------
      log.info('Phase 1: Lyrics generation');

      const formatOptions: FormatAwareGenerationOptions = {
        formatId: FORMAT_ID,
        genre: request.genre,
        language,
      };

      const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey: GEMINI_API_KEY,
        temperature: 0.9,
      });

      const breakdownPrompt = buildBreakdownPrompt(request.idea, formatOptions);
      const breakdownResult = await model.withStructuredOutput(LyricsBreakdownSchema).invoke(breakdownPrompt);

      const screenplayPrompt = buildScreenplayPrompt(breakdownResult.acts, formatOptions);
      const screenplayResult = await model.withStructuredOutput(LyricsScreenplaySchema).invoke(screenplayPrompt);

      // Lyrics are stored in dialogue lines
      const screenplay: ScreenplayScene[] = screenplayResult.scenes.map((s, i) => ({
        id: `scene_${i}`,
        sceneNumber: i + 1,
        heading: s.heading,
        action: s.action,
        dialogue: s.dialogue.filter(d => d.text.trim().length > 0),
        charactersPresent: [],
      }));

      const lyrics = screenplay.map(s => ({
        section: s.heading,
        lines: s.dialogue.map(d => d.text),
      }));

      // ----------------------------------------------------------------
      // Phase 2: Music composition (beat metadata) — Requirement 8.3
      // ----------------------------------------------------------------
      log.info('Phase 2: Music composition');

      const wordCount = countScriptWords(screenplay);
      const durationCheck = validateDurationConstraint(wordCount, metadata);
      if (!durationCheck.valid) {
        log.warn(`Duration constraint: ${durationCheck.message}`);
      }

      const bpm = GENRE_BPM[request.genre ?? 'Pop'] ?? 120;
      const estimatedDuration = durationCheck.estimatedSeconds;

      // Generate beat metadata for synchronization
      const beatMetadata: BeatMetadata = generateBeatMetadata(bpm, estimatedDuration);

      log.info(`Beat metadata: ${bpm} BPM, ${beatMetadata.beats.length} beats over ${estimatedDuration.toFixed(1)}s`);

      // Persist state
      const state: StoryModeState = {
        id: sessionId,
        topic: request.idea,
        breakdown: breakdownResult.acts.map(a => `${a.title}: ${a.narrativeBeat}`).join('\n'),
        screenplay,
        characters: [],
        shotlist: [],
        currentStep: 'screenplay',
        updatedAt: Date.now(),
        formatId: FORMAT_ID,
        language,
      };
      storyModeStore.set(sessionId, state);

      // Checkpoint 1: Lyrics and Music — Requirement 8.5
      const lyricsApproval = await checkpoints.createCheckpoint('lyrics-and-music', {
        sceneCount: screenplay.length,
        scenes: screenplay.map(s => ({
          heading: s.heading,
          action: s.action.slice(0, 120),
        })),
        lyrics: lyrics.map(l => ({
          section: l.section,
          lineCount: l.lines.length,
        })),
        bpm,
        estimatedDuration,
        beatCount: beatMetadata.beats.length,
      });
      if (!lyricsApproval.approved) {
        log.info('Lyrics rejected by user');
        checkpoints.dispose();
        return { success: false, error: 'Lyrics rejected by user', partialResults: { lyrics, beatMetadata } };
      }

      // ----------------------------------------------------------------
      // Phase 3: Beat-synchronized visual generation — Requirement 8.4
      // ----------------------------------------------------------------
      log.info('Phase 3: Beat-synchronized visual generation');

      const visualTasks: Task<{ sceneId: string; imageUrl: string }>[] = screenplay.map((scene, i) => ({
        id: `visual_${i}`,
        type: 'visual' as const,
        priority: 1,
        retryable: true,
        timeout: 60_000,
        execute: async () => {
          const guide = buildImageStyleGuide({
            scene: scene.action,
            style: `${request.genre ?? 'Pop'} Music Video`,
            background: scene.heading,
            mood: breakdownResult.acts[i]?.emotionalHook ?? 'energetic',
          });

          const imageUrl = await generateImageFromPrompt(
            scene.action,
            `${request.genre ?? 'Pop'} Music Video`,
            '',
            '16:9',
            true,
            undefined,
            sessionId,
            i,
            guide,
          );
          return { sceneId: scene.id, imageUrl };
        },
      }));

      const visualResults = await this.parallelEngine.execute(visualTasks, {
        concurrencyLimit: metadata.concurrencyLimit,
        retryAttempts: 2,
        retryDelay: 1000,
        exponentialBackoff: true,
      });

      const visuals = visualResults
        .filter(r => r.success && r.data)
        .map(r => r.data!);

      log.info(`Visuals generated: ${visuals.length}/${screenplay.length}`);

      // Align visual transitions to beats — Requirement 8.6
      const rawTransitionTimes = screenplay.map((_, i) =>
        (i / screenplay.length) * estimatedDuration
      );
      const beatAlignedTransitions = alignTransitionsToBeat(rawTransitionTimes, beatMetadata.beats);
      log.info(`Transitions aligned to beats: ${beatAlignedTransitions.map(t => t.toFixed(2)).join(', ')}s`);

      // Update state
      state.shotlist = visuals.map((v, i) => ({
        id: `shot_${i}`,
        sceneId: v.sceneId,
        shotNumber: i + 1,
        description: screenplay[i]?.action ?? '',
        cameraAngle: 'Dynamic',
        movement: 'Cut',
        lighting: 'Atmospheric',
        dialogue: screenplay[i]?.dialogue.map(d => d.text).join(' ') ?? '',
        imageUrl: v.imageUrl,
      }));
      state.currentStep = 'shotlist';
      state.updatedAt = Date.now();
      storyModeStore.set(sessionId, state);

      // Checkpoint 2: Visual Preview — Requirement 8.5
      const visualApproval = await checkpoints.createCheckpoint('visual-preview', {
        visuals: visuals.map(v => ({
          sceneId: v.sceneId,
          imageUrl: v.imageUrl,
        })),
        visualCount: visuals.length,
        totalScenes: screenplay.length,
      });
      if (!visualApproval.approved) {
        log.info('Visuals rejected by user');
        checkpoints.dispose();
        return {
          success: false,
          error: 'Visuals rejected by user',
          partialResults: { lyrics, beatMetadata, visuals },
        };
      }

      // ----------------------------------------------------------------
      // Phase 4: Vocal narration (sung delivery) — Requirement 8.3
      // ----------------------------------------------------------------
      log.info('Phase 4: Vocal generation');

      const scenes: Scene[] = screenplay.map((s, i) => ({
        id: s.id,
        name: s.heading,
        duration: estimatedDuration / screenplay.length,
        visualDescription: s.action,
        narrationScript: s.dialogue.map(d => d.text).join('\n'),
        emotionalTone: 'friendly',
      }));

      const voiceConfig = getFormatVoiceForLanguage(FORMAT_ID, language);
      const narratorConfig: NarratorConfig = {
        defaultVoice: voiceConfig.voiceName,
        videoPurpose: 'music_video',
        language: language as any,
        styleOverride: voiceConfig.stylePrompt,
      };

      const narrationSegments: NarrationSegment[] = [];
      for (const scene of scenes) {
        try {
          const segment = await narrateScene(scene, narratorConfig, sessionId);
          narrationSegments.push(segment);
        } catch (err) {
          log.warn(`Vocal generation failed for scene ${scene.id}:`, err);
        }
      }

      log.info(`Vocal generation complete: ${narrationSegments.length}/${scenes.length} segments`);

      // ----------------------------------------------------------------
      // Phase 5: Assembly with beat synchronization — Requirement 8.6
      // ----------------------------------------------------------------
      log.info('Phase 5: Beat-synchronized assembly');

      const totalDuration = narrationSegments.reduce((sum, s) => sum + s.audioDuration, 0);

      const assemblyRules = buildAssemblyRules(FORMAT_ID, {
        totalDuration,
        beatMetadata,
      });

      // Checkpoint 3: Final Assembly — Requirement 8.5
      const assemblyApproval = await checkpoints.createCheckpoint('final-assembly', {
        sceneCount: screenplay.length,
        visualCount: visuals.length,
        narrationCount: narrationSegments.length,
        totalDuration,
      });
      if (!assemblyApproval.approved) {
        log.info('Assembly rejected by user');
        checkpoints.dispose();
        return {
          success: false,
          error: 'Assembly rejected by user',
          partialResults: { lyrics, beatMetadata, visuals, narrationSegments },
        };
      }

      state.currentStep = 'production';
      state.updatedAt = Date.now();
      state.checkpoints = checkpoints.getAllCheckpoints();
      storyModeStore.set(sessionId, state);

      checkpoints.dispose();

      log.info(`Music Video pipeline complete: ${screenplay.length} sections, ${visuals.length} visuals, ${beatMetadata.beats.length} beats`);

      return {
        success: true,
        partialResults: {
          sessionId,
          screenplay,
          lyrics,
          visuals,
          narrationSegments,
          assemblyRules,
          beatMetadata,
          beatAlignedTransitions,
          totalDuration,
        },
      };

    } catch (error) {
      checkpoints.dispose();
      const msg = error instanceof Error ? error.message : String(error);
      log.error('Music Video pipeline failed:', msg);
      return { success: false, error: msg };
    }
  }
}
````

## File: packages/shared/src/services/pipelines/newsPolitics.ts
````typescript
/**
 * News/Politics Pipeline
 *
 * Produces factual reporting videos with balanced perspectives, source citations,
 * and a professional neutral tone.
 * Phases: Research → Script → Visuals → Audio → Assembly.
 *
 * Requirements: 9.1–9.6
 */

import type { FormatMetadata, VideoFormat, Scene, NarrationSegment, ScreenplayScene } from '../../types';
import type { FormatPipeline, PipelineRequest, PipelineResult, PipelineCallbacks } from '../formatRouter';
import { formatRegistry } from '../formatRegistry';
import { ResearchService, type ResearchResult } from '../researchService';
import {
  buildBreakdownPrompt,
  buildScreenplayPrompt,
  countScriptWords,
  validateDurationConstraint,
  type FormatAwareGenerationOptions,
} from '../ai/storyPipeline';
import { ParallelExecutionEngine, type Task } from '../parallelExecutionEngine';
import { CheckpointSystem } from '../checkpointSystem';
import { narrateScene, getFormatVoiceForLanguage, type NarratorConfig } from '../narratorService';
import { generateImageFromPrompt } from '../imageService';
import { buildImageStyleGuide } from '../prompt/imageStyleGuide';
import { buildAssemblyRules } from '../ffmpeg/formatAssembly';
import { detectLanguage } from '../languageDetector';
import { storyModeStore } from '../ai/production/store';
import type { StoryModeState } from '../ai/production/types';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { GEMINI_API_KEY, MODELS } from '../shared/apiClient';
import { z } from 'zod';
import { agentLogger } from '../logger';

const FORMAT_ID: VideoFormat = 'news-politics';
const log = agentLogger.child('NewsPoliticsPipeline');

// ============================================================================
// Schemas
// ============================================================================

const BreakdownSchema = z.object({
  acts: z.array(z.object({
    title: z.string(),
    emotionalHook: z.string(),
    narrativeBeat: z.string(),
  })).min(3).max(5),
});

const ScreenplaySchema = z.object({
  scenes: z.array(z.object({
    heading: z.string(),
    action: z.string(),
    dialogue: z.array(z.object({
      speaker: z.string().max(30),
      text: z.string().min(1),
    })),
  })).min(3).max(7),
});

// ============================================================================
// Pipeline Implementation
// ============================================================================

export class NewsPoliticsPipeline implements FormatPipeline {
  private researchService: ResearchService;
  private parallelEngine: ParallelExecutionEngine;

  constructor(
    researchService?: ResearchService,
    parallelEngine?: ParallelExecutionEngine,
  ) {
    this.researchService = researchService ?? new ResearchService();
    this.parallelEngine = parallelEngine ?? new ParallelExecutionEngine();
  }

  getMetadata(): FormatMetadata {
    return formatRegistry.getFormat(FORMAT_ID)!;
  }

  async validate(request: PipelineRequest): Promise<boolean> {
    return !!request.idea && request.idea.trim().length > 0;
  }

  async execute(request: PipelineRequest, callbacks?: PipelineCallbacks): Promise<PipelineResult> {
    const sessionId = `news_${Date.now()}_${Math.random().toString(36).slice(2, 8)}`;
    const language = request.language ?? detectLanguage(request.idea);
    const metadata = this.getMetadata();

    let cancelled = false;
    const checkpoints = new CheckpointSystem({
      maxCheckpoints: metadata.checkpointCount,
      onCheckpointCreated: callbacks?.onCheckpointCreated,
    });
    callbacks?.onCheckpointSystemCreated?.(checkpoints);
    callbacks?.onCancelRequested?.(() => { cancelled = true; checkpoints.dispose(); });

    log.info(`Starting News/Politics pipeline: "${request.idea.slice(0, 60)}..." [${language}]`);

    try {
      // ----------------------------------------------------------------
      // Phase 1: Balanced research from multiple sources — Requirements 9.2
      // ----------------------------------------------------------------
      log.info('Phase 1: Balanced multi-source research');

      const researchResult: ResearchResult = await this.researchService.research({
        topic: request.idea,
        language,
        depth: 'medium',
        sources: ['web', 'knowledge-base', ...(request.referenceDocuments?.length ? ['references' as const] : [])],
        maxResults: 12,
        referenceDocuments: request.referenceDocuments,
      });

      log.info(`Research complete: ${researchResult.sources.length} sources, confidence=${researchResult.confidence.toFixed(2)}`);

      // Checkpoint 1: Research and Sources — Requirement 9.5
      const researchApproval = await checkpoints.createCheckpoint('research-and-sources', {
        sourceCount: researchResult.sources.length,
        confidence: researchResult.confidence,
        topics: researchResult.sources.map(s => s.title).slice(0, 5),
        summaryPreview: researchResult.summary.slice(0, 300),
      });
      if (!researchApproval.approved) {
        log.info('Research rejected by user');
        checkpoints.dispose();
        return { success: false, error: 'Research rejected by user', partialResults: { research: researchResult } };
      }

      // ----------------------------------------------------------------
      // Phase 2: Factual, balanced script with citations — Requirement 9.3
      // ----------------------------------------------------------------
      log.info('Phase 2: Factual script generation');

      const formatOptions: FormatAwareGenerationOptions = {
        formatId: FORMAT_ID,
        genre: request.genre,
        language,
        researchSummary: researchResult.summary,
        researchCitations: researchResult.citations.map(c => c.text).join('; '),
      };

      const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey: GEMINI_API_KEY,
        temperature: 0.5,
      });

      const breakdownPrompt = buildBreakdownPrompt(request.idea, formatOptions);
      const breakdownResult = await model.withStructuredOutput(BreakdownSchema).invoke(breakdownPrompt);

      const screenplayPrompt = buildScreenplayPrompt(breakdownResult.acts, formatOptions);
      const screenplayResult = await model.withStructuredOutput(ScreenplaySchema).invoke(screenplayPrompt);

      const screenplay: ScreenplayScene[] = screenplayResult.scenes.map((s, i) => ({
        id: `scene_${i}`,
        sceneNumber: i + 1,
        heading: s.heading,
        action: s.action,
        dialogue: s.dialogue.filter(d => d.text.trim().length > 0),
        charactersPresent: [],
      }));

      const wordCount = countScriptWords(screenplay);
      const durationCheck = validateDurationConstraint(wordCount, metadata);
      if (!durationCheck.valid) {
        log.warn(`Duration constraint: ${durationCheck.message}`);
      }

      // Persist state
      const state: StoryModeState = {
        id: sessionId,
        topic: request.idea,
        breakdown: breakdownResult.acts.map(a => `${a.title}: ${a.narrativeBeat}`).join('\n'),
        screenplay,
        characters: [],
        shotlist: [],
        currentStep: 'screenplay',
        updatedAt: Date.now(),
        formatId: FORMAT_ID,
        language,
      };
      storyModeStore.set(sessionId, state);

      // Checkpoint 2: Script Review — Requirement 9.5
      const scriptApproval = await checkpoints.createCheckpoint('script-review', {
        sceneCount: screenplay.length,
        scenes: screenplay.map(s => ({
          heading: s.heading,
          actionPreview: s.action.slice(0, 120),
        })),
      });
      if (!scriptApproval.approved) {
        log.info('Script rejected by user');
        checkpoints.dispose();
        return { success: false, error: 'Script rejected by user', partialResults: { screenplay, research: researchResult } };
      }

      // ----------------------------------------------------------------
      // Phase 3: News-style graphics and data visualizations — Requirement 9.4
      // ----------------------------------------------------------------
      log.info('Phase 3: News-style visual generation');

      const visualTasks: Task<{ sceneId: string; imageUrl: string }>[] = screenplay.map((scene, i) => ({
        id: `visual_${i}`,
        type: 'visual' as const,
        priority: 1,
        retryable: true,
        timeout: 60_000,
        execute: async () => {
          const guide = buildImageStyleGuide({
            scene: scene.action,
            style: 'News Broadcast',
            background: scene.heading,
            mood: breakdownResult.acts[i]?.emotionalHook ?? 'serious',
          });

          const imageUrl = await generateImageFromPrompt(
            scene.action,
            'News Broadcast',
            '',
            '16:9',
            true,
            undefined,
            sessionId,
            i,
            guide,
          );
          return { sceneId: scene.id, imageUrl };
        },
      }));

      const visualResults = await this.parallelEngine.execute(visualTasks, {
        concurrencyLimit: metadata.concurrencyLimit,
        retryAttempts: 2,
        retryDelay: 1000,
        exponentialBackoff: true,
      });

      const visuals = visualResults
        .filter(r => r.success && r.data)
        .map(r => r.data!);

      log.info(`Visuals generated: ${visuals.length}/${screenplay.length}`);

      // Update state
      state.shotlist = visuals.map((v, i) => ({
        id: `shot_${i}`,
        sceneId: v.sceneId,
        shotNumber: i + 1,
        description: screenplay[i]?.action ?? '',
        cameraAngle: 'Medium',
        movement: 'Static',
        lighting: 'Studio',
        dialogue: screenplay[i]?.dialogue[0]?.text ?? '',
        imageUrl: v.imageUrl,
      }));
      state.currentStep = 'shotlist';
      state.updatedAt = Date.now();
      storyModeStore.set(sessionId, state);

      // ----------------------------------------------------------------
      // Phase 4: Professional neutral voiceover — Requirements 9.6
      // ----------------------------------------------------------------
      log.info('Phase 4: Audio generation (neutral voice)');

      const scenes: Scene[] = screenplay.map((s, i) => ({
        id: s.id,
        name: s.heading,
        duration: durationCheck.estimatedSeconds / screenplay.length,
        visualDescription: s.action,
        narrationScript: s.dialogue.map(d => d.text).join(' '),
        emotionalTone: 'dramatic',
      }));

      const voiceConfig = getFormatVoiceForLanguage(FORMAT_ID, language);
      const narratorConfig: NarratorConfig = {
        defaultVoice: voiceConfig.voiceName,
        videoPurpose: 'documentary',
        language: language as any,
        styleOverride: voiceConfig.stylePrompt,
      };

      const narrationSegments: NarrationSegment[] = [];
      for (const scene of scenes) {
        try {
          const segment = await narrateScene(scene, narratorConfig, sessionId);
          narrationSegments.push(segment);
        } catch (err) {
          log.warn(`Narration failed for scene ${scene.id}:`, err);
        }
      }

      log.info(`Narration complete: ${narrationSegments.length}/${scenes.length} segments`);

      // ----------------------------------------------------------------
      // Phase 5: Assembly — Requirement 9.1
      // ----------------------------------------------------------------
      log.info('Phase 5: Assembly');

      const totalDuration = narrationSegments.reduce((sum, s) => sum + s.audioDuration, 0);
      const assemblyRules = buildAssemblyRules(FORMAT_ID, { totalDuration });

      // Checkpoint 3: Final Assembly — Requirement 9.5
      const assemblyApproval = await checkpoints.createCheckpoint('final-assembly', {
        sceneCount: screenplay.length,
        visualCount: visuals.length,
        narrationCount: narrationSegments.length,
        totalDuration,
      });
      if (!assemblyApproval.approved) {
        log.info('Assembly rejected by user');
        checkpoints.dispose();
        return {
          success: false,
          error: 'Assembly rejected by user',
          partialResults: { screenplay, visuals, narrationSegments, research: researchResult },
        };
      }

      state.currentStep = 'production';
      state.updatedAt = Date.now();
      state.checkpoints = checkpoints.getAllCheckpoints();
      storyModeStore.set(sessionId, state);

      checkpoints.dispose();

      log.info(`News/Politics pipeline complete: ${screenplay.length} scenes, ${visuals.length} visuals, ${researchResult.citations.length} citations`);

      return {
        success: true,
        partialResults: {
          sessionId,
          screenplay,
          visuals,
          narrationSegments,
          assemblyRules,
          research: researchResult,
          totalDuration,
        },
      };

    } catch (error) {
      checkpoints.dispose();
      const msg = error instanceof Error ? error.message : String(error);
      log.error('News/Politics pipeline failed:', msg);
      return { success: false, error: msg };
    }
  }
}
````

## File: packages/shared/src/services/pipelines/shorts.ts
````typescript
/**
 * Shorts/Reels Pipeline
 *
 * Produces vertical short-form videos (15–60s) optimized for mobile with
 * hook-first engagement and fast-paced visuals.
 * Phases: Script → Visuals → Audio → Assembly.
 *
 * Requirements: 6.1–6.5
 */

import type { FormatMetadata, VideoFormat, Scene, NarrationSegment, ScreenplayScene } from '../../types';
import type { FormatPipeline, PipelineRequest, PipelineResult, PipelineCallbacks } from '../formatRouter';
import { formatRegistry } from '../formatRegistry';
import {
  buildBreakdownPrompt,
  buildScreenplayPrompt,
  countScriptWords,
  validateDurationConstraint,
  type FormatAwareGenerationOptions,
} from '../ai/storyPipeline';
import { ParallelExecutionEngine, type Task } from '../parallelExecutionEngine';
import { CheckpointSystem } from '../checkpointSystem';
import { narrateScene, getFormatVoiceForLanguage, type NarratorConfig } from '../narratorService';
import { generateImageFromPrompt } from '../imageService';
import { buildImageStyleGuide } from '../prompt/imageStyleGuide';
import { buildAssemblyRules } from '../ffmpeg/formatAssembly';
import { detectLanguage } from '../languageDetector';
import { storyModeStore } from '../ai/production/store';
import type { StoryModeState } from '../ai/production/types';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { GEMINI_API_KEY, MODELS } from '../shared/apiClient';
import { z } from 'zod';
import { agentLogger } from '../logger';

const FORMAT_ID: VideoFormat = 'shorts';
const log = agentLogger.child('ShortsPipeline');

// ============================================================================
// Schemas
// ============================================================================

const BreakdownSchema = z.object({
  acts: z.array(z.object({
    title: z.string(),
    emotionalHook: z.string(),
    narrativeBeat: z.string(),
  })).min(2).max(3),
});

const ScreenplaySchema = z.object({
  scenes: z.array(z.object({
    heading: z.string(),
    action: z.string(),
    dialogue: z.array(z.object({
      speaker: z.string().max(30),
      text: z.string().min(1),
    })),
  })).min(2).max(4),
});

// ============================================================================
// Pipeline Implementation
// ============================================================================

export class ShortsPipeline implements FormatPipeline {
  private parallelEngine: ParallelExecutionEngine;

  constructor(parallelEngine?: ParallelExecutionEngine) {
    this.parallelEngine = parallelEngine ?? new ParallelExecutionEngine();
  }

  getMetadata(): FormatMetadata {
    return formatRegistry.getFormat(FORMAT_ID)!;
  }

  async validate(request: PipelineRequest): Promise<boolean> {
    return !!request.idea && request.idea.trim().length > 0;
  }

  async execute(request: PipelineRequest, callbacks?: PipelineCallbacks): Promise<PipelineResult> {
    const sessionId = `sht_${Date.now()}_${Math.random().toString(36).slice(2, 8)}`;
    const language = request.language ?? detectLanguage(request.idea);
    const metadata = this.getMetadata();

    let cancelled = false;
    const checkpoints = new CheckpointSystem({
      maxCheckpoints: metadata.checkpointCount,
      onCheckpointCreated: callbacks?.onCheckpointCreated,
    });
    callbacks?.onCheckpointSystemCreated?.(checkpoints);
    callbacks?.onCancelRequested?.(() => { cancelled = true; checkpoints.dispose(); });

    log.info(`Starting Shorts pipeline: "${request.idea.slice(0, 60)}..." [${language}]`);

    try {
      // ----------------------------------------------------------------
      // Phase 1: Hook-first script generation — Requirements 6.1, 6.3
      // ----------------------------------------------------------------
      log.info('Phase 1: Script generation (hook-first)');

      const formatOptions: FormatAwareGenerationOptions = {
        formatId: FORMAT_ID,
        genre: request.genre,
        language,
      };

      const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey: GEMINI_API_KEY,
        temperature: 0.85,
      });

      // Step 1a: Breakdown — first act must be the hook
      const breakdownPrompt = buildBreakdownPrompt(request.idea, formatOptions);
      const breakdownResult = await model.withStructuredOutput(BreakdownSchema).invoke(breakdownPrompt);

      // Step 1b: Screenplay — opening scene is the hook
      const screenplayPrompt = buildScreenplayPrompt(breakdownResult.acts, formatOptions);
      const screenplayResult = await model.withStructuredOutput(ScreenplaySchema).invoke(screenplayPrompt);

      const screenplay: ScreenplayScene[] = screenplayResult.scenes.map((s, i) => ({
        id: `scene_${i}`,
        sceneNumber: i + 1,
        heading: s.heading,
        action: s.action,
        dialogue: s.dialogue.filter(d => d.text.trim().length > 0),
        charactersPresent: [],
      }));

      const wordCount = countScriptWords(screenplay);
      const durationCheck = validateDurationConstraint(wordCount, metadata);
      if (!durationCheck.valid) {
        log.warn(`Duration constraint: ${durationCheck.message}`);
      }

      // Persist state
      const state: StoryModeState = {
        id: sessionId,
        topic: request.idea,
        breakdown: breakdownResult.acts.map(a => `${a.title}: ${a.narrativeBeat}`).join('\n'),
        screenplay,
        characters: [],
        shotlist: [],
        currentStep: 'screenplay',
        updatedAt: Date.now(),
        formatId: FORMAT_ID,
        language,
      };
      storyModeStore.set(sessionId, state);

      // Checkpoint 1: Hook Preview — Requirement 6.5
      const hookApproval = await checkpoints.createCheckpoint('hook-preview', {
        sceneCount: screenplay.length,
        scenes: screenplay.map(s => ({
          heading: s.heading,
          action: s.action.length > 200 ? s.action.slice(0, 200) + '...' : s.action,
        })),
      });
      if (!hookApproval.approved) {
        log.info('Hook rejected by user');
        checkpoints.dispose();
        return { success: false, error: 'Hook rejected by user', partialResults: { screenplay } };
      }

      // ----------------------------------------------------------------
      // Phase 2: Vertical visual generation (9:16) — Requirements 6.2, 6.4
      // ----------------------------------------------------------------
      log.info('Phase 2: Vertical visual generation (9:16)');

      const visualTasks: Task<{ sceneId: string; imageUrl: string }>[] = screenplay.map((scene, i) => ({
        id: `visual_${i}`,
        type: 'visual' as const,
        priority: 1,
        retryable: true,
        timeout: 60_000,
        execute: async () => {
          const guide = buildImageStyleGuide({
            scene: scene.action,
            style: 'Fast-Paced Vertical',
            background: scene.heading,
            mood: breakdownResult.acts[i]?.emotionalHook ?? 'exciting',
          });

          const imageUrl = await generateImageFromPrompt(
            scene.action,
            'Fast-Paced Vertical',
            '',
            '9:16',
            true,
            undefined,
            sessionId,
            i,
            guide,
          );
          return { sceneId: scene.id, imageUrl };
        },
      }));

      const visualResults = await this.parallelEngine.execute(visualTasks, {
        concurrencyLimit: metadata.concurrencyLimit,
        retryAttempts: 2,
        retryDelay: 800,
        exponentialBackoff: true,
      });

      const visuals = visualResults
        .filter(r => r.success && r.data)
        .map(r => r.data!);

      log.info(`Visuals generated: ${visuals.length}/${screenplay.length}`);

      // Update state
      state.shotlist = visuals.map((v, i) => ({
        id: `shot_${i}`,
        sceneId: v.sceneId,
        shotNumber: i + 1,
        description: screenplay[i]?.action ?? '',
        cameraAngle: 'Close-Up',
        movement: 'Fast',
        lighting: 'Vibrant',
        dialogue: screenplay[i]?.dialogue[0]?.text ?? '',
        imageUrl: v.imageUrl,
      }));
      state.currentStep = 'shotlist';
      state.updatedAt = Date.now();
      storyModeStore.set(sessionId, state);

      // ----------------------------------------------------------------
      // Phase 3: Audio generation — Requirements 6.1
      // ----------------------------------------------------------------
      log.info('Phase 3: Audio generation');

      const scenes: Scene[] = screenplay.map((s, i) => ({
        id: s.id,
        name: s.heading,
        duration: durationCheck.estimatedSeconds / screenplay.length,
        visualDescription: s.action,
        narrationScript: s.dialogue.map(d => d.text).join(' '),
        emotionalTone: 'urgent' as const,
      }));

      const voiceConfig = getFormatVoiceForLanguage(FORMAT_ID, language);
      const narratorConfig: NarratorConfig = {
        defaultVoice: voiceConfig.voiceName,
        videoPurpose: 'social_short',
        language: language as any,
        styleOverride: voiceConfig.stylePrompt,
      };

      const narrationSegments: NarrationSegment[] = [];
      for (const scene of scenes) {
        try {
          const segment = await narrateScene(scene, narratorConfig, sessionId);
          narrationSegments.push(segment);
        } catch (err) {
          log.warn(`Narration failed for scene ${scene.id}:`, err);
        }
      }

      log.info(`Narration complete: ${narrationSegments.length}/${scenes.length} segments`);

      // ----------------------------------------------------------------
      // Phase 4: Assembly optimized for mobile — Requirement 6.5
      // ----------------------------------------------------------------
      log.info('Phase 4: Assembly');

      const totalDuration = narrationSegments.reduce((sum, s) => sum + s.audioDuration, 0);
      const assemblyRules = buildAssemblyRules(FORMAT_ID, { totalDuration });

      // Checkpoint 2: Final Assembly — Requirement 6.5
      const assemblyApproval = await checkpoints.createCheckpoint('final-assembly', {
        sceneCount: screenplay.length,
        visualCount: visuals.length,
        narrationCount: narrationSegments.length,
        totalDuration,
      });
      if (!assemblyApproval.approved) {
        log.info('Assembly rejected by user');
        checkpoints.dispose();
        return {
          success: false,
          error: 'Assembly rejected by user',
          partialResults: { screenplay, visuals, narrationSegments },
        };
      }

      state.currentStep = 'production';
      state.updatedAt = Date.now();
      state.checkpoints = checkpoints.getAllCheckpoints();
      storyModeStore.set(sessionId, state);

      checkpoints.dispose();

      log.info(`Shorts pipeline complete: ${screenplay.length} scenes, ${visuals.length} visuals (9:16)`);

      return {
        success: true,
        partialResults: {
          sessionId,
          screenplay,
          visuals,
          narrationSegments,
          assemblyRules,
          totalDuration,
          aspectRatio: '9:16',
        },
      };

    } catch (error) {
      checkpoints.dispose();
      const msg = error instanceof Error ? error.message : String(error);
      log.error('Shorts pipeline failed:', msg);
      return { success: false, error: msg };
    }
  }
}
````

## File: packages/shared/src/services/pipelines/youtubeNarrator.ts
````typescript
/**
 * YouTube Narrator Pipeline
 *
 * Produces long-form YouTube narrator videos with conversational narration
 * and B-roll visuals. Phases: Research → Script → Visuals → Audio → Assembly.
 *
 * Requirements: 3.1–3.7
 */

import type { FormatMetadata, VideoFormat, Scene, NarrationSegment, ScreenplayScene } from '../../types';
import type { FormatPipeline, PipelineRequest, PipelineResult, PipelineCallbacks } from '../formatRouter';
import { formatRegistry } from '../formatRegistry';
import { ResearchService, type ResearchResult } from '../researchService';
import {
  buildBreakdownPrompt,
  buildScreenplayPrompt,
  generateVoiceoverScripts,
  countScriptWords,
  validateDurationConstraint,
  type FormatAwareGenerationOptions,
} from '../ai/storyPipeline';
import { ParallelExecutionEngine, type Task } from '../parallelExecutionEngine';
import { CheckpointSystem, type CheckpointApproval } from '../checkpointSystem';
import { narrateScene, getFormatVoiceForLanguage, type NarratorConfig } from '../narratorService';
import { generateImageFromPrompt } from '../imageService';
import { buildImageStyleGuide } from '../prompt/imageStyleGuide';
import { buildAssemblyRules } from '../ffmpeg/formatAssembly';
import { detectLanguage } from '../languageDetector';
import { storyModeStore } from '../ai/production/store';
import type { StoryModeState } from '../ai/production/types';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { GEMINI_API_KEY, MODELS } from '../shared/apiClient';
import { z } from 'zod';
import { agentLogger } from '../logger';

const FORMAT_ID: VideoFormat = 'youtube-narrator';
const log = agentLogger.child('YouTubeNarratorPipeline');

// ============================================================================
// Schemas
// ============================================================================

const BreakdownSchema = z.object({
  acts: z.array(z.object({
    title: z.string(),
    emotionalHook: z.string(),
    narrativeBeat: z.string(),
  })).min(3).max(5),
});

const ScreenplaySchema = z.object({
  scenes: z.array(z.object({
    heading: z.string(),
    action: z.string(),
    dialogue: z.array(z.object({
      speaker: z.string().max(30),
      text: z.string().min(1),
    })),
  })).min(3).max(8),
});

// ============================================================================
// Pipeline Implementation
// ============================================================================

export class YouTubeNarratorPipeline implements FormatPipeline {
  private researchService: ResearchService;
  private parallelEngine: ParallelExecutionEngine;

  constructor(
    researchService?: ResearchService,
    parallelEngine?: ParallelExecutionEngine,
  ) {
    this.researchService = researchService ?? new ResearchService();
    this.parallelEngine = parallelEngine ?? new ParallelExecutionEngine();
  }

  getMetadata(): FormatMetadata {
    return formatRegistry.getFormat(FORMAT_ID)!;
  }

  async validate(request: PipelineRequest): Promise<boolean> {
    return !!request.idea && request.idea.trim().length > 0;
  }

  async execute(request: PipelineRequest, callbacks?: PipelineCallbacks): Promise<PipelineResult> {
    const sessionId = `yt_${Date.now()}_${Math.random().toString(36).slice(2, 8)}`;
    const language = request.language ?? detectLanguage(request.idea);
    const metadata = this.getMetadata();

    let cancelled = false;
    const checkpoints = new CheckpointSystem({
      maxCheckpoints: metadata.checkpointCount,
      onCheckpointCreated: callbacks?.onCheckpointCreated,
    });
    callbacks?.onCheckpointSystemCreated?.(checkpoints);
    callbacks?.onCancelRequested?.(() => { cancelled = true; checkpoints.dispose(); });

    log.info(`Starting YouTube Narrator pipeline: "${request.idea.slice(0, 60)}..." [${language}]`);

    try {
      // ----------------------------------------------------------------
      // Phase 1: Research (parallel web queries) — Requirement 3.5
      // ----------------------------------------------------------------
      log.info('Phase 1: Research');

      let researchResult: ResearchResult | undefined;

      researchResult = await this.researchService.research({
        topic: request.idea,
        language,
        depth: 'medium',
        sources: ['web', 'knowledge-base', ...(request.referenceDocuments?.length ? ['references' as const] : [])],
        maxResults: 10,
        referenceDocuments: request.referenceDocuments,
      });

      log.info(`Research complete: ${researchResult.sources.length} sources, confidence=${researchResult.confidence.toFixed(2)}`);

      // ----------------------------------------------------------------
      // Phase 2: Script generation — Requirements 3.1, 3.2
      // ----------------------------------------------------------------
      log.info('Phase 2: Script generation');

      const formatOptions: FormatAwareGenerationOptions = {
        formatId: FORMAT_ID,
        genre: request.genre,
        language,
        researchSummary: researchResult.summary,
        researchCitations: researchResult.citations.map(c => c.text).join('; '),
      };

      // Step 2a: Breakdown
      const model = new ChatGoogleGenerativeAI({
        model: MODELS.TEXT,
        apiKey: GEMINI_API_KEY,
        temperature: 0.7,
      });

      const breakdownPrompt = buildBreakdownPrompt(request.idea, formatOptions);
      const breakdownResult = await model.withStructuredOutput(BreakdownSchema).invoke(breakdownPrompt);

      // Step 2b: Screenplay
      const screenplayPrompt = buildScreenplayPrompt(breakdownResult.acts, formatOptions);
      const screenplayResult = await model.withStructuredOutput(ScreenplaySchema).invoke(screenplayPrompt);

      const screenplay: ScreenplayScene[] = screenplayResult.scenes.map((s, i) => ({
        id: `scene_${i}`,
        sceneNumber: i + 1,
        heading: s.heading,
        action: s.action,
        dialogue: s.dialogue.filter(d => d.text.trim().length > 0),
        charactersPresent: [],
      }));

      // Duration validation
      const wordCount = countScriptWords(screenplay);
      const durationCheck = validateDurationConstraint(wordCount, metadata);
      if (!durationCheck.valid) {
        log.warn(`Duration constraint: ${durationCheck.message}`);
      }

      // Persist state
      const state: StoryModeState = {
        id: sessionId,
        topic: request.idea,
        breakdown: breakdownResult.acts.map(a => `${a.title}: ${a.narrativeBeat}`).join('\n'),
        screenplay,
        characters: [],
        shotlist: [],
        currentStep: 'screenplay',
        updatedAt: Date.now(),
        formatId: FORMAT_ID,
        language,
      };
      storyModeStore.set(sessionId, state);

      // Checkpoint 1: Script Review — Requirement 3.6
      const scriptApproval = await checkpoints.createCheckpoint('script-review', {
        sceneCount: screenplay.length,
        scenes: screenplay.map(s => ({
          heading: s.heading,
          action: s.action.length > 200 ? s.action.slice(0, 200) + '...' : s.action,
        })),
        estimatedDuration: `${Math.round(durationCheck.estimatedSeconds)}s`,
      });
      if (!scriptApproval.approved) {
        log.info('Script rejected by user');
        checkpoints.dispose();
        return { success: false, error: 'Script rejected by user', partialResults: { screenplay } };
      }

      // ----------------------------------------------------------------
      // Phase 3: Visual generation (B-roll, 16:9) — Requirement 3.3
      // ----------------------------------------------------------------
      log.info('Phase 3: Visual generation');

      const visualTasks: Task<{ sceneId: string; imageUrl: string }>[] = screenplay.map((scene, i) => ({
        id: `visual_${i}`,
        type: 'visual' as const,
        priority: 1,
        retryable: true,
        timeout: 60_000,
        execute: async () => {
          const guide = buildImageStyleGuide({
            scene: scene.action,
            style: 'B-roll Documentary',
            background: scene.heading,
            mood: breakdownResult.acts[i]?.emotionalHook ?? 'informative',
          });

          const imageUrl = await generateImageFromPrompt(
            scene.action,
            'B-roll Documentary',
            '',
            '16:9',
            true,
            undefined,
            sessionId,
            i,
            guide,
          );
          return { sceneId: scene.id, imageUrl };
        },
      }));

      const visualResults = await this.parallelEngine.execute(visualTasks, {
        concurrencyLimit: metadata.concurrencyLimit,
        retryAttempts: 2,
        retryDelay: 1000,
        exponentialBackoff: true,
      });

      const visuals = visualResults
        .filter(r => r.success && r.data)
        .map(r => r.data!);

      log.info(`Visuals generated: ${visuals.length}/${screenplay.length}`);

      // Update state with visuals
      state.shotlist = visuals.map((v, i) => ({
        id: `shot_${i}`,
        sceneId: v.sceneId,
        shotNumber: i + 1,
        description: screenplay[i]?.action ?? '',
        cameraAngle: 'Wide',
        movement: 'Static',
        lighting: 'Natural',
        dialogue: screenplay[i]?.dialogue[0]?.text ?? '',
        imageUrl: v.imageUrl,
      }));
      state.currentStep = 'shotlist';
      state.updatedAt = Date.now();
      storyModeStore.set(sessionId, state);

      // Checkpoint 2: Visual Preview — Requirement 3.6
      const visualApproval = await checkpoints.createCheckpoint('visual-preview', {
        visuals: visuals.map(v => ({
          sceneId: v.sceneId,
          imageUrl: v.imageUrl,
        })),
        visualCount: visuals.length,
        totalScenes: screenplay.length,
      });
      if (!visualApproval.approved) {
        log.info('Visuals rejected by user');
        checkpoints.dispose();
        return { success: false, error: 'Visuals rejected by user', partialResults: { screenplay, visuals } };
      }

      // ----------------------------------------------------------------
      // Phase 4: Audio generation (conversational voice) — Requirement 3.4
      // ----------------------------------------------------------------
      log.info('Phase 4: Audio generation');

      // Generate voiceover scripts with delivery markers
      const voiceoverMap = await generateVoiceoverScripts(
        screenplay,
        breakdownResult.acts.map(a => a.emotionalHook),
      );

      // Convert screenplay to Scene[] for narrator service
      const scenes: Scene[] = screenplay.map((s, i) => ({
        id: s.id,
        name: s.heading,
        duration: durationCheck.estimatedSeconds / screenplay.length,
        visualDescription: s.action,
        narrationScript: voiceoverMap.get(s.id) ?? s.dialogue.map(d => d.text).join(' '),
        emotionalTone: 'friendly' as const,
      }));

      const voiceConfig = getFormatVoiceForLanguage(FORMAT_ID, language);
      const narratorConfig: NarratorConfig = {
        defaultVoice: voiceConfig.voiceName,
        videoPurpose: 'documentary',
        language: language as any,
        styleOverride: voiceConfig.stylePrompt,
      };

      const narrationSegments: NarrationSegment[] = [];
      for (const scene of scenes) {
        try {
          const segment = await narrateScene(scene, narratorConfig, sessionId);
          narrationSegments.push(segment);
        } catch (err) {
          log.warn(`Narration failed for scene ${scene.id}:`, err);
        }
      }

      log.info(`Narration complete: ${narrationSegments.length}/${scenes.length} segments`);

      // ----------------------------------------------------------------
      // Phase 5: Assembly — Requirements 3.1
      // ----------------------------------------------------------------
      log.info('Phase 5: Assembly');

      const totalDuration = narrationSegments.reduce((sum, s) => sum + s.audioDuration, 0);
      const assemblyRules = buildAssemblyRules(FORMAT_ID, { totalDuration });

      // Checkpoint 3: Final Assembly — Requirement 3.6
      const assemblyApproval = await checkpoints.createCheckpoint('final-assembly', {
        sceneCount: screenplay.length,
        visualCount: visuals.length,
        narrationCount: narrationSegments.length,
        totalDuration,
      });
      if (!assemblyApproval.approved) {
        log.info('Assembly rejected by user');
        checkpoints.dispose();
        return {
          success: false,
          error: 'Assembly rejected by user',
          partialResults: { screenplay, visuals, narrationSegments },
        };
      }

      state.currentStep = 'production';
      state.updatedAt = Date.now();
      state.checkpoints = checkpoints.getAllCheckpoints();
      storyModeStore.set(sessionId, state);

      checkpoints.dispose();

      log.info(`YouTube Narrator pipeline complete: ${screenplay.length} scenes, ${visuals.length} visuals, ${narrationSegments.length} narrations`);

      return {
        success: true,
        partialResults: {
          sessionId,
          screenplay,
          visuals,
          narrationSegments,
          assemblyRules,
          research: researchResult,
          totalDuration,
        },
      };

    } catch (error) {
      checkpoints.dispose();
      const msg = error instanceof Error ? error.message : String(error);
      log.error('YouTube Narrator pipeline failed:', msg);
      return { success: false, error: msg };
    }
  }
}
````

## File: packages/shared/src/services/projectService.ts
````typescript
/**
 * Project Service
 *
 * CRUD operations for user projects stored in Firestore.
 * Projects are stored under /users/{userId}/projects/{projectId}
 * Export history is stored as a subcollection /users/{userId}/projects/{projectId}/exports
 */
import {
  doc,
  setDoc,
  getDoc,
  deleteDoc,
  collection,
  query,
  orderBy,
  limit,
  getDocs,
  serverTimestamp,
  Timestamp,
  addDoc,
  updateDoc,
  type DocumentData,
} from 'firebase/firestore';
import { getFirebaseDb, isFirebaseConfigured } from './firebase/config';
import { getCurrentUser } from './firebase/authService';

// ============================================================
// Types
// ============================================================

export type ProjectType = 'production' | 'story' | 'visualizer';
export type ProjectStatus = 'draft' | 'in_progress' | 'completed' | 'archived';

export interface Project {
  id: string;
  userId: string;
  title: string;
  description?: string;
  type: ProjectType;
  status: ProjectStatus;

  // Timestamps
  createdAt: Date;
  updatedAt: Date;
  lastAccessedAt?: Date;

  // Visual metadata
  thumbnailUrl?: string;
  duration?: number; // seconds

  // Project config
  style?: string;
  topic?: string;
  language?: string;

  // Progress tracking
  sceneCount?: number;
  hasVisuals?: boolean;
  hasNarration?: boolean;
  hasMusic?: boolean;
  hasExport?: boolean;

  // Cloud storage reference
  cloudSessionId: string;

  // Tags for filtering
  tags?: string[];
  isFavorite?: boolean;
}

export interface ExportRecord {
  id: string;
  projectId: string;
  format: 'mp4' | 'webm' | 'gif';
  quality: 'draft' | 'standard' | 'high' | 'ultra';
  aspectRatio: '16:9' | '9:16' | '1:1';
  cloudUrl?: string;
  localUrl?: string;
  fileSize?: number;
  duration?: number;
  createdAt: Date;
  settings?: Record<string, unknown>;
}

export interface CreateProjectInput {
  title: string;
  type: ProjectType;
  topic?: string;
  style?: string;
  description?: string;
}

export interface UpdateProjectInput {
  title?: string;
  description?: string;
  status?: ProjectStatus;
  thumbnailUrl?: string;
  duration?: number;
  style?: string;
  topic?: string;
  sceneCount?: number;
  hasVisuals?: boolean;
  hasNarration?: boolean;
  hasMusic?: boolean;
  hasExport?: boolean;
  tags?: string[];
  isFavorite?: boolean;
}

// ============================================================
// Helpers
// ============================================================

function generateProjectId(): string {
  const timestamp = Date.now();
  const random = Math.random().toString(36).substring(2, 10);
  return `proj_${timestamp}_${random}`;
}

function generateCloudSessionId(projectId: string): string {
  return `production_${projectId}`;
}

function timestampToDate(timestamp: unknown): Date {
  if (timestamp instanceof Timestamp) {
    return timestamp.toDate();
  }
  if (timestamp instanceof Date) {
    return timestamp;
  }
  return new Date();
}

function docToProject(data: DocumentData): Project {
  return {
    id: data.id,
    userId: data.userId,
    title: data.title || 'Untitled Project',
    description: data.description,
    type: data.type || 'production',
    status: data.status || 'draft',
    createdAt: timestampToDate(data.createdAt),
    updatedAt: timestampToDate(data.updatedAt),
    lastAccessedAt: data.lastAccessedAt ? timestampToDate(data.lastAccessedAt) : undefined,
    thumbnailUrl: data.thumbnailUrl,
    duration: data.duration,
    style: data.style,
    topic: data.topic,
    language: data.language,
    sceneCount: data.sceneCount,
    hasVisuals: data.hasVisuals,
    hasNarration: data.hasNarration,
    hasMusic: data.hasMusic,
    hasExport: data.hasExport,
    cloudSessionId: data.cloudSessionId,
    tags: data.tags || [],
    isFavorite: data.isFavorite || false,
  };
}

function docToExportRecord(data: DocumentData): ExportRecord {
  return {
    id: data.id,
    projectId: data.projectId,
    format: data.format || 'mp4',
    quality: data.quality || 'standard',
    aspectRatio: data.aspectRatio || '16:9',
    cloudUrl: data.cloudUrl,
    localUrl: data.localUrl,
    fileSize: data.fileSize,
    duration: data.duration,
    createdAt: timestampToDate(data.createdAt),
    settings: data.settings,
  };
}

// ============================================================
// Project CRUD Operations
// ============================================================

/**
 * Create a new project
 */
export async function createProject(input: CreateProjectInput): Promise<Project | null> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    console.warn('[ProjectService] Cannot create - no auth or Firebase not configured');
    return null;
  }

  try {
    const projectId = generateProjectId();
    const cloudSessionId = generateCloudSessionId(projectId);
    const now = serverTimestamp();

    const projectData = {
      id: projectId,
      userId: user.uid,
      title: input.title,
      description: input.description || '',
      type: input.type,
      status: 'draft' as ProjectStatus,
      topic: input.topic || '',
      style: input.style || '',
      createdAt: now,
      updatedAt: now,
      lastAccessedAt: now,
      cloudSessionId,
      sceneCount: 0,
      hasVisuals: false,
      hasNarration: false,
      hasMusic: false,
      hasExport: false,
      tags: [],
      isFavorite: false,
    };

    const docRef = doc(db, 'users', user.uid, 'projects', projectId);
    await setDoc(docRef, projectData);

    console.log(`[ProjectService] Created project ${projectId}`);

    // Return with proper dates
    return {
      ...projectData,
      createdAt: new Date(),
      updatedAt: new Date(),
      lastAccessedAt: new Date(),
    };
  } catch (error) {
    console.error('[ProjectService] Failed to create project:', error);
    return null;
  }
}

/**
 * Get a project by ID
 */
export async function getProject(projectId: string): Promise<Project | null> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return null;
  }

  try {
    const docRef = doc(db, 'users', user.uid, 'projects', projectId);
    const docSnap = await getDoc(docRef);

    if (!docSnap.exists()) {
      console.log(`[ProjectService] Project ${projectId} not found`);
      return null;
    }

    const project = docToProject(docSnap.data());

    // Verify ownership
    if (project.userId !== user.uid) {
      console.warn('[ProjectService] Project belongs to different user');
      return null;
    }

    console.log(`[ProjectService] Loaded project ${projectId}`);
    return project;
  } catch (error) {
    console.error('[ProjectService] Failed to get project:', error);
    return null;
  }
}

/**
 * Update a project
 */
export async function updateProject(
  projectId: string,
  updates: UpdateProjectInput
): Promise<boolean> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return false;
  }

  try {
    const docRef = doc(db, 'users', user.uid, 'projects', projectId);

    await updateDoc(docRef, {
      ...updates,
      updatedAt: serverTimestamp(),
    });

    console.log(`[ProjectService] Updated project ${projectId}`);
    return true;
  } catch (error) {
    console.error('[ProjectService] Failed to update project:', error);
    return false;
  }
}

/**
 * Delete a project
 */
export async function deleteProject(projectId: string): Promise<boolean> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return false;
  }

  try {
    // First verify ownership
    const docRef = doc(db, 'users', user.uid, 'projects', projectId);
    const docSnap = await getDoc(docRef);

    if (!docSnap.exists()) {
      console.log(`[ProjectService] Project ${projectId} not found`);
      return false;
    }

    const data = docSnap.data();
    if (data.userId !== user.uid) {
      console.warn('[ProjectService] Cannot delete - project belongs to different user');
      return false;
    }

    // Delete the project document
    // Note: Exports subcollection will remain orphaned but Firebase doesn't cascade deletes
    // For full cleanup, we would need to delete exports first or use Cloud Functions
    await deleteDoc(docRef);

    console.log(`[ProjectService] Deleted project ${projectId}`);
    return true;
  } catch (error) {
    console.error('[ProjectService] Failed to delete project:', error);
    return false;
  }
}

/**
 * List user's projects
 */
export async function listUserProjects(maxResults: number = 50): Promise<Project[]> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return [];
  }

  try {
    const projectsRef = collection(db, 'users', user.uid, 'projects');
    const q = query(
      projectsRef,
      orderBy('updatedAt', 'desc'),
      limit(maxResults)
    );

    const snapshot = await getDocs(q);
    const projects: Project[] = [];

    snapshot.forEach((docSnapshot) => {
      projects.push(docToProject(docSnapshot.data()));
    });

    console.log(`[ProjectService] Listed ${projects.length} projects`);
    return projects;
  } catch (error) {
    console.error('[ProjectService] Failed to list projects:', error);
    return [];
  }
}

/**
 * Mark project as accessed (updates lastAccessedAt)
 */
export async function markProjectAccessed(projectId: string): Promise<void> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) return;

  try {
    const docRef = doc(db, 'users', user.uid, 'projects', projectId);
    await updateDoc(docRef, {
      lastAccessedAt: serverTimestamp(),
    });
  } catch (error) {
    // Non-critical, just log
    console.warn('[ProjectService] Failed to mark accessed:', error);
  }
}

/**
 * Toggle project favorite status
 */
export async function toggleFavorite(projectId: string): Promise<boolean> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return false;
  }

  try {
    const docRef = doc(db, 'users', user.uid, 'projects', projectId);
    const docSnap = await getDoc(docRef);

    if (!docSnap.exists()) {
      return false;
    }

    const currentFavorite = docSnap.data().isFavorite || false;

    await updateDoc(docRef, {
      isFavorite: !currentFavorite,
      updatedAt: serverTimestamp(),
    });

    return true;
  } catch (error) {
    console.error('[ProjectService] Failed to toggle favorite:', error);
    return false;
  }
}

// ============================================================
// Export History Operations
// ============================================================

/**
 * Save an export record
 */
export async function saveExportRecord(
  projectId: string,
  exportData: Omit<ExportRecord, 'id' | 'projectId' | 'createdAt'>
): Promise<ExportRecord | null> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return null;
  }

  try {
    const exportsRef = collection(db, 'users', user.uid, 'projects', projectId, 'exports');

    const exportDoc = {
      ...exportData,
      projectId,
      createdAt: serverTimestamp(),
    };

    const docRef = await addDoc(exportsRef, exportDoc);

    // Also update project to mark hasExport
    await updateProject(projectId, { hasExport: true });

    console.log(`[ProjectService] Saved export record ${docRef.id}`);

    return {
      id: docRef.id,
      projectId,
      ...exportData,
      createdAt: new Date(),
    };
  } catch (error) {
    console.error('[ProjectService] Failed to save export record:', error);
    return null;
  }
}

/**
 * Get export history for a project
 */
export async function getExportHistory(
  projectId: string,
  maxResults: number = 20
): Promise<ExportRecord[]> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return [];
  }

  try {
    const exportsRef = collection(db, 'users', user.uid, 'projects', projectId, 'exports');
    const q = query(
      exportsRef,
      orderBy('createdAt', 'desc'),
      limit(maxResults)
    );

    const snapshot = await getDocs(q);
    const exports: ExportRecord[] = [];

    snapshot.forEach((docSnapshot) => {
      const data = docSnapshot.data();
      exports.push(docToExportRecord({ ...data, id: docSnapshot.id }));
    });

    return exports;
  } catch (error) {
    console.error('[ProjectService] Failed to get export history:', error);
    return [];
  }
}

// ============================================================
// Utility Functions
// ============================================================

/**
 * Check if project service is available
 */
export function isProjectServiceAvailable(): boolean {
  return isFirebaseConfigured() && getCurrentUser() !== null;
}

/**
 * Get project count for current user
 */
export async function getProjectCount(): Promise<number> {
  const projects = await listUserProjects(1000);
  return projects.length;
}

/**
 * Search projects by title
 */
export async function searchProjects(searchTerm: string): Promise<Project[]> {
  // Firestore doesn't support full-text search natively
  // So we fetch all and filter client-side (acceptable for personal projects)
  const projects = await listUserProjects(100);

  const term = searchTerm.toLowerCase();
  return projects.filter(
    (p) =>
      p.title.toLowerCase().includes(term) ||
      p.topic?.toLowerCase().includes(term) ||
      p.description?.toLowerCase().includes(term)
  );
}

/**
 * Get recent projects (last 5 accessed)
 */
export async function getRecentProjects(): Promise<Project[]> {
  const db = getFirebaseDb();
  const user = getCurrentUser();

  if (!db || !user) {
    return [];
  }

  try {
    const projectsRef = collection(db, 'users', user.uid, 'projects');
    const q = query(
      projectsRef,
      orderBy('lastAccessedAt', 'desc'),
      limit(5)
    );

    const snapshot = await getDocs(q);
    const projects: Project[] = [];

    snapshot.forEach((docSnapshot) => {
      projects.push(docToProject(docSnapshot.data()));
    });

    return projects;
  } catch (error) {
    console.error('[ProjectService] Failed to get recent projects:', error);
    return [];
  }
}

/**
 * Get favorite projects
 */
export async function getFavoriteProjects(): Promise<Project[]> {
  const projects = await listUserProjects(100);
  return projects.filter((p) => p.isFavorite);
}
````

## File: packages/shared/src/services/projectTemplatesService.ts
````typescript
/**
 * Project Templates Service
 * 
 * Provides pre-built story templates with genre-specific starting points
 * and quick-start wizards for common video production workflows.
 */

import type { StoryState, ScreenplayScene } from '@/types';

export interface TemplateScene {
  sceneNumber: number;
  heading: string;
  action: string;
  setting?: string;
  emotionalBeat?: string;
  duration?: number;
}

export interface ProjectTemplate {
  id: string;
  name: string;
  description: string;
  genre: string;
  category: 'narrative' | 'commercial' | 'educational' | 'social' | 'experimental';
  thumbnail?: string;
  estimatedDuration: string;
  difficulty: 'beginner' | 'intermediate' | 'advanced';
  tags: string[];
  visualStyle: string;
  aspectRatio: string;
  templateScenes: TemplateScene[];
  suggestedVisualStyles: string[];
  suggestedAspectRatios: string[];
}

export interface QuickStartWizard {
  id: string;
  name: string;
  description: string;
  steps: WizardStep[];
  resultTemplate: string;
}

export interface WizardStep {
  id: string;
  title: string;
  description: string;
  type: 'text' | 'select' | 'multiselect' | 'number' | 'textarea';
  field: string;
  options?: { value: string; label: string }[];
  placeholder?: string;
  required?: boolean;
  validation?: {
    min?: number;
    max?: number;
    minLength?: number;
    maxLength?: number;
  };
}

function generateId(): string {
  return `scene_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;
}

function convertTemplateToScreenplayScenes(scenes: TemplateScene[]): ScreenplayScene[] {
  return scenes.map(scene => ({
    id: generateId(),
    sceneNumber: scene.sceneNumber,
    heading: scene.heading,
    action: scene.action,
    dialogue: [],
    charactersPresent: [],
  }));
}

const TEMPLATES: ProjectTemplate[] = [
  // Narrative Templates
  {
    id: 'short-film-drama',
    name: 'Short Film - Drama',
    description: 'A compelling 3-5 minute dramatic short with emotional arc, character development, and cinematic visuals.',
    genre: 'Drama',
    category: 'narrative',
    estimatedDuration: '3-5 minutes',
    difficulty: 'intermediate',
    tags: ['drama', 'emotional', 'character-driven', 'cinematic'],
    visualStyle: 'Cinematic',
    aspectRatio: '16:9',
    suggestedVisualStyles: ['Cinematic', 'Film Noir', 'Golden Hour', 'Moody'],
    suggestedAspectRatios: ['16:9', '2.39:1'],
    templateScenes: [
      { sceneNumber: 1, heading: 'INT. LOCATION - DAY', action: 'Opening scene establishing the protagonist and their world.', emotionalBeat: 'Introduction', duration: 45 },
      { sceneNumber: 2, heading: 'EXT. LOCATION - DAY', action: 'The inciting incident that disrupts the protagonist\'s life.', emotionalBeat: 'Conflict', duration: 60 },
      { sceneNumber: 3, heading: 'INT. LOCATION - NIGHT', action: 'Rising tension and emotional confrontation.', emotionalBeat: 'Climax', duration: 90 },
      { sceneNumber: 4, heading: 'EXT. LOCATION - DAY', action: 'Resolution and character transformation.', emotionalBeat: 'Resolution', duration: 45 },
    ],
  },
  {
    id: 'horror-short',
    name: 'Horror Short',
    description: 'A tense 2-3 minute horror piece with atmospheric tension, jump scares, and unsettling imagery.',
    genre: 'Horror',
    category: 'narrative',
    estimatedDuration: '2-3 minutes',
    difficulty: 'intermediate',
    tags: ['horror', 'suspense', 'atmospheric', 'thriller'],
    visualStyle: 'Dark Cinematic',
    aspectRatio: '16:9',
    suggestedVisualStyles: ['Dark Cinematic', 'Desaturated', 'High Contrast', 'Gothic'],
    suggestedAspectRatios: ['16:9', '2.39:1'],
    templateScenes: [
      { sceneNumber: 1, heading: 'INT. DARK LOCATION - NIGHT', action: 'Establishing an unsettling atmosphere. Something feels wrong.', emotionalBeat: 'Unease', duration: 30 },
      { sceneNumber: 2, heading: 'INT. DARK LOCATION - NIGHT', action: 'Strange occurrences begin. Tension builds.', emotionalBeat: 'Dread', duration: 45 },
      { sceneNumber: 3, heading: 'INT. DARK LOCATION - NIGHT', action: 'The horror reveals itself. Peak terror moment.', emotionalBeat: 'Terror', duration: 60 },
      { sceneNumber: 4, heading: 'INT/EXT. LOCATION - NIGHT', action: 'Ambiguous ending leaving audience unsettled.', emotionalBeat: 'Lingering dread', duration: 30 },
    ],
  },
  {
    id: 'comedy-sketch',
    name: 'Comedy Sketch',
    description: 'A punchy 1-2 minute comedy sketch with setup, escalation, and punchline.',
    genre: 'Comedy',
    category: 'narrative',
    estimatedDuration: '1-2 minutes',
    difficulty: 'beginner',
    tags: ['comedy', 'humor', 'sketch', 'funny'],
    visualStyle: 'Bright',
    aspectRatio: '16:9',
    suggestedVisualStyles: ['Bright', 'Colorful', 'Sitcom', 'Casual'],
    suggestedAspectRatios: ['16:9', '9:16'],
    templateScenes: [
      { sceneNumber: 1, heading: 'INT. EVERYDAY LOCATION - DAY', action: 'Setup: Establish the normal situation and characters.', emotionalBeat: 'Setup', duration: 20 },
      { sceneNumber: 2, heading: 'INT. SAME LOCATION - DAY', action: 'Complication: Something absurd happens or is revealed.', emotionalBeat: 'Escalation', duration: 30 },
      { sceneNumber: 3, heading: 'INT. SAME LOCATION - DAY', action: 'Punchline: The comedic payoff that lands the joke.', emotionalBeat: 'Punchline', duration: 20 },
    ],
  },
  {
    id: 'sci-fi-concept',
    name: 'Sci-Fi Concept',
    description: 'A visually stunning 2-4 minute science fiction piece exploring futuristic themes.',
    genre: 'Science Fiction',
    category: 'narrative',
    estimatedDuration: '2-4 minutes',
    difficulty: 'advanced',
    tags: ['sci-fi', 'futuristic', 'technology', 'visionary'],
    visualStyle: 'Cyberpunk',
    aspectRatio: '2.39:1',
    suggestedVisualStyles: ['Cyberpunk', 'Clean Futuristic', 'Neon Noir', 'Blade Runner'],
    suggestedAspectRatios: ['2.39:1', '16:9'],
    templateScenes: [
      { sceneNumber: 1, heading: 'EXT. FUTURISTIC CITY - NIGHT', action: 'Establishing the world and its advanced technology.', emotionalBeat: 'Wonder', duration: 40 },
      { sceneNumber: 2, heading: 'INT. HIGH-TECH FACILITY - DAY', action: 'Introduction of the central concept or conflict.', emotionalBeat: 'Discovery', duration: 50 },
      { sceneNumber: 3, heading: 'EXT/INT. VARIOUS - DAY/NIGHT', action: 'The implications of the technology unfold.', emotionalBeat: 'Consequence', duration: 60 },
      { sceneNumber: 4, heading: 'EXT. FUTURISTIC VISTA - DAWN', action: 'Philosophical conclusion about humanity and technology.', emotionalBeat: 'Reflection', duration: 40 },
    ],
  },
  {
    id: 'fantasy-adventure',
    name: 'Fantasy Adventure',
    description: 'An epic 3-5 minute fantasy journey with magical elements and heroic moments.',
    genre: 'Fantasy',
    category: 'narrative',
    estimatedDuration: '3-5 minutes',
    difficulty: 'advanced',
    tags: ['fantasy', 'adventure', 'magic', 'epic'],
    visualStyle: 'Epic Fantasy',
    aspectRatio: '2.39:1',
    suggestedVisualStyles: ['Epic Fantasy', 'Painterly', 'Magical Realism', 'Lord of the Rings'],
    suggestedAspectRatios: ['2.39:1', '16:9'],
    templateScenes: [
      { sceneNumber: 1, heading: 'EXT. MAGICAL REALM - DAY', action: 'Establishing the magical world and its beauty.', emotionalBeat: 'Wonder', duration: 45 },
      { sceneNumber: 2, heading: 'INT. ANCIENT STRUCTURE - DAY', action: 'The hero receives their quest or calling.', emotionalBeat: 'Call to adventure', duration: 50 },
      { sceneNumber: 3, heading: 'EXT. PERILOUS TERRAIN - DAY', action: 'The hero faces trials and demonstrates courage.', emotionalBeat: 'Challenge', duration: 80 },
      { sceneNumber: 4, heading: 'EXT. TRIUMPHANT LOCATION - SUNSET', action: 'Victory and transformation of the hero.', emotionalBeat: 'Triumph', duration: 50 },
    ],
  },

  // Commercial Templates
  {
    id: 'product-showcase',
    name: 'Product Showcase',
    description: 'A sleek 30-60 second product video highlighting features and benefits.',
    genre: 'Commercial',
    category: 'commercial',
    estimatedDuration: '30-60 seconds',
    difficulty: 'beginner',
    tags: ['product', 'commercial', 'marketing', 'showcase'],
    visualStyle: 'Clean Minimal',
    aspectRatio: '16:9',
    suggestedVisualStyles: ['Clean Minimal', 'Premium', 'Tech Modern', 'Lifestyle'],
    suggestedAspectRatios: ['16:9', '1:1', '9:16'],
    templateScenes: [
      { sceneNumber: 1, heading: 'PRODUCT REVEAL', action: 'Dramatic reveal of the product from darkness to light.', emotionalBeat: 'Intrigue', duration: 10 },
      { sceneNumber: 2, heading: 'FEATURE HIGHLIGHTS', action: 'Showcase key features with dynamic camera movements.', emotionalBeat: 'Desire', duration: 20 },
      { sceneNumber: 3, heading: 'IN USE', action: 'Product in real-world use showing benefits.', emotionalBeat: 'Connection', duration: 15 },
      { sceneNumber: 4, heading: 'CALL TO ACTION', action: 'Final product shot with branding and CTA.', emotionalBeat: 'Action', duration: 10 },
    ],
  },
  {
    id: 'brand-story',
    name: 'Brand Story',
    description: 'An emotional 1-2 minute brand video connecting with audience values.',
    genre: 'Commercial',
    category: 'commercial',
    estimatedDuration: '1-2 minutes',
    difficulty: 'intermediate',
    tags: ['brand', 'story', 'emotional', 'values'],
    visualStyle: 'Cinematic',
    aspectRatio: '16:9',
    suggestedVisualStyles: ['Cinematic', 'Documentary', 'Warm', 'Authentic'],
    suggestedAspectRatios: ['16:9', '2.39:1'],
    templateScenes: [
      { sceneNumber: 1, heading: 'THE PROBLEM', action: 'Showing the challenge or need the audience faces.', emotionalBeat: 'Recognition', duration: 20 },
      { sceneNumber: 2, heading: 'THE JOURNEY', action: 'The brand\'s story and passion for solving the problem.', emotionalBeat: 'Trust', duration: 30 },
      { sceneNumber: 3, heading: 'THE SOLUTION', action: 'How the brand delivers value and changes lives.', emotionalBeat: 'Hope', duration: 30 },
      { sceneNumber: 4, heading: 'THE VISION', action: 'The future the brand is building with its customers.', emotionalBeat: 'Inspiration', duration: 20 },
    ],
  },

  // Social Media Templates
  {
    id: 'tiktok-reel',
    name: 'TikTok/Reels Video',
    description: 'A fast-paced 15-60 second vertical video optimized for social engagement.',
    genre: 'Social',
    category: 'social',
    estimatedDuration: '15-60 seconds',
    difficulty: 'beginner',
    tags: ['tiktok', 'reels', 'social', 'viral', 'short-form'],
    visualStyle: 'Trendy',
    aspectRatio: '9:16',
    suggestedVisualStyles: ['Trendy', 'Bold', 'High Energy', 'Aesthetic'],
    suggestedAspectRatios: ['9:16'],
    templateScenes: [
      { sceneNumber: 1, heading: 'HOOK', action: 'Grab attention in the first 3 seconds.', emotionalBeat: 'Curiosity', duration: 5 },
      { sceneNumber: 2, heading: 'CONTENT', action: 'Deliver the main message or entertainment.', emotionalBeat: 'Engagement', duration: 15 },
      { sceneNumber: 3, heading: 'PAYOFF', action: 'Satisfying ending that encourages sharing.', emotionalBeat: 'Satisfaction', duration: 10 },
    ],
  },

  // Educational Templates
  {
    id: 'explainer-video',
    name: 'Explainer Video',
    description: 'A clear 2-3 minute educational video breaking down complex topics.',
    genre: 'Educational',
    category: 'educational',
    estimatedDuration: '2-3 minutes',
    difficulty: 'intermediate',
    tags: ['educational', 'explainer', 'tutorial', 'informative'],
    visualStyle: 'Clean',
    aspectRatio: '16:9',
    suggestedVisualStyles: ['Clean', 'Illustrated', 'Infographic', 'Modern'],
    suggestedAspectRatios: ['16:9'],
    templateScenes: [
      { sceneNumber: 1, heading: 'INTRODUCTION', action: 'Introduce the topic and why it matters.', emotionalBeat: 'Interest', duration: 20 },
      { sceneNumber: 2, heading: 'THE PROBLEM', action: 'Explain the challenge or question being addressed.', emotionalBeat: 'Understanding', duration: 30 },
      { sceneNumber: 3, heading: 'THE EXPLANATION', action: 'Break down the concept step by step.', emotionalBeat: 'Clarity', duration: 60 },
      { sceneNumber: 4, heading: 'KEY TAKEAWAYS', action: 'Recap the main points and next steps.', emotionalBeat: 'Confidence', duration: 30 },
    ],
  },

  // Experimental Templates
  {
    id: 'music-video-concept',
    name: 'Music Video Concept',
    description: 'An artistic 3-4 minute visual narrative designed to accompany music.',
    genre: 'Music Video',
    category: 'experimental',
    estimatedDuration: '3-4 minutes',
    difficulty: 'advanced',
    tags: ['music', 'artistic', 'visual', 'experimental'],
    visualStyle: 'Artistic',
    aspectRatio: '16:9',
    suggestedVisualStyles: ['Artistic', 'Abstract', 'Neon', 'Dreamlike'],
    suggestedAspectRatios: ['16:9', '2.39:1', '1:1'],
    templateScenes: [
      { sceneNumber: 1, heading: 'INTRO/VERSE 1', action: 'Set the mood and introduce visual themes.', emotionalBeat: 'Atmosphere', duration: 45 },
      { sceneNumber: 2, heading: 'CHORUS 1', action: 'Visual explosion matching musical peak.', emotionalBeat: 'Energy', duration: 30 },
      { sceneNumber: 3, heading: 'VERSE 2/BRIDGE', action: 'Development of visual narrative and themes.', emotionalBeat: 'Evolution', duration: 60 },
      { sceneNumber: 4, heading: 'FINAL CHORUS/OUTRO', action: 'Peak visual moment and satisfying conclusion.', emotionalBeat: 'Climax', duration: 45 },
    ],
  },
  {
    id: 'documentary-mini',
    name: 'Mini Documentary',
    description: 'A thoughtful 3-5 minute documentary piece exploring a subject in depth.',
    genre: 'Documentary',
    category: 'experimental',
    estimatedDuration: '3-5 minutes',
    difficulty: 'advanced',
    tags: ['documentary', 'real', 'interview', 'investigative'],
    visualStyle: 'Documentary',
    aspectRatio: '16:9',
    suggestedVisualStyles: ['Documentary', 'Raw', 'Journalistic', 'Authentic'],
    suggestedAspectRatios: ['16:9', '2.39:1'],
    templateScenes: [
      { sceneNumber: 1, heading: 'COLD OPEN', action: 'Hook the viewer with intriguing footage or statement.', emotionalBeat: 'Intrigue', duration: 30 },
      { sceneNumber: 2, heading: 'CONTEXT', action: 'Provide background and context for the subject.', emotionalBeat: 'Understanding', duration: 60 },
      { sceneNumber: 3, heading: 'DEEP DIVE', action: 'Explore the subject through multiple perspectives.', emotionalBeat: 'Depth', duration: 120 },
      { sceneNumber: 4, heading: 'CONCLUSION', action: 'Leave the viewer with something to think about.', emotionalBeat: 'Reflection', duration: 40 },
    ],
  },
];

const QUICK_START_WIZARDS: QuickStartWizard[] = [
  {
    id: 'story-from-idea',
    name: 'Story from Idea',
    description: 'Turn your idea into a complete storyboard in minutes.',
    steps: [
      {
        id: 'idea',
        title: 'Your Story Idea',
        description: 'Describe your story in a few sentences.',
        type: 'textarea',
        field: 'idea',
        placeholder: 'A young inventor discovers a way to communicate with plants...',
        required: true,
        validation: { minLength: 20, maxLength: 500 },
      },
      {
        id: 'genre',
        title: 'Genre',
        description: 'What genre best fits your story?',
        type: 'select',
        field: 'genre',
        required: true,
        options: [
          { value: 'Drama', label: 'Drama' },
          { value: 'Comedy', label: 'Comedy' },
          { value: 'Horror', label: 'Horror' },
          { value: 'Science Fiction', label: 'Science Fiction' },
          { value: 'Fantasy', label: 'Fantasy' },
          { value: 'Thriller', label: 'Thriller' },
          { value: 'Romance', label: 'Romance' },
          { value: 'Documentary', label: 'Documentary' },
        ],
      },
      {
        id: 'duration',
        title: 'Target Duration',
        description: 'How long should the final video be?',
        type: 'select',
        field: 'duration',
        required: true,
        options: [
          { value: '30', label: '30 seconds' },
          { value: '60', label: '1 minute' },
          { value: '120', label: '2 minutes' },
          { value: '180', label: '3 minutes' },
          { value: '300', label: '5 minutes' },
        ],
      },
      {
        id: 'style',
        title: 'Visual Style',
        description: 'Choose the look and feel.',
        type: 'select',
        field: 'visualStyle',
        required: true,
        options: [
          { value: 'Cinematic', label: 'Cinematic' },
          { value: 'Anime', label: 'Anime' },
          { value: 'Photorealistic', label: 'Photorealistic' },
          { value: 'Illustrated', label: 'Illustrated' },
          { value: 'Painterly', label: 'Painterly' },
          { value: 'Minimalist', label: 'Minimalist' },
        ],
      },
    ],
    resultTemplate: 'short-film-drama',
  },
  {
    id: 'product-video',
    name: 'Product Video',
    description: 'Create a professional product showcase video.',
    steps: [
      {
        id: 'product',
        title: 'Product Name',
        description: 'What product are you showcasing?',
        type: 'text',
        field: 'productName',
        placeholder: 'UltraWidget Pro',
        required: true,
      },
      {
        id: 'features',
        title: 'Key Features',
        description: 'List the main features to highlight (one per line).',
        type: 'textarea',
        field: 'features',
        placeholder: 'Fast charging\nWater resistant\nCompact design',
        required: true,
      },
      {
        id: 'audience',
        title: 'Target Audience',
        description: 'Who is this video for?',
        type: 'text',
        field: 'audience',
        placeholder: 'Tech-savvy millennials',
        required: true,
      },
      {
        id: 'platform',
        title: 'Platform',
        description: 'Where will this video be published?',
        type: 'select',
        field: 'platform',
        required: true,
        options: [
          { value: 'youtube', label: 'YouTube (16:9)' },
          { value: 'instagram', label: 'Instagram Feed (1:1)' },
          { value: 'tiktok', label: 'TikTok/Reels (9:16)' },
          { value: 'website', label: 'Website (16:9)' },
        ],
      },
    ],
    resultTemplate: 'product-showcase',
  },
  {
    id: 'social-content',
    name: 'Social Media Content',
    description: 'Create engaging short-form content for social platforms.',
    steps: [
      {
        id: 'topic',
        title: 'Content Topic',
        description: 'What is your video about?',
        type: 'text',
        field: 'topic',
        placeholder: '5 productivity tips for remote workers',
        required: true,
      },
      {
        id: 'platform',
        title: 'Platform',
        description: 'Which platform is this for?',
        type: 'select',
        field: 'platform',
        required: true,
        options: [
          { value: 'tiktok', label: 'TikTok' },
          { value: 'instagram-reels', label: 'Instagram Reels' },
          { value: 'youtube-shorts', label: 'YouTube Shorts' },
        ],
      },
      {
        id: 'tone',
        title: 'Tone',
        description: 'What vibe should the video have?',
        type: 'select',
        field: 'tone',
        required: true,
        options: [
          { value: 'funny', label: 'Funny/Humorous' },
          { value: 'informative', label: 'Informative/Educational' },
          { value: 'inspiring', label: 'Inspiring/Motivational' },
          { value: 'aesthetic', label: 'Aesthetic/Calming' },
          { value: 'dramatic', label: 'Dramatic/Intense' },
        ],
      },
    ],
    resultTemplate: 'tiktok-reel',
  },
];

export function getAllTemplates(): ProjectTemplate[] {
  return TEMPLATES;
}

export function getTemplateById(id: string): ProjectTemplate | undefined {
  return TEMPLATES.find(t => t.id === id);
}

export function getTemplatesByCategory(category: ProjectTemplate['category']): ProjectTemplate[] {
  return TEMPLATES.filter(t => t.category === category);
}

export function getTemplatesByGenre(genre: string): ProjectTemplate[] {
  return TEMPLATES.filter(t => t.genre.toLowerCase() === genre.toLowerCase());
}

export function getTemplatesByDifficulty(difficulty: ProjectTemplate['difficulty']): ProjectTemplate[] {
  return TEMPLATES.filter(t => t.difficulty === difficulty);
}

export function searchTemplates(query: string): ProjectTemplate[] {
  const lowerQuery = query.toLowerCase();
  return TEMPLATES.filter(t => 
    t.name.toLowerCase().includes(lowerQuery) ||
    t.description.toLowerCase().includes(lowerQuery) ||
    t.genre.toLowerCase().includes(lowerQuery) ||
    t.tags.some(tag => tag.toLowerCase().includes(lowerQuery))
  );
}

export function getAllWizards(): QuickStartWizard[] {
  return QUICK_START_WIZARDS;
}

export function getWizardById(id: string): QuickStartWizard | undefined {
  return QUICK_START_WIZARDS.find(w => w.id === id);
}

export function applyTemplate(template: ProjectTemplate): Partial<StoryState> {
  return {
    currentStep: 'breakdown',
    genre: template.genre,
    visualStyle: template.visualStyle,
    aspectRatio: template.aspectRatio,
    breakdown: convertTemplateToScreenplayScenes(template.templateScenes),
  };
}

export function getTemplateCategories(): Array<{
  id: ProjectTemplate['category'];
  name: string;
  description: string;
  icon: string;
}> {
  return [
    { id: 'narrative', name: 'Narrative', description: 'Story-driven films and shorts', icon: '🎬' },
    { id: 'commercial', name: 'Commercial', description: 'Product and brand videos', icon: '💼' },
    { id: 'educational', name: 'Educational', description: 'Explainers and tutorials', icon: '📚' },
    { id: 'social', name: 'Social Media', description: 'Short-form content', icon: '📱' },
    { id: 'experimental', name: 'Experimental', description: 'Art and music videos', icon: '🎨' },
  ];
}
````

## File: packages/shared/src/services/prompt/imageStyleGuide.ts
````typescript
/**
 * Image Style Guide - Structured JSON prompts for image generation
 *
 * Replaces ad-hoc string concatenation with a typed JSON object that gets
 * serialized as the prompt text sent to image models (Imagen, DeAPI Flux, Gemini).
 *
 * Inspired by: https://dev.to/worldlinetech/json-style-guides-for-controlled-image-generation
 */

import { getStyleEnhancement } from "./styleEnhancements";
import { type Persona } from "./personaData";
import {
  IMAGE_STYLE_MODIFIERS,
  DEFAULT_NEGATIVE_CONSTRAINTS,
} from "../../constants";

// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------

export interface ImageStyleGuideSubject {
  type: string; // "person" | "object" | "animal" | "environment"
  description: string;
  position?: string; // "center" | "left-third" | "foreground" etc.
  pose?: string;
  expression?: string;
  interaction?: string;
}

export interface ImageStyleGuide {
  scene: string;
  subjects: ImageStyleGuideSubject[];
  style: {
    preset: string;
    keywords: string[];
    medium: string;
  };
  color_palette: string[];
  lighting: {
    source: string;
    quality: string;
    direction?: string;
  };
  mood: string;
  background: string;
  composition: {
    shot_type: string;
    camera_angle: string;
    framing?: string;
  };
  camera: {
    lens?: string;
    depth_of_field?: string;
    focus?: string;
  };
  textures?: string[];
  effects?: string[];
  avoid: string[];
}

// ---------------------------------------------------------------------------
// Per-style default tables
// ---------------------------------------------------------------------------

interface StyleDefaults {
  lighting: ImageStyleGuide["lighting"];
  color_palette: string[];
  textures: string[];
  effects: string[];
  camera: ImageStyleGuide["camera"];
  composition: ImageStyleGuide["composition"];
  mood: string;
}

const STYLE_DEFAULTS: Record<string, StyleDefaults> = {
  cinematic: {
    lighting: { source: "golden hour", quality: "soft diffused", direction: "backlit" },
    color_palette: ["warm amber", "deep shadow", "desaturated teal"],
    textures: ["35mm film grain"],
    effects: ["anamorphic lens flare", "bokeh"],
    camera: { lens: "35mm", depth_of_field: "shallow", focus: "subject" },
    composition: { shot_type: "medium shot", camera_angle: "eye-level", framing: "rule of thirds" },
    mood: "dramatic cinematic",
  },
  "anime / manga": {
    lighting: { source: "soft ambient", quality: "cel-shaded flat", direction: "front-lit" },
    color_palette: ["vibrant saturated", "pastel highlights", "deep ink shadows"],
    textures: ["clean linework", "screentone shading"],
    effects: ["speed lines", "sparkle effects"],
    camera: { lens: "50mm", depth_of_field: "deep", focus: "character" },
    composition: { shot_type: "medium shot", camera_angle: "slightly low angle", framing: "dynamic diagonal" },
    mood: "expressive anime",
  },
  cyberpunk: {
    lighting: { source: "neon tube lighting", quality: "harsh directional", direction: "side-lit" },
    color_palette: ["teal", "magenta", "electric blue", "deep black"],
    textures: ["rain-slicked asphalt", "holographic sheen"],
    effects: ["chromatic aberration", "glitch artifacts", "lens flare"],
    camera: { lens: "35mm", depth_of_field: "selective", focus: "subject" },
    composition: { shot_type: "wide shot", camera_angle: "low angle", framing: "asymmetric" },
    mood: "dystopian neon",
  },
  watercolor: {
    lighting: { source: "natural daylight", quality: "soft diffused" },
    color_palette: ["transparent washes", "bleeding edges", "raw paper white"],
    textures: ["cold-pressed paper", "pigment granulation"],
    effects: ["water bloom", "wet-on-wet bleed"],
    camera: { depth_of_field: "deep" },
    composition: { shot_type: "medium shot", camera_angle: "eye-level", framing: "organic" },
    mood: "dreamy ethereal",
  },
  "oil painting": {
    lighting: { source: "studio north light", quality: "warm directional", direction: "side-lit" },
    color_palette: ["rich saturated pigments", "warm earthy tones", "deep shadows"],
    textures: ["canvas weave", "impasto knife texture", "thick paint ridges"],
    effects: ["linseed oil sheen", "glazed translucent layers"],
    camera: { depth_of_field: "deep" },
    composition: { shot_type: "portrait", camera_angle: "eye-level", framing: "classical" },
    mood: "rich classical",
  },
  "pixel art": {
    lighting: { source: "flat ambient", quality: "uniform" },
    color_palette: ["limited 16-color palette", "vibrant primaries"],
    textures: ["dithering patterns", "aliased hard edges"],
    effects: ["scanline overlay", "CRT curvature"],
    camera: { depth_of_field: "deep" },
    composition: { shot_type: "wide shot", camera_angle: "isometric", framing: "tile-based" },
    mood: "nostalgic retro",
  },
  surrealist: {
    lighting: { source: "impossible light sources", quality: "dreamlike diffused" },
    color_palette: ["muted earth tones", "unexpected accent colors"],
    textures: ["smooth hyper-real surfaces"],
    effects: ["melting distortion", "floating objects"],
    camera: { lens: "wide angle", depth_of_field: "deep" },
    composition: { shot_type: "wide shot", camera_angle: "eye-level", framing: "center framing" },
    mood: "uncanny dreamlike",
  },
  "dark fantasy": {
    lighting: { source: "flickering torchlight", quality: "volumetric", direction: "backlit" },
    color_palette: ["deep crimson", "charcoal black", "sickly green"],
    textures: ["weathered stone", "rusted metal"],
    effects: ["volumetric fog", "blood moon glow"],
    camera: { lens: "35mm", depth_of_field: "shallow", focus: "subject" },
    composition: { shot_type: "medium shot", camera_angle: "low angle", framing: "asymmetric" },
    mood: "ominous gothic",
  },
  "commercial / ad": {
    lighting: { source: "studio softbox", quality: "high-key professional", direction: "front-lit" },
    color_palette: ["clean whites", "brand accent colors"],
    textures: ["smooth pristine surfaces"],
    effects: ["subtle rim light"],
    camera: { lens: "85mm", depth_of_field: "shallow", focus: "product" },
    composition: { shot_type: "hero shot", camera_angle: "eye-level", framing: "center framing" },
    mood: "aspirational clean",
  },
  "minimalist / tutorial": {
    lighting: { source: "flat ambient", quality: "uniform even" },
    color_palette: ["limited palette", "clean whites", "accent color"],
    textures: ["flat vector", "clean geometric"],
    effects: [],
    camera: { depth_of_field: "deep" },
    composition: { shot_type: "wide shot", camera_angle: "isometric", framing: "center framing" },
    mood: "clear educational",
  },
  "comic book": {
    lighting: { source: "dramatic spot light", quality: "harsh directional" },
    color_palette: ["vibrant primaries", "bold black outlines"],
    textures: ["halftone dots", "Ben-Day dots"],
    effects: ["Kirby crackle energy", "dynamic action lines"],
    camera: { depth_of_field: "deep" },
    composition: { shot_type: "dynamic angle", camera_angle: "low angle", framing: "dramatic foreshortening" },
    mood: "energetic heroic",
  },
  "corporate / brand": {
    lighting: { source: "soft ambient", quality: "even professional" },
    color_palette: ["professional blue", "clean white", "light gray"],
    textures: ["flat vector", "clean geometric shapes"],
    effects: ["abstract blob backgrounds"],
    camera: { depth_of_field: "deep" },
    composition: { shot_type: "medium shot", camera_angle: "eye-level", framing: "balanced" },
    mood: "trustworthy professional",
  },
  photorealistic: {
    lighting: { source: "natural ambient", quality: "soft diffused" },
    color_palette: ["natural color temperature", "true-to-life tones"],
    textures: ["realistic skin texture", "natural material surfaces"],
    effects: ["shallow depth bokeh"],
    camera: { lens: "50mm", depth_of_field: "shallow", focus: "subject" },
    composition: { shot_type: "medium shot", camera_angle: "eye-level", framing: "rule of thirds" },
    mood: "authentic documentary",
  },
  // Special preset for character reference sheets
  character_sheet: {
    lighting: { source: "studio softbox", quality: "soft diffused", direction: "rim light accent" },
    color_palette: ["neutral tones", "clean whites"],
    textures: [],
    effects: [],
    camera: { lens: "85mm", depth_of_field: "shallow", focus: "subject" },
    composition: { shot_type: "medium shot", camera_angle: "eye-level", framing: "center framing" },
    mood: "neutral professional",
  },
};

/** Fallback defaults when a style is not found in the table. */
const FALLBACK_DEFAULTS: StyleDefaults = STYLE_DEFAULTS["cinematic"]!;

// ---------------------------------------------------------------------------
// Builder
// ---------------------------------------------------------------------------

export interface BuildImageStyleGuideParams {
  /** Scene/action description. Falls back to promptText if not provided. */
  scene?: string;
  /** Pre-built subjects array. If not provided, derived from globalSubject. */
  subjects?: ImageStyleGuideSubject[];
  /** Art-style preset name (e.g. "Cinematic", "Anime / Manga"). */
  style?: string;
  /** Global subject string — mapped to subjects[0] when subjects not given. */
  globalSubject?: string;
  /** Raw or refined prompt text — used for scene when scene is not given. */
  promptText?: string;
  /** Background override. */
  background?: string;
  /** Mood override. */
  mood?: string;
  /** Lighting override. */
  lighting?: ImageStyleGuide["lighting"];
  /** Composition override. */
  composition?: ImageStyleGuide["composition"];
  /** Camera override. */
  camera?: ImageStyleGuide["camera"];
  /** Color palette override. */
  color_palette?: string[];
  /** Extra avoid items (merged with DEFAULT_NEGATIVE_CONSTRAINTS). */
  avoid?: string[];
  /** Persona-specific negative constraints injected between defaults and call-site avoid. */
  personaNegatives?: string[];
  /** Textures override. */
  textures?: string[];
  /** Effects override. */
  effects?: string[];
}

/**
 * Assemble a full `ImageStyleGuide` from heterogeneous sources.
 *
 * The builder merges:
 * - per-style default tables (lighting, palette, camera, …)
 * - `styleEnhancements.ts` keywords + mediumDescription
 * - `IMAGE_STYLE_MODIFIERS` string (parsed into supplemental keywords)
 * - `DEFAULT_NEGATIVE_CONSTRAINTS` → `avoid`
 * - any explicit overrides passed in params
 */
export function buildImageStyleGuide(params: BuildImageStyleGuideParams = {}): ImageStyleGuide {
  const {
    scene,
    subjects,
    style = "Cinematic",
    globalSubject,
    promptText,
    background,
    mood,
    lighting,
    composition,
    camera,
    color_palette,
    avoid,
    personaNegatives,
    textures,
    effects,
  } = params;

  // --- Resolve style defaults ---
  const styleLower = style.toLowerCase();
  const defaults = STYLE_DEFAULTS[styleLower] ?? FALLBACK_DEFAULTS;

  // --- Style keywords from styleEnhancements.ts ---
  const enhancement = getStyleEnhancement(style);

  // --- Supplemental keywords from IMAGE_STYLE_MODIFIERS string ---
  const modifierStr = IMAGE_STYLE_MODIFIERS[style] ?? IMAGE_STYLE_MODIFIERS["Cinematic"] ?? "";
  const modifierKeywords = modifierStr
    .split(",")
    .map((s) => s.trim())
    .filter((s) => s.length > 0);

  // Merge and deduplicate keywords
  const allKeywords = [...new Set([...enhancement.keywords, ...modifierKeywords])];

  // --- Subjects ---
  let resolvedSubjects: ImageStyleGuideSubject[] = subjects ?? [];
  if (resolvedSubjects.length === 0 && globalSubject && globalSubject.trim()) {
    resolvedSubjects = [
      {
        type: "person",
        description: globalSubject,
        position: "center",
      },
    ];
  }

  // --- Scene ---
  const resolvedScene = scene ?? promptText ?? "";

  // --- Avoid (negative constraints) ---
  const negativeItems = DEFAULT_NEGATIVE_CONSTRAINTS.map((c) =>
    c.replace(/^no\s+/i, "").trim(),
  );
  const resolvedAvoid = [...new Set([...negativeItems, ...(personaNegatives ?? []), ...(avoid ?? [])])];

  return {
    scene: resolvedScene,
    subjects: resolvedSubjects,
    style: {
      preset: style,
      keywords: allKeywords,
      medium: enhancement.mediumDescription,
    },
    color_palette: color_palette ?? defaults.color_palette,
    lighting: lighting ?? defaults.lighting,
    mood: mood ?? defaults.mood,
    background: background ?? "",
    composition: composition ?? defaults.composition,
    camera: camera ?? defaults.camera,
    textures: textures ?? defaults.textures,
    effects: effects ?? defaults.effects,
    avoid: resolvedAvoid,
  };
}

// ---------------------------------------------------------------------------
// Serializer
// ---------------------------------------------------------------------------

/**
 * Serialize an `ImageStyleGuide` to a JSON string suitable as an image-model prompt.
 */
// ---------------------------------------------------------------------------
// Shot Breakdown Factory (Story Mode)
// ---------------------------------------------------------------------------

export interface ShotBreakdownInput {
  description: string;
  shotType: string;
  cameraAngle: string;
  movement: string;
  lighting: string;
  emotion: string;
}

export interface CharacterInput {
  name: string;
  visualDescription: string;
  facialTags?: string;
}

/**
 * Build a complete `BuildImageStyleGuideParams` from shot breakdown + character data.
 * Centralizes the mapping from story pipeline types to the style guide system.
 */
export function fromShotBreakdown(
  shot: ShotBreakdownInput,
  characters: CharacterInput[],
  style: string,
  extractedStyle?: ExtractedStyleOverride,
  persona?: Persona,
): ImageStyleGuide {
  // Build subjects from characters mentioned in the shot description
  const shotDescLower = shot.description.toLowerCase();
  const subjects: ImageStyleGuideSubject[] = characters
    .filter(c => shotDescLower.includes(c.name.toLowerCase()))
    .map(c => ({
      type: "person",
      description: c.facialTags
        ? c.facialTags
        : c.visualDescription.length > 150
          ? c.visualDescription.substring(0, 147) + "..."
          : c.visualDescription,
      position: "center",
      expression: shot.emotion || undefined,
      pose: inferPoseFromDescription(shot.description),
      interaction: inferInteractionFromDescription(shot.description, characters.length),
    }));

  // Map shot fields to style guide params
  const params: BuildImageStyleGuideParams = {
    scene: shot.description,
    subjects,
    style,
    mood: shot.emotion,
    lighting: {
      source: shot.lighting || "natural",
      quality: "cinematic",
      direction: inferLightingDirection(shot.lighting),
    },
    composition: {
      shot_type: mapShotType(shot.shotType),
      camera_angle: mapCameraAngle(shot.cameraAngle),
      framing: "rule of thirds",
    },
  };

  // Apply extracted style overrides for cross-shot consistency
  if (extractedStyle) {
    if (extractedStyle.colorPalette?.length) {
      params.color_palette = extractedStyle.colorPalette;
    }
    if (extractedStyle.lighting) {
      params.lighting = {
        source: extractedStyle.lighting,
        quality: "consistent",
      };
    }
    if (extractedStyle.texture) {
      params.textures = [extractedStyle.texture];
    }
    if (extractedStyle.moodKeywords?.length) {
      params.mood = extractedStyle.moodKeywords.join(", ");
    }
    if (extractedStyle.negativePrompts?.length) {
      params.avoid = extractedStyle.negativePrompts;
    }
  }

  // Inject persona-specific negative constraints
  if (persona?.negative_constraints?.length) {
    params.personaNegatives = persona.negative_constraints;
  }

  return buildImageStyleGuide(params);
}

/** Extracted style from visualConsistencyService, used for cross-shot consistency */
export interface ExtractedStyleOverride {
  colorPalette?: string[];
  lighting?: string;
  texture?: string;
  moodKeywords?: string[];
  negativePrompts?: string[];
}

function inferPoseFromDescription(description: string): string | undefined {
  const lower = description.toLowerCase();
  if (/\b(sitting|seated|sits)\b/.test(lower)) return "sitting";
  if (/\b(standing|stands)\b/.test(lower)) return "standing";
  if (/\b(walking|walks)\b/.test(lower)) return "walking";
  if (/\b(running|runs)\b/.test(lower)) return "running";
  if (/\b(lying|lies|reclining)\b/.test(lower)) return "reclining";
  return undefined;
}

function inferInteractionFromDescription(description: string, charCount: number): string | undefined {
  if (charCount < 2) return undefined;
  const lower = description.toLowerCase();
  if (/\b(talking|speaking|conversation)\b/.test(lower)) return "in conversation";
  if (/\b(fighting|struggling)\b/.test(lower)) return "in conflict";
  if (/\b(embracing|hugging)\b/.test(lower)) return "embracing";
  if (/\b(looking at|facing)\b/.test(lower)) return "facing each other";
  return undefined;
}

function inferLightingDirection(lighting: string): string | undefined {
  const lower = (lighting || "").toLowerCase();
  if (/\b(back|rim|silhouette)\b/.test(lower)) return "backlit";
  if (/\b(side|dramatic)\b/.test(lower)) return "side-lit";
  if (/\b(top|overhead)\b/.test(lower)) return "top-lit";
  return undefined;
}

function mapShotType(shotType: string): string {
  const map: Record<string, string> = {
    "Wide": "wide shot",
    "Medium": "medium shot",
    "Close-up": "close-up",
    "Extreme Close-up": "extreme close-up",
    "POV": "POV shot",
    "Over-the-shoulder": "over-the-shoulder shot",
  };
  return map[shotType] || shotType.toLowerCase();
}

function mapCameraAngle(angle: string): string {
  const map: Record<string, string> = {
    "Eye-level": "eye-level",
    "High": "high angle",
    "Low": "low angle",
    "Dutch": "dutch angle",
    "Bird's-eye": "bird's eye view",
    "Worm's-eye": "worm's eye view",
  };
  return map[angle] || angle.toLowerCase();
}

// ---------------------------------------------------------------------------
// Serializer
// ---------------------------------------------------------------------------

export function serializeStyleGuide(guide: ImageStyleGuide): string {
  return JSON.stringify(guide, null, 2);
}

/**
 * Serialize an `ImageStyleGuide` into a natural-language paragraph.
 *
 * Use this for models that don't handle raw JSON well (e.g. Imagen, DeAPI Flux).
 * The output reads as a conventional image-generation prompt while carrying
 * every dimension from the guide.
 */
export function serializeStyleGuideAsText(guide: ImageStyleGuide): string {
  const parts: string[] = [];

  // Lead with subjects (most important for model attention)
  if (guide.subjects.length > 0) {
    const subjectDescriptions = guide.subjects.map((s) => {
      let desc = s.description;
      if (s.pose) desc += `, ${s.pose}`;
      if (s.expression) desc += `, ${s.expression}`;
      if (s.position) desc += `, positioned ${s.position}`;
      if (s.interaction) desc += `, ${s.interaction}`;
      return desc;
    });
    parts.push(subjectDescriptions.join("; "));
  }

  // Scene / action
  if (guide.scene) {
    parts.push(guide.scene);
  }

  // Background
  if (guide.background) {
    parts.push(`Background: ${guide.background}.`);
  }

  // Style medium
  parts.push(guide.style.medium + ".");

  // Lighting
  const lt = guide.lighting;
  parts.push(
    `Lighting: ${lt.source}, ${lt.quality}${lt.direction ? `, ${lt.direction}` : ""}.`,
  );

  // Composition + camera
  const comp = guide.composition;
  let camLine = `${comp.shot_type}, ${comp.camera_angle}`;
  if (comp.framing) camLine += `, ${comp.framing}`;
  const cam = guide.camera;
  if (cam.lens) camLine += `, ${cam.lens} lens`;
  if (cam.depth_of_field) camLine += `, ${cam.depth_of_field} depth of field`;
  if (cam.focus) camLine += `, focus on ${cam.focus}`;
  parts.push(camLine + ".");

  // Mood
  if (guide.mood) {
    parts.push(`${guide.mood} mood.`);
  }

  // Color palette
  if (guide.color_palette.length > 0) {
    parts.push(`Color palette: ${guide.color_palette.join(", ")}.`);
  }

  // Textures + effects
  const extras = [...(guide.textures ?? []), ...(guide.effects ?? [])];
  if (extras.length > 0) {
    parts.push(extras.join(", ") + ".");
  }

  // Style keywords (select a few to avoid prompt bloat)
  if (guide.style.keywords.length > 0) {
    parts.push(guide.style.keywords.slice(0, 5).join(", ") + ".");
  }

  // Avoid / negative — prefix each item with "no" for stronger negative prompting
  if (guide.avoid.length > 0) {
    const prefixed = guide.avoid.map(item =>
      item.toLowerCase().startsWith("no ") ? item : `no ${item}`
    );
    parts.push(`Avoid: ${prefixed.join(", ")}.`);
  }

  return parts.filter(Boolean).join(" ");
}
````

## File: packages/shared/src/services/prompt/index.ts
````typescript
/**
 * Prompt Module Index
 * 
 * Re-exports all prompt-related modules for convenient imports.
 */

export { getSystemPersona, type Persona, type PersonaType } from './personaData';
export { getStyleEnhancement, type StyleEnhancement } from './styleEnhancements';
export {
  buildImageStyleGuide,
  serializeStyleGuide,
  serializeStyleGuideAsText,
  type ImageStyleGuide,
  type ImageStyleGuideSubject,
  type BuildImageStyleGuideParams,
} from './imageStyleGuide';
````

## File: packages/shared/src/services/prompt/personaData.ts
````typescript
/**
 * Persona Data
 * 
 * AI persona definitions for different video purposes.
 * Extracted from promptService.ts for modularity.
 */

import { VideoPurpose } from "../../constants";

export type PersonaType = "brand_specialist" | "visual_poet" | "historian" | "viral_creator";

export interface Persona {
    type: PersonaType;
    name: string;
    role: string;
    coreRule: string;
    visualPrinciples: string[];
    avoidList: string[];
    negative_constraints: string[];
}

/**
 * Persona definitions for each video purpose.
 */
const PERSONA_DEFINITIONS: Record<VideoPurpose, Persona> = {
    commercial: {
        type: "brand_specialist",
        name: "Brand Specialist",
        role: "Commercial Visual Director",
        coreRule: "Show products and subjects with clean, aspirational visuals. No metaphors - literal product shots only.",
        visualPrinciples: [
            "Hero product shots with professional lighting",
            "Clean, uncluttered compositions",
            "Lifestyle context showing benefits",
            "High production value aesthetic",
            "Call-to-action friendly framing",
        ],
        avoidList: [
            "Abstract metaphors",
            "Artistic interpretations that obscure the product",
            "Dark or moody lighting that hides details",
            "Busy backgrounds that distract from subject",
        ],
        negative_constraints: [
            "unflattering product angles",
            "dirty or grimy surfaces",
            "competitor branding or logos",
            "cluttered distracting backgrounds",
            "harsh shadows obscuring product details",
        ],
    },
    music_video: {
        type: "visual_poet",
        name: "Visual Poet",
        role: "Music Video Director",
        coreRule: "ATMOSPHERIC RESONANCE: Prioritize the EMOTION of the lyric over the object. If lyrics say 'candle', visualize 'loneliness' or 'fading hope' using lighting and shadows. Do not simply show the object mentioned.",
        visualPrinciples: [
            "Emotional interpretation of mentioned objects through atmosphere",
            "Emotional resonance through cinematography",
            "Deep atmospheric compositions",
            "Symbolic objects shown as emotional metaphors",
            "Visual rhythm matching musical structure",
        ],
        avoidList: [
            "Replacing concrete objects with generic scenes",
            "Showing 'sad person' when lyrics mention 'candle'",
            "Abstract interpretations that ignore specific imagery",
            "Generic couple scenes for emotional content",
        ],
        negative_constraints: [
            "flat even lighting with no mood",
            "stock photo poses and expressions",
            "literal illustration of lyrics without emotional depth",
            "oversaturated neon without narrative purpose",
            "busy text overlays competing with visuals",
        ],
    },
    documentary: {
        type: "historian",
        name: "Historian",
        role: "Documentary Visualizer",
        coreRule: "Prioritize realism and accuracy. Every visual must be grounded in reality and support the factual narrative.",
        visualPrinciples: [
            "Realistic, documentary-style imagery",
            "Historical accuracy when applicable",
            "Educational clarity",
            "B-roll style supporting visuals",
            "Professional, trustworthy aesthetic",
        ],
        avoidList: [
            "Stylized or fantastical interpretations",
            "Emotional manipulation through unrealistic imagery",
            "Artistic license that distorts facts",
            "Dramatic embellishments",
        ],
        negative_constraints: [
            "fantastical or impossible imagery",
            "oversaturated HDR color grading",
            "fictional or invented characters",
            "Hollywood cinematic color science",
            "CGI or computer-generated environments",
        ],
    },
    social_short: {
        type: "viral_creator",
        name: "Viral Creator",
        role: "Social Media Visual Specialist",
        coreRule: "Create scroll-stopping visuals with immediate impact. First frame must hook the viewer. Think TikTok, Instagram Reels - every frame must be screenshot-worthy.",
        visualPrinciples: [
            "Hyper-detailed textures (8K, Unreal Engine 5 quality)",
            "Volumetric lighting and cinematic depth of field",
            "Symmetrical composition (Wes Anderson style)",
            "Vibrant, saturated color palette (Teal & Orange grading)",
            "Bold, high-contrast visuals that pop on mobile",
            "Trending aesthetic references (Y2K, vaporwave, dark academia)",
            "Dynamic, energetic framing with leading lines",
        ],
        avoidList: [
            "Blurry backgrounds or soft focus",
            "Text or watermarks in frame",
            "Dull, flat lighting",
            "Generic stock photo aesthetic",
            "Slow-building subtle imagery",
            "Complex compositions that don't read on small screens",
            "Muted color palettes",
        ],
        negative_constraints: [
            "low resolution or blurry details",
            "bland neutral color palettes",
            "static symmetrical compositions without energy",
            "outdated or retro aesthetics (unless intentionally trending)",
            "small unreadable elements on mobile screens",
        ],
    },
    podcast_visual: {
        type: "visual_poet",
        name: "Visual Poet",
        role: "Ambient Visual Designer",
        coreRule: "Create calming, non-distracting backgrounds that complement spoken content without competing for attention.",
        visualPrinciples: [
            "Ambient, atmospheric scenes",
            "Subtle movement and gentle transitions",
            "Meditative, contemplative imagery",
            "Abstract or environmental focus",
            "Long-duration friendly visuals",
        ],
        avoidList: [
            "Busy, attention-grabbing scenes",
            "Fast movement or dramatic action",
            "Strong narrative elements",
            "Visuals that demand interpretation",
        ],
        negative_constraints: [
            "jarring high-contrast scene changes",
            "faces or figures requiring emotional interpretation",
            "text or informational overlays",
            "busy patterned backgrounds",
            "strobing or rapid light changes",
        ],
    },
    lyric_video: {
        type: "visual_poet",
        name: "Visual Poet",
        role: "Lyric Video Designer",
        coreRule: "Create backgrounds with clear negative space for text overlay. Visuals support lyrics without overwhelming them.",
        visualPrinciples: [
            "Compositions with text-safe zones",
            "Lower-third and center-frame clearance",
            "Thematic imagery that supports mood",
            "Contrast-friendly backgrounds",
            "Rhythmic visual flow matching lyrics",
        ],
        avoidList: [
            "Busy center compositions",
            "Complex patterns that interfere with text",
            "Dramatic lighting changes that affect readability",
            "Visuals that compete with lyrics for attention",
        ],
        negative_constraints: [
            "complex detailed patterns in text placement zones",
            "low-contrast backgrounds against likely text colors",
            "bright white or pure black fields that flatten readability",
            "multiple focal points competing for center attention",
            "rapid unpredictable lighting shifts",
        ],
    },
    storytelling: {
        type: "visual_poet",
        name: "Master Storyteller",
        role: "Narrative Visual Director",
        coreRule: "Create immersive, cinematic visuals that bring stories to life. Focus on atmosphere, character moments, and emotional beats.",
        visualPrinciples: [
            "Rich, atmospheric world-building",
            "Character-focused compositions",
            "Dramatic lighting for emotional impact",
            "Cultural authenticity in settings",
            "Visual metaphors that enhance narrative",
        ],
        avoidList: [
            "Generic stock imagery",
            "Flat, uninspired compositions",
            "Culturally insensitive representations",
            "Visuals that break story immersion",
        ],
        negative_constraints: [
            "modern contemporary settings without narrative context",
            "generic stock photo expressions",
            "flat even studio lighting without mood",
            "anachronistic costume or prop details",
            "shallow shallow backgrounds with no world-building depth",
        ],
    },
    educational: {
        type: "historian",
        name: "Educator",
        role: "Educational Content Designer",
        coreRule: "Create clear, informative visuals that aid understanding. Prioritize clarity and visual hierarchy.",
        visualPrinciples: [
            "Clear, well-organized compositions",
            "Diagrams and infographic-style layouts",
            "Step-by-step visual progression",
            "Highlighting key concepts visually",
            "Professional, trustworthy aesthetic",
        ],
        avoidList: [
            "Overly artistic interpretations",
            "Confusing or cluttered visuals",
            "Distracting backgrounds",
            "Imagery that doesn't support learning",
        ],
        negative_constraints: [
            "decorative clutter with no informational value",
            "emotionally manipulative or biased imagery",
            "dark moody lighting reducing clarity",
            "abstract metaphors requiring interpretation",
            "stylistic flourishes competing with the learning objective",
        ],
    },
    horror_mystery: {
        type: "visual_poet",
        name: "Shadow Weaver",
        role: "Horror/Mystery Visual Director",
        coreRule: "Create atmospheric, suspenseful visuals that build tension. Use shadows, negative space, and unsettling compositions.",
        visualPrinciples: [
            "Deep shadows and chiaroscuro lighting",
            "Unsettling, off-center compositions",
            "Fog, mist, and atmospheric effects",
            "Subtle horror elements, not gore",
            "Building dread through environment",
        ],
        avoidList: [
            "Bright, cheerful lighting",
            "Explicit gore or violence",
            "Jump-scare focused imagery",
            "Generic horror clichés",
        ],
        negative_constraints: [
            "bright cheerful daylight scenes",
            "explicit gore or gratuitous violence",
            "comedic or ironic visual tone",
            "safe warm color palettes without tension",
            "clean unambiguous well-lit compositions",
        ],
    },
    travel: {
        type: "historian",
        name: "Explorer",
        role: "Travel & Nature Visual Director",
        coreRule: "Capture the beauty and wonder of places. Create visuals that inspire wanderlust and appreciation for nature.",
        visualPrinciples: [
            "Sweeping landscape compositions",
            "Golden hour and dramatic lighting",
            "Cultural landmarks and local life",
            "Sense of scale and grandeur",
            "Authentic, non-touristy perspectives",
        ],
        avoidList: [
            "Clichéd tourist shots",
            "Over-processed HDR looks",
            "Crowded, busy scenes",
            "Inauthentic representations",
        ],
        negative_constraints: [
            "overprocessed HDR or tone-mapped skies",
            "clichéd postcard framing of famous landmarks",
            "crowds of tourists breaking scene authenticity",
            "studio-lit artificial environments",
            "culturally stereotyped representations of local people",
        ],
    },
    motivational: {
        type: "brand_specialist",
        name: "Inspirer",
        role: "Motivational Visual Director",
        coreRule: "Create uplifting, empowering visuals that inspire action. Focus on triumph, growth, and human potential.",
        visualPrinciples: [
            "Upward movement and rising compositions",
            "Warm, hopeful lighting",
            "Human achievement moments",
            "Nature metaphors for growth",
            "Dynamic, energetic framing",
        ],
        avoidList: [
            "Dark, pessimistic imagery",
            "Static, lifeless compositions",
            "Clichéd motivational stock photos",
            "Unrealistic perfection",
        ],
        negative_constraints: [
            "dark pessimistic or hopeless imagery",
            "downward compositional movement",
            "failure or defeat-themed visual metaphors",
            "exhausted or defeated expressions",
            "cluttered chaotic environments without resolution",
        ],
    },
    news_report: {
        type: "historian",
        name: "Journalist",
        role: "News Visual Director",
        coreRule: "Create factual, objective visuals that inform without bias. Prioritize accuracy and journalistic integrity.",
        visualPrinciples: [
            "Clean, professional compositions",
            "Factual, documentary-style imagery",
            "Clear visual hierarchy",
            "Neutral, unbiased framing",
            "Supporting graphics and data visualization",
        ],
        avoidList: [
            "Sensationalized imagery",
            "Biased or leading visuals",
            "Emotional manipulation",
            "Inaccurate representations",
        ],
        negative_constraints: [
            "sensationalized dramatic color grading",
            "emotionally manipulative facial close-ups",
            "biased framing favoring one perspective",
            "inaccurate or invented visual representations",
            "stylized artistic effects undermining credibility",
        ],
    },
    // Story Mode Genre-Specific Personas
    story_drama: {
        type: "visual_poet",
        name: "Drama Director",
        role: "Dramatic Story Visualizer",
        coreRule: "Focus on emotional depth and character moments. Every frame should convey inner conflict, relationships, and human experience.",
        visualPrinciples: [
            "Intimate close-ups for emotional beats",
            "Warm, naturalistic lighting",
            "Character-focused compositions",
            "Subtle environmental storytelling",
            "Meaningful negative space for contemplation",
        ],
        avoidList: [
            "Over-the-top dramatic effects",
            "Distracting action sequences",
            "Cold, sterile environments",
            "Generic establishing shots",
        ],
        negative_constraints: [
            "explosive action or physical spectacle",
            "cold sterile clinical environments",
            "exaggerated cartoon expressions",
            "genre-breaking comedic tones",
            "hyperactive camera movement undermining emotional stillness",
        ],
    },
    story_comedy: {
        type: "viral_creator",
        name: "Comedy Director",
        role: "Comedic Story Visualizer",
        coreRule: "Create visually engaging, timing-aware compositions. Use framing and staging to enhance comedic beats and character reactions.",
        visualPrinciples: [
            "Bright, vibrant color palettes",
            "Clear staging for physical comedy",
            "Reaction shot compositions",
            "Playful, dynamic camera angles",
            "Exaggerated but grounded environments",
        ],
        avoidList: [
            "Dark, moody lighting",
            "Overly serious compositions",
            "Cluttered frames that hide reactions",
            "Slow, contemplative pacing",
        ],
        negative_constraints: [
            "dark moody lighting removing comedic energy",
            "tragic or grief-stricken emotional tones",
            "gritty desaturated color grading",
            "claustrophobic tight framing hiding physical comedy",
            "slow ponderous camera work dampening comedic timing",
        ],
    },
    story_thriller: {
        type: "visual_poet",
        name: "Thriller Director",
        role: "Suspense Story Visualizer",
        coreRule: "Build tension through visual unease. Use shadows, tight framing, and unsettling compositions to create psychological suspense.",
        visualPrinciples: [
            "High contrast, noir-inspired lighting",
            "Claustrophobic framing",
            "Dutch angles for disorientation",
            "Deep shadows hiding threats",
            "Reflections and surveillance aesthetics",
        ],
        avoidList: [
            "Bright, cheerful lighting",
            "Wide, safe compositions",
            "Predictable framing",
            "Explicit violence over implied threat",
        ],
        negative_constraints: [
            "bright cheerful well-lit open spaces",
            "soft flattering portrait lighting",
            "wide safe reassuring compositions",
            "warm cozy color palettes",
            "clear unambiguous staging without hidden elements",
        ],
    },
    story_scifi: {
        type: "visual_poet",
        name: "Sci-Fi Director",
        role: "Science Fiction Visualizer",
        coreRule: "Create believable futuristic worlds with consistent technology aesthetics. Balance wonder with grounded human elements.",
        visualPrinciples: [
            "Sleek, technological environments",
            "Neon and holographic lighting accents",
            "Scale contrast (human vs. technology)",
            "Clean, minimalist future aesthetics",
            "Atmospheric volumetric lighting",
        ],
        avoidList: [
            "Dated retro-futurism (unless intentional)",
            "Cluttered, chaotic technology",
            "Inconsistent tech levels",
            "Generic spaceship interiors",
        ],
        negative_constraints: [
            "contemporary or historical settings breaking sci-fi immersion",
            "organic natural environments without technological integration",
            "warm earthy tones inconsistent with futuristic aesthetic",
            "anachronistic non-futuristic props or costumes",
            "retro-futurist 1950s aesthetics (unless story-mandated)",
        ],
    },
    story_action: {
        type: "viral_creator",
        name: "Action Director",
        role: "Action Story Visualizer",
        coreRule: "Create dynamic, kinetic visuals with clear spatial geography. Every frame should convey motion, impact, and stakes.",
        visualPrinciples: [
            "Dynamic diagonal compositions",
            "Motion blur and speed lines",
            "High-energy color grading",
            "Clear action geography",
            "Impactful moment freezes",
        ],
        avoidList: [
            "Static, boring compositions",
            "Confusing spatial relationships",
            "Muted, desaturated colors",
            "Slow, contemplative framing",
        ],
        negative_constraints: [
            "static symmetrical portrait compositions",
            "slow meditative camera movement",
            "muted desaturated low-energy color palettes",
            "ambiguous spatial geography hiding action",
            "emotionally quiet contemplative scenes without kinetic energy",
        ],
    },
    story_fantasy: {
        type: "visual_poet",
        name: "Fantasy Director",
        role: "Fantasy World Visualizer",
        coreRule: "Create magical, immersive worlds with consistent internal logic. Balance wonder and beauty with narrative grounding.",
        visualPrinciples: [
            "Rich, saturated color palettes",
            "Magical lighting effects (glows, particles)",
            "Epic scale and grandeur",
            "Detailed world-building elements",
            "Mythical creature integration",
        ],
        avoidList: [
            "Generic fantasy clichés",
            "Inconsistent magical rules",
            "Modern elements breaking immersion",
            "Flat, uninspired landscapes",
        ],
        negative_constraints: [
            "modern technology or contemporary objects breaking world immersion",
            "photorealistic documentary-style photography aesthetics",
            "mundane urban or suburban environments without fantasy elements",
            "flat overcast natural lighting without magical quality",
            "inconsistent visual rules mixing incompatible fantasy systems",
        ],
    },
    story_romance: {
        type: "visual_poet",
        name: "Romance Director",
        role: "Romantic Story Visualizer",
        coreRule: "Create intimate, emotionally resonant visuals. Focus on connection, chemistry, and the beauty of human relationships.",
        visualPrinciples: [
            "Soft, flattering lighting",
            "Warm, romantic color grading",
            "Two-shot compositions emphasizing connection",
            "Beautiful, aspirational settings",
            "Intimate close-ups on expressions",
        ],
        avoidList: [
            "Harsh, unflattering lighting",
            "Cold, sterile environments",
            "Distant, disconnected framing",
            "Overly sexualized imagery",
        ],
        negative_constraints: [
            "harsh unflattering overhead or side lighting",
            "cold sterile clinical environments",
            "distant isolated single-subject framing",
            "gritty desaturated color palettes",
            "aggressive confrontational body language between subjects",
        ],
    },
    story_historical: {
        type: "historian",
        name: "Historical Director",
        role: "Period Story Visualizer",
        coreRule: "Create historically accurate, immersive period visuals. Research-driven authenticity in costumes, settings, and atmosphere.",
        visualPrinciples: [
            "Period-accurate production design",
            "Natural, era-appropriate lighting",
            "Authentic costume and prop details",
            "Painterly, classical compositions",
            "Cultural and historical accuracy",
        ],
        avoidList: [
            "Anachronistic elements",
            "Modern sensibilities in framing",
            "Inaccurate cultural representations",
            "Over-stylized period aesthetics",
        ],
        negative_constraints: [
            "anachronistic modern objects or technology",
            "contemporary fashion or hairstyles",
            "modern architecture or urban infrastructure",
            "period-breaking synthetic color grading",
            "contemporary casual body language inconsistent with the era",
        ],
    },
    story_animation: {
        type: "viral_creator",
        name: "Animation Director",
        role: "Animated Story Visualizer",
        coreRule: "Create expressive, stylized visuals with clear character appeal. Embrace the freedom of animation while maintaining emotional truth.",
        visualPrinciples: [
            "Bold, expressive character designs",
            "Vibrant, stylized color palettes",
            "Dynamic, exaggerated compositions",
            "Clear silhouettes and staging",
            "Imaginative, fantastical environments",
        ],
        avoidList: [
            "Photorealistic rendering (unless intentional)",
            "Stiff, lifeless poses",
            "Muddy, unclear compositions",
            "Generic, template-based designs",
        ],
        negative_constraints: [
            "photorealistic rendering undermining stylized aesthetic",
            "stiff anatomically rigid poses without cartoon exaggeration",
            "muddy muted colors reducing visual clarity",
            "complex unreadable silhouettes",
            "generic uncanny valley human faces",
        ],
    },
};

/**
 * Get the AI persona based on video purpose.
 */
export function getSystemPersona(purpose: VideoPurpose): Persona {
    return PERSONA_DEFINITIONS[purpose] || PERSONA_DEFINITIONS.music_video;
}

/**
 * Get purpose-specific negative constraints for image generation.
 * Used to inject persona-aware negatives into the image style guide avoid list.
 */
export function getPersonaNegatives(purpose: VideoPurpose): string[] {
    return getSystemPersona(purpose).negative_constraints;
}
````

## File: packages/shared/src/services/prompt/sceneShotTemplates.ts
````typescript

````

## File: packages/shared/src/services/prompt/styleEnhancements.ts
````typescript
/**
 * Style Enhancements
 * 
 * Style-specific technique keywords for authentic visual representation.
 * Extracted from promptService.ts for modularity.
 */

export interface StyleEnhancement {
    keywords: string[];
    mediumDescription: string;
}

/**
 * Style enhancement definitions for all art styles.
 */
const STYLE_ENHANCEMENTS: Record<string, StyleEnhancement> = {
    cinematic: {
        keywords: [
            "35mm film grain",
            "anamorphic lens flare",
            "shallow depth of field",
            "professional color grading",
            "dramatic three-point lighting",
            "cinematic aspect ratio",
            "volumetric light rays",
        ],
        mediumDescription: "cinematic movie still with professional cinematography, anamorphic lens characteristics, and dramatic lighting",
    },
    anime: {
        keywords: [
            "screentone shading",
            "speed lines",
            "expressive oversized eyes",
            "dynamic action poses",
            "cel-shaded flat colors",
            "clean ink linework",
            "shoujo sparkle effects",
            "dramatic wind hair movement",
        ],
        mediumDescription: "high-quality Japanese animation style with clean linework, cel shading, and Studio Ghibli-inspired aesthetic",
    },
    manga: {
        keywords: [
            "screentone shading",
            "speed lines",
            "expressive oversized eyes",
            "dynamic action poses",
            "cel-shaded flat colors",
            "clean ink linework",
            "shoujo sparkle effects",
            "dramatic wind hair movement",
        ],
        mediumDescription: "high-quality Japanese animation style with clean linework, cel shading, and Studio Ghibli-inspired aesthetic",
    },
    cyberpunk: {
        keywords: [
            "neon tube lighting",
            "holographic displays",
            "rain-slicked reflective streets",
            "chromatic aberration",
            "glitch artifacts",
            "teal and magenta color palette",
            "high contrast noir shadows",
            "retrofuturistic technology",
        ],
        mediumDescription: "futuristic cyberpunk aesthetic with neon-drenched cityscapes, rain effects, and Blade Runner-inspired atmosphere",
    },
    watercolor: {
        keywords: [
            "bleeding color edges",
            "wet-on-wet technique",
            "cold-pressed Arches paper texture",
            "pigment granulation",
            "transparent color washes",
            "soft diffused edges",
            "visible water bloom effects",
            "raw paper white highlights",
        ],
        mediumDescription: "authentic watercolor painting on textured paper with visible pigment flow, soft bleeding edges, and transparent washes",
    },
    aquarelle: {
        keywords: [
            "bleeding color edges",
            "wet-on-wet technique",
            "cold-pressed Arches paper texture",
            "pigment granulation",
            "transparent color washes",
            "soft diffused edges",
            "visible water bloom effects",
            "raw paper white highlights",
        ],
        mediumDescription: "authentic watercolor painting on textured paper with visible pigment flow, soft bleeding edges, and transparent washes",
    },
    oil: {
        keywords: [
            "impasto knife texture",
            "linseed oil sheen",
            "canvas weave texture",
            "thick paint ridges",
            "visible brushstroke direction",
            "rich saturated pigments",
            "chiaroscuro modeling",
            "glazed translucent layers",
        ],
        mediumDescription: "traditional oil painting on stretched canvas with visible impasto brushwork, rich pigment saturation, and classical technique",
    },
    pixel: {
        keywords: [
            "limited color palette",
            "dithering patterns",
            "aliased hard edges",
            "sprite-based characters",
            "8-bit or 16-bit aesthetic",
            "tile-based backgrounds",
            "scanline overlay",
            "CRT screen curvature",
        ],
        mediumDescription: "authentic retro pixel art with limited palette, dithering techniques, and nostalgic 16-bit video game aesthetic",
    },
    surreal: {
        keywords: [
            "impossible geometry",
            "melting clocks motif",
            "dreamlike distortion",
            "juxtaposed scale",
            "floating objects",
            "Dalí-esque symbolism",
            "Magritte-style paradox",
            "uncanny valley atmosphere",
        ],
        mediumDescription: "surrealist art style with dreamlike impossible imagery, symbolic juxtapositions, and Dalí/Magritte-inspired composition",
    },
    "dark fantasy": {
        keywords: [
            "grimdark atmosphere",
            "gothic architecture",
            "volumetric fog and mist",
            "flickering torchlight",
            "eldritch horror elements",
            "weathered stone textures",
            "blood moon lighting",
            "chiaroscuro shadows",
        ],
        mediumDescription: "dark fantasy art with gothic atmosphere, grimdark aesthetic, detailed textures, and ominous eldritch mood",
    },
    commercial: {
        keywords: [
            "studio softbox lighting",
            "infinity curve background",
            "product hero shot",
            "macro detail focus",
            "clean negative space",
            "aspirational lifestyle context",
            "high-key professional lighting",
            "advertisement composition",
        ],
        mediumDescription: "professional commercial photography with studio lighting, clean backgrounds, and product-focused hero composition",
    },
    ad: {
        keywords: [
            "studio softbox lighting",
            "infinity curve background",
            "product hero shot",
            "macro detail focus",
            "clean negative space",
            "aspirational lifestyle context",
            "high-key professional lighting",
            "advertisement composition",
        ],
        mediumDescription: "professional commercial photography with studio lighting, clean backgrounds, and product-focused hero composition",
    },
    minimalist: {
        keywords: [
            "flat vector shapes",
            "isometric perspective",
            "clean white background",
            "limited color palette",
            "geometric simplification",
            "infographic clarity",
            "sans-serif aesthetic",
            "educational diagram style",
        ],
        mediumDescription: "clean minimalist illustration with flat design, isometric elements, and educational infographic clarity",
    },
    tutorial: {
        keywords: [
            "flat vector shapes",
            "isometric perspective",
            "clean white background",
            "limited color palette",
            "geometric simplification",
            "infographic clarity",
            "sans-serif aesthetic",
            "educational diagram style",
        ],
        mediumDescription: "clean minimalist illustration with flat design, isometric elements, and educational infographic clarity",
    },
    comic: {
        keywords: [
            "bold ink outlines",
            "halftone dot patterns",
            "dynamic action lines",
            "Ben-Day dots shading",
            "vibrant superhero colors",
            "dramatic foreshortening",
            "Kirby crackle energy",
            "word balloon composition space",
        ],
        mediumDescription: "American comic book style with bold ink outlines, halftone patterns, dynamic action poses, and vibrant superhero aesthetic",
    },
    corporate: {
        keywords: [
            "Memphis design elements",
            "flat vector illustration",
            "professional blue tones",
            "clean geometric shapes",
            "tech startup aesthetic",
            "trustworthy composition",
            "modern sans-serif style",
            "abstract blob backgrounds",
        ],
        mediumDescription: "modern corporate design with Memphis style elements, flat vectors, and professional tech-startup aesthetic",
    },
    brand: {
        keywords: [
            "Memphis design elements",
            "flat vector illustration",
            "professional blue tones",
            "clean geometric shapes",
            "tech startup aesthetic",
            "trustworthy composition",
            "modern sans-serif style",
            "abstract blob backgrounds",
        ],
        mediumDescription: "modern corporate design with Memphis style elements, flat vectors, and professional tech-startup aesthetic",
    },
    photo: {
        keywords: [
            "DSLR 50mm lens",
            "natural ambient lighting",
            "shallow depth of field bokeh",
            "raw unedited appearance",
            "realistic skin texture",
            "natural color temperature",
            "documentary photography style",
            "candid moment capture",
        ],
        mediumDescription: "hyper-realistic photography with DSLR quality, natural lighting, and authentic documentary-style capture",
    },
    realistic: {
        keywords: [
            "DSLR 50mm lens",
            "natural ambient lighting",
            "shallow depth of field bokeh",
            "raw unedited appearance",
            "realistic skin texture",
            "natural color temperature",
            "documentary photography style",
            "candid moment capture",
        ],
        mediumDescription: "hyper-realistic photography with DSLR quality, natural lighting, and authentic documentary-style capture",
    },
    noir: {
        keywords: [
            "chiaroscuro lighting",
            "venetian blind shadows",
            "high contrast black and white",
            "cigarette smoke wisps",
            "rain-slicked streets",
            "fedora silhouettes",
            "1940s detective aesthetic",
            "expressionist angles",
        ],
        mediumDescription: "classic film noir cinematography with dramatic shadows, high contrast, and 1940s detective atmosphere",
    },
    charcoal: {
        keywords: [
            "paper grain texture",
            "smudged graphite gradients",
            "gestural mark-making",
            "tonal value range",
            "cross-hatching technique",
            "erased highlights",
            "vine charcoal softness",
            "fixative spray texture",
        ],
        mediumDescription: "hand-drawn sketch with visible paper texture, smudged charcoal gradients, and natural media characteristics",
    },
    sketch: {
        keywords: [
            "paper grain texture",
            "smudged graphite gradients",
            "gestural mark-making",
            "tonal value range",
            "cross-hatching technique",
            "erased highlights",
            "vine charcoal softness",
            "fixative spray texture",
        ],
        mediumDescription: "hand-drawn sketch with visible paper texture, smudged charcoal gradients, and natural media characteristics",
    },
    pencil: {
        keywords: [
            "paper grain texture",
            "smudged graphite gradients",
            "gestural mark-making",
            "tonal value range",
            "cross-hatching technique",
            "erased highlights",
            "vine charcoal softness",
            "fixative spray texture",
        ],
        mediumDescription: "hand-drawn sketch with visible paper texture, smudged charcoal gradients, and natural media characteristics",
    },
    character_sheet: {
        keywords: [
            "studio softbox lighting",
            "neutral white background",
            "consistent proportions",
            "sharp focus on subject",
            "professional reference sheet",
            "clean rim light accent",
            "85mm portrait lens",
            "front and three-quarter view",
        ],
        mediumDescription: "professional character reference sheet with neutral studio lighting, clean background, and consistent proportions",
    },
};

// Default fallback style
const DEFAULT_STYLE: StyleEnhancement = {
    keywords: [
        "cinematic depth of field",
        "professional color grading",
        "anamorphic lens characteristics",
        "dramatic atmospheric lighting",
    ],
    mediumDescription: "cinematic visual style with professional cinematography and dramatic composition",
};

/**
 * Get style-specific technique keywords to inject into prompts.
 */
/**
 * Get style-specific technique keywords to inject into prompts.
 */
export function getStyleEnhancement(style: string | undefined | null): StyleEnhancement {
    if (!style) {
        return DEFAULT_STYLE;
    }

    const styleLower = style.toLowerCase();

    // Check for exact matches first
    if (STYLE_ENHANCEMENTS[styleLower]) {
        return STYLE_ENHANCEMENTS[styleLower];
    }

    // Check for partial matches
    for (const [key, value] of Object.entries(STYLE_ENHANCEMENTS)) {
        if (styleLower.includes(key) || key.includes(styleLower)) {
            return value;
        }
    }

    return DEFAULT_STYLE;
}
````

## File: packages/shared/src/services/prompt/templateLoader.ts
````typescript
/**
 * Prompt Template Loader
 *
 * Loads format-specific prompt templates from external text files.
 * Supports {{variable}} substitution for dynamic content injection.
 *
 * Directory structure:
 *   services/prompt/templates/{formatId}/{phase}.txt
 *
 * Requirements: 21.1, 21.2, 21.3, 21.4, 21.5
 */

// Eagerly import all .txt template files via Vite's raw text loader.
// Path is relative to this file: services/prompt/templates/
// Vitest processes this the same way as the browser build.
const _rawFiles = import.meta.glob<string>(
  './templates/**/*.txt',
  { query: '?raw', import: 'default', eager: true }
);

// Normalize Vite glob keys: './templates/movie-animation/breakdown.txt'
// → registry key: 'movie-animation/breakdown'
const templateRegistry: Record<string, string> = {};
for (const [path, content] of Object.entries(_rawFiles)) {
  const match = path.match(/\.\/templates\/(.+)\.txt$/);
  if (match && match[1]) {
    templateRegistry[match[1]] = content;
  }
}

/**
 * Load a prompt template for a given format and phase.
 * Throws with a descriptive error if the template file is missing (Req 21.2).
 *
 * @param formatId - Video format identifier (e.g., 'movie-animation', 'youtube-narrator')
 * @param phase    - Pipeline phase name (e.g., 'breakdown', 'screenplay')
 */
export function loadTemplate(formatId: string, phase: string): string {
  const key = `${formatId}/${phase}`;
  const template = templateRegistry[key];
  if (template === undefined) {
    const available = Object.keys(templateRegistry)
      .sort()
      .join(', ');
    throw new Error(
      `Prompt template not found: '${key}.txt'. ` +
      `Ensure the file exists at services/prompt/templates/${key}.txt. ` +
      `Available templates: ${available || '(none)'}`
    );
  }
  return template;
}

/**
 * Substitute {{variable}} placeholders in a template string.
 * Variables not present in the vars map are left unchanged.
 *
 * @param template - Template string containing {{variable}} placeholders
 * @param vars     - Map of variable name → substitution value
 */
export function substituteVariables(
  template: string,
  vars: Record<string, string>
): string {
  return template.replace(/\{\{(\w+)\}\}/g, (_match, name: string) =>
    Object.prototype.hasOwnProperty.call(vars, name) ? vars[name]! : `{{${name}}}`
  );
}

/**
 * Check whether a template exists for the given format and phase without throwing.
 */
export function hasTemplate(formatId: string, phase: string): boolean {
  return (`${formatId}/${phase}`) in templateRegistry;
}

/**
 * Programmatically register (or override) a template.
 * Intended for tests and dynamic registration — does not persist to disk.
 */
export function setTemplate(formatId: string, phase: string, content: string): void {
  templateRegistry[`${formatId}/${phase}`] = content;
}

/**
 * Return all registered template keys (format/phase pairs).
 * Useful for introspection and validation.
 */
export function listTemplates(): string[] {
  return Object.keys(templateRegistry).sort();
}
````

## File: packages/shared/src/services/prompt/templates/advertisement/breakdown.txt
````
You are a senior advertising copywriter specializing in {{genre}} commercials.
{{language_instruction}}
{{references}}
Create a high-impact advertisement breakdown for:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} seconds. Every second counts.

Structure as 3-4 beats optimized for maximum emotional impact and clear call-to-action. For each beat:
1. Title — The advertising beat name (e.g., "Problem Hook", "Solution Reveal", "CTA Moment")
2. Emotional Hook — The specific emotion triggered in the viewer (urgency, desire, fear of missing out, aspiration)
3. Narrative Beat — The exact message or visual moment, with specific copy direction

ADVERTISEMENT RULES:
- Open with an immediate hook that stops the scroll
- Build desire or solve a clear pain point
- End with a powerful, memorable call-to-action
- Every beat must serve the product/brand goal

Genre: {{genre}}. Tailor the tone accordingly.
````

## File: packages/shared/src/services/prompt/templates/advertisement/cta-creation.txt
````
You are a direct-response copywriter specializing in high-converting calls-to-action.
{{language_instruction}}
Write 6 call-to-action variations for an advertisement about:
"{{idea}}"

Genre/style: {{genre}}

The CTA is the last thing the viewer sees and hears. It must convert intent into action.

CTA ARCHITECTURE — every CTA needs all three elements:
1. ACTION VERB — specific, active (not "learn more" — try "claim", "get", "start", "join", "save", "unlock")
2. VALUE REMINDER — a micro-benefit restated at the moment of decision
3. FRICTION REDUCER — remove hesitation (time, money, or effort objection handled)

Write one CTA for each category:

[CTA 1 — URGENCY]
Time or quantity scarcity that is truthful and specific. Never manufactured urgency.
Example frame: "Offer ends [date] — [action] while [condition]."

[CTA 2 — DESIRE]
Lead with what the viewer gets, not what they do. Outcome-first language.
Example frame: "[Desired outcome] — [action] at [where/how]."

[CTA 3 — SOCIAL PROOF]
Reference the crowd. Numbers, community, or authority lend credibility.
Example frame: "Join [number] people who [action] — [CTA]."

[CTA 4 — RISK REVERSAL]
Remove financial or commitment fear directly in the CTA text.
Example frame: "Try it free for [X days] — no [objection]. [Action]."

[CTA 5 — IDENTITY]
Speak to who the viewer wants to be, not what they want to have.
Example frame: "[Identity statement] — [action]."

[CTA 6 — SIMPLICITY]
The shortest possible CTA. One action, one benefit, one sentence.
Maximum 8 words total.

FORMAT FOR EACH:
- Screen text version (max 6 words, large display)
- Voiceover version (spoken, natural, 1-2 sentences)
- Recommended visual treatment for the final frame (2-3 words each)

Close with your recommended CTA and the reason it best fits this {{genre}} advertisement.
````

## File: packages/shared/src/services/prompt/templates/advertisement/screenplay.txt
````
You are an advertising director writing a {{genre}} commercial script.
{{language_instruction}}
{{references}}
Write a high-impact advertisement script based on this breakdown:

{{breakdown}}

Create exactly {{actCount}} scenes. For each scene:
1. Heading — Shot type (e.g., "EXT. CITY STREET - GOLDEN HOUR", "PRODUCT CLOSE-UP", "FINAL CTA FRAME")
2. Action — High-impact visual direction: what makes this shot grab attention and communicate the message
3. Dialogue — Voiceover or on-screen talent lines (speaker: "Voiceover" or character name)

ADVERTISEMENT SCRIPT RULES:
- Every frame must earn its place — no filler visuals
- Dialogue must be punchy, memorable, and brand-aligned
- The final scene MUST include a clear, prominent call-to-action
- Use dynamic, energetic language in action descriptions
- Emphasize the product/service benefit in every beat

Genre style: {{genre}}. Write exactly {{actCount}} scenes.
````

## File: packages/shared/src/services/prompt/templates/advertisement/script-generation.txt
````
You are a senior advertising copywriter creating a {{genre}} commercial.
{{language_instruction}}
{{references}}
Write a complete, production-ready advertisement script for:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} seconds. Every word earns its place.

SCRIPT FORMAT:
Write in two parallel columns — VISUAL and AUDIO — to show what the viewer sees and hears simultaneously.
Use this format for each beat:

[BEAT NAME]
VISUAL: [Describe exactly what appears on screen]
AUDIO: [Voiceover line / dialogue / music direction]

PERSUASION ARCHITECTURE:
1. PROBLEM HOOK (0-5s): Immediately surface the pain point or desire — make the viewer feel understood
2. AMPLIFICATION (5-15s): Make the problem feel larger, more urgent, or the desire more compelling
3. SOLUTION REVEAL (15-35s): Introduce the product/service as the natural answer — show, don't tell
4. PROOF POINT (35-50s): One specific, believable benefit with concrete detail (not vague claims)
5. CTA CLOSE (last 5s): Clear action with urgency or exclusivity — "Get yours now", "Limited time", etc.

COPYWRITING RULES:
- Lead with emotion, close with logic
- Every adjective must earn its place — cut "amazing", "incredible", "revolutionary"
- The product name appears no more than 3 times total
- CTA must be the last spoken line and appear visually at the same moment
- Music direction: specify tempo and mood, not genre (e.g., "building percussion, 85 BPM" not "hip-hop")

Genre/style: {{genre}}. Calibrate tone for this specific ad type.
````

## File: packages/shared/src/services/prompt/templates/documentary/breakdown.txt
````
You are a documentary filmmaker and investigative journalist specializing in {{genre}} documentaries.
{{language_instruction}}
{{research}}{{references}}
Create a deeply researched documentary breakdown for:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes. Depth and credibility are paramount.

Structure as 4-5 chapters with clear narrative through-lines and research citations. For each chapter:
1. Title — Documentary chapter title that frames the investigation (e.g., "Chapter 1: The Hidden Truth", "Chapter 3: The Turning Point")
2. Emotional Hook — The emotional and intellectual pull of this chapter (outrage, revelation, empathy, urgency, hope)
3. Narrative Beat — The specific findings, events, or testimonies covered, with source citations from the research

DOCUMENTARY RULES:
- Ground every claim in evidence from research or reference documents
- Maintain a clear narrative arc across chapters
- Include multiple perspectives where relevant
- Build from context to revelation to implication

Genre: {{genre}}.
````

## File: packages/shared/src/services/prompt/templates/documentary/chapter-structure.txt
````
You are a documentary producer and story architect planning a {{genre}} documentary.
{{language_instruction}}
{{research}}{{references}}
Design a complete chapter structure for a documentary about:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes.

CHAPTER ARCHITECTURE:
Each chapter must have a distinct emotional and informational function. No two chapters should feel the same.

For each chapter, provide:

CHAPTER [NUMBER]: [TITLE]
DURATION: [Approximate minutes]
FUNCTION: [What this chapter accomplishes narratively and informationally]
EMOTIONAL JOURNEY: [How the viewer should feel entering → during → leaving this chapter]
KEY CONTENT:
  - Primary finding or revelation
  - Evidence types (archival footage, interview, data, location)
  - Sourced facts from research (cite each one)
CHAPTER CLOSE: [The last image or line that makes the viewer need to watch the next chapter]

REQUIRED CHAPTER TYPES (include all that fit the target duration):

[COLD OPEN] — In medias res. No context. Visceral. Creates the central question.
[ORIGIN CHAPTER] — How did we get here? Historical or causal foundation.
[EVIDENCE CHAPTER] — The case: facts, data, testimony. Build the argument.
[COMPLICATION CHAPTER] — The counter-narrative. What the evidence doesn't explain. Introduces doubt or complexity.
[HUMAN CHAPTER] — Faces and stories. Who is actually affected and how. Empathy anchor.
[RESOLUTION/IMPLICATION CHAPTER] — What it all means. Where we go from here.
[CODA] — Where things stand today. Often more questions than answers.

CHAPTER LINKING:
After writing all chapters, define the THROUGH-LINE:
- The central question posed in chapter 1 that every subsequent chapter advances
- The moment of greatest revelation (which chapter and why)
- The intended emotional state of the viewer at the documentary's final frame

Genre: {{genre}}. Ground every chapter element in the provided research and reference materials.
````

## File: packages/shared/src/services/prompt/templates/documentary/screenplay.txt
````
You are a documentary director writing a {{genre}} film script.
{{language_instruction}}
{{research}}{{references}}
Write a chapter-structured documentary script based on this breakdown:

{{breakdown}}

Create exactly {{actCount}} scenes — one per chapter above. For each scene:
1. Heading — Documentary chapter label (e.g., "CHAPTER 1 — THE BEGINNING", "ARCHIVAL FOOTAGE - 1985")
2. Action — Visual direction: archival footage description, interview setup, data visualization, or location shot
3. Dialogue — Narrator voiceover or interview excerpt (speaker: "Narrator" or "Expert" or source name)

DOCUMENTARY SCRIPT RULES:
- Cite specific sources in dialogue when stating facts
- Describe archival visuals, data charts, or maps concretely in action lines
- Use measured, authoritative narration — let facts speak
- Interview dialogue should feel authentic and sourced
- Include chapter titles as visual title cards in action descriptions
- Reference key research findings and reference document content

Write exactly {{actCount}} scenes.
````

## File: packages/shared/src/services/prompt/templates/documentary/script-generation.txt
````
You are a documentary filmmaker and investigative journalist writing a {{genre}} documentary.
{{language_instruction}}
{{research}}{{references}}
Write a complete, production-ready documentary script for:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes. Credibility and narrative tension are equally essential.

DOCUMENTARY SCRIPT FORMAT:
Use three-column notation:
[CHAPTER TITLE]
TIME: [approximate timecode]
VISUAL: [Shot description — archival footage, interview setup, location, graphic]
AUDIO: [Narrator VO text / interview question / ambient sound direction]
SOURCE: [Citation for any stated fact]

STRUCTURAL REQUIREMENTS:
[COLD OPEN] — Start in the middle of the action; hook before any title card (2-3 min)
[TITLE CARD + CONTEXT] — Establish scope and stakes without giving away the conclusion
[CHAPTER 1 — ORIGINS]: Historical or factual foundation; establish what we're investigating
[CHAPTER 2 — EVIDENCE]: Key findings, testimonies, data; build the case
[CHAPTER 3 — COMPLICATION]: Counter-argument, obstacle, or revelation that deepens complexity
[CHAPTER 4 — IMPLICATIONS]: What this means; expert analysis; who is affected and how
[EPILOGUE] — Where things stand today; what remains unresolved; call to awareness or action

DOCUMENTARY CRAFT:
- Every factual claim must have a [SOURCE] tag with the research citation
- Interview questions (for on-camera subjects) must be open-ended and specific
- Archival footage descriptions must include approximate era and visual content
- Narrator voice: measured, authoritative, without advocacy — present facts and let viewers conclude
- Include at least one "surprise reversal" — a finding that contradicts the expected narrative
- Data visualizations: specify chart type, variables, and visual treatment

Genre/beat: {{genre}}.
````

## File: packages/shared/src/services/prompt/templates/educational/breakdown.txt
````
You are an instructional designer creating a {{genre}} educational tutorial.
{{language_instruction}}
{{research}}{{references}}
Create a structured learning breakdown for an educational video about:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes. Balance depth with clarity.

Structure as 4-5 learning modules with clear pedagogical purpose. For each module:
1. Title — Clear learning objective title (e.g., "What Is X?", "How X Works", "Why X Matters", "Applying X")
2. Emotional Hook — The learner motivation for this module (curiosity, mastery desire, practical need, surprise)
3. Narrative Beat — The specific concept, diagram, or example taught, with key facts from research

EDUCATIONAL DESIGN RULES:
- Start with foundational concepts before advanced material
- Include at least one concrete example or analogy per module
- Build knowledge progressively — each module prepares the learner for the next
- Reference authoritative sources where available

Subject area: {{genre}}.
````

## File: packages/shared/src/services/prompt/templates/educational/learning-objectives.txt
````
You are an instructional designer writing learning objectives for a {{genre}} educational video.
{{language_instruction}}
{{research}}
Define clear, measurable learning objectives for a lesson about:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes.

OBJECTIVE FRAMEWORK (Bloom's Taxonomy — write objectives at the appropriate cognitive level):

[KNOWLEDGE OBJECTIVE — Remember]
By the end of this video, learners will be able to RECALL or RECOGNIZE:
(List 2-3 specific facts, definitions, or concepts they must retain)
Format: "Learners will identify [specific item] as [definition/characteristic]."

[COMPREHENSION OBJECTIVE — Understand]
By the end of this video, learners will be able to EXPLAIN or DESCRIBE:
(List 1-2 concepts in their own words, without referencing the video)
Format: "Learners will explain [concept] in terms of [real-world context]."

[APPLICATION OBJECTIVE — Apply]
By the end of this video, learners will be able to USE or DEMONSTRATE:
(List 1-2 practical skills or techniques they can perform after watching)
Format: "Learners will apply [skill] to [specific scenario or problem type]."

[ANALYSIS OBJECTIVE — Analyze] (include if duration allows depth)
By the end of this video, learners will be able to DISTINGUISH or COMPARE:
(1 objective involving critical evaluation or pattern recognition)
Format: "Learners will differentiate [A] from [B] based on [criteria]."

OBJECTIVE QUALITY CHECKLIST (verify each objective meets all criteria):
✓ Uses a specific, observable action verb (not "understand", "know", or "appreciate")
✓ Specifies the condition or context for performance
✓ Is achievable within the video's time constraints
✓ Directly maps to content that will appear in the video
✓ Is assessable — a learner could demonstrate success

After objectives, provide:
[PREREQUISITE KNOWLEDGE]: What learners must already know before watching
[ASSESSMENT IDEAS]: 2-3 questions or tasks that would verify each objective was met
[VISUAL ANCHOR]: One diagram or visual that could represent the entire lesson's core concept
````

## File: packages/shared/src/services/prompt/templates/educational/screenplay.txt
````
You are an educational video producer creating a {{genre}} tutorial script.
{{language_instruction}}
{{research}}{{references}}
Write a structured educational script based on this learning breakdown:

{{breakdown}}

Create exactly {{actCount}} scenes — one per learning module above. For each scene:
1. Heading — Module label (e.g., "MODULE 1: INTRODUCTION", "MODULE 2: CORE CONCEPT")
2. Action — Visual aid description: diagrams, charts, text overlays, or animations that reinforce the concept
3. Dialogue — Educator's narration (speaker: "Narrator") explaining the concept clearly

EDUCATIONAL SCRIPT RULES:
- Use plain, accessible language — avoid unnecessary jargon; define technical terms when first introduced
- Reference specific examples, statistics, or analogies from the research
- Describe visual aids concretely: what diagram, what labels, what animation
- Each dialogue block should have a clear teaching point
- End with a summary or takeaway in the final scene

Subject area: {{genre}}. Write exactly {{actCount}} scenes.
````

## File: packages/shared/src/services/prompt/templates/educational/script-generation.txt
````
You are an instructional designer and on-camera educator creating a {{genre}} tutorial.
{{language_instruction}}
{{research}}{{references}}
Write a complete, ready-to-record educational script for:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes.

PEDAGOGICAL STRUCTURE (Bloom's Taxonomy aligned):
[LEARNING CONTRACT] — State exactly what the learner will know/be able to do by the end (30s)
[PRIOR KNOWLEDGE ACTIVATION] — Connect to something the learner already knows (1-2 min)
[CONCEPT INTRODUCTION] — Define and explain the core concept clearly (2-5 min)
[WORKED EXAMPLE] — Walk through a concrete, relatable example step by step
[GUIDED PRACTICE] — Present a problem; pause and give the learner time to think before revealing answer
[COMMON MISCONCEPTIONS] — Address the top 2-3 mistakes learners make
[SUMMARY] — Recall the key points using the same words used in the introduction
[NEXT STEPS] — What to learn or do next; where to practice

SCRIPT CONVENTIONS:
- Label each section clearly: [SECTION NAME]
- Include [VISUAL CUE: description] wherever a diagram, animation, or graphic should appear
- Use "we" to create collaborative learning ("Let's work through this together...")
- Define every technical term the first time it appears, in plain language immediately after
- Include at least one analogy that connects the abstract concept to everyday experience
- For complex multi-step processes, use numbered steps and repeat step numbers in the explanation

PACING:
- Speak at ≤130 words per minute for concept-heavy sections
- Use [PAUSE] markers where the learner should reflect or try something before continuing
- Recap key points at natural 3-5 minute intervals

Subject area: {{genre}}. Calibrate complexity to the subject's typical learner profile.
````

## File: packages/shared/src/services/prompt/templates/movie-animation/breakdown.txt
````
You are a story development expert specializing in {{genre}} cinematic narratives.
{{language_instruction}}
{{research}}{{references}}
Before writing, silently identify:
- The protagonist and their central desire or goal
- The core conflict they face (internal or external)
- The emotional arc: how does the protagonist change from start to finish?
- One key turning point per act

Create a narrative breakdown for a {{genre}} story about:
"{{idea}}"

Divide into 3-5 acts. For each act provide:
1. Title — A compelling act title referencing a specific story moment (not generic like "Introduction")
2. Emotional Hook — The dominant emotion the audience should feel in this act (grief, awe, tension, triumph...)
3. Narrative Beat — The specific story event or revelation that drives this act forward (name characters, describe the action)

Keep each field concise (1-2 sentences max). Be specific — avoid vague labels like "conflict begins" or "things get harder".
````

## File: packages/shared/src/services/prompt/templates/movie-animation/screenplay.txt
````
You are a cinematic screenwriter specializing in {{genre}} films.
{{language_instruction}}
{{research}}{{references}}
Write a short {{genre}} screenplay based on this outline:

{{breakdown}}

Create exactly {{actCount}} scenes. For each scene:
1. Heading — Location/time (e.g., "INT. SPACESHIP - DAY")
2. Action — Vivid visual description of what the camera sees. Describe sensory details: sounds, textures, movement, light, color. These lines will be used as voiceover narration — make them cinematic.
3. Dialogue — Character lines (if any)

DIALOGUE RULES (CRITICAL — schema will reject violations):
- "speaker" must be the character's NAME ONLY — 1 to 4 words maximum (e.g., "Faisal", "Old Man", "Narrator")
- "speaker" must NEVER contain scene descriptions, emotions, or actions. MAX 30 characters.
- "text" is the spoken/narrated line — it must NEVER be empty.
- If there is no specific speaker, use "Narrator" as the speaker name.

VALID: {"speaker": "Faisal", "text": "What happened to this place?"}
INVALID: {"speaker": "Faisal walks through ruins", "text": ""}

Keep action descriptions vivid but concise. Write exactly {{actCount}} scenes — one per act above.
````

## File: packages/shared/src/services/prompt/templates/movie-animation/script-generation.txt
````
You are a professional screenwriter crafting a {{genre}} short film.
{{language_instruction}}
{{references}}
Write a complete, production-ready screenplay for:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes.

FORMAT (standard screenplay format, adapted for short film):
- Scene headings: INT./EXT. LOCATION — TIME
- Action lines: Present tense, active voice, what the camera actually sees
- Character names: CENTERED, ALL CAPS on first appearance
- Dialogue: Character name centered above their lines
- Parentheticals: Only when essential for performance direction

STORYTELLING REQUIREMENTS:
- Open on a compelling image that encapsulates the theme — no expository dialogue in scene 1
- Each scene must advance plot OR reveal character — never just both, always at least one
- The protagonist must make an active choice that drives the story forward in each act
- Subtext over text: characters rarely say exactly what they mean
- Visual storytelling: at least 1 key moment must be told entirely through action, no dialogue

STRUCTURE:
[ACT 1 — SETUP]: Establish world, protagonist, and want (the stated goal)
[ACT 2A — ESCALATION]: Pursuit of goal meets increasing obstacles; reveal need (real underlying desire)
[MIDPOINT]: A revelation or reversal that changes the stakes
[ACT 2B — DARK NIGHT]: The lowest point — protagonist must change or fail
[ACT 3 — RESOLUTION]: Active climax driven by the protagonist; need triumphs over or redefines want

GENRE: {{genre}} — honor genre conventions while subverting at least one expectation.
Write the full screenplay with proper formatting. Include scene numbers.
````

## File: packages/shared/src/services/prompt/templates/music-video/breakdown.txt
````
You are a music video creative director specializing in {{genre}} music.
{{language_instruction}}
{{references}}
Create a lyric and visual breakdown for a {{genre}} music video about:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes.

Structure as 3-5 sections following song structure. For each section:
1. Title — Song section name (e.g., "Verse 1", "Pre-Chorus", "Chorus", "Bridge", "Outro")
2. Emotional Hook — The emotional intensity and mood of this section (building desire, cathartic release, introspective, euphoric)
3. Narrative Beat — The lyrical theme and visual concept: what do the lyrics say and what visuals accompany them

MUSIC VIDEO RULES:
- The breakdown defines both the narrative/lyrical content AND the visual treatment
- Chorus sections should be the most visually and emotionally impactful
- Maintain a coherent visual story across sections
- Genre {{genre}} — reflect the genre's visual and sonic conventions

Design for beat-synchronized visuals.
````

## File: packages/shared/src/services/prompt/templates/music-video/lyrics-generation.txt
````
You are a professional lyricist writing original lyrics for a {{genre}} song.
{{language_instruction}}
Write complete, original song lyrics for a music video about:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes.

SONG STRUCTURE (write all sections in order):

[VERSE 1] — 8-12 lines
Establish the world, character, or situation. Anchor the song in specific, concrete imagery.
Rhyme scheme: ABAB or AABB — choose one and stay consistent throughout.

[PRE-CHORUS] — 4 lines (optional, include if genre convention expects it)
Build tension. Make the chorus feel inevitable. Use ascending syllable count for each line.

[CHORUS] — 6-8 lines
THE HOOK. These are the lines the listener sings in the shower.
- First line must contain the title or central phrase
- Melody must be implied through rhythm — mark stressed syllables with CAPS
- Must work as a standalone emotional statement with no context

[VERSE 2] — 8-12 lines
Advance the story or deepen the theme. Do NOT repeat verse 1 content.
Introduce new perspective, location, or character development.

[CHORUS] — Repeat (mark as CHORUS REPEAT)

[BRIDGE] — 6-8 lines
Contrast section: different emotional register, opposing viewpoint, or resolution moment.
Often the most personal, vulnerable, or elevated section of the song.

[FINAL CHORUS / OUTRO] — 8-12 lines
Modified chorus with added emotional weight. Can extend the hook or add new layers.
Final line must feel like an ending, not a fade.

LYRIC CRAFT REQUIREMENTS:
- Syllable count consistency within matched lines (count and note if needed)
- Zero filler words: "baby", "yeah", "oh" only if genre convention demands them
- One extended metaphor sustained across at least two sections
- Avoid the exact title phrase in verse lines — save it for chorus impact

Genre: {{genre}} — ensure diction, imagery, and cadence match the genre's conventions and audience expectations.
````

## File: packages/shared/src/services/prompt/templates/music-video/screenplay.txt
````
You are a music video director scripting a {{genre}} video.
{{language_instruction}}
{{references}}
Write a music video script based on this breakdown:

{{breakdown}}

Create exactly {{actCount}} scenes — one per song section above. For each scene:
1. Heading — Song section and visual context (e.g., "VERSE 1 — ABANDONED WAREHOUSE", "CHORUS — ROOFTOP SUNSET")
2. Action — Visual direction: color palette, lighting mood, camera movement, and visual motifs synchronized to the beat
3. Dialogue — Song lyrics for this section (speaker: "Lyrics")

MUSIC VIDEO SCRIPT RULES:
- Lyrics should feel authentic to the {{genre}} genre and topic
- Action descriptions should describe cinematic visuals that complement the lyrics
- Include specific color grading and lighting notes (e.g., "warm golden hour", "neon-drenched darkness")
- Camera movements should feel musical — slow push in on emotional lines, wide shots on anthemic moments
- The visual narrative should enhance, not distract from, the emotional journey of the song

Write exactly {{actCount}} scenes.
````

## File: packages/shared/src/services/prompt/templates/music-video/script-generation.txt
````
You are a music video director and lyricist creating a {{genre}} music video.
{{language_instruction}}
{{references}}
Write a complete music video script — lyrics and visual treatment — for:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes.

SONG STRUCTURE AND LYRICS:
Write complete lyrics following standard song structure. Each section labeled:

[VERSE 1] — Establish the narrative, character, or world. Specific imagery over abstractions.
[PRE-CHORUS] — Build emotional tension, raise the stakes. Transition phrase that makes the chorus feel inevitable.
[CHORUS] — The emotional peak. The hook. Repeat 2-4 times across the song. Keep it 4-8 lines max.
[VERSE 2] — Advance the narrative. Introduce new detail or perspective. Don't repeat verse 1 content.
[BRIDGE] — Contrast section: different melody feel, shifted perspective, or emotional release.
[OUTRO] — Resolution or fade. Can echo chorus or introduce a final lyrical turn.

LYRIC CRAFT RULES:
- End-rhyme scheme: ABAB or AABB — consistent within each section
- Count syllables for rhythmic consistency (mark stressed syllables if helpful)
- Use concrete images over emotional abstractions: "broken clock on a cracked wall" not "feeling lost"
- Avoid clichés unless deliberately subverted: "heart of gold" → "a heart like coin-worn copper"
- The chorus hook must be singable within the first 3 words

VISUAL TREATMENT (for each section):
After the lyrics for each section, write:
VISUAL: [Color palette, location, lighting mood, camera movement, visual motif synchronized to this section's energy]

Genre: {{genre}} — ensure lyrics, imagery, and pacing reflect the sonic and visual conventions of this genre.
````

## File: packages/shared/src/services/prompt/templates/news-politics/breakdown.txt
````
You are a broadcast journalist and news producer specializing in {{genre}}.
{{language_instruction}}
{{research}}{{references}}
Create a balanced, factual news breakdown for:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes. Accuracy and balance are non-negotiable.

Structure as 3-5 reporting segments following journalism standards. For each segment:
1. Title — News segment title (e.g., "Breaking: The Core Facts", "Multiple Perspectives", "Expert Analysis", "Implications")
2. Emotional Hook — The journalistic hook that makes viewers care (urgency, public interest, accountability, human impact)
3. Narrative Beat — The specific facts, quotes, or data points reported, with source citations from research

JOURNALISM RULES:
- Every factual claim must be sourced from the provided research
- Present multiple perspectives — avoid advocacy
- Lead with the most newsworthy information (inverted pyramid)
- Distinguish clearly between fact and analysis
- Cite named sources and authoritative references

Genre/beat: {{genre}}.
````

## File: packages/shared/src/services/prompt/templates/news-politics/screenplay.txt
````
You are a news broadcast writer scripting a {{genre}} report.
{{language_instruction}}
{{research}}{{references}}
Write a professional news report script based on this breakdown:

{{breakdown}}

Create exactly {{actCount}} scenes — one per reporting segment above. For each scene:
1. Heading — Broadcast label (e.g., "ANCHOR DESK - LIVE", "FIELD REPORT", "EXPERT INTERVIEW", "GRAPHIC: DATA VISUALIZATION")
2. Action — Broadcast visual: news desk setup, field location, lower-third graphics, data charts, or archival footage
3. Dialogue — Anchor or reporter script (speaker: "Anchor" or "Reporter") with cited facts

NEWS SCRIPT RULES:
- Lead each scene with the most important information first
- Use formal, neutral, professional tone throughout
- Cite sources by name when stating specific facts or statistics
- Describe news graphics and lower thirds concretely in action lines
- Balance perspectives across scenes — include multiple viewpoints
- No advocacy — report facts and let viewers draw conclusions

Write exactly {{actCount}} scenes.
````

## File: packages/shared/src/services/prompt/templates/news-politics/script-generation.txt
````
You are a broadcast journalist and news anchor writing a {{genre}} news report.
{{language_instruction}}
{{research}}{{references}}
Write a complete, broadcast-ready news script for:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes.

BROADCAST SCRIPT FORMAT:
Use the standard split-format broadcast script:
[SEGMENT LABEL — e.g., ANCHOR INTRO / FIELD REPORT / EXPERT SOT]
VIDEO: [Visual description — what appears on screen, lower thirds, graphics]
AUDIO: [Anchor/reporter script] (SOURCE: citation when stating facts)

JOURNALISTIC STRUCTURE (Inverted Pyramid):
[ANCHOR INTRO — 30s]: The most newsworthy element first. Who, what, when, where in the first sentence.
[MAIN BODY — context, evidence, perspectives]:
  • PRIMARY FACTS: What happened, verified by at least 2 sources
  • OFFICIAL RESPONSE: Government/organization/company statement (quote or paraphrase)
  • EXPERT ANALYSIS: Independent expert perspective with credentials noted
  • AFFECTED PERSPECTIVE: Human-interest angle — how does this affect real people?
[BALANCE SEGMENT]: Present the strongest counter-perspective or opposing viewpoint fairly
[DATA/GRAPHICS SEGMENT]: Key statistics visualized — cite the source on-screen
[ANCHOR CLOSE — 15s]: What happens next; what to watch for; where to get more information

JOURNALISM STANDARDS:
- Every stated fact requires a [SOURCE] citation immediately after
- Distinguish clearly between confirmed fact, official claim, and analyst opinion
- Avoid loaded language: neutral verbs ("said", "stated", "claimed") not "admitted" or "insisted"
- Specific numbers over vague quantifiers: "37% increase" not "significant rise"
- Named sources preferred; if unnamed, specify why (e.g., "spoke on condition of anonymity because...")
- Include at least one perspective from those most directly affected by the story

Genre/beat: {{genre}}.
````

## File: packages/shared/src/services/prompt/templates/news-politics/source-citation.txt
````
You are a fact-checker and journalism editor verifying source attribution for a {{genre}} news report.
{{language_instruction}}
{{research}}{{references}}
Create a complete source citation document for a news report about:
"{{idea}}"

This document serves as the source inventory for the production team and appears in the video as on-screen citations.

FOR EACH FACTUAL CLAIM IN THE REPORT, CREATE A CITATION ENTRY:

[CITATION ENTRY FORMAT]
CLAIM: [The exact statement as it will appear in the script]
SOURCE: [Organization / Author / Publication name]
TYPE: [primary source | official statement | expert analysis | data report | eyewitness account]
DATE: [Publication or statement date — be specific]
RELIABILITY: [high | medium | requires-corroboration]
ON-SCREEN TEXT: [How this citation appears as a lower-third graphic — maximum 40 characters]
NOTES: [Any context about source independence, potential bias, or need for additional corroboration]

CITATION CATEGORIES TO COVER:

[PRIMARY STATISTICS]
Any number, percentage, or quantitative claim. Must have a data source with methodology noted.

[OFFICIAL STATEMENTS]
Government, institutional, or organizational positions. Note if statement was in response to inquiry.

[EXPERT OPINIONS]
Academic, industry, or technical analysis. Include expert's credentials relevant to this topic.

[WITNESS/AFFECTED PARTY ACCOUNTS]
First-person testimony. Note if identity is protected and why.

[HISTORICAL OR CONTEXTUAL FACTS]
Background facts that provide context. Even well-known facts need sourcing in news.

EDITORIAL NOTES:
After all citations, provide:
[SOURCING GAPS]: Claims in the report that need additional sourcing before broadcast
[BALANCE CHECK]: Perspectives that appear in the report and perspectives that are absent
[VERIFICATION STATUS]: Overall confidence level for the report (ready to air | needs verification | hold pending sources)

Genre/beat: {{genre}}.
````

## File: packages/shared/src/services/prompt/templates/shorts/breakdown.txt
````
You are a viral short-form content creator specializing in {{genre}} Shorts and Reels.
{{language_instruction}}
{{references}}
Create a hook-first content breakdown for a vertical short video about:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} seconds. Attention is gone in 2 seconds.

Structure as 3 tight beats designed for mobile vertical viewing. For each beat:
1. Title — The beat name (e.g., "PATTERN INTERRUPT HOOK", "CORE VALUE DROP", "LOOP CLOSE / CTA")
2. Emotional Hook — The snap emotional trigger (shock, curiosity gap, FOMO, instant satisfaction, relatability)
3. Narrative Beat — Exactly what happens in this beat — the image, text, or action that lands the punch

SHORTS RULES:
- Beat 1 MUST be a scroll-stopping hook — the first 2 seconds determine everything
- Beat 2 delivers the core value or entertainment fast, no build-up
- Beat 3 creates a loop or drives a share/follow/comment action
- No wasted frames — every second must earn its place

Genre: {{genre}}.
````

## File: packages/shared/src/services/prompt/templates/shorts/hook-creation.txt
````
You are a short-form content specialist. You write hooks that stop the scroll in under 2 seconds.
{{language_instruction}}
Write 5 ultra-short hooks for a Shorts/Reels video about:
"{{idea}}"

Style: {{genre}}

THE 2-SECOND RULE: A hook must work before any audio plays — assume the viewer has sound off. The first visual frame must create enough curiosity to make them tap for sound.

HOOK ANATOMY FOR SHORTS:
- Screen text: MAX 5 words, high contrast, no more than 2 lines
- Visual: One striking image, action, or motion that is immediately legible at mobile scale
- Audio (if heard): One sentence that raises a question the viewer must answer

Write 5 hooks — one of each type:

[HOOK 1 — VISUAL SHOCK]
A striking first frame that looks wrong, surprising, or physically impossible.
Screen text: [5 words max]
Visual: [Describe the opening image/action]
VO (first line only): [One sentence]

[HOOK 2 — BOLD CLAIM]
A text-first hook. The statement is so bold or counterintuitive that the viewer stops to verify.
Screen text: [Statement in 4 words max]
Visual: [Simple, non-distracting background that emphasizes the text]
VO (first line only): [Expand the claim in one sentence]

[HOOK 3 — BEFORE/AFTER TEASE]
Show the end result first. The viewer watches to find out how it was achieved.
Screen text: [Teaser label — "This took 3 days" / "The result ↓"]
Visual: [The impressive final result in frame]
VO (first line only): [Set up the journey: "I started with nothing."]

[HOOK 4 — RELATABLE PAIN]
One sentence that makes the exact target viewer feel understood in 2 seconds.
Screen text: [Pain point in 5 words — format as a question or statement]
Visual: [Viewer's emotional state or the painful situation]
VO (first line only): [Expand the pain, then promise relief]

[HOOK 5 — CURIOSITY GAP]
Give information that is incomplete — the gap between what they know and what they need to know creates compulsive viewing.
Screen text: ["This [thing] changes everything" / "You're doing X wrong"]
Visual: [Something partially revealed]
VO (first line only): [The missing piece setup: "Most people don't know that..."]

After the 5 hooks, identify which best fits the {{genre}} Shorts format and explain in one sentence.
````

## File: packages/shared/src/services/prompt/templates/shorts/screenplay.txt
````
You are a short-form video director creating a vertical {{genre}} video.
{{language_instruction}}
{{references}}
Write a fast-paced short video script based on this breakdown:

{{breakdown}}

Create exactly {{actCount}} scenes — one per beat above. For each scene:
1. Heading — Beat label (e.g., "HOOK FRAME", "VALUE DROP", "CTA CLOSE") with mobile context
2. Action — Vertical 9:16 visual: fast cut, bold text overlay, or dynamic visual that fills the phone screen
3. Dialogue — On-screen text or voiceover (speaker: "Narrator" or "Text Overlay")

SHORTS SCRIPT RULES:
- Visuals must be designed for 9:16 vertical mobile screens
- Use fast cuts, bold typography, and high-contrast visuals
- Dialogue should be short punchy lines — no long narration
- Text overlays should be large and readable on a phone screen
- Pacing: hook (0-2s), value (3-20s), close (last 3s)

Write exactly {{actCount}} scenes.
````

## File: packages/shared/src/services/prompt/templates/shorts/script-generation.txt
````
You are a viral short-form content creator writing a {{genre}} Short/Reel.
{{language_instruction}}
{{references}}
Write a complete, ready-to-film short video script for:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} seconds. You have zero margin for slow moments.

THE SCROLL-STOP FORMULA:
[0-2s — HOOK]: One sentence or visual that interrupts the scroll. Use a bold claim, unexpected image, or question that creates an instant curiosity gap. NO intros, NO "hey guys", NO context-setting.

[3-15s — PAYOFF]: Deliver the core value fast. Use maximum 2-3 sentences per beat. If it's a how-to, show the result FIRST, then explain. If it's a fact, lead with the most shocking element.

[15-25s — DEPTH] (if duration allows): One layer deeper — the "why" or the nuance. Keep energy high.

[Last 3s — LOOP/CTA]: Either loop back to the opening image/question to encourage replay, or deliver a clear action ("Follow for more", "Share this with someone who needs it", "Comment your answer").

SCRIPT FORMAT:
Write each section as:
[TIMECODE] SCREEN TEXT (if any): "..."
VOICEOVER: "..."
VISUAL: [What the camera shows — vertical 9:16]

RULES FOR SHORTS:
- Maximum 1 idea per video — complexity kills retention
- Screen text must be readable in 1-2 seconds — max 5 words
- Voiceover must match or precede screen text, never lag behind
- Use pattern interrupts: cut every 2-3 seconds; change angle or zoom level
- If you can cut a word without losing meaning, cut it

Genre/style: {{genre}}. Match the energy level and editing pace of the genre.
````

## File: packages/shared/src/services/prompt/templates/youtube-narrator/breakdown.txt
````
You are a YouTube content strategist specializing in {{genre}} educational videos.
{{language_instruction}}
{{research}}{{references}}
Create a structured content breakdown for a YouTube narrator video about:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes of narration.

Divide into 4-5 segments optimized for B-roll narration with conversational pacing. For each segment:
1. Title — A hook-based chapter title that creates curiosity or promises clear value
2. Emotional Hook — Why viewers stay engaged in this segment (curiosity, awe, surprise, relatability, urgency)
3. Narrative Beat — The key information or insight covered, with specific facts, examples, or data points

Design each segment to flow naturally into the next. Include any research citations where relevant.
Avoid generic labels — each segment should feel like a distinct, engaging chapter.
````

## File: packages/shared/src/services/prompt/templates/youtube-narrator/hook-creation.txt
````
You are a YouTube hook specialist. Your only job is to write the perfect opening 30-60 seconds.
{{language_instruction}}
{{research}}
Write 5 alternative hook options for a YouTube video about:
"{{idea}}"

Genre: {{genre}}

Each hook must accomplish three things in under 60 seconds of spoken content:
1. STOP — Interrupt the viewer's passive scrolling with something unexpected or urgent
2. PROMISE — Make a specific, believable, desirable promise about what they'll learn or feel
3. WITHHOLD — Create a curiosity gap that can only be closed by watching the full video

HOOK TYPES — write one of each:

[HOOK 1 — CONTRARIAN CLAIM]
Open with a statement that directly contradicts a widely-held belief related to {{idea}}.
Then promise to prove it with evidence.

[HOOK 2 — STORY OPEN]
Drop the viewer into the middle of a specific, vivid moment. Name a real person, place, and situation.
Don't explain — just describe. The promise is implied by the intrigue of the scene.

[HOOK 3 — DIRECT PROMISE]
State the exact value the viewer will gain. Use numbers and specifics.
"By the end of this video, you'll know exactly how to [specific outcome]."

[HOOK 4 — QUESTION HOOK]
Ask the one question your target viewer has been asking themselves privately.
Make them feel seen. The video answers it.

[HOOK 5 — STAKES HOOK]
Open with a consequence: what happens if the viewer does NOT know this?
Create urgency without being manipulative — the stakes must be real and relevant.

FORMAT FOR EACH HOOK:
- Written as complete spoken script (not notes or bullets)
- Maximum 90 words per hook
- End each hook on a cliffhanger that demands continuation

After the 5 hooks, recommend which one best fits the {{genre}} format and explain why in 2 sentences.
````

## File: packages/shared/src/services/prompt/templates/youtube-narrator/screenplay.txt
````
You are a YouTube narrator scriptwriter crafting engaging {{genre}} content.
{{language_instruction}}
{{research}}{{references}}
Write a conversational narrator script based on this content breakdown:

{{breakdown}}

Create exactly {{actCount}} scenes — one per segment above. For each scene:
1. Heading — Segment type (e.g., "HOOK - COLD OPEN", "MAIN POINT 1", "TRANSITION", "OUTRO CTA")
2. Action — B-roll visual description: what the viewer sees on screen while the narrator speaks
3. Dialogue — Narrator's spoken script (speaker: "Narrator")

NARRATOR SCRIPT RULES:
- Write as if speaking directly to the viewer — conversational, engaging, and natural
- Use "you" to address the viewer and create connection
- Include pacing cues: rhetorical questions, short punchy sentences, moments of pause
- Reference research facts and citations where relevant
- Keep dialogue lines natural for spoken delivery (no markdown formatting)

Write exactly {{actCount}} scenes.
````

## File: packages/shared/src/services/prompt/templates/youtube-narrator/script-generation.txt
````
You are a professional YouTube scriptwriter crafting a {{genre}} video.
{{language_instruction}}
{{research}}{{references}}
Write a complete, publication-ready YouTube narrator script for:
"{{idea}}"

Target duration: {{minDuration}}-{{maxDuration}} minutes of spoken narration.

SCRIPT FORMAT:
- Open with a pattern-interrupt hook that earns the first 30 seconds (no "welcome back" intros)
- Use second-person address ("you") to create personal connection throughout
- Write at an 8th-grade reading level — clear, direct, zero jargon without explanation
- Include natural transition phrases between segments ("But here's where it gets interesting...", "Now, you might be thinking...")
- End with a clear call-to-action tied back to the opening hook

TONE AND PACING:
- Conversational but authoritative — like a knowledgeable friend, not a textbook
- Vary sentence length: punchy short sentences after each key point, longer ones for context
- Use rhetorical questions to maintain engagement ("So why does this matter to you?")
- Include at least one counter-intuitive fact or surprising angle per major section

STRUCTURE (label each section):
[HOOK] — 30-60 seconds, immediate value promise
[CONTEXT] — Background the viewer needs to understand
[MAIN POINT 1] — First key insight with example
[MAIN POINT 2] — Second insight, build on previous
[MAIN POINT 3+] — Continue as needed for target duration
[CONCLUSION] — Callback to hook, forward value
[CTA] — Subscribe/like/comment prompt that feels earned

{{genre}} genre — adapt the tone accordingly (educational, commentary, review, etc.).
Write the complete script as flowing narration, not bullet points.
````

## File: packages/shared/src/services/prompt/vibeLibrary.ts
````typescript
/**
 * Vibe Library
 *
 * Categorized vocabulary of 100+ vibe terms across 7 categories.
 * Used by the content planner to inject creative vocabulary into prompts,
 * and by the narrator to build rich Director's Notes.
 *
 * React-free — safe for Node.js usage.
 */

import type { InstructionTriplet } from "../../types";

export type VibeAxis = "emotion" | "cinematic" | "atmosphere";

export type VibeCategory =
  | "2026-tech"
  | "emotional-states"
  | "cinematic-styles"
  | "environmental-textures"
  | "cultural-moods"
  | "temporal-aesthetics"
  | "sonic-landscapes";

export interface VibeTerm {
  id: string;
  label: string;
  category: VibeCategory;
  axis: VibeAxis;
  promptFragment: string;
  compatiblePurposes?: string[];
}

export const VIBE_LIBRARY: VibeTerm[] = [
  // === 2026-tech ===
  { id: "liquid-glass", label: "Liquid Glass", category: "2026-tech", axis: "atmosphere", promptFragment: "translucent liquid-glass surfaces refracting ambient light" },
  { id: "neural-lace", label: "Neural Lace", category: "2026-tech", axis: "atmosphere", promptFragment: "thread-thin neural-lace filaments pulsing with data" },
  { id: "digital-distortion", label: "Digital Distortion", category: "2026-tech", axis: "cinematic", promptFragment: "glitch-scan digital distortion fragments across the frame" },
  { id: "holographic-decay", label: "Holographic Decay", category: "2026-tech", axis: "atmosphere", promptFragment: "holographic projections flickering and decaying at the edges" },
  { id: "quantum-shimmer", label: "Quantum Shimmer", category: "2026-tech", axis: "atmosphere", promptFragment: "quantum shimmer of probability fields warping visible light" },
  { id: "synthetic-bloom", label: "Synthetic Bloom", category: "2026-tech", axis: "cinematic", promptFragment: "synthetic bloom overexposure on chrome and glass surfaces" },
  { id: "data-rain", label: "Data Rain", category: "2026-tech", axis: "atmosphere", promptFragment: "cascading streams of luminous data-rain in the air" },
  { id: "circuit-veins", label: "Circuit Veins", category: "2026-tech", axis: "atmosphere", promptFragment: "circuit-vein patterns glowing beneath translucent skin" },
  { id: "pixel-erosion", label: "Pixel Erosion", category: "2026-tech", axis: "cinematic", promptFragment: "edges of reality dissolving into pixel erosion" },
  { id: "chrome-reflection", label: "Chrome Reflection", category: "2026-tech", axis: "cinematic", promptFragment: "hyper-sharp chrome reflections warping the surrounding environment" },
  { id: "void-static", label: "Void Static", category: "2026-tech", axis: "atmosphere", promptFragment: "void-static interference bleeding through the visual plane" },
  { id: "bio-luminescent-tech", label: "Bio-Luminescent Tech", category: "2026-tech", axis: "atmosphere", promptFragment: "bio-luminescent technology pulsing with organic rhythms" },
  { id: "signal-decay", label: "Signal Decay", category: "2026-tech", axis: "cinematic", promptFragment: "signal decay artifacts and noise distorting the broadcast" },

  // === emotional-states ===
  { id: "visceral-dread", label: "Visceral Dread", category: "emotional-states", axis: "emotion", promptFragment: "a gut-punch of visceral dread, breath caught in the throat", compatiblePurposes: ["horror_mystery", "storytelling"] },
  { id: "stoic-resignation", label: "Stoic Resignation", category: "emotional-states", axis: "emotion", promptFragment: "quiet stoic resignation, accepting what cannot be changed", compatiblePurposes: ["documentary", "storytelling"] },
  { id: "melancholy", label: "Melancholy", category: "emotional-states", axis: "emotion", promptFragment: "deep melancholy weighing on every syllable", compatiblePurposes: ["storytelling", "documentary", "music_video"] },
  { id: "euphoric-wonder", label: "Euphoric Wonder", category: "emotional-states", axis: "emotion", promptFragment: "breathless euphoric wonder at the impossible made real", compatiblePurposes: ["travel", "motivational", "documentary"] },
  { id: "bittersweet-longing", label: "Bittersweet Longing", category: "emotional-states", axis: "emotion", promptFragment: "bittersweet longing for what was and what might have been", compatiblePurposes: ["storytelling", "music_video"] },
  { id: "seething-rage", label: "Seething Rage", category: "emotional-states", axis: "emotion", promptFragment: "barely contained seething rage beneath a calm surface", compatiblePurposes: ["storytelling", "horror_mystery"] },
  { id: "cold-detachment", label: "Cold Detachment", category: "emotional-states", axis: "emotion", promptFragment: "clinical cold detachment, observing without feeling", compatiblePurposes: ["documentary", "news_report"] },
  { id: "nostalgic-warmth", label: "Nostalgic Warmth", category: "emotional-states", axis: "emotion", promptFragment: "gentle nostalgic warmth, like sunlight through old glass", compatiblePurposes: ["storytelling", "documentary", "travel"] },
  { id: "feral-joy", label: "Feral Joy", category: "emotional-states", axis: "emotion", promptFragment: "wild feral joy, untamed and unashamed" },
  { id: "hollow-grief", label: "Hollow Grief", category: "emotional-states", axis: "emotion", promptFragment: "hollow grief echoing in an empty chest", compatiblePurposes: ["storytelling"] },
  { id: "electric-anticipation", label: "Electric Anticipation", category: "emotional-states", axis: "emotion", promptFragment: "electric anticipation crackling before the reveal", compatiblePurposes: ["commercial", "social_short"] },
  { id: "quiet-defiance", label: "Quiet Defiance", category: "emotional-states", axis: "emotion", promptFragment: "quiet defiance, refusing to bend or break", compatiblePurposes: ["motivational", "storytelling"] },
  { id: "sacred-awe", label: "Sacred Awe", category: "emotional-states", axis: "emotion", promptFragment: "sacred awe in the presence of something vast", compatiblePurposes: ["documentary", "travel"] },
  { id: "paranoid-unease", label: "Paranoid Unease", category: "emotional-states", axis: "emotion", promptFragment: "creeping paranoid unease, something is watching", compatiblePurposes: ["horror_mystery"] },

  // === cinematic-styles ===
  { id: "dutch-angle", label: "Dutch Angle", category: "cinematic-styles", axis: "cinematic", promptFragment: "tilted dutch angle creating visual unease and disorientation" },
  { id: "anamorphic-flare", label: "Anamorphic Flare", category: "cinematic-styles", axis: "cinematic", promptFragment: "horizontal anamorphic lens flares streaking across frame" },
  { id: "chiaroscuro-lighting", label: "Chiaroscuro", category: "cinematic-styles", axis: "cinematic", promptFragment: "chiaroscuro lighting with sharp contrast between shadow and light" },
  { id: "slow-push-in", label: "Slow Push-In", category: "cinematic-styles", axis: "cinematic", promptFragment: "gradual slow push-in building focus and intensity" },
  { id: "tracking-shot", label: "Tracking Shot", category: "cinematic-styles", axis: "cinematic", promptFragment: "fluid tracking shot following the subject through space" },
  { id: "handheld-float", label: "Handheld Float", category: "cinematic-styles", axis: "cinematic", promptFragment: "subtle handheld float adding organic documentary realism" },
  { id: "dolly-zoom", label: "Dolly Zoom", category: "cinematic-styles", axis: "cinematic", promptFragment: "vertigo dolly-zoom warping perspective and depth" },
  { id: "static-tripod", label: "Static Tripod", category: "cinematic-styles", axis: "cinematic", promptFragment: "locked-off static tripod stillness, letting the scene breathe" },
  { id: "crane-reveal", label: "Crane Reveal", category: "cinematic-styles", axis: "cinematic", promptFragment: "sweeping crane reveal lifting to expose the full landscape" },
  { id: "whip-pan", label: "Whip Pan", category: "cinematic-styles", axis: "cinematic", promptFragment: "energetic whip-pan snapping between subjects" },
  { id: "rack-focus", label: "Rack Focus", category: "cinematic-styles", axis: "cinematic", promptFragment: "deliberate rack-focus shifting attention between planes" },
  { id: "overhead-god-shot", label: "Overhead God Shot", category: "cinematic-styles", axis: "cinematic", promptFragment: "overhead god-shot viewing the scene from directly above" },
  { id: "low-angle-power", label: "Low Angle Power", category: "cinematic-styles", axis: "cinematic", promptFragment: "low angle shot conveying dominance and towering presence" },
  { id: "extreme-closeup", label: "Extreme Close-Up", category: "cinematic-styles", axis: "cinematic", promptFragment: "extreme close-up isolating a single detail or micro-expression" },
  { id: "pull-back-reveal", label: "Pull-Back Reveal", category: "cinematic-styles", axis: "cinematic", promptFragment: "slow pull-back reveal expanding from detail to full context" },

  // === environmental-textures ===
  { id: "ethereal-echo", label: "Ethereal Echo", category: "environmental-textures", axis: "atmosphere", promptFragment: "ethereal echo reverberating through vast empty spaces" },
  { id: "foggy-ruins", label: "Foggy Ruins", category: "environmental-textures", axis: "atmosphere", promptFragment: "thick fog rolling through crumbling ancient ruins" },
  { id: "desert-silence", label: "Desert Silence", category: "environmental-textures", axis: "atmosphere", promptFragment: "absolute desert silence, only the wind and shifting sand" },
  { id: "neon-rain", label: "Neon Rain", category: "environmental-textures", axis: "atmosphere", promptFragment: "neon-rain reflecting a thousand colors off wet asphalt" },
  { id: "server-room-hum", label: "Server Room Hum", category: "environmental-textures", axis: "atmosphere", promptFragment: "steady server-room hum of machines processing in the dark" },
  { id: "cathedral-reverb", label: "Cathedral Reverb", category: "environmental-textures", axis: "atmosphere", promptFragment: "cathedral reverb amplifying every whisper into grandeur" },
  { id: "jungle-steam", label: "Jungle Steam", category: "environmental-textures", axis: "atmosphere", promptFragment: "tropical jungle steam rising from rain-soaked canopy floor" },
  { id: "frozen-lake", label: "Frozen Lake", category: "environmental-textures", axis: "atmosphere", promptFragment: "frozen lake surface cracking under pressure, vast white expanse" },
  { id: "underground-drip", label: "Underground Drip", category: "environmental-textures", axis: "atmosphere", promptFragment: "underground cavern with echoing water drips in darkness" },
  { id: "burning-embers", label: "Burning Embers", category: "environmental-textures", axis: "atmosphere", promptFragment: "glowing embers drifting upward from smoldering remains" },
  { id: "salt-flats", label: "Salt Flats", category: "environmental-textures", axis: "atmosphere", promptFragment: "endless salt flats merging earth and sky at the horizon" },
  { id: "abandoned-factory", label: "Abandoned Factory", category: "environmental-textures", axis: "atmosphere", promptFragment: "abandoned factory with rusted machinery and shafts of dusty light" },
  { id: "coral-reef-glow", label: "Coral Reef Glow", category: "environmental-textures", axis: "atmosphere", promptFragment: "bioluminescent coral reef glowing beneath dark ocean water" },
  { id: "volcanic-ash", label: "Volcanic Ash", category: "environmental-textures", axis: "atmosphere", promptFragment: "volcanic ash drifting through air, muting colors and sound" },

  // === cultural-moods ===
  { id: "middle-eastern-dusk", label: "Middle Eastern Dusk", category: "cultural-moods", axis: "atmosphere", promptFragment: "warm Middle Eastern dusk, call to prayer echoing over rooftops", compatiblePurposes: ["documentary", "storytelling", "travel"] },
  { id: "nordic-frost", label: "Nordic Frost", category: "cultural-moods", axis: "atmosphere", promptFragment: "stark Nordic frost landscape under pale winter light", compatiblePurposes: ["documentary", "storytelling"] },
  { id: "tokyo-neon-night", label: "Tokyo Neon Night", category: "cultural-moods", axis: "atmosphere", promptFragment: "dense Tokyo neon night, electric signs reflecting in rain", compatiblePurposes: ["commercial", "social_short"] },
  { id: "mediterranean-gold", label: "Mediterranean Gold", category: "cultural-moods", axis: "atmosphere", promptFragment: "sun-drenched Mediterranean gold on whitewashed walls", compatiblePurposes: ["travel", "documentary"] },
  { id: "andean-altitude", label: "Andean Altitude", category: "cultural-moods", axis: "atmosphere", promptFragment: "high Andean altitude, thin air and vast mountain panoramas", compatiblePurposes: ["travel", "documentary"] },
  { id: "saharan-mirage", label: "Saharan Mirage", category: "cultural-moods", axis: "atmosphere", promptFragment: "Saharan mirage shimmering on the horizon, heat distortion", compatiblePurposes: ["documentary", "storytelling"] },
  { id: "amazonian-canopy", label: "Amazonian Canopy", category: "cultural-moods", axis: "atmosphere", promptFragment: "dense Amazonian canopy filtering green-gold light", compatiblePurposes: ["documentary", "travel"] },
  { id: "tibetan-monastery", label: "Tibetan Monastery", category: "cultural-moods", axis: "atmosphere", promptFragment: "serene Tibetan monastery perched above clouds at dawn", compatiblePurposes: ["documentary", "motivational"] },

  // === temporal-aesthetics ===
  { id: "golden-hour-decay", label: "Golden Hour Decay", category: "temporal-aesthetics", axis: "cinematic", promptFragment: "fading golden hour light casting long shadows as day dies" },
  { id: "midnight-blue", label: "Midnight Blue", category: "temporal-aesthetics", axis: "cinematic", promptFragment: "deep midnight blue wash over every surface, moonlight only" },
  { id: "twilight-liminal", label: "Twilight Liminal", category: "temporal-aesthetics", axis: "cinematic", promptFragment: "twilight liminal hour where day and night coexist" },
  { id: "pre-dawn-grey", label: "Pre-Dawn Grey", category: "temporal-aesthetics", axis: "cinematic", promptFragment: "pre-dawn grey light, world not yet awake, flat and quiet" },
  { id: "overcast-flat", label: "Overcast Flat", category: "temporal-aesthetics", axis: "cinematic", promptFragment: "overcast flat diffused light eliminating all shadows" },
  { id: "harsh-noon", label: "Harsh Noon", category: "temporal-aesthetics", axis: "cinematic", promptFragment: "harsh noon sun creating deep overhead shadows and blown highlights" },
  { id: "magic-hour", label: "Magic Hour", category: "temporal-aesthetics", axis: "cinematic", promptFragment: "magic hour warm glow painting everything in amber and rose" },
  { id: "blue-hour-mystery", label: "Blue Hour Mystery", category: "temporal-aesthetics", axis: "cinematic", promptFragment: "blue hour mystery, deep saturated blues before full darkness" },

  // === sonic-landscapes ===
  { id: "tension-drone", label: "Tension Drone", category: "sonic-landscapes", axis: "atmosphere", promptFragment: "low-frequency tension drone building beneath the surface", compatiblePurposes: ["horror_mystery", "storytelling"] },
  { id: "whisper-static", label: "Whisper Static", category: "sonic-landscapes", axis: "atmosphere", promptFragment: "barely audible whisper-static, like voices behind a wall", compatiblePurposes: ["horror_mystery"] },
  { id: "heartbeat-pulse", label: "Heartbeat Pulse", category: "sonic-landscapes", axis: "atmosphere", promptFragment: "rhythmic heartbeat pulse driving the pace of the scene", compatiblePurposes: ["horror_mystery", "storytelling"] },
  { id: "wind-chime-decay", label: "Wind Chime Decay", category: "sonic-landscapes", axis: "atmosphere", promptFragment: "distant wind-chime decay fading into silence" },
  { id: "industrial-grind", label: "Industrial Grind", category: "sonic-landscapes", axis: "atmosphere", promptFragment: "harsh industrial grind of metal on metal" },
  { id: "choir-swell", label: "Choir Swell", category: "sonic-landscapes", axis: "atmosphere", promptFragment: "ethereal choir swell rising to fill the space with voices", compatiblePurposes: ["motivational", "documentary"] },
  { id: "rain-on-metal", label: "Rain on Metal", category: "sonic-landscapes", axis: "atmosphere", promptFragment: "staccato rain-on-metal percussion in an open structure" },
  { id: "deep-ocean-pressure", label: "Deep Ocean Pressure", category: "sonic-landscapes", axis: "atmosphere", promptFragment: "deep ocean pressure, muffled and immense in the abyss" },
  { id: "crackling-fire", label: "Crackling Fire", category: "sonic-landscapes", axis: "atmosphere", promptFragment: "intimate crackling fire casting dancing orange light" },
  { id: "distant-thunder", label: "Distant Thunder", category: "sonic-landscapes", axis: "atmosphere", promptFragment: "distant rolling thunder promising storms on the horizon" },
];

/**
 * Scenario templates with suggested triplets per narrative arc beat.
 */
export interface ScenarioTemplate {
  id: string;
  name: string;
  description: string;
  arcBeats: Array<{
    beat: string;
    suggestedTriplet: InstructionTriplet;
  }>;
}

export const SCENARIO_TEMPLATES: ScenarioTemplate[] = [
  {
    id: "ghost-protocol",
    name: "The Ghost Protocol",
    description: "A mystery unraveling in abandoned or haunted spaces, building from unease to revelation.",
    arcBeats: [
      { beat: "Setup", suggestedTriplet: { primaryEmotion: "paranoid-unease", cinematicDirection: "handheld-float", environmentalAtmosphere: "foggy-ruins" } },
      { beat: "Discovery", suggestedTriplet: { primaryEmotion: "electric-anticipation", cinematicDirection: "slow-push-in", environmentalAtmosphere: "underground-drip" } },
      { beat: "Confrontation", suggestedTriplet: { primaryEmotion: "visceral-dread", cinematicDirection: "dutch-angle", environmentalAtmosphere: "tension-drone" } },
      { beat: "Revelation", suggestedTriplet: { primaryEmotion: "sacred-awe", cinematicDirection: "crane-reveal", environmentalAtmosphere: "cathedral-reverb" } },
    ],
  },
  {
    id: "silent-signal",
    name: "The Silent Signal",
    description: "A technological thriller about intercepting something that was never meant to be found.",
    arcBeats: [
      { beat: "Intercept", suggestedTriplet: { primaryEmotion: "cold-detachment", cinematicDirection: "static-tripod", environmentalAtmosphere: "server-room-hum" } },
      { beat: "Decode", suggestedTriplet: { primaryEmotion: "electric-anticipation", cinematicDirection: "extreme-closeup", environmentalAtmosphere: "whisper-static" } },
      { beat: "Chase", suggestedTriplet: { primaryEmotion: "seething-rage", cinematicDirection: "tracking-shot", environmentalAtmosphere: "neon-rain" } },
      { beat: "Transmission", suggestedTriplet: { primaryEmotion: "euphoric-wonder", cinematicDirection: "pull-back-reveal", environmentalAtmosphere: "data-rain" } },
    ],
  },
  {
    id: "desert-crossing",
    name: "The Desert Crossing",
    description: "An epic journey through desolation toward something transformative.",
    arcBeats: [
      { beat: "Departure", suggestedTriplet: { primaryEmotion: "quiet-defiance", cinematicDirection: "crane-reveal", environmentalAtmosphere: "desert-silence" } },
      { beat: "Ordeal", suggestedTriplet: { primaryEmotion: "stoic-resignation", cinematicDirection: "handheld-float", environmentalAtmosphere: "saharan-mirage" } },
      { beat: "Oasis", suggestedTriplet: { primaryEmotion: "nostalgic-warmth", cinematicDirection: "slow-push-in", environmentalAtmosphere: "middle-eastern-dusk" } },
      { beat: "Arrival", suggestedTriplet: { primaryEmotion: "sacred-awe", cinematicDirection: "pull-back-reveal", environmentalAtmosphere: "golden-hour-decay" } },
    ],
  },
  {
    id: "neon-descent",
    name: "Neon Descent",
    description: "A dive into a cyberpunk underworld, from surface glamour to hidden truth.",
    arcBeats: [
      { beat: "Surface", suggestedTriplet: { primaryEmotion: "electric-anticipation", cinematicDirection: "anamorphic-flare", environmentalAtmosphere: "tokyo-neon-night" } },
      { beat: "Descent", suggestedTriplet: { primaryEmotion: "paranoid-unease", cinematicDirection: "dolly-zoom", environmentalAtmosphere: "neon-rain" } },
      { beat: "Underworld", suggestedTriplet: { primaryEmotion: "visceral-dread", cinematicDirection: "chiaroscuro-lighting", environmentalAtmosphere: "industrial-grind" } },
      { beat: "Emergence", suggestedTriplet: { primaryEmotion: "feral-joy", cinematicDirection: "whip-pan", environmentalAtmosphere: "neural-lace" } },
    ],
  },
];

/**
 * Get vibe terms filtered by axis and optionally by video purpose.
 */
export function getVibeTerms(axis?: VibeAxis, purpose?: string): VibeTerm[] {
  let terms = VIBE_LIBRARY;

  if (axis) {
    terms = terms.filter(t => t.axis === axis);
  }

  if (purpose) {
    terms = terms.filter(t =>
      !t.compatiblePurposes || t.compatiblePurposes.includes(purpose)
    );
  }

  return terms;
}

/**
 * Look up vibe terms in the library and return their prompt fragments
 * for a given InstructionTriplet.
 */
export function tripletToPromptFragments(triplet: InstructionTriplet): {
  emotionFragment: string;
  cinematicFragment: string;
  atmosphereFragment: string;
} {
  const emotionTerm = VIBE_LIBRARY.find(t => t.id === triplet.primaryEmotion);
  const cinematicTerm = VIBE_LIBRARY.find(t => t.id === triplet.cinematicDirection);
  const atmosphereTerm = VIBE_LIBRARY.find(t => t.id === triplet.environmentalAtmosphere);

  return {
    emotionFragment: emotionTerm?.promptFragment ?? triplet.primaryEmotion,
    cinematicFragment: cinematicTerm?.promptFragment ?? triplet.cinematicDirection,
    atmosphereFragment: atmosphereTerm?.promptFragment ?? triplet.environmentalAtmosphere,
  };
}
````

## File: packages/shared/src/services/promptFormatService.ts
````typescript
/**
 * Prompt Format Service
 * 
 * Provides format specifications, JSON schema requirements, and examples
 * for LLM prompts to encourage consistent JSON output.
 * 
 * Feature: agent-director-json-parsing-fix
 * Requirements: 5.1, 5.2, 5.3, 5.4, 5.5
 */

// --- Types and Interfaces ---

/**
 * JSON Schema definition for storyboard output.
 * Requirements: 5.2
 */
export interface StoryboardJSONSchema {
  type: 'object';
  properties: {
    prompts: {
      type: 'array';
      items: {
        type: 'object';
        properties: {
          text: { type: 'string'; description: string };
          mood: { type: 'string'; description: string };
          timestamp: { type: 'string'; description: string };
        };
        required: string[];
      };
    };
  };
  required: string[];
}

/**
 * Format specification for LLM prompts.
 * Requirements: 5.1
 */
export interface FormatSpecification {
  outputFormat: 'json';
  jsonStructure: string;
  validationRules: string[];
  commonMistakes: string[];
}

/**
 * Example response for LLM prompts.
 * Requirements: 5.3
 */
export interface ResponseExample {
  description: string;
  example: string;
  notes: string[];
}

/**
 * Format correction pattern.
 * Requirements: 5.4
 */
export interface FormatCorrectionPattern {
  name: string;
  pattern: RegExp;
  correction: (...args: string[]) => string;
  description: string;
}

/**
 * Successful response pattern for library.
 * Requirements: 5.5
 */
export interface SuccessfulResponsePattern {
  id: string;
  pattern: string;
  frequency: number;
  lastSeen: string;
  characteristics: string[];
}

/**
 * Result of format correction preprocessing.
 * Requirements: 5.4
 */
export interface FormatCorrectionResult {
  corrected: string;
  wasModified: boolean;
  appliedCorrections: string[];
  confidence: number;
}

// --- JSON Schema Definitions ---

/**
 * Standard JSON schema for storyboard output.
 * Requirements: 5.2
 */
export const STORYBOARD_JSON_SCHEMA: StoryboardJSONSchema = {
  type: 'object',
  properties: {
    prompts: {
      type: 'array',
      items: {
        type: 'object',
        properties: {
          text: {
            type: 'string',
            description: 'Detailed visual prompt (60-120 words) starting with a concrete subject'
          },
          mood: {
            type: 'string',
            description: 'Emotional tone of the scene (e.g., melancholic, hopeful, intense)'
          },
          timestamp: {
            type: 'string',
            description: 'Timestamp in MM:SS format (e.g., "01:30")'
          }
        },
        required: ['text', 'mood', 'timestamp']
      }
    }
  },
  required: ['prompts']
};

/**
 * Analysis output JSON schema.
 * Requirements: 5.2
 */
export const ANALYSIS_JSON_SCHEMA = {
  type: 'object',
  properties: {
    sections: {
      type: 'array',
      items: {
        type: 'object',
        properties: {
          name: { type: 'string', description: 'Section name (e.g., Intro, Verse 1, Chorus)' },
          startTimestamp: { type: 'string', description: 'Start timestamp in MM:SS format' },
          endTimestamp: { type: 'string', description: 'End timestamp in MM:SS format' },
          type: { type: 'string', enum: ['intro', 'verse', 'pre-chorus', 'chorus', 'bridge', 'outro', 'transition', 'key_point', 'conclusion'] },
          emotionalIntensity: { type: 'number', minimum: 1, maximum: 10 }
        },
        required: ['name', 'startTimestamp', 'endTimestamp', 'type', 'emotionalIntensity']
      }
    },
    emotionalArc: {
      type: 'object',
      properties: {
        opening: { type: 'string' },
        peak: { type: 'string' },
        resolution: { type: 'string' }
      },
      required: ['opening', 'peak', 'resolution']
    },
    themes: { type: 'array', items: { type: 'string' } },
    motifs: { type: 'array', items: { type: 'string' } },
    visualScenes: {
      type: 'array',
      items: {
        type: 'object',
        properties: {
          visualPrompt: { type: 'string', description: 'Full 60-100 word Midjourney-style prompt' },
          subjectContext: { type: 'string', description: 'Narrative significance of the scene' },
          timestamp: { type: 'string', description: 'MM:SS format' },
          emotionalTone: { type: 'string', description: 'Single word emotional tone' }
        },
        required: ['visualPrompt', 'subjectContext', 'timestamp', 'emotionalTone']
      }
    }
  },
  required: ['sections', 'emotionalArc', 'themes', 'motifs']
};

// --- Format Specifications ---

/**
 * Get format specification for storyboard generation.
 * Requirements: 5.1
 */
export function getStoryboardFormatSpecification(): FormatSpecification {
  return {
    outputFormat: 'json',
    jsonStructure: `{
  "prompts": [
    {
      "text": "string (60-120 words, starts with concrete subject)",
      "mood": "string (emotional tone)",
      "timestamp": "string (MM:SS format)"
    }
  ]
}`,
    validationRules: [
      'Output MUST be valid JSON - no markdown code blocks, no extra text',
      'The "prompts" array MUST contain the exact number of items requested',
      'Each prompt "text" MUST be 60-120 words and start with a concrete subject noun',
      'Each "timestamp" MUST be in MM:SS format (e.g., "01:30")',
      'Each "mood" MUST be a single descriptive word or short phrase',
      'Do NOT include trailing commas after the last item in arrays or objects',
      'Do NOT include comments in the JSON output',
      'All string values MUST use double quotes, not single quotes'
    ],
    commonMistakes: [
      'Wrapping JSON in markdown code blocks (```json ... ```)',
      'Adding explanatory text before or after the JSON',
      'Using single quotes instead of double quotes',
      'Including trailing commas',
      'Forgetting required fields',
      'Using incorrect timestamp format (use MM:SS, not HH:MM:SS)',
      'Starting prompts with prepositions instead of concrete subjects'
    ]
  };
}

/**
 * Get format specification for analysis output.
 * Requirements: 5.1
 */
export function getAnalysisFormatSpecification(): FormatSpecification {
  return {
    outputFormat: 'json',
    jsonStructure: `{
  "sections": [
    {
      "name": "string",
      "startTimestamp": "MM:SS",
      "endTimestamp": "MM:SS",
      "type": "intro|verse|chorus|bridge|outro|...",
      "emotionalIntensity": number (1-10)
    }
  ],
  "emotionalArc": {
    "opening": "string",
    "peak": "string",
    "resolution": "string"
  },
  "themes": ["string"],
  "motifs": ["string"],
  "visualScenes": [
    {
      "visualPrompt": "string (60-100 words, full Midjourney-style prompt)",
      "subjectContext": "string (who/what and significance)",
      "timestamp": "MM:SS",
      "emotionalTone": "string (single word)"
    }
  ]
}`,
    validationRules: [
      'Output MUST be valid JSON - no markdown code blocks',
      'The "type" field MUST be lowercase (e.g., "verse" not "Verse")',
      'Timestamps MUST be in MM:SS format',
      'emotionalIntensity MUST be a number between 1 and 10',
      'Each visualScene.visualPrompt MUST be 60-100 words',
      'All required fields must be present'
    ],
    commonMistakes: [
      'Using capitalized type values (use "verse" not "Verse")',
      'Omitting the visualScenes array',
      'Using HH:MM:SS format instead of MM:SS',
      'Wrapping output in markdown code blocks',
      'Writing short visualPrompts instead of full 60-100 word prompts'
    ]
  };
}

// --- Response Examples ---

/**
 * Get example responses for storyboard generation.
 * Requirements: 5.3
 */
export function getStoryboardExamples(): ResponseExample[] {
  return [
    {
      description: 'Correct storyboard output with 3 prompts',
      example: `{
  "prompts": [
    {
      "text": "A weathered wooden door slowly creaks open, revealing a dimly lit hallway with peeling wallpaper and dust motes floating in a single beam of afternoon light. The camera holds at eye level, capturing the texture of aged brass doorknob and the shadow patterns cast across worn floorboards. Atmosphere of quiet abandonment and nostalgic melancholy.",
      "mood": "melancholic",
      "timestamp": "00:15"
    },
    {
      "text": "A lone figure in a vintage leather jacket stands at the edge of a rain-slicked rooftop, city lights blurring into bokeh circles below. Medium shot from behind, emphasizing isolation against the vast urban sprawl. Neon signs reflect off wet concrete, creating pools of magenta and teal. Cold wind suggested by slightly lifted collar.",
      "mood": "contemplative",
      "timestamp": "00:45"
    },
    {
      "text": "Weathered hands carefully unfold a yellowed photograph, fingers trembling slightly. Extreme close-up reveals the texture of aged paper and faded ink. Soft window light illuminates dust particles suspended in air. The image within the photo remains partially obscured, suggesting memory and loss.",
      "mood": "nostalgic",
      "timestamp": "01:15"
    }
  ]
}`,
      notes: [
        'Each prompt starts with a concrete subject (door, figure, hands)',
        'Prompts are 60-120 words with specific visual details',
        'Timestamps are in MM:SS format',
        'No markdown code blocks or extra text',
        'Valid JSON with proper formatting'
      ]
    }
  ];
}

/**
 * Get example responses for analysis output.
 * Requirements: 5.3
 */
export function getAnalysisExamples(): ResponseExample[] {
  return [
    {
      description: 'Correct analysis output',
      example: `{
  "sections": [
    {
      "name": "Intro",
      "startTimestamp": "00:00",
      "endTimestamp": "00:30",
      "type": "intro",
      "emotionalIntensity": 3
    },
    {
      "name": "Verse 1",
      "startTimestamp": "00:30",
      "endTimestamp": "01:15",
      "type": "verse",
      "emotionalIntensity": 5
    }
  ],
  "emotionalArc": {
    "opening": "Quiet introspection",
    "peak": "Emotional breakthrough",
    "resolution": "Peaceful acceptance"
  },
  "themes": ["loss", "memory", "hope"],
  "motifs": ["fading light", "empty rooms", "old photographs"],
  "visualScenes": [
    {
      "visualPrompt": "An elderly prophet with a flowing white beard stands before a weathered stone altar on a desolate mountaintop, his arm raised with a ceremonial knife, golden divine light piercing through dark storm clouds above, wind whipping his garments, low angle dramatic composition, cinematic rim lighting on his silhouette",
      "subjectContext": "Prophet Ibrahim at the moment of ultimate test before divine intervention",
      "timestamp": "00:45",
      "emotionalTone": "anguished"
    }
  ]
}`,
      notes: [
        'Type values are lowercase (intro, verse, not Intro, Verse)',
        'Timestamps in MM:SS format',
        'emotionalIntensity is a number, not a string',
        'visualScenes contains full 60-100 word Midjourney-style prompts'
      ]
    }
  ];
}

// --- Format Correction Patterns ---

/**
 * Patterns for correcting common format issues before parsing.
 * Requirements: 5.4
 */
export const FORMAT_CORRECTION_PATTERNS: FormatCorrectionPattern[] = [
  {
    name: 'remove_markdown_json_blocks',
    pattern: /```json\s*([\s\S]*?)```/gi,
    correction: (match: string) => {
      const inner = match.replace(/```json\s*/gi, '').replace(/```\s*$/gi, '');
      return inner.trim();
    },
    description: 'Remove markdown JSON code block wrappers'
  },
  {
    name: 'remove_markdown_blocks',
    pattern: /```\s*([\s\S]*?)```/gi,
    correction: (match: string) => {
      const inner = match.replace(/```\s*/gi, '');
      return inner.trim();
    },
    description: 'Remove generic markdown code block wrappers'
  },
  {
    name: 'fix_trailing_commas_array',
    pattern: /,(\s*\])/g,
    correction: () => '$1',
    description: 'Remove trailing commas before closing brackets'
  },
  {
    name: 'fix_trailing_commas_object',
    pattern: /,(\s*\})/g,
    correction: () => '$1',
    description: 'Remove trailing commas before closing braces'
  },
  {
    name: 'fix_single_quotes',
    pattern: /'([^']+)'(\s*:)/g,
    correction: (match: string) => match.replace(/'/g, '"'),
    description: 'Convert single-quoted keys to double quotes'
  },
  {
    name: 'remove_js_comments',
    pattern: /\/\/[^\n]*/g,
    correction: () => '',
    description: 'Remove JavaScript-style single-line comments'
  },
  {
    name: 'remove_multiline_comments',
    pattern: /\/\*[\s\S]*?\*\//g,
    correction: () => '',
    description: 'Remove multi-line comments'
  },
  {
    name: 'fix_unquoted_keys',
    pattern: /([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)(\s*:)/g,
    correction: (match: string, prefix: string, key: string, suffix: string) => `${prefix}"${key}"${suffix}`,
    description: 'Quote unquoted object keys'
  },
  {
    name: 'normalize_newlines_in_strings',
    pattern: /("(?:[^"\\]|\\.)*")/g,
    correction: (match: string) => {
      // Replace literal newlines inside strings with \n
      return match.replace(/\n/g, '\\n').replace(/\r/g, '\\r').replace(/\t/g, '\\t');
    },
    description: 'Escape newlines inside string values'
  },
  {
    name: 'remove_leading_text',
    pattern: /^[^{[]*(?=[{[])/,
    correction: () => '',
    description: 'Remove any text before the JSON starts'
  },
  {
    name: 'remove_trailing_text',
    pattern: /(?<=[}\]])[^}\]]*$/,
    correction: () => '',
    description: 'Remove any text after the JSON ends'
  }
];

// --- Successful Response Pattern Library ---

/**
 * Library of successful response patterns.
 * Requirements: 5.5
 */
class ResponsePatternLibrary {
  private patterns: Map<string, SuccessfulResponsePattern> = new Map();

  /**
   * Add a successful response pattern to the library.
   */
  addPattern(response: string, characteristics: string[]): void {
    const id = this.generatePatternId(response);
    const existing = this.patterns.get(id);

    if (existing) {
      existing.frequency++;
      existing.lastSeen = new Date().toISOString();
    } else {
      this.patterns.set(id, {
        id,
        pattern: this.extractPattern(response),
        frequency: 1,
        lastSeen: new Date().toISOString(),
        characteristics
      });
    }
  }

  /**
   * Get the most common successful patterns.
   */
  getTopPatterns(limit: number = 5): SuccessfulResponsePattern[] {
    return Array.from(this.patterns.values())
      .sort((a, b) => b.frequency - a.frequency)
      .slice(0, limit);
  }

  /**
   * Check if a response matches known successful patterns.
   */
  matchesKnownPattern(response: string): boolean {
    const responsePattern = this.extractPattern(response);
    return this.patterns.has(this.generatePatternId(response)) ||
      Array.from(this.patterns.values()).some(p =>
        this.patternSimilarity(p.pattern, responsePattern) > 0.8
      );
  }

  /**
   * Get pattern characteristics for a response.
   */
  getPatternCharacteristics(response: string): string[] {
    const characteristics: string[] = [];

    // Check structure
    if (response.includes('"prompts"')) {
      characteristics.push('has_prompts_array');
    }
    if (response.includes('"sections"')) {
      characteristics.push('has_sections_array');
    }
    if (response.includes('"text"')) {
      characteristics.push('has_text_field');
    }
    if (response.includes('"mood"')) {
      characteristics.push('has_mood_field');
    }
    if (response.includes('"timestamp"')) {
      characteristics.push('has_timestamp_field');
    }

    // Check format
    if (!response.includes('```')) {
      characteristics.push('no_markdown_blocks');
    }
    if (response.trim().startsWith('{') || response.trim().startsWith('[')) {
      characteristics.push('starts_with_json');
    }

    return characteristics;
  }

  /**
   * Generate a unique ID for a pattern.
   */
  private generatePatternId(response: string): string {
    const characteristics = this.getPatternCharacteristics(response);
    return characteristics.sort().join('|');
  }

  /**
   * Extract a structural pattern from a response.
   */
  private extractPattern(response: string): string {
    // Simplify the response to its structural pattern
    return response
      .replace(/"[^"]*"/g, '""')  // Replace string values
      .replace(/\d+/g, '0')       // Replace numbers
      .replace(/\s+/g, ' ')       // Normalize whitespace
      .trim();
  }

  /**
   * Calculate similarity between two patterns.
   */
  private patternSimilarity(pattern1: string, pattern2: string): number {
    const tokens1 = new Set(pattern1.split(/\s+/));
    const tokens2 = new Set(pattern2.split(/\s+/));

    let intersection = 0;
    tokens1.forEach(t => { if (tokens2.has(t)) intersection++; });

    const union = tokens1.size + tokens2.size - intersection;
    return union === 0 ? 0 : intersection / union;
  }

  /**
   * Clear the pattern library.
   */
  clear(): void {
    this.patterns.clear();
  }

  /**
   * Get the total number of patterns.
   */
  get size(): number {
    return this.patterns.size;
  }
}

// Singleton instance
export const responsePatternLibrary = new ResponsePatternLibrary();

// --- Format Correction Preprocessing ---

/**
 * Attempt to fix common formatting issues before parsing.
 * Requirements: 5.4
 * 
 * @param content - The raw LLM response content
 * @returns FormatCorrectionResult with corrected content and metadata
 */
export function preprocessFormatCorrection(content: string): FormatCorrectionResult {
  if (!content || typeof content !== 'string') {
    return {
      corrected: content || '',
      wasModified: false,
      appliedCorrections: [],
      confidence: 0
    };
  }

  let corrected = content;
  const appliedCorrections: string[] = [];

  // Apply each correction pattern
  for (const pattern of FORMAT_CORRECTION_PATTERNS) {
    const before = corrected;

    if (pattern.name === 'fix_unquoted_keys') {
      // Special handling for unquoted keys pattern
      corrected = corrected.replace(pattern.pattern, (...args: string[]) => {
        return pattern.correction(...args);
      });
    } else if (pattern.name === 'remove_markdown_json_blocks' || pattern.name === 'remove_markdown_blocks') {
      // Extract content from markdown blocks
      const matches = corrected.match(pattern.pattern);
      if (matches && matches.length > 0) {
        // Take the first match and extract the content
        corrected = pattern.correction(matches[0]);
        appliedCorrections.push(pattern.name);
      }
    } else {
      corrected = corrected.replace(pattern.pattern, pattern.correction as unknown as string);
    }

    if (corrected !== before) {
      if (!appliedCorrections.includes(pattern.name)) {
        appliedCorrections.push(pattern.name);
      }
    }
  }

  // Trim whitespace
  corrected = corrected.trim();

  // Calculate confidence based on corrections applied
  let confidence = 1.0;
  if (appliedCorrections.length > 0) {
    // Reduce confidence based on number of corrections needed
    confidence = Math.max(0.5, 1.0 - (appliedCorrections.length * 0.1));
  }

  // Verify the result looks like valid JSON
  if (!corrected.startsWith('{') && !corrected.startsWith('[')) {
    confidence = Math.min(confidence, 0.3);
  }

  return {
    corrected,
    wasModified: appliedCorrections.length > 0,
    appliedCorrections,
    confidence
  };
}

/**
 * Check if content needs format correction.
 * Requirements: 5.4
 */
export function needsFormatCorrection(content: string): boolean {
  if (!content || typeof content !== 'string') {
    return false;
  }

  // Check for common issues
  const issues = [
    content.includes('```'),           // Markdown code blocks
    /,\s*[\]}]/.test(content),         // Trailing commas
    /'[^']+'\s*:/.test(content),       // Single-quoted keys
    /\/\//.test(content),              // JS comments
    /\/\*/.test(content),              // Multi-line comments
    !content.trim().startsWith('{') && !content.trim().startsWith('[')  // Doesn't start with JSON
  ];

  return issues.some(issue => issue);
}

// --- Prompt Enhancement Functions ---

/**
 * Generate format specification text for inclusion in prompts.
 * Requirements: 5.1
 */
export function generateFormatSpecificationText(type: 'storyboard' | 'analysis'): string {
  const spec = type === 'storyboard'
    ? getStoryboardFormatSpecification()
    : getAnalysisFormatSpecification();

  return `
OUTPUT FORMAT REQUIREMENTS:
- Output MUST be valid JSON only - no markdown, no code blocks, no explanatory text
- Structure: ${spec.jsonStructure.split('\n').map(l => '  ' + l).join('\n')}

VALIDATION RULES:
${spec.validationRules.map(r => `- ${r}`).join('\n')}

COMMON MISTAKES TO AVOID:
${spec.commonMistakes.map(m => `- ${m}`).join('\n')}
`;
}

/**
 * Generate example text for inclusion in prompts.
 * Requirements: 5.3
 */
export function generateExampleText(type: 'storyboard' | 'analysis'): string {
  const examples = type === 'storyboard'
    ? getStoryboardExamples()
    : getAnalysisExamples();

  if (examples.length === 0) {
    return '';
  }

  const example = examples[0];
  if (!example) return '';

  return `
EXAMPLE OF CORRECT OUTPUT:
${example.example}

NOTES:
${example.notes.map(n => `- ${n}`).join('\n')}
`;
}

/**
 * Generate complete format guidance for prompts.
 * Requirements: 5.1, 5.2, 5.3
 */
export function generateCompleteFormatGuidance(type: 'storyboard' | 'analysis'): string {
  const schema = type === 'storyboard' ? STORYBOARD_JSON_SCHEMA : ANALYSIS_JSON_SCHEMA;
  const specText = generateFormatSpecificationText(type);
  const exampleText = generateExampleText(type);

  return `
${specText}

JSON SCHEMA:
${JSON.stringify(schema, null, 2)}

${exampleText}

CRITICAL: Return ONLY the JSON object. No markdown, no code blocks, no additional text.
`;
}

// Note: Types are already exported inline with their definitions above
````

## File: packages/shared/src/services/promptService.ts
````typescript
/**
 * Prompt Service
 * Handles prompt generation and refinement for image/video generation.
 */

import { Type } from "@google/genai";
import { ImagePrompt } from "../types";
import { parseSRTTimestamp } from "../utils/srtParser";
import { ai, MODELS, withRetry } from "./shared/apiClient";
import { CAMERA_ANGLES, LIGHTING_MOODS, VideoPurpose } from "../constants";
import type { ImageStyleGuide } from "./prompt/imageStyleGuide";
import { normalizeForSimilarity, countWords } from "./utils/textProcessing";

// --- NEW STYLE INJECTOR ---

/**
 * Forces a consistent visual style across all assets to prevent "Style Soup".
 */
export function injectMasterStyle(basePrompt: string, stylePreset: string = "cinematic"): string {
    // 1. Clean the prompt of conflicting instructions
    let cleanPrompt = basePrompt.replace(/photorealistic|cartoon|3d render|sketch/gi, "").trim();

    // 2. Dynamic style prefix based on stylePreset
    const stylePrefixes: Record<string, string> = {
        "cinematic": "Cinematic film shot",
        "anime": "Anime-style illustration",
        "watercolor": "Watercolor painting",
        "sketch": "Pencil sketch drawing",
        "oil painting": "Oil painting on canvas",
        "photorealistic": "Photorealistic photograph",
        "documentary": "Documentary-style photograph",
        "noir": "Film noir shot",
        "vaporwave": "Vaporwave aesthetic",
        "cyberpunk": "Cyberpunk-themed visual",
        "fantasy": "Fantasy art illustration",
        "manga": "Manga-style drawing",
        "comic book": "Comic book illustration",
        "3d render": "3D rendered image",
        "pixel art": "Pixel art creation",
        "chibi": "Chibi-style art",
        "steampunk": "Steampunk illustration",
        "gothic": "Gothic art style",
        "minimalist": "Minimalist design",
        "abstract": "Abstract art piece",
        "vintage": "Vintage-style photograph"
    };

    // Get the appropriate prefix, default to the style name if not found
    const prefix = stylePrefixes[stylePreset.toLowerCase()] || `${stylePreset} aesthetic`;

    // 3. Define the Master Style (The "Glue")
    // This ensures consistent colors, lighting, and texture across all generated clips.
    const MASTER_STYLE = `${prefix}, ${stylePreset} aesthetic, consistent color grading, soft volumetric lighting, 35mm film grain, high coherence, highly detailed, 8k resolution. Negative prompt: text, watermark, bad quality, distorted, cgi artifacts, cartoon.`;

    // 4. Combine
    return `${cleanPrompt}. ${MASTER_STYLE}`;
}

// Re-export from extracted modules for backward compatibility
export { getSystemPersona, type Persona, type PersonaType } from './prompt/personaData';
export { getStyleEnhancement, type StyleEnhancement } from './prompt/styleEnhancements';
export { normalizeForSimilarity, countWords };

// --- Motion Prompt Result ---

/**
 * Structured result from generateMotionPrompt().
 * Separates camera movement from subject/environment physics to avoid
 * instruction dilution in video models that conflate the two.
 */
export interface MotionPromptResult {
  /** Camera only: movement type, direction, speed (≤25 words). */
  camera_motion: string;
  /** Environment/subject: wind, particles, flame, cloth, water (≤25 words). */
  subject_physics: string;
  /** Combined string for single-string video APIs: "{camera_motion}. {subject_physics}" */
  combined: string;
}

// --- Types ---

export type PromptRefinementIntent =
  | "auto"
  | "more_detailed"
  | "more_cinematic"
  | "more_consistent_subject"
  | "shorten"
  | "fix_repetition";

// --- Lint Issue Types ---

export type PromptLintIssueCode =
  | "too_short"
  | "too_long"
  | "repetitive"
  | "missing_subject"
  | "no_leading_subject"
  | "contains_text_instruction"
  | "contains_logos_watermarks"
  | "weak_visual_specificity"
  | "generic_conflict";

export interface PromptLintIssue {
  code: PromptLintIssueCode;
  message: string;
  severity: "warn" | "error";
}

interface PromptResponseItem {
  text: string;
  mood: string;
  timestamp: string;
}

// --- Helper Functions ---

/**
 * Calculate Jaccard similarity between two strings.
 */
export function jaccardSimilarity(a: string, b: string): number {
  const sa = new Set(normalizeForSimilarity(a).split(" ").filter(Boolean));
  const sb = new Set(normalizeForSimilarity(b).split(" ").filter(Boolean));
  if (sa.size === 0 && sb.size === 0) return 1;
  if (sa.size === 0 || sb.size === 0) return 0;

  let inter = 0;
  Array.from(sa).forEach(w => { if (sb.has(w)) inter++; });
  const union = sa.size + sb.size - inter;
  return union === 0 ? 0 : inter / union;
}

// NOTE: getSystemPersona and getStyleEnhancement have been moved to
// ./prompt/personaData.ts and ./prompt/styleEnhancements.ts respectively

/**
 * Lint a prompt for common issues.
 */
export function lintPrompt(params: {
  promptText: string;
  globalSubject?: string;
  previousPrompts?: string[];
}): PromptLintIssue[] {
  const { promptText, globalSubject, previousPrompts } = params;
  const issues: PromptLintIssue[] = [];

  const words = countWords(promptText);

  // Reduced threshold from 18 to 10 - modern models (Imagen 3, Flux) perform better with concise prompts
  if (words < 10) {
    issues.push({
      code: "too_short",
      message:
        "Prompt is very short; add setting, lighting, camera/composition, and mood to reduce generic outputs.",
      severity: "warn",
    });
  }

  if (words > 180) {
    issues.push({
      code: "too_long",
      message:
        "Prompt is very long; consider removing redundant adjectives to reduce model confusion.",
      severity: "warn",
    });
  }

  const norm = normalizeForSimilarity(promptText);

  // Check if prompt starts with a concrete subject (article + noun pattern)
  const leadingSubjectPattern = /^(a|an|the|two|three|several|many|some|all|every|their|his|her|its|our|\w+ed|\w+ing)\s+\w+/i;
  const trimmedPrompt = promptText.trim();
  if (!leadingSubjectPattern.test(trimmedPrompt) && words > 5) {
    // If it doesn't match the pattern but starts with a capital letter + word, it might be a noun
    // We only flag if it looks like a preposition or weak start
    const firstWord = trimmedPrompt.split(/\s+/)[0]?.toLowerCase();
    const weakStarts = ["in", "through", "on", "at", "by", "with", "from", "when", "while", "during", "beneath", "under", "above"];

    if (firstWord && weakStarts.includes(firstWord)) {
      issues.push({
        code: "no_leading_subject",
        message:
          `Prompt starts with a preposition ("${firstWord}"). It should start with a concrete subject (e.g., 'A lone figure...', 'The vintage car...', 'Weathered hands...')`,
        severity: "warn",
      });
    }
  }

  if (/\btext\b|\bsubtitles\b|\bcaption\b|\btypography\b|\blabel\b|\bwords\b|\btitle\b/.test(norm)) {
    issues.push({
      code: "contains_text_instruction",
      message:
        "Prompt mentions text/subtitles/typography/labels; this often causes unwanted text in images.",
      severity: "warn",
    });
  }

  if (/\blogo\b|\bwatermark\b|\bbrand\b/.test(norm)) {
    issues.push({
      code: "contains_logos_watermarks",
      message:
        "Prompt mentions logos/watermarks/brands; this often increases unwanted marks in images.",
      severity: "warn",
    });
  }

  // Check for lighting quality (not just presence)
  const hasLighting = /\b(lighting|lit|glow|neon|sunset|dawn|fog|mist|smoke|backlight|sidelight|rim light|golden hour|blue hour|ambient|diffused|harsh|soft light|volumetric)\b/.test(norm);
  const hasQualityLighting = /\b(golden hour|blue hour|backlight|sidelight|rim light|volumetric|diffused|harsh direct|soft diffused|dappled|ambient glow|fire flicker|candlelight|moonlight)\b/.test(norm);

  // Check for camera angle (required)
  const hasCameraAngle = /\b(close-up|wide shot|medium shot|portrait|overhead|low angle|high angle|bird.?s?.?eye|dutch angle|eye level|over.?the.?shoulder|tracking|dolly|crane|steadicam|handheld|establishing shot|extreme close-up)\b/.test(norm);

  // Check for color/palette
  const hasColorPalette = /\b(color palette|palette|monochrome|pastel|vibrant|muted|teal|orange|amber|crimson|slate|desaturated|warm tones|cool tones)\b/.test(norm);

  // Check for lens/depth
  const hasLensDepth = /\b(depth of field|bokeh|lens|35mm|50mm|anamorphic|shallow focus|deep focus|rack focus)\b/.test(norm);

  const hasVisualAnchors = hasLighting || hasCameraAngle || hasColorPalette || hasLensDepth;

  if (!hasVisualAnchors) {
    issues.push({
      code: "weak_visual_specificity",
      message:
        "Prompt lacks visual anchors (camera, lighting, palette). Add at least 1–2 to improve composition consistency.",
      severity: "warn",
    });
  }

  // NEW: Specific check for missing camera angle
  if (!hasCameraAngle && words > 20) {
    issues.push({
      code: "weak_visual_specificity",
      message:
        "Prompt missing camera angle/shot type. Add one of: close-up, wide shot, medium shot, low angle, high angle, bird's eye, dutch angle, over-the-shoulder, tracking shot.",
      severity: "warn",
    });
  }

  // NEW: Check for lighting quality (not just presence)
  if (hasLighting && !hasQualityLighting && words > 30) {
    issues.push({
      code: "weak_visual_specificity",
      message:
        "Prompt has generic lighting. Specify quality: golden hour, backlight, rim light, soft diffused, harsh direct, volumetric, dappled through leaves, etc.",
      severity: "warn",
    });
  }

  if (globalSubject && globalSubject.trim().length > 0) {
    // ENFORCE EXACT MATCH: Check if globalSubject appears exactly in first 20 words
    const promptWords = promptText.trim().split(/\s+/).slice(0, 20);
    const firstTwentyWords = promptWords.join(" ").toLowerCase();
    const exactSubject = globalSubject.trim().toLowerCase();
    
    // Check for exact match of the globalSubject phrase
    const hasExactMatch = firstTwentyWords.includes(exactSubject);
    
    if (!hasExactMatch) {
      issues.push({
        code: "missing_subject",
        message:
          `Prompt MUST start with the exact Global Subject "${globalSubject}" in the first 20 words. Current first 20 words: "${firstTwentyWords}"`,
        severity: "error",
      });
    }
  }

  if (previousPrompts && previousPrompts.length > 0) {
    const sims = previousPrompts.map((p) => jaccardSimilarity(p, promptText));
    const maxSim = Math.max(...sims);
    if (maxSim >= 0.72) {
      issues.push({
        code: "repetitive",
        message:
          "Prompt is very similar to another scene; vary setting/camera/lighting to avoid repetitive images.",
        severity: "warn",
      });
    }
  }

  // Generic conflict detection - flag common cliche tropes
  // But skip for action-oriented styles where conflict is expected
  const conflictPatterns = /\b(arguing|slamming|yelling|fighting|screaming\s+at|shouting\s+match|couple\s+fighting|heated\s+argument|angry\s+confrontation)\b/i;
  if (conflictPatterns.test(norm)) {
    // Note: This check is style-agnostic in lintPrompt. The caller should consider
    // suppressing this warning for action/anime styles where conflict is intentional.
    issues.push({
      code: "generic_conflict",
      message:
        "Generic conflict imagery detected (arguing, fighting). Consider visual metaphors: glass breaking, door closing, wilting flower, fading photograph, storm clouds gathering.",
      severity: "warn",
    });
  }

  return issues;
}

/**
 * Get purpose-specific instructions for prompt generation.
 */
export const getPurposeGuidance = (purpose: VideoPurpose): string => {
  const guidance: Record<VideoPurpose, string> = {
    music_video: `
PURPOSE: Music Video (Cinematic, Emotional)
- Create dramatic, emotionally resonant scenes that amplify the music's feeling
- Use cinematic compositions with depth and layers
- Match visual intensity to musical intensity (verse=calm, chorus=dynamic)
- Aim for 4-6 second average scene duration
- Include atmospheric elements (particles, light rays, reflections)`,

    social_short: `
PURPOSE: Social Media Short (TikTok/Reels/Shorts)
- Bold, eye-catching visuals that pop on small screens
- High contrast, vibrant colors, immediate visual impact
- Fast-paced energy, dynamic compositions
- Vertical-friendly framing (subject centered, minimal side detail)
- Trendy aesthetics, modern and relatable imagery`,

    documentary: `
PURPOSE: Documentary/Educational
- Realistic, grounded visuals that inform and explain
- B-roll style imagery that supports narration
- Clear, unambiguous scenes that illustrate concepts
- Professional, trustworthy aesthetic
- Mix of wide establishing shots and detail close-ups`,

    commercial: `
PURPOSE: Commercial/Advertisement
- Clean, polished, aspirational imagery
- Product/subject should be hero of each frame
- Lifestyle-oriented scenes showing benefits/emotions
- Professional lighting, minimal distractions
- Call-to-action friendly compositions`,

    podcast_visual: `
PURPOSE: Podcast/Audio Visualization
- Ambient, non-distracting background visuals
- Abstract or environmental scenes
- Calm, steady imagery that doesn't compete with spoken content
- Subtle movement potential, meditative quality
- Longer scene durations (8-15 seconds)`,

    lyric_video: `
PURPOSE: Lyric Video
- Compositions with clear negative space for text overlay
- Avoid busy centers where lyrics will appear
- Backgrounds that provide contrast for readability
- Thematic imagery that supports but doesn't overwhelm
- Consider lower-third and center-frame text placement areas`,

    storytelling: `
PURPOSE: Storytelling/Narrative
- Narrative-driven imagery that follows a story arc
- Character-focused scenes with emotional depth
- Settings that establish time, place, and mood
- Visual metaphors and symbolic imagery
- Dramatic lighting to enhance storytelling moments`,

    educational: `
PURPOSE: Educational Content
- Clear, informative visuals that support learning
- Diagrams and illustrative imagery when appropriate
- Professional, trustworthy aesthetic
- Consistent visual language throughout
- Balance between engaging and instructional`,

    horror_mystery: `
PURPOSE: Horror/Mystery
- Dark, atmospheric, and suspenseful imagery
- Use of shadows, fog, and low-key lighting
- Unsettling compositions with negative space
- Subtle hints of danger or the unknown
- Moody color palettes (desaturated, cool tones)`,

    travel: `
PURPOSE: Travel/Nature
- Stunning landscape and scenic imagery
- Wide establishing shots that capture scale
- Cultural and environmental authenticity
- Golden hour and natural lighting preferred
- Sense of wonder and exploration`,

    motivational: `
PURPOSE: Motivational/Inspirational
- Uplifting, empowering imagery
- Dynamic compositions suggesting progress
- Warm, hopeful lighting
- Aspirational subjects and settings
- Visual metaphors for growth and achievement`,

    news_report: `
PURPOSE: News Report/Journalistic
- Factual, objective visual style
- Clear, unambiguous imagery
- Professional, trustworthy aesthetic
- B-roll that supports factual narration
- Neutral color grading, minimal stylization`,

    // Story Mode Genre-Specific Guidance
    story_drama: `
PURPOSE: Drama Story
- Emotional depth and character-focused moments
- Intimate close-ups for emotional beats
- Warm, naturalistic lighting
- Subtle environmental storytelling
- Meaningful pauses and contemplative compositions`,

    story_comedy: `
PURPOSE: Comedy Story
- Bright, vibrant, energetic visuals
- Clear staging for comedic timing
- Reaction shots and character expressions
- Playful, dynamic camera angles
- Exaggerated but grounded environments`,

    story_thriller: `
PURPOSE: Thriller Story
- High tension, suspenseful atmosphere
- Noir-inspired high contrast lighting
- Claustrophobic, unsettling framing
- Deep shadows and hidden threats
- Dutch angles for psychological unease`,

    story_scifi: `
PURPOSE: Sci-Fi Story
- Futuristic, technological environments
- Neon accents and holographic elements
- Scale contrast between human and technology
- Clean, minimalist future aesthetics
- Atmospheric volumetric lighting`,

    story_action: `
PURPOSE: Action Story
- Dynamic, kinetic compositions
- Motion blur and speed emphasis
- High-energy color grading
- Clear spatial geography for action
- Impactful freeze-frame moments`,

    story_fantasy: `
PURPOSE: Fantasy Story
- Magical, immersive world-building
- Rich, saturated color palettes
- Epic scale and grandeur
- Mystical lighting effects (glows, particles)
- Mythical creatures and enchanted environments`,

    story_romance: `
PURPOSE: Romance Story
- Intimate, emotionally resonant visuals
- Soft, flattering lighting
- Warm, romantic color grading
- Two-shot compositions emphasizing connection
- Beautiful, aspirational settings`,

    story_historical: `
PURPOSE: Historical Story
- Period-accurate production design
- Natural, era-appropriate lighting
- Authentic costumes and settings
- Painterly, classical compositions
- Cultural and historical authenticity`,

    story_animation: `
PURPOSE: Animated Story
- Bold, expressive character designs
- Vibrant, stylized color palettes
- Dynamic, exaggerated compositions
- Clear silhouettes and staging
- Imaginative, fantastical environments`,
  };

  return guidance[purpose] || guidance.music_video;
};


/**
 * Enhanced prompt generation instruction with visual storytelling.
 * Now injects rich style keywords from styleEnhancements.ts for authentic visual representation.
 * Also injects persona data from personaData.ts for purpose-specific director guidance.
 */
export const getPromptGenerationInstruction = (
  style: string,
  mode: "lyrics" | "story",
  content: string,
  globalSubject: string = "",
  purpose: VideoPurpose = "music_video",
) => {
  // Import style enhancement data for rich visual keywords
  const { getStyleEnhancement } = require('./prompt/styleEnhancements');
  const { getSystemPersona } = require('./prompt/personaData');
  const styleData = getStyleEnhancement(style);
  const persona = getSystemPersona(purpose);

  const contentType =
    mode === "lyrics" ? "song lyrics" : "spoken-word/narrative transcript";
  const purposeGuidance = getPurposeGuidance(purpose);

  // Build rich style block with actual technique keywords
  const richStyleBlock = `
ART STYLE: "${style}"
VISUAL GUIDELINES (MANDATORY - apply to ALL prompts):
${styleData.keywords.map((k: string) => `- ${k}`).join('\n')}
AESTHETIC GOAL: ${styleData.mediumDescription}`;

  // Build persona block with director-specific guidance
  const personaBlock = `
DIRECTOR PERSONA: ${persona.name} (${persona.role})
CORE DIRECTIVE: ${persona.coreRule}
VISUAL PRINCIPLES:
${persona.visualPrinciples.map((p: string) => `- ${p}`).join('\n')}
STRICTLY AVOID:
${persona.avoidList.map((a: string) => `- ${a}`).join('\n')}`;

  const subjectBlock = globalSubject.trim()
    ? `
MAIN SUBJECT (must appear consistently in relevant scenes):
"${globalSubject}"
- Keep this subject's appearance, clothing, and key features consistent
- Reference specific visual details (hair color, outfit, distinguishing features)
- The subject should be the visual anchor across scenes
- CRITICAL: Every prompt MUST start with "${globalSubject}" or a direct reference to them`
    : `
MAIN SUBJECT: None specified
- Create cohesive scenes with consistent environmental/thematic elements
- If characters appear, maintain their appearance across scenes`;

  const structureGuidance =
    mode === "lyrics"
      ? `
SONG STRUCTURE ANALYSIS:
1. Identify sections: Intro, Verse, Pre-Chorus, Chorus, Bridge, Outro
2. Verses = introspective, storytelling, character moments
3. Choruses = emotional peaks, dynamic visuals, wider shots
4. Bridge = visual contrast, unexpected angle or setting
5. Match energy: quiet sections → intimate close-ups; loud sections → epic wide shots`
      : `
NARRATIVE STRUCTURE ANALYSIS:
1. Identify segments: Introduction, Key Points, Transitions, Conclusion
2. Opening = establishing context, setting the scene
3. Main content = illustrating concepts, showing examples
4. Transitions = visual bridges between ideas
5. Conclusion = reinforcing main message, memorable closing image`;

  const visualVariety = `
VISUAL VARIETY REQUIREMENTS:
- Camera angles to use across scenes: ${CAMERA_ANGLES.slice(0, 6).join(", ")}
- Lighting variations: ${LIGHTING_MOODS.slice(0, 5).join(", ")}
- NEVER repeat the same camera angle in consecutive scenes
- Create an emotional arc: establish → build → climax → resolve
- Each prompt must specify: subject, action/pose, setting, lighting, camera angle, mood`;

  return `You are a professional music video director and visual storyteller creating an image storyboard.
${personaBlock}

TASK: Analyze this ${contentType} and generate a visual storyboard with detailed image prompts.
${richStyleBlock}
${subjectBlock}
${purposeGuidance}
${structureGuidance}
${visualVariety}

PROMPT WRITING RULES:
1. FORMAT: "[Subject Description], [Action], [Environment], [Lighting/Style]"
2. If a Global Subject is defined ("${globalSubject}"), every prompt MUST start with exactly that phrase.
   - CORRECT: "${globalSubject || 'The subject'} standing in a neon rainstorm..."
   - INCORRECT: "A lonely figure standing..." (Ambiguous - causes subject drift)
   - INCORRECT: "Neon rain falls on ${globalSubject || 'the subject'}..." (Passive - subject not leading)
3. EVERY prompt MUST begin with a concrete subject noun (e.g., "A lone figure...", "A vintage car...", "A glowing orb...", "Weathered hands...")
4. Each prompt must be 40-120 words with SPECIFIC visual details
5. MANDATORY CHECKLIST for each prompt (include ALL of these):
   - Subject: WHO or WHAT is in the scene (concrete noun, not abstract)
   - Action/Pose: What the subject is doing
   - Setting: WHERE the scene takes place
   - Lighting: Type and quality (e.g., "golden hour backlighting", "harsh overhead fluorescent", "soft diffused window light")
   - Texture: At least one tactile detail (e.g., "weathered wood grain", "rain-slicked asphalt", "velvet fabric")
   - Camera: Shot type and angle (e.g., "extreme close-up at eye level", "wide establishing shot from low angle")
   - Atmosphere: Mood and ambient details
6. Focus ONLY on visual elements: subjects, lighting, textures, colors, camera angles, atmosphere
7. NO generic phrases like "beautiful", "stunning", "amazing" - be SPECIFIC with descriptors
8. Reference the main subject by their specific features, not just "the subject"
9. Vary compositions: rule-of-thirds, centered, symmetrical, asymmetrical
10. Include sensory details: textures, materials, weather, time of day

EMOTIONAL ARC:
- Scene 1-2: Establish mood and setting (wide shots, context)
- Scene 3-5: Build intensity (medium shots, character focus)
- Scene 6-8: Peak emotion (dynamic angles, close-ups, action)
- Scene 9-12: Resolution/reflection (pull back, contemplative)

CONTENT TO ANALYZE:
${content.slice(0, 15000)}

OUTPUT: Generate 8-12 prompts as JSON with 'prompts' array.
Each item: { "text": "detailed visual prompt starting with concrete subject", "mood": "emotional tone", "timestamp": "MM:SS" }

Timestamps should align with natural section breaks in the content.`;
};

/**
 * Refine an image prompt using AI.
 */
async function refineImagePromptWithAI(params: {
  promptText: string;
  style: string;
  globalSubject?: string;
  aspectRatio?: string;
  intent?: PromptRefinementIntent;
  issues?: PromptLintIssue[];
}): Promise<string> {
  const {
    promptText,
    style,
    globalSubject = "",
    aspectRatio = "16:9",
    intent = "auto",
    issues = [],
  } = params;

  const issueSummary =
    issues.length > 0
      ? issues.map((i) => `- (${i.code}) ${i.message}`).join("\n")
      : "- (none)";

  const response = await ai.models.generateContent({
    model: MODELS.TEXT,
    contents: `You are a prompt engineer for high-quality image generation.
Rewrite the user's prompt to improve visual clarity, cinematic composition, and subject consistency while preserving intent.

Global Subject (must remain consistent across scenes):
${globalSubject ? globalSubject : "(none)"}

Chosen Style Preset:
${style}

Aspect Ratio:
${aspectRatio}

User Intent:
${intent}

Detected Issues:
${issueSummary}

Requirements:
- Output ONLY a JSON object: { "prompt": string }.
- Keep it a single prompt suitable for an image model.
- Make it vivid and specific (setting, lighting, camera/composition, color palette, mood).
- Keep style consistent with the chosen preset.
- Focus EXCLUSIVELY on visual elements: subjects, lighting, textures, colors, camera angles, atmosphere.
- If Global Subject is provided, it MUST be the primary focus. Restate its key identifiers explicitly (face, hair, outfit, materials) so the subject stays 100% consistent across scenes.
- If the subject is a person, use consistent descriptors.
- Ensure the prompt STARTS with the subject name or a concrete description of it.
- Keep length 70–130 words for maximum detail.
- Use specific visual descriptors (e.g., "amber light", "weathered oak", "muted teal") rather than generic phrases.

User Prompt:
${promptText}`,
    config: {
      responseMimeType: "application/json",
      responseSchema: {
        type: Type.OBJECT,
        properties: {
          prompt: { type: Type.STRING },
        },
        required: ["prompt"],
      },
    },
  });

  const jsonStr = response.text;
  if (!jsonStr) return promptText;

  try {
    const parsed = JSON.parse(jsonStr) as { prompt: string };
    return parsed.prompt?.trim() ? parsed.prompt.trim() : promptText;
  } catch {
    return promptText;
  }
}


// --- Main Services ---

/**
 * Internal function to generate prompts from content.
 */
const generatePrompts = async (
  srtContent: string,
  style: string,
  mode: "lyrics" | "story",
  globalSubject: string = "",
  purpose: VideoPurpose = "music_video",
): Promise<ImagePrompt[]> => {
  return withRetry(async () => {
    try {
      const response = await ai.models.generateContent({
        model: MODELS.TEXT,
        contents: getPromptGenerationInstruction(
          style,
          mode,
          srtContent,
          globalSubject,
          purpose,
        ),
        config: {
          responseMimeType: "application/json",
          responseSchema: {
            type: Type.OBJECT,
            properties: {
              prompts: {
                type: Type.ARRAY,
                items: {
                  type: Type.OBJECT,
                  properties: {
                    text: { type: Type.STRING },
                    mood: { type: Type.STRING },
                    timestamp: { type: Type.STRING },
                  },
                  required: ["text", "mood", "timestamp"],
                },
              },
            },
          },
        },
      });

      const jsonStr = response.text;
      if (!jsonStr) throw new Error("No prompts generated");

      const parsed = JSON.parse(jsonStr) as { prompts: PromptResponseItem[] };

      return parsed.prompts.map((p, index: number) => ({
        text: p.text,
        mood: p.mood,
        timestamp: p.timestamp,
        id: `prompt-${Date.now()}-${index}`,
        timestampSeconds: parseSRTTimestamp(p.timestamp) ?? 0,
      }));
    } catch (error) {
      console.error("Prompt generation error:", error);
      return [];
    }
  });
};

/**
 * Generate image prompts from song lyrics.
 */
export const generatePromptsFromLyrics = (
  srtContent: string,
  style: string = "Cinematic",
  globalSubject: string = "",
  purpose: VideoPurpose = "music_video",
) => generatePrompts(srtContent, style, "lyrics", globalSubject, purpose);

/**
 * Generate image prompts from story/narrative content.
 */
export const generatePromptsFromStory = (
  srtContent: string,
  style: string = "Cinematic",
  globalSubject: string = "",
  purpose: VideoPurpose = "documentary",
) => generatePrompts(srtContent, style, "story", globalSubject, purpose);

/**
 * Refine an image prompt with linting and optional AI enhancement.
 */
export const refineImagePrompt = async (params: {
  promptText: string;
  style?: string;
  globalSubject?: string;
  aspectRatio?: string;
  intent?: PromptRefinementIntent;
  previousPrompts?: string[];
}): Promise<{ refinedPrompt: string; issues: PromptLintIssue[] }> => {
  const {
    promptText,
    style = "Cinematic",
    globalSubject = "",
    aspectRatio = "16:9",
    intent = "auto",
    previousPrompts = [],
  } = params;

  const issues = lintPrompt({ promptText, globalSubject, previousPrompts });

  // Only run an AI rewrite if it looks low quality or the user explicitly requests a change.
  const shouldRefine =
    intent !== "auto" ||
    issues.some(
      (i) =>
        i.code === "too_short" ||
        i.code === "repetitive" ||
        i.code === "missing_subject",
    );

  if (!shouldRefine) {
    return { refinedPrompt: promptText.trim(), issues };
  }

  const refinedPrompt = await withRetry(async () => {
    return refineImagePromptWithAI({
      promptText,
      style,
      globalSubject,
      aspectRatio,
      intent,
      issues,
    });
  });

  return { refinedPrompt, issues };
};

/**
 * Refine an image prompt and return a partial ImageStyleGuide via AI.
 * The AI fills in structured fields (mood, lighting, background, composition, etc.)
 * based on the raw prompt text + style + subject context.
 *
 * Existing `refineImagePrompt()` callers are unaffected — only `imageService.ts`
 * uses this guide variant.
 */
export const refineImagePromptAsGuide = async (params: {
  promptText: string;
  style?: string;
  globalSubject?: string;
  aspectRatio?: string;
  intent?: PromptRefinementIntent;
  previousPrompts?: string[];
}): Promise<{
  guide: Partial<ImageStyleGuide>;
  issues: PromptLintIssue[];
}> => {
  const {
    promptText,
    style = "Cinematic",
    globalSubject = "",
    aspectRatio = "16:9",
    intent = "auto",
    previousPrompts = [],
  } = params;

  const issues = lintPrompt({ promptText, globalSubject, previousPrompts });

  const shouldRefine =
    intent !== "auto" ||
    issues.some(
      (i) =>
        i.code === "too_short" ||
        i.code === "repetitive" ||
        i.code === "missing_subject",
    );

  if (!shouldRefine) {
    // Return a minimal guide with just the scene text populated
    return { guide: { scene: promptText.trim() }, issues };
  }

  const issueSummary =
    issues.length > 0
      ? issues.map((i) => `- (${i.code}) ${i.message}`).join("\n")
      : "- (none)";

  const guide = await withRetry(async () => {
    const response = await ai.models.generateContent({
      model: MODELS.TEXT,
      contents: `You are a prompt engineer for high-quality image generation.
Refine the user's prompt into structured fields for a JSON style guide.

Global Subject: ${globalSubject || "(none)"}
Style Preset: ${style}
Aspect Ratio: ${aspectRatio}
User Intent: ${intent}
Detected Issues:
${issueSummary}

Requirements:
- Output a JSON object with these optional fields:
  scene (string), mood (string), background (string),
  lighting (object with source, quality, direction),
  composition (object with shot_type, camera_angle, framing),
  camera (object with lens, depth_of_field, focus),
  color_palette (array of strings),
  textures (array of strings),
  effects (array of strings).
- Only include fields where you can meaningfully improve the prompt.
- The "scene" field should be a vivid, specific rewrite of the user's prompt (70-130 words).
- If Global Subject is provided, ensure it is the primary focus in the scene.
- Be specific with visual descriptors.

User Prompt:
${promptText}`,
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            scene: { type: Type.STRING },
            mood: { type: Type.STRING },
            background: { type: Type.STRING },
            lighting: {
              type: Type.OBJECT,
              properties: {
                source: { type: Type.STRING },
                quality: { type: Type.STRING },
                direction: { type: Type.STRING },
              },
            },
            composition: {
              type: Type.OBJECT,
              properties: {
                shot_type: { type: Type.STRING },
                camera_angle: { type: Type.STRING },
                framing: { type: Type.STRING },
              },
            },
            camera: {
              type: Type.OBJECT,
              properties: {
                lens: { type: Type.STRING },
                depth_of_field: { type: Type.STRING },
                focus: { type: Type.STRING },
              },
            },
            color_palette: { type: Type.ARRAY, items: { type: Type.STRING } },
            textures: { type: Type.ARRAY, items: { type: Type.STRING } },
            effects: { type: Type.ARRAY, items: { type: Type.STRING } },
          },
        },
      },
    });

    const jsonStr = response.text;
    if (!jsonStr) return { scene: promptText.trim() } as Partial<ImageStyleGuide>;

    try {
      return JSON.parse(jsonStr) as Partial<ImageStyleGuide>;
    } catch {
      return { scene: promptText.trim() } as Partial<ImageStyleGuide>;
    }
  });

  return { guide, issues };
};

/**
 * Generate a professional, cinematic video prompt for Veo 3.1.
 * Transforms a scene description into a production-quality video generation prompt
 * with camera movements, lighting, pacing, and cinematic techniques.
 *
 * @param sceneDescription - The scene's visual description
 * @param style - Art style (Cinematic, Documentary, etc.)
 * @param mood - Emotional tone of the scene
 * @param globalSubject - Subject to keep consistent
 * @param videoPurpose - Purpose of the video (documentary, commercial, etc.)
 * @param durationSeconds - Target video duration (4, 6, or 8 seconds)
 */
export const generateProfessionalVideoPrompt = async (
  sceneDescription: string,
  style: string = "Cinematic",
  mood: string = "dramatic",
  globalSubject: string = "",
  videoPurpose: string = "documentary",
  durationSeconds: number = 6,
): Promise<string> => {
  return withRetry(async () => {
    const response = await ai.models.generateContent({
      model: MODELS.TEXT,
      contents: `You are an elite cinematographer and video director creating prompts for Google Veo 3.1 (state-of-the-art AI video generation).

SCENE DESCRIPTION TO TRANSFORM:
"${sceneDescription}"

PRODUCTION PARAMETERS:
- Visual Style: ${style}
- Emotional Tone: ${mood}
- Video Purpose: ${videoPurpose}
- Duration: ${durationSeconds} seconds
${globalSubject ? `- Main Subject (MUST be featured prominently): ${globalSubject}` : ''}

YOUR TASK: Transform the scene description into a professional video generation prompt that will produce cinematic, broadcast-quality footage.

MANDATORY ELEMENTS TO INCLUDE:
1. **CAMERA WORK** (choose appropriate technique):
   - Movement: dolly in/out, tracking shot, crane up/down, steadicam follow, handheld, static with subject motion
   - Speed: slow push (contemplative), medium pace (narrative), dynamic movement (action)
   - Framing: wide establishing → medium → close-up progression, or single powerful composition

2. **LIGHTING DESIGN**:
   - Key light direction and quality (harsh, soft, diffused, directional)
   - Mood lighting (golden hour, blue hour, moonlight, neon, practical lights)
   - Contrast ratio (high contrast noir, low contrast ethereal, natural)
   - Light motivation (window light, fire glow, screen glow, sun rays)

3. **MOTION CHOREOGRAPHY**:
   - Subject movement within frame (walking, turning, gesturing, still with breath)
   - Environmental motion (wind in hair/clothes, particles, smoke, water, reflections)
   - Parallax and depth (foreground elements moving differently than background)

4. **ATMOSPHERE & ENVIRONMENT**:
   - Volumetric effects (fog, haze, dust particles, rain, snow, lens flares)
   - Environmental storytelling elements
   - Time of day and weather conditions

5. **TEMPORAL PACING**:
   - For ${durationSeconds}s video, describe a mini-arc:
     - Seconds 0-2: Establish (wide or reveal)
     - Seconds 2-4: Focus (move to subject)
     - Seconds 4-${durationSeconds}: Conclude (emotional beat or pullback)

STYLE-SPECIFIC GUIDANCE:
${style === "Cinematic" ? `
- 35mm film grain, anamorphic lens flares, 2.39:1 cinematic feel
- Deep shadows, rich highlights, film color science
- Motivated camera movement, deliberate pacing
- Production design worthy of major motion pictures` : ''}
${style === "Documentary" ? `
- Handheld authenticity with stabilization
- Natural lighting, observational approach
- Intimate close-ups, revealing wide shots
- Truth-seeking camera that follows action` : ''}
${style === "Commercial / Ad" ? `
- Pristine, polished, aspirational visuals
- Hero lighting on products/subjects
- Smooth, confident camera movements
- High production value, clean compositions` : ''}
${style === "Anime / Manga" ? `
- Anime-style dynamic camera angles
- Wind and particle effects
- Dramatic speed lines and motion blur
- Expressive character moments` : ''}

OUTPUT FORMAT:
Generate a single, detailed video prompt (150-250 words) that reads as professional direction.
Start directly with the scene action (no preamble like "A video of...").
Write in present tense, active voice.
Be specific about timing and progression within the ${durationSeconds} seconds.

Return JSON: { "videoPrompt": "your detailed prompt here" }`,
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            videoPrompt: { type: Type.STRING },
          },
          required: ["videoPrompt"],
        },
      },
    });

    const jsonStr = response.text;
    if (!jsonStr) {
      // Fallback: return enhanced version of original
      return `${style} shot: ${sceneDescription}. Smooth camera movement, professional lighting, cinematic composition. ${mood} atmosphere with environmental details.`;
    }

    try {
      const parsed = JSON.parse(jsonStr) as { videoPrompt: string };
      if (parsed.videoPrompt && parsed.videoPrompt.length > 50) {
        return parsed.videoPrompt;
      }
      return `${style} shot: ${sceneDescription}. Smooth camera movement, professional lighting. ${mood} mood.`;
    } catch {
      return `${style}: ${sceneDescription}. Cinematic quality, ${mood} lighting.`;
    }
  });
};

/**
 * Generate a motion-optimized prompt for video animation.
 * Transforms a static image description into an animation-focused prompt
 * that specifies camera movements, environmental effects, and subtle animations.
 *
 * Returns a `MotionPromptResult` with separate `camera_motion` and `subject_physics`
 * fields plus a pre-joined `combined` string for single-string APIs.
 */
export const generateMotionPrompt = async (
  imagePrompt: string,
  mood: string = "cinematic",
  globalSubject: string = "",
): Promise<MotionPromptResult> => {
  return withRetry(async () => {
    const response = await ai.models.generateContent({
      model: MODELS.TEXT,
      contents: `You are a professional video director creating motion instructions for animating a still image.

STATIC IMAGE DESCRIPTION:
${imagePrompt}

MOOD: ${mood}

${globalSubject ? `MAIN SUBJECT (keep stationary/subtle movement only): ${globalSubject}` : ""}

TASK: Generate TWO separate motion descriptions (≤25 words each):
1. camera_motion — Camera ONLY: movement type, direction, speed (e.g. "slow push-in", "gentle pan left", "static with parallax depth")
2. subject_physics — Environment/subject ONLY: wind, particles, light, cloth, water (e.g. "leaves gently swaying", "candle flame flickering", "fog drifting")

RULES:
- Keep the main subject relatively static (subtle breathing, blinking, hair movement only)
- camera_motion must describe ONLY camera behaviour — no environment or subject action
- subject_physics must describe ONLY environment/subject physical motion — no camera references
- The animation is only 1-2 seconds, so describe subtle, looping motion
- NO scene changes, NO new elements, NO action sequences
- Use present continuous tense

OUTPUT: Return JSON with fields "camera_motion" and "subject_physics"`,
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            camera_motion: { type: Type.STRING },
            subject_physics: { type: Type.STRING },
          },
          required: ["camera_motion", "subject_physics"],
        },
      },
    });

    const jsonStr = response.text;
    if (!jsonStr) {
      const camera_motion = `slow cinematic push-in`;
      const subject_physics = `gentle ${mood} atmospheric movement`;
      return { camera_motion, subject_physics, combined: `${camera_motion}. ${subject_physics}` };
    }

    try {
      const parsed = JSON.parse(jsonStr) as { camera_motion: string; subject_physics: string };
      const camera_motion = parsed.camera_motion || `slow camera movement`;
      const subject_physics = parsed.subject_physics || `subtle ${mood} atmosphere`;
      return { camera_motion, subject_physics, combined: `${camera_motion}. ${subject_physics}` };
    } catch {
      const camera_motion = `slow cinematic camera movement`;
      const subject_physics = `${mood} ambiance with gentle environmental motion`;
      return { camera_motion, subject_physics, combined: `${camera_motion}. ${subject_physics}` };
    }
  });
};

/**
 * Compress a long image generation prompt into a concise keyword-first form.
 *
 * Story-mode shots can exceed 200 words after persona + style + character injection.
 * Long prompts cause "instruction dilution" where models ignore later details.
 * This compressor front-loads the critical visual elements (subject → action →
 * lighting → mood → style) and trims to ≤80 words.
 *
 * Short prompts (≤100 words) are returned unchanged to avoid unnecessary API calls.
 * All errors fall back to the original prompt — this is a non-fatal enhancement.
 */
export const compressPromptForGeneration = async (prompt: string): Promise<string> => {
  if (countWords(prompt) <= 100) return prompt;

  try {
    const response = await ai.models.generateContent({
      model: MODELS.TEXT,
      contents: `Rewrite the following image generation prompt into comma-separated visual keywords.
Max 80 words. Front-load: subject → action/pose → lighting → mood → style keyword.
Preserve: main subject, key action, lighting quality, mood, one style keyword.
Drop: filler, repeated adjectives, verbose background descriptions.
Output ONLY the rewritten prompt as a single comma-separated line.

PROMPT:
${prompt}`,
      config: { temperature: 0.1 },
    });

    const compressed = response.text?.trim();
    if (!compressed || compressed.length < 20) return prompt;
    if (countWords(compressed) >= countWords(prompt)) return prompt;

    return compressed;
  } catch {
    return prompt;
  }
};
````

## File: packages/shared/src/services/qualityMonitorService.ts
````typescript
/**
 * Quality Monitor Service
 * 
 * Tracks and analyzes video production quality metrics.
 * Provides insights for improving AI output and user experience.
 */

import { ContentPlan, Scene, NarrationSegment, ValidationResult } from "../types";
import { VideoSFXPlan } from "./sfxService";
import { VideoPurpose } from "../constants";
import { getEffectiveLegacyTone } from "./tripletUtils";

// --- Quality Metrics Types ---

export interface SceneQualityMetrics {
  sceneId: string;
  sceneName: string;
  
  // Timing metrics
  duration: number;
  narrationDuration: number | null;
  timingSync: number; // 0-100, how well narration matches scene duration
  wordsPerSecond: number;
  
  // Content metrics
  visualDescriptionLength: number;
  visualDescriptionQuality: "poor" | "fair" | "good" | "excellent";
  narrationWordCount: number;
  narrationQuality: "poor" | "fair" | "good" | "excellent";
  
  // SFX metrics
  hasSfx: boolean;
  sfxRelevance: number; // 0-100, how relevant the SFX is to scene
  hasAudioUrl: boolean;
  
  // Issues
  issues: string[];
  suggestions: string[];
}

export interface ProductionQualityReport {
  // Overall scores
  overallScore: number; // 0-100
  contentScore: number;
  timingScore: number;
  visualScore: number;
  audioScore: number;
  
  // Metadata
  title: string;
  videoPurpose: VideoPurpose;
  totalDuration: number;
  sceneCount: number;
  timestamp: Date;
  
  // Per-scene breakdown
  sceneMetrics: SceneQualityMetrics[];
  
  // Aggregated insights
  strengths: string[];
  weaknesses: string[];
  actionableImprovements: string[];
  
  // Technical metrics
  avgWordsPerSecond: number;
  avgSceneDuration: number;
  visualCoverage: number; // % of scenes with visuals
  audioCoverage: number; // % of scenes with narration
  sfxCoverage: number; // % of scenes with SFX
  
  // AI performance metrics
  aiSfxAccuracy: number; // % of AI-suggested SFX that were valid
  contentPlannerCreativity: "low" | "medium" | "high";
}

// --- Quality Analysis Functions ---

/**
 * Analyze visual description quality based on specificity and length.
 */
function analyzeVisualDescription(description: string): {
  quality: "poor" | "fair" | "good" | "excellent";
  score: number;
  issues: string[];
} {
  const issues: string[] = [];
  let score = 50;
  
  const length = description.length;
  const words = description.split(/\s+/).length;
  
  // Length scoring
  if (length < 30) {
    score -= 20;
    issues.push("Visual description too short - add more detail");
  } else if (length >= 80 && length <= 180) {
    score += 20;
  } else if (length > 200) {
    score -= 10;
    issues.push("Visual description may be truncated");
  }
  
  // Check for concrete visual elements
  const concreteKeywords = /color|light|shadow|close-up|wide shot|background|foreground|texture|movement|expression/i;
  if (concreteKeywords.test(description)) {
    score += 15;
  } else {
    issues.push("Add concrete visual elements (lighting, camera angle, colors)");
  }
  
  // Check for abstract/vague language
  const vagueKeywords = /beautiful|amazing|wonderful|nice|good|great|interesting/i;
  if (vagueKeywords.test(description)) {
    score -= 10;
    issues.push("Replace vague adjectives with specific visual details");
  }
  
  // Check for action/movement
  const actionKeywords = /moving|walking|running|flowing|rising|falling|spinning|floating/i;
  if (actionKeywords.test(description)) {
    score += 10;
  }
  
  score = Math.max(0, Math.min(100, score));
  
  const quality = score >= 80 ? "excellent" : score >= 60 ? "good" : score >= 40 ? "fair" : "poor";
  
  return { quality, score, issues };
}

/**
 * Analyze narration quality based on pacing and engagement.
 */
function analyzeNarration(script: string, duration: number): {
  quality: "poor" | "fair" | "good" | "excellent";
  score: number;
  wordsPerSecond: number;
  issues: string[];
} {
  const issues: string[] = [];
  let score = 50;
  
  const words = script.split(/\s+/).filter(w => w.length > 0);
  const wordCount = words.length;
  const wordsPerSecond = wordCount / duration;
  
  // Ideal speaking rate is 2.0-2.8 words/second
  if (wordsPerSecond < 1.5) {
    score -= 15;
    issues.push("Narration too sparse - add more content or reduce scene duration");
  } else if (wordsPerSecond > 3.2) {
    score -= 20;
    issues.push("Narration too dense - reduce words or increase scene duration");
  } else if (wordsPerSecond >= 2.0 && wordsPerSecond <= 2.8) {
    score += 20;
  }
  
  // Check for sentence variety
  const sentences = script.split(/[.!?]+/).filter(s => s.trim().length > 0);
  const avgSentenceLength = wordCount / Math.max(1, sentences.length);
  
  if (avgSentenceLength > 25) {
    score -= 10;
    issues.push("Sentences too long - break into shorter, punchier phrases");
  } else if (avgSentenceLength < 5) {
    score -= 5;
    issues.push("Sentences very short - consider combining for better flow");
  }
  
  // Check for engagement markers
  const engagementMarkers = /imagine|discover|witness|experience|journey|secret|reveal|transform/i;
  if (engagementMarkers.test(script)) {
    score += 15;
  }
  
  // Check for questions (engagement technique)
  if (/\?/.test(script)) {
    score += 5;
  }
  
  score = Math.max(0, Math.min(100, score));
  
  const quality = score >= 80 ? "excellent" : score >= 60 ? "good" : score >= 40 ? "fair" : "poor";
  
  return { quality, score, wordsPerSecond, issues };
}

/**
 * Analyze SFX relevance to scene content.
 */
function analyzeSfxRelevance(
  scene: Scene,
  sfxId: string | null,
  sfxName: string | null
): { relevance: number; issues: string[] } {
  if (!sfxId || !sfxName) {
    return { relevance: 0, issues: ["No SFX assigned to scene"] };
  }
  
  const issues: string[] = [];
  let relevance = 50;
  
  const sceneText = `${scene.visualDescription} ${scene.narrationScript} ${scene.name}`.toLowerCase();
  
  // Check keyword matches
  const sfxKeywordMap: Record<string, string[]> = {
    "desert-wind": ["desert", "sand", "dune", "sahara", "dry", "wind"],
    "desert-night": ["desert", "night", "stars", "quiet", "peaceful"],
    "ocean-waves": ["ocean", "sea", "beach", "wave", "water", "coast"],
    "forest-ambience": ["forest", "tree", "bird", "nature", "wood"],
    "rain-gentle": ["rain", "storm", "water", "cozy"],
    "thunderstorm": ["storm", "thunder", "lightning", "dramatic"],
    "city-traffic": ["city", "urban", "traffic", "street", "car"],
    "cafe-ambience": ["cafe", "coffee", "restaurant", "social"],
    "marketplace": ["market", "bazaar", "souk", "crowd", "vendor"],
    "eerie-ambience": ["horror", "scary", "ghost", "haunted", "dark", "eerie"],
    "mystical-drone": ["magic", "mystical", "fantasy", "ethereal", "ancient"],
    "whispers": ["ghost", "spirit", "whisper", "supernatural"],
    "heartbeat": ["tension", "suspense", "fear", "anxiety"],
    "tension-drone": ["tension", "suspense", "thriller", "dark"],
    "hopeful-pad": ["hope", "positive", "uplifting", "inspiring"],
    "epic-strings": ["epic", "dramatic", "emotional", "powerful"],
    "middle-eastern": ["arabic", "middle east", "oriental", "desert", "arabian"],
  };
  
  const keywords = sfxKeywordMap[sfxId] || [];
  const matches = keywords.filter(kw => sceneText.includes(kw));
  
  if (matches.length >= 2) {
    relevance = 90;
  } else if (matches.length === 1) {
    relevance = 70;
  } else {
    relevance = 40;
    issues.push(`SFX "${sfxName}" may not match scene content well`);
  }
  
  return { relevance, issues };
}

/**
 * Generate a comprehensive quality report for a production.
 */
export function generateQualityReport(
  contentPlan: ContentPlan,
  narrationSegments: NarrationSegment[],
  sfxPlan: VideoSFXPlan | null,
  validation: ValidationResult | null,
  videoPurpose: VideoPurpose
): ProductionQualityReport {
  const sceneMetrics: SceneQualityMetrics[] = [];
  
  let totalTimingScore = 0;
  let totalVisualScore = 0;
  let totalNarrationScore = 0;
  let totalSfxRelevance = 0;
  let totalWordsPerSecond = 0;
  let scenesWithSfx = 0;
  let scenesWithAudio = 0;
  let aiSfxValid = 0;
  let aiSfxTotal = 0;
  
  const allIssues: string[] = [];
  const allSuggestions: string[] = [];
  
  // Analyze each scene
  contentPlan.scenes.forEach((scene, index) => {
    const narration = narrationSegments.find(n => n.sceneId === scene.id);
    const sfxScene = sfxPlan?.scenes.find(s => s.sceneId === scene.id);
    
    // Visual analysis
    const visualAnalysis = analyzeVisualDescription(scene.visualDescription);
    totalVisualScore += visualAnalysis.score;
    
    // Narration analysis
    const narrationAnalysis = analyzeNarration(scene.narrationScript, scene.duration);
    totalNarrationScore += narrationAnalysis.score;
    totalWordsPerSecond += narrationAnalysis.wordsPerSecond;
    
    // Timing sync
    let timingSync = 100;
    if (narration) {
      scenesWithAudio++;
      const diff = Math.abs(narration.audioDuration - scene.duration);
      timingSync = Math.max(0, 100 - (diff * 10));
    }
    totalTimingScore += timingSync;
    
    // SFX analysis
    let sfxRelevance = 0;
    let hasSfx = false;
    let hasAudioUrl = false;
    const sfxIssues: string[] = [];
    
    if (sfxScene?.ambientTrack) {
      hasSfx = true;
      scenesWithSfx++;
      hasAudioUrl = !!sfxScene.ambientTrack.audioUrl;
      
      const sfxAnalysis = analyzeSfxRelevance(
        scene,
        sfxScene.ambientTrack.id,
        sfxScene.ambientTrack.name
      );
      sfxRelevance = sfxAnalysis.relevance;
      sfxIssues.push(...sfxAnalysis.issues);
      totalSfxRelevance += sfxRelevance;
    }
    
    // Track AI SFX accuracy
    if (scene.ambientSfx) {
      aiSfxTotal++;
      if (sfxScene?.ambientTrack?.id === scene.ambientSfx) {
        aiSfxValid++;
      }
    }
    
    // Combine issues
    const sceneIssues = [
      ...visualAnalysis.issues,
      ...narrationAnalysis.issues,
      ...sfxIssues,
    ];
    
    if (timingSync < 80) {
      sceneIssues.push("Narration timing doesn't match scene duration");
    }
    
    allIssues.push(...sceneIssues.map(i => `Scene ${index + 1}: ${i}`));
    
    sceneMetrics.push({
      sceneId: scene.id,
      sceneName: scene.name,
      duration: scene.duration,
      narrationDuration: narration?.audioDuration || null,
      timingSync,
      wordsPerSecond: narrationAnalysis.wordsPerSecond,
      visualDescriptionLength: scene.visualDescription.length,
      visualDescriptionQuality: visualAnalysis.quality,
      narrationWordCount: scene.narrationScript.split(/\s+/).length,
      narrationQuality: narrationAnalysis.quality,
      hasSfx,
      sfxRelevance,
      hasAudioUrl,
      issues: sceneIssues,
      suggestions: [],
    });
  });
  
  const sceneCount = contentPlan.scenes.length;
  
  // Calculate aggregate scores
  const contentScore = Math.round((totalVisualScore + totalNarrationScore) / (sceneCount * 2));
  const timingScore = Math.round(totalTimingScore / sceneCount);
  const visualScore = Math.round(totalVisualScore / sceneCount);
  const audioScore = scenesWithAudio > 0 
    ? Math.round((totalNarrationScore / sceneCount + (scenesWithAudio / sceneCount) * 100) / 2)
    : 0;
  
  // Overall score (weighted average)
  const overallScore = Math.round(
    contentScore * 0.35 +
    timingScore * 0.25 +
    visualScore * 0.25 +
    audioScore * 0.15
  );
  
  // Generate insights
  const strengths: string[] = [];
  const weaknesses: string[] = [];
  const actionableImprovements: string[] = [];
  
  // Analyze strengths
  if (visualScore >= 75) strengths.push("Strong visual descriptions");
  if (timingScore >= 85) strengths.push("Excellent narration-scene timing sync");
  if (scenesWithSfx / sceneCount >= 0.8) strengths.push("Good SFX coverage");
  if (totalWordsPerSecond / sceneCount >= 2.0 && totalWordsPerSecond / sceneCount <= 2.8) {
    strengths.push("Optimal narration pacing");
  }
  
  // Analyze weaknesses
  if (visualScore < 60) weaknesses.push("Visual descriptions need more detail");
  if (timingScore < 70) weaknesses.push("Narration timing issues");
  if (scenesWithSfx / sceneCount < 0.5) weaknesses.push("Limited SFX coverage");
  if (totalWordsPerSecond / sceneCount > 3.0) weaknesses.push("Narration too fast");
  if (totalWordsPerSecond / sceneCount < 1.8) weaknesses.push("Narration too sparse");
  
  // Generate actionable improvements
  if (visualScore < 70) {
    actionableImprovements.push("Add specific visual elements: lighting, camera angles, colors, textures");
  }
  if (timingScore < 80) {
    actionableImprovements.push("Adjust scene durations to match narration length");
  }
  if (scenesWithSfx / sceneCount < 0.7) {
    actionableImprovements.push("Add ambient SFX to more scenes for immersion");
  }
  if (contentScore < 70) {
    actionableImprovements.push("Use more engaging language: questions, sensory words, action verbs");
  }
  
  // Determine creativity level
  const avgDescLength = sceneMetrics.reduce((sum, s) => sum + s.visualDescriptionLength, 0) / sceneCount;
  const hasVariedTones = new Set(contentPlan.scenes.map(s => getEffectiveLegacyTone(s))).size >= 2;
  const contentPlannerCreativity: "low" | "medium" | "high" = 
    avgDescLength > 120 && hasVariedTones ? "high" :
    avgDescLength > 80 ? "medium" : "low";
  
  return {
    overallScore,
    contentScore,
    timingScore,
    visualScore,
    audioScore,
    title: contentPlan.title,
    videoPurpose,
    totalDuration: contentPlan.totalDuration,
    sceneCount,
    timestamp: new Date(),
    sceneMetrics,
    strengths,
    weaknesses,
    actionableImprovements,
    avgWordsPerSecond: totalWordsPerSecond / sceneCount,
    avgSceneDuration: contentPlan.totalDuration / sceneCount,
    visualCoverage: 100, // Assuming all scenes have visuals
    audioCoverage: (scenesWithAudio / sceneCount) * 100,
    sfxCoverage: (scenesWithSfx / sceneCount) * 100,
    aiSfxAccuracy: aiSfxTotal > 0 ? (aiSfxValid / aiSfxTotal) * 100 : 100,
    contentPlannerCreativity,
  };
}

/**
 * Get a summary string for quick display.
 */
export function getQualitySummary(report: ProductionQualityReport): string {
  const emoji = report.overallScore >= 80 ? "🌟" : report.overallScore >= 60 ? "✅" : "⚠️";
  return `${emoji} Quality: ${report.overallScore}/100 | Content: ${report.contentScore} | Timing: ${report.timingScore} | Visual: ${report.visualScore}`;
}

/**
 * Export report as JSON for analytics.
 */
export function exportReportAsJson(report: ProductionQualityReport): string {
  return JSON.stringify(report, null, 2);
}

/**
 * Store report in localStorage for history tracking.
 */
export function saveReportToHistory(report: ProductionQualityReport): void {
  const historyKey = "lyriclens_quality_history";
  const existing = localStorage.getItem(historyKey);
  const history: ProductionQualityReport[] = existing ? JSON.parse(existing) : [];
  
  // Keep last 20 reports
  history.unshift(report);
  if (history.length > 20) {
    history.pop();
  }
  
  localStorage.setItem(historyKey, JSON.stringify(history));
}

/**
 * Get quality history from localStorage.
 */
export function getQualityHistory(): ProductionQualityReport[] {
  const historyKey = "lyriclens_quality_history";
  const existing = localStorage.getItem(historyKey);
  return existing ? JSON.parse(existing) : [];
}

/**
 * Calculate average scores from history.
 */
export function getHistoricalAverages(): {
  avgOverall: number;
  avgContent: number;
  avgTiming: number;
  avgVisual: number;
  trend: "improving" | "stable" | "declining";
} | null {
  const history = getQualityHistory();
  if (history.length < 2) return null;
  
  const avgOverall = history.reduce((sum, r) => sum + r.overallScore, 0) / history.length;
  const avgContent = history.reduce((sum, r) => sum + r.contentScore, 0) / history.length;
  const avgTiming = history.reduce((sum, r) => sum + r.timingScore, 0) / history.length;
  const avgVisual = history.reduce((sum, r) => sum + r.visualScore, 0) / history.length;
  
  // Calculate trend (compare recent 3 vs older 3)
  const recent = history.slice(0, 3);
  const older = history.slice(-3);
  const recentAvg = recent.reduce((sum, r) => sum + r.overallScore, 0) / recent.length;
  const olderAvg = older.reduce((sum, r) => sum + r.overallScore, 0) / older.length;
  
  const trend = recentAvg > olderAvg + 5 ? "improving" :
                recentAvg < olderAvg - 5 ? "declining" : "stable";
  
  return { avgOverall, avgContent, avgTiming, avgVisual, trend };
}
````

## File: packages/shared/src/services/researchService.ts
````typescript
/**
 * Research Service
 *
 * Performs parallel web/knowledge queries for content-heavy formats.
 * Uses the Parallel Execution Engine for concurrent query execution,
 * aggregates and deduplicates results, and prioritizes reference documents.
 *
 * Requirements: 3.5, 7.2, 7.6, 11.1–11.6, 20.2, 22.2
 */

import { ParallelExecutionEngine, Task } from './parallelExecutionEngine';
import { ai, MODELS } from './shared/apiClient';
import { Type } from '@google/genai';
import { chunkContent, tokenize, jaccardSimilarity } from './utils/textProcessing';

// Grounded response shape (candidates[0].groundingMetadata)
interface GroundingChunk {
  web?: { uri?: string; title?: string };
}
interface GroundingMetadata {
  webSearchQueries?: string[];
  groundingChunks?: GroundingChunk[];
}

// ============================================================================
// Types and Interfaces
// ============================================================================

export interface ResearchQuery {
  topic: string;
  language: 'ar' | 'en';
  depth: 'shallow' | 'medium' | 'deep';
  sources: ('web' | 'knowledge-base' | 'references')[];
  maxResults: number;
  referenceDocuments?: IndexedDocument[];
}

export interface ResearchResult {
  sources: Source[];
  summary: string;
  citations: Citation[];
  confidence: number;
  /** True when some queries failed but partial results were returned */
  partial?: boolean;
  /** Number of queries that failed */
  failedQueries?: number;
}

export interface Source {
  id: string;
  title: string;
  content: string;
  url?: string;
  type: 'web' | 'knowledge-base' | 'reference';
  relevance: number;
  language: 'ar' | 'en';
}

export interface Citation {
  sourceId: string;
  text: string;
  position: number;
}

export interface IndexedDocument {
  id: string;
  filename: string;
  content: string;
  chunks: string[];
  metadata: Record<string, any>;
}

// ============================================================================
// Constants
// ============================================================================

/** Number of parallel sub-queries per depth level */
const DEPTH_QUERY_COUNTS: Record<ResearchQuery['depth'], number> = {
  shallow: 3,
  medium: 5,
  deep: 8,
};

/** Content chunk size in characters for document indexing */
const CHUNK_SIZE = 1000;

/**
 * Jaccard similarity threshold above which two sources are considered duplicates.
 * Property 12: sources with similarity > 0.9 are deduplicated.
 */
const DEDUP_SIMILARITY_THRESHOLD = 0.9;

/** Relevance score assigned to reference document sources (highest priority) */
const REFERENCE_RELEVANCE_BASE = 1.0;

/** Maximum relevance score for web/knowledge-base sources */
const QUERY_RELEVANCE_MAX = 0.85;

/** Sub-query aspect templates for diversifying research coverage */
const QUERY_ASPECTS_EN = [
  'overview and definition',
  'historical background and context',
  'current state and recent developments',
  'key facts and statistics',
  'expert analysis and perspectives',
  'related topics and connections',
  'challenges and controversies',
  'future implications and trends',
];

const QUERY_ASPECTS_AR = [
  'نظرة عامة وتعريف',
  'الخلفية التاريخية والسياق',
  'الوضع الراهن والتطورات الأخيرة',
  'الحقائق والإحصائيات الرئيسية',
  'التحليل وآراء الخبراء',
  'الموضوعات ذات الصلة والروابط',
  'التحديات والجدل',
  'الآثار المستقبلية والاتجاهات',
];

// ============================================================================
// Research Service
// ============================================================================

export class ResearchService {
  private engine: ParallelExecutionEngine;

  constructor(engine?: ParallelExecutionEngine) {
    this.engine = engine ?? new ParallelExecutionEngine();
  }

  // --------------------------------------------------------------------------
  // Public API
  // --------------------------------------------------------------------------

  /**
   * Research a topic by executing multiple queries in parallel.
   * Returns aggregated, deduplicated, and prioritized sources.
   * On partial failure, returns available results with `partial: true`.
   *
   * Requirements: 11.1, 11.2, 11.3, 11.4, 11.5, 11.6
   */
  async research(query: ResearchQuery): Promise<ResearchResult> {
    const tasks = this.buildTasks(query);

    // Execute all queries concurrently (Req 11.1)
    const results = await this.engine.execute(tasks, {
      concurrencyLimit: 5,
      retryAttempts: 2,
      retryDelay: 1000,
      exponentialBackoff: true,
    });

    // Collect sources — partial failure handled by gathering successful results (Req 11.4)
    const allSources: Source[] = [];
    let failedCount = 0;

    for (const result of results) {
      if (result.success && result.data) {
        allSources.push(...result.data);
      } else {
        failedCount++;
      }
    }

    // Deduplicate results (Req 11.2)
    const uniqueSources = this.deduplicateSources(allSources);

    // Sort: references first with highest relevance, then by relevance desc (Req 11.3)
    const sortedSources = this.sortSources(uniqueSources);

    // Limit to maxResults
    const limitedSources = sortedSources.slice(0, query.maxResults);

    const summary = this.buildSummary(limitedSources, query.topic, query.language);
    const citations = this.buildCitations(limitedSources);
    const confidence = this.calculateConfidence(limitedSources, failedCount, tasks.length);

    return {
      sources: limitedSources,
      summary,
      citations,
      confidence,
      partial: failedCount > 0,
      failedQueries: failedCount,
    };
  }

  /**
   * Extract text from uploaded File objects, chunk them, and return IndexedDocuments.
   * Documents that fail to parse are skipped (logged only) — no exception thrown.
   *
   * Requirements: 11.3, 22.1, 22.2, 22.4, 22.5
   */
  async prioritizeReferences(documents: File[]): Promise<IndexedDocument[]> {
    const indexed: IndexedDocument[] = [];

    for (const doc of documents) {
      try {
        const content = await extractFileContent(doc);
        const chunks = chunkContent(content, CHUNK_SIZE);

        indexed.push({
          id: crypto.randomUUID(),
          filename: doc.name,
          content,
          chunks,
          metadata: {
            type: doc.type,
            size: doc.size,
            lastModified: doc.lastModified,
          },
        });
      } catch (err) {
        // Graceful: log and continue (Req 22.5)
        console.warn(`[ResearchService] Failed to parse "${doc.name}":`, err);
      }
    }

    return indexed;
  }

  // --------------------------------------------------------------------------
  // Task Building
  // --------------------------------------------------------------------------

  private buildTasks(query: ResearchQuery): Task<Source[]>[] {
    const tasks: Task<Source[]>[] = [];
    const aspects = query.language === 'ar' ? QUERY_ASPECTS_AR : QUERY_ASPECTS_EN;
    const queryCount = DEPTH_QUERY_COUNTS[query.depth];

    // Web and knowledge-base queries (Req 11.5)
    const needsKnowledgeQuery =
      query.sources.includes('web') || query.sources.includes('knowledge-base');

    if (needsKnowledgeQuery) {
      const type = query.sources.includes('web') ? 'web' : 'knowledge-base';

      for (let i = 0; i < queryCount; i++) {
        const aspect = aspects[i % aspects.length];
        const subQuery = `${query.topic} — ${aspect}`;

        tasks.push({
          id: `query-${i}`,
          type: 'research',
          priority: 1,
          retryable: true,
          timeout: 30_000,
          execute: () => this.executeKnowledgeQuery(subQuery, query.topic, query.language, type),
        });
      }
    }

    // Reference document queries — higher priority than web results (Req 11.3)
    if (query.sources.includes('references') && query.referenceDocuments?.length) {
      for (let i = 0; i < query.referenceDocuments.length; i++) {
        const doc = query.referenceDocuments[i]!;

        tasks.push({
          id: `ref-${i}`,
          type: 'research',
          priority: 2, // Higher than web queries
          retryable: false,
          timeout: 10_000,
          execute: () => this.executeReferenceQuery(doc, query.topic, query.language),
        });
      }
    }

    return tasks;
  }

  // --------------------------------------------------------------------------
  // Query Executors
  // --------------------------------------------------------------------------

  /**
   * Query Gemini for factual content about a topic aspect.
   * Used for both 'web' and 'knowledge-base' source types.
   */
  async executeKnowledgeQuery(
    subQuery: string,
    topic: string,
    language: 'ar' | 'en',
    type: 'web' | 'knowledge-base'
  ): Promise<Source[]> {
    const langInstruction =
      language === 'ar'
        ? 'Respond entirely in Arabic.'
        : 'Respond entirely in English.';

    const prompt =
      language === 'ar'
        ? `أنت باحث متخصص. قدم معلومات موثوقة وشاملة حول: "${subQuery}".
استجب بتنسيق JSON مع المصادر التالية. ${langInstruction}`
        : `You are a specialized researcher. Provide reliable and comprehensive information about: "${subQuery}".
Respond with JSON containing research sources. ${langInstruction}`;

    const responseSchema = {
      type: Type.OBJECT,
      properties: {
        sources: {
          type: Type.ARRAY,
          items: {
            type: Type.OBJECT,
            properties: {
              title: { type: Type.STRING },
              content: { type: Type.STRING },
              relevance: { type: Type.NUMBER },
            },
            required: ['title', 'content', 'relevance'],
          },
        },
      },
      required: ['sources'],
    };

    const response = await ai.models.generateContent({
      model: MODELS.TEXT_GROUNDED,
      contents: prompt,
      config: {
        tools: [{ googleSearch: {} }],
        responseMimeType: 'application/json',
        responseSchema,
      },
    });

    // Extract real web sources from grounding metadata (when available)
    const groundingMeta = (response.candidates?.[0] as { groundingMetadata?: GroundingMetadata } | undefined)
      ?.groundingMetadata;
    const groundedUrls: Record<number, string> = {};
    if (groundingMeta?.groundingChunks) {
      groundingMeta.groundingChunks.forEach((chunk, i) => {
        if (chunk.web?.uri) groundedUrls[i] = chunk.web.uri;
      });
    }

    const raw: { sources?: Array<{ title: string; content: string; relevance: number }> } =
      JSON.parse(response.text ?? '{"sources":[]}');

    return (raw.sources ?? []).map((s, i) => ({
      id: crypto.randomUUID(),
      title: s.title,
      content: s.content,
      url: groundedUrls[i],
      type,
      relevance: Math.min(QUERY_RELEVANCE_MAX, Math.max(0, s.relevance ?? 0.5)),
      language,
    }));
  }

  /**
   * Convert an IndexedDocument into Source records for the results.
   * Reference documents always get the highest relevance score.
   */
  async executeReferenceQuery(
    doc: IndexedDocument,
    topic: string,
    language: 'ar' | 'en'
  ): Promise<Source[]> {
    // Use the most relevant chunks (first N that mention the topic, or all chunks)
    const relevantChunks = doc.chunks.filter((c) =>
      c.toLowerCase().includes(topic.toLowerCase())
    );
    const chunksToUse = relevantChunks.length > 0 ? relevantChunks : doc.chunks;

    return chunksToUse.map((chunk, idx) => ({
      id: crypto.randomUUID(),
      title: `${doc.filename} — Part ${idx + 1}`,
      content: chunk,
      type: 'reference' as const,
      relevance: REFERENCE_RELEVANCE_BASE,
      language,
    }));
  }

  // --------------------------------------------------------------------------
  // Deduplication
  // --------------------------------------------------------------------------

  /**
   * Remove sources whose content similarity exceeds DEDUP_SIMILARITY_THRESHOLD.
   * Keeps the first occurrence (or reference type when there's a tie).
   *
   * Property 12: sources with Jaccard similarity > 90% are removed.
   */
  deduplicateSources(sources: Source[]): Source[] {
    const unique: Source[] = [];

    for (const candidate of sources) {
      const isDuplicate = unique.some(
        (existing) => jaccardSimilarity(tokenize(existing.content, 3), tokenize(candidate.content, 3)) > DEDUP_SIMILARITY_THRESHOLD
      );

      if (!isDuplicate) {
        unique.push(candidate);
      }
    }

    return unique;
  }

  // --------------------------------------------------------------------------
  // Sorting
  // --------------------------------------------------------------------------

  /**
   * Sort sources: 'reference' type first, then by relevance descending.
   *
   * Property 13: reference documents appear first and have higher relevance.
   */
  sortSources(sources: Source[]): Source[] {
    return [...sources].sort((a, b) => {
      // References always come first
      if (a.type === 'reference' && b.type !== 'reference') return -1;
      if (b.type === 'reference' && a.type !== 'reference') return 1;
      // Then sort by relevance descending
      return b.relevance - a.relevance;
    });
  }

  // --------------------------------------------------------------------------
  // Summary & Citations
  // --------------------------------------------------------------------------

  private buildSummary(sources: Source[], topic: string, language: 'ar' | 'en'): string {
    if (sources.length === 0) {
      return language === 'ar'
        ? `لم يتم العثور على معلومات كافية حول: ${topic}`
        : `No sufficient information found for: ${topic}`;
    }

    // Concatenate top-3 source snippets as a simple summary
    const snippets = sources
      .slice(0, 3)
      .map((s) => s.content.slice(0, 200))
      .join(' ... ');

    return language === 'ar'
      ? `ملخص البحث حول "${topic}": ${snippets}`
      : `Research summary for "${topic}": ${snippets}`;
  }

  private buildCitations(sources: Source[]): Citation[] {
    return sources.map((source, idx) => ({
      sourceId: source.id,
      text: source.title,
      position: idx,
    }));
  }

  private calculateConfidence(
    sources: Source[],
    failedCount: number,
    totalTasks: number
  ): number {
    if (totalTasks === 0) return 0;
    if (sources.length === 0) return 0;

    const successRate = 1 - failedCount / totalTasks;
    const avgRelevance =
      sources.reduce((sum, s) => sum + s.relevance, 0) / sources.length;

    return Math.min(1, successRate * avgRelevance);
  }
}

// ============================================================================
// File Content Extraction (browser + Node compatible)
// ============================================================================

/**
 * Extract text content from a File object.
 * Supports: text/plain, application/pdf (basic), and application/vnd.openxmlformats-officedocument.wordprocessingml.document (DOCX stub).
 *
 * Requirements: 22.1, 22.4, 22.5
 */
export async function extractFileContent(file: File): Promise<string> {
  const name = file.name.toLowerCase();

  if (file.type === 'text/plain' || name.endsWith('.txt') || name.endsWith('.md')) {
    return readFileAsText(file);
  }

  if (file.type === 'application/pdf' || name.endsWith('.pdf')) {
    // Basic PDF: read as text (works for simple/searchable PDFs without binary decoding)
    const text = await readFileAsText(file);
    // Strip binary artifacts — keep only printable ASCII and spaces
    return text.replace(/[^\x20-\x7E\n\r\t\u0600-\u06FF]/g, ' ').replace(/\s+/g, ' ').trim();
  }

  if (
    file.type === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document' ||
    name.endsWith('.docx')
  ) {
    // DOCX: read as text (simplified — strips XML tags)
    const text = await readFileAsText(file);
    return text.replace(/<[^>]+>/g, ' ').replace(/\s+/g, ' ').trim();
  }

  throw new Error(
    `Unsupported file format: "${file.name}". Supported formats: PDF, TXT, DOCX.`
  );
}

function readFileAsText(file: File): Promise<string> {
  // Browser environment
  if (typeof FileReader !== 'undefined') {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => resolve(reader.result as string);
      reader.onerror = () => reject(new Error(`Failed to read file: ${file.name}`));
      reader.readAsText(file, 'utf-8');
    });
  }

  // Node.js / server environment
  return file.text();
}

// ============================================================================
// Singleton Export
// ============================================================================

export const researchService = new ResearchService();
````

## File: packages/shared/src/services/secureApiClient.ts
````typescript
/**
 * Secure API Client
 * 
 * Handles API calls through backend proxy to avoid exposing API keys in client bundle.
 * In production, all API calls should go through the backend server.
 */

const API_BASE_URL = import.meta.env.DEV ? 'http://localhost:3001' : '';

export interface ApiResponse<T = any> {
  success: boolean;
  data?: T;
  error?: string;
}

/**
 * Make a secure API call through the backend proxy
 */
async function secureApiCall<T = any>(
  endpoint: string,
  options: RequestInit = {}
): Promise<ApiResponse<T>> {
  try {
    const response = await fetch(`${API_BASE_URL}/api${endpoint}`, {
      ...options,
      headers: {
        'Content-Type': 'application/json',
        ...options.headers,
      },
    });

    if (!response.ok) {
      throw new Error(`API call failed: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return { success: true, data };
  } catch (error) {
    console.error(`[SecureApiClient] ${endpoint} failed:`, error);
    return {
      success: false,
      error: error instanceof Error ? error.message : String(error),
    };
  }
}

/**
 * Gemini API calls through backend proxy
 */
export const geminiApi = {
  async generateContent(prompt: string, options?: any): Promise<ApiResponse> {
    return secureApiCall('/gemini/generate', {
      method: 'POST',
      body: JSON.stringify({ prompt, options }),
    });
  },

  async generateImage(prompt: string, options?: any): Promise<ApiResponse> {
    return secureApiCall('/gemini/image', {
      method: 'POST',
      body: JSON.stringify({ prompt, options }),
    });
  },
};

/**
 * DEAPI calls through backend proxy
 */
export const deapiApi = {
  async generateImage(prompt: string, options?: any): Promise<ApiResponse> {
    return secureApiCall('/deapi/image', {
      method: 'POST',
      body: JSON.stringify({ prompt, options }),
    });
  },

  async animateImage(imageUrl: string, options?: any): Promise<ApiResponse> {
    return secureApiCall('/deapi/animate', {
      method: 'POST',
      body: JSON.stringify({ imageUrl, options }),
    });
  },
};

/**
 * Check if we're in development mode with direct API access
 */
export function isDevelopmentWithDirectApi(): boolean {
  return import.meta.env.DEV && !!import.meta.env.VITE_GEMINI_API_KEY;
}

/**
 * Get API key for development mode only
 * @deprecated Use backend proxy instead
 */
export function getDevApiKey(service: 'gemini' | 'deapi'): string | null {
  if (!import.meta.env.DEV) {
    console.warn('[SecureApiClient] API keys not available in production - use backend proxy');
    return null;
  }

  switch (service) {
    case 'gemini':
      return import.meta.env.VITE_GEMINI_API_KEY || null;
    case 'deapi':
      return import.meta.env.VITE_DEAPI_API_KEY || null;
    default:
      return null;
  }
}
````

## File: packages/shared/src/services/sfxService.ts
````typescript
/**
 * SFX Service
 * 
 * Mood-based ambient sound effects for video production.
 * Generates or selects ambient audio based on scene mood, video purpose, and visual descriptions.
 * 
 * Features:
 * - Mood-based ambient sound selection
 * - Scene-specific SFX suggestions
 * - Transition sounds between scenes
 * - Background music recommendations
 */

import { Scene, EmotionalTone, AmbientSFX, SFXCategory, SceneSFXPlan, VideoSFXPlan } from "../types";
import { VideoPurpose } from "../constants";
import { getEffectiveLegacyTone } from "./tripletUtils";

// Re-export types that other modules need
export type { VideoSFXPlan, SceneSFXPlan, AmbientSFX };

// --- Ambient Sound Library ---

/**
 * Pre-defined ambient sound library organized by category and mood.
 * These can be replaced with actual audio URLs or generated on-demand.
 */
const AMBIENT_LIBRARY: AmbientSFX[] = [
  // === NATURE ===
  {
    id: "desert-wind",
    name: "Desert Wind",
    description: "Gentle wind blowing across sand dunes with occasional gusts",
    category: "nature",
    moods: ["calm", "dramatic", "professional"],
    keywords: ["desert", "sand", "dune", "sahara", "arabian", "wind", "dry"],
    duration: 0,
    suggestedVolume: 0.3,
  },
  {
    id: "desert-night",
    name: "Desert Night",
    description: "Quiet desert night with distant wind and subtle insect sounds",
    category: "nature",
    moods: ["calm", "dramatic"],
    keywords: ["desert", "night", "stars", "quiet", "peaceful", "arabian"],
    duration: 0,
    suggestedVolume: 0.25,
  },
  {
    id: "ocean-waves",
    name: "Ocean Waves",
    description: "Rhythmic ocean waves crashing on shore",
    category: "nature",
    moods: ["calm", "friendly", "professional"],
    keywords: ["ocean", "sea", "beach", "waves", "water", "coast", "shore"],
    duration: 0,
    suggestedVolume: 0.35,
  },
  {
    id: "forest-ambience",
    name: "Forest Ambience",
    description: "Birds chirping, leaves rustling, peaceful forest atmosphere",
    category: "nature",
    moods: ["calm", "friendly"],
    keywords: ["forest", "trees", "birds", "nature", "woods", "peaceful"],
    duration: 0,
    suggestedVolume: 0.3,
  },
  {
    id: "rain-gentle",
    name: "Gentle Rain",
    description: "Soft rain falling with occasional distant thunder",
    category: "weather",
    moods: ["calm", "dramatic"],
    keywords: ["rain", "storm", "water", "weather", "cozy"],
    duration: 0,
    suggestedVolume: 0.35,
  },
  {
    id: "thunderstorm",
    name: "Thunderstorm",
    description: "Heavy rain with thunder and lightning",
    category: "weather",
    moods: ["dramatic", "urgent"],
    keywords: ["storm", "thunder", "lightning", "rain", "dramatic", "intense"],
    duration: 0,
    suggestedVolume: 0.4,
  },
  {
    id: "wind-howling",
    name: "Howling Wind",
    description: "Strong wind howling through mountains or buildings",
    category: "weather",
    moods: ["dramatic", "urgent"],
    keywords: ["wind", "storm", "mountain", "cold", "winter", "harsh"],
    duration: 0,
    suggestedVolume: 0.35,
  },

  // === URBAN ===
  {
    id: "city-traffic",
    name: "City Traffic",
    description: "Urban traffic sounds with cars, horns, and city bustle",
    category: "urban",
    moods: ["professional", "urgent"],
    keywords: ["city", "urban", "traffic", "cars", "street", "downtown"],
    duration: 0,
    suggestedVolume: 0.25,
  },
  {
    id: "cafe-ambience",
    name: "Café Ambience",
    description: "Coffee shop atmosphere with quiet chatter and cups clinking",
    category: "urban",
    moods: ["friendly", "calm"],
    keywords: ["cafe", "coffee", "restaurant", "social", "cozy"],
    duration: 0,
    suggestedVolume: 0.2,
  },
  {
    id: "marketplace",
    name: "Marketplace",
    description: "Busy marketplace with vendors and crowd murmur",
    category: "urban",
    moods: ["friendly", "professional"],
    keywords: ["market", "bazaar", "souk", "crowd", "vendors", "busy"],
    duration: 0,
    suggestedVolume: 0.25,
  },

  // === SUPERNATURAL/MYSTERY ===
  {
    id: "eerie-ambience",
    name: "Eerie Ambience",
    description: "Unsettling atmospheric sounds with subtle drones",
    category: "supernatural",
    moods: ["dramatic"],
    keywords: ["horror", "scary", "ghost", "haunted", "mystery", "dark", "eerie"],
    duration: 0,
    suggestedVolume: 0.3,
  },
  {
    id: "mystical-drone",
    name: "Mystical Drone",
    description: "Ethereal, otherworldly ambient drone",
    category: "supernatural",
    moods: ["dramatic", "calm"],
    keywords: ["magic", "mystical", "fantasy", "ethereal", "spiritual", "ancient"],
    duration: 0,
    suggestedVolume: 0.25,
  },
  {
    id: "whispers",
    name: "Distant Whispers",
    description: "Faint, unintelligible whispers creating unease",
    category: "supernatural",
    moods: ["dramatic"],
    keywords: ["ghost", "spirit", "haunted", "whisper", "scary", "horror"],
    duration: 0,
    suggestedVolume: 0.15,
  },
  {
    id: "heartbeat",
    name: "Heartbeat",
    description: "Slow, tense heartbeat building suspense",
    category: "supernatural",
    moods: ["dramatic", "urgent"],
    keywords: ["tension", "suspense", "fear", "anxiety", "thriller"],
    duration: 0,
    suggestedVolume: 0.3,
  },

  // === TRANSITIONS ===
  {
    id: "whoosh-soft",
    name: "Soft Whoosh",
    description: "Gentle transition whoosh sound",
    category: "transition",
    moods: ["friendly", "calm", "professional"],
    keywords: ["transition", "change", "scene"],
    duration: 1,
    suggestedVolume: 0.4,
  },
  {
    id: "whoosh-dramatic",
    name: "Dramatic Whoosh",
    description: "Powerful cinematic whoosh for dramatic transitions",
    category: "transition",
    moods: ["dramatic", "urgent"],
    keywords: ["transition", "dramatic", "cinematic"],
    duration: 1.5,
    suggestedVolume: 0.5,
  },
  {
    id: "impact-deep",
    name: "Deep Impact",
    description: "Low, resonant impact sound for emphasis",
    category: "transition",
    moods: ["dramatic", "urgent"],
    keywords: ["impact", "hit", "dramatic", "emphasis"],
    duration: 2,
    suggestedVolume: 0.5,
  },
  {
    id: "shimmer",
    name: "Shimmer",
    description: "Magical shimmer/sparkle transition sound",
    category: "transition",
    moods: ["friendly", "calm"],
    keywords: ["magic", "sparkle", "transition", "light"],
    duration: 1.5,
    suggestedVolume: 0.35,
  },

  // === MUSICAL BEDS ===
  {
    id: "tension-drone",
    name: "Tension Drone",
    description: "Low, building tension musical bed",
    category: "musical",
    moods: ["dramatic", "urgent"],
    keywords: ["tension", "suspense", "thriller", "dark"],
    duration: 0,
    suggestedVolume: 0.2,
  },
  {
    id: "hopeful-pad",
    name: "Hopeful Pad",
    description: "Warm, uplifting ambient pad",
    category: "musical",
    moods: ["friendly", "calm"],
    keywords: ["hope", "positive", "uplifting", "warm", "inspiring"],
    duration: 0,
    suggestedVolume: 0.2,
  },
  {
    id: "epic-strings",
    name: "Epic Strings",
    description: "Cinematic string swell for dramatic moments",
    category: "musical",
    moods: ["dramatic"],
    keywords: ["epic", "cinematic", "dramatic", "emotional", "powerful"],
    duration: 0,
    suggestedVolume: 0.25,
  },
  {
    id: "middle-eastern",
    name: "Middle Eastern Ambience",
    description: "Traditional Middle Eastern musical atmosphere",
    category: "musical",
    moods: ["dramatic", "calm", "professional"],
    keywords: ["arabic", "middle east", "oriental", "traditional", "desert", "arabian"],
    duration: 0,
    suggestedVolume: 0.2,
  },
];

// --- Video Purpose to SFX Mapping ---

const PURPOSE_SFX_PREFERENCES: Record<VideoPurpose, {
  preferredCategories: SFXCategory[];
  transitionStyle: "soft" | "dramatic" | "none";
  useBackgroundMusic: boolean;
  masterVolume: number;
}> = {
  documentary: {
    preferredCategories: ["ambient", "nature", "urban"],
    transitionStyle: "soft",
    useBackgroundMusic: false,
    masterVolume: 0.3,
  },
  storytelling: {
    preferredCategories: ["ambient", "nature", "supernatural", "musical"],
    transitionStyle: "dramatic",
    useBackgroundMusic: true,
    masterVolume: 0.35,
  },
  horror_mystery: {
    preferredCategories: ["supernatural", "weather", "musical"],
    transitionStyle: "dramatic",
    useBackgroundMusic: true,
    masterVolume: 0.4,
  },
  music_video: {
    preferredCategories: ["musical", "ambient"],
    transitionStyle: "none",
    useBackgroundMusic: false,
    masterVolume: 0.2,
  },
  social_short: {
    preferredCategories: ["transition", "action"],
    transitionStyle: "dramatic",
    useBackgroundMusic: false,
    masterVolume: 0.4,
  },
  commercial: {
    preferredCategories: ["musical", "ambient"],
    transitionStyle: "soft",
    useBackgroundMusic: true,
    masterVolume: 0.25,
  },
  podcast_visual: {
    preferredCategories: ["ambient"],
    transitionStyle: "none",
    useBackgroundMusic: false,
    masterVolume: 0.15,
  },
  lyric_video: {
    preferredCategories: ["musical"],
    transitionStyle: "none",
    useBackgroundMusic: false,
    masterVolume: 0.2,
  },
  educational: {
    preferredCategories: ["ambient", "musical"],
    transitionStyle: "soft",
    useBackgroundMusic: true,
    masterVolume: 0.2,
  },
  travel: {
    preferredCategories: ["nature", "ambient", "urban"],
    transitionStyle: "soft",
    useBackgroundMusic: true,
    masterVolume: 0.3,
  },
  motivational: {
    preferredCategories: ["musical", "ambient"],
    transitionStyle: "dramatic",
    useBackgroundMusic: true,
    masterVolume: 0.3,
  },
  news_report: {
    preferredCategories: ["ambient"],
    transitionStyle: "none",
    useBackgroundMusic: false,
    masterVolume: 0.15,
  },
  story_drama: {
    preferredCategories: ["ambient", "musical"],
    transitionStyle: "dramatic",
    useBackgroundMusic: true,
    masterVolume: 0.35,
  },
  story_comedy: {
    preferredCategories: ["musical", "action"],
    transitionStyle: "soft",
    useBackgroundMusic: true,
    masterVolume: 0.25,
  },
  story_thriller: {
    preferredCategories: ["supernatural", "weather", "ambient"],
    transitionStyle: "dramatic",
    useBackgroundMusic: true,
    masterVolume: 0.4,
  },
  story_scifi: {
    preferredCategories: ["ambient", "musical"],
    transitionStyle: "dramatic",
    useBackgroundMusic: true,
    masterVolume: 0.3,
  },
  story_action: {
    preferredCategories: ["action", "transition", "ambient"],
    transitionStyle: "dramatic",
    useBackgroundMusic: true,
    masterVolume: 0.4,
  },
  story_fantasy: {
    preferredCategories: ["musical", "nature", "supernatural"],
    transitionStyle: "dramatic",
    useBackgroundMusic: true,
    masterVolume: 0.35,
  },
  story_romance: {
    preferredCategories: ["musical", "ambient", "nature"],
    transitionStyle: "soft",
    useBackgroundMusic: true,
    masterVolume: 0.25,
  },
  story_historical: {
    preferredCategories: ["ambient", "musical"],
    transitionStyle: "soft",
    useBackgroundMusic: true,
    masterVolume: 0.3,
  },
  story_animation: {
    preferredCategories: ["musical", "action", "transition"],
    transitionStyle: "dramatic",
    useBackgroundMusic: true,
    masterVolume: 0.3,
  },
};

// --- Core Functions ---

/**
 * Extract keywords from scene visual description and narration.
 */
function extractSceneKeywords(scene: Scene): string[] {
  const text = `${scene.visualDescription} ${scene.narrationScript} ${scene.name}`.toLowerCase();

  // Common keyword patterns
  const keywords: string[] = [];

  // Nature keywords
  if (/desert|sand|dune|sahara/i.test(text)) keywords.push("desert");
  if (/ocean|sea|beach|wave|coast/i.test(text)) keywords.push("ocean");
  if (/forest|tree|wood|jungle/i.test(text)) keywords.push("forest");
  if (/mountain|hill|peak|cliff/i.test(text)) keywords.push("mountain");
  if (/rain|storm|thunder/i.test(text)) keywords.push("rain", "storm");
  if (/wind|breeze|gust/i.test(text)) keywords.push("wind");
  if (/night|dark|moon|star/i.test(text)) keywords.push("night");

  // Urban keywords
  if (/city|urban|street|traffic/i.test(text)) keywords.push("city", "urban");
  if (/market|bazaar|souk|shop/i.test(text)) keywords.push("market");
  if (/cafe|coffee|restaurant/i.test(text)) keywords.push("cafe");

  // Mood keywords
  if (/ghost|spirit|haunt|eerie|creepy/i.test(text)) keywords.push("ghost", "eerie");
  if (/magic|mystical|ancient|spell/i.test(text)) keywords.push("magic", "mystical");
  if (/horror|scary|fear|terror/i.test(text)) keywords.push("horror", "scary");
  if (/tension|suspense|thriller/i.test(text)) keywords.push("tension", "suspense");
  if (/hope|positive|uplift|inspir/i.test(text)) keywords.push("hope", "inspiring");
  if (/epic|dramatic|powerful/i.test(text)) keywords.push("epic", "dramatic");

  // Cultural keywords
  if (/arab|middle east|oriental|kuwait|egypt/i.test(text)) keywords.push("arabic", "middle east");
  if (/asian|japan|china|korea/i.test(text)) keywords.push("asian");

  return [...new Set(keywords)];
}

/**
 * Score how well an SFX matches a scene.
 */
function scoreSFXMatch(sfx: AmbientSFX, scene: Scene, sceneKeywords: string[], preferences: typeof PURPOSE_SFX_PREFERENCES[VideoPurpose]): number {
  let score = 0;

  // Category preference (0-30 points)
  const categoryIndex = preferences.preferredCategories.indexOf(sfx.category);
  if (categoryIndex !== -1) {
    score += 30 - (categoryIndex * 5);
  }

  // Mood match (0-25 points)
  if (sfx.moods.includes(getEffectiveLegacyTone(scene))) {
    score += 25;
  }

  // Keyword match (0-45 points, 5 per match)
  const keywordMatches = sceneKeywords.filter(kw =>
    sfx.keywords.some(sfxKw => sfxKw.includes(kw) || kw.includes(sfxKw))
  );
  score += Math.min(45, keywordMatches.length * 9);

  return score;
}

/**
 * Find the best ambient SFX for a scene.
 * Prioritizes AI-suggested SFX from content planner if available.
 */
export function findAmbientForScene(
  scene: Scene,
  videoPurpose: VideoPurpose,
  excludeIds: string[] = []
): AmbientSFX | null {
  // First, check if AI suggested an SFX for this scene
  if (scene.ambientSfx) {
    const aiSuggested = AMBIENT_LIBRARY.find(sfx => sfx.id === scene.ambientSfx);
    if (aiSuggested && !excludeIds.includes(aiSuggested.id)) {
      console.log(`[SFX] Using AI-suggested SFX for scene "${scene.name}": ${aiSuggested.name}`);
      return aiSuggested;
    }
    // If AI suggested an invalid ID, log warning and fall back to keyword matching
    if (!aiSuggested) {
      console.warn(`[SFX] AI suggested unknown SFX ID "${scene.ambientSfx}" for scene "${scene.name}", falling back to keyword matching`);
    }
  }

  const preferences = PURPOSE_SFX_PREFERENCES[videoPurpose];
  const sceneKeywords = extractSceneKeywords(scene);

  // Filter to ambient/loopable sounds only
  const candidates = AMBIENT_LIBRARY.filter(sfx =>
    sfx.duration === 0 && // Loopable
    sfx.category !== "transition" &&
    !excludeIds.includes(sfx.id)
  );

  if (candidates.length === 0) return null;

  // Score and sort
  const scored = candidates.map(sfx => ({
    sfx,
    score: scoreSFXMatch(sfx, scene, sceneKeywords, preferences),
  })).sort((a, b) => b.score - a.score);

  // Return best match if score is above threshold
  const bestMatch = scored[0];
  if (bestMatch && bestMatch.score >= 20) {
    return bestMatch.sfx;
  }

  return null;
}

/**
 * Get transition sound based on video purpose and mood.
 */
export function getTransitionSound(
  videoPurpose: VideoPurpose,
  mood: EmotionalTone
): AmbientSFX | null {
  const preferences = PURPOSE_SFX_PREFERENCES[videoPurpose];

  if (preferences.transitionStyle === "none") {
    return null;
  }

  const transitions = AMBIENT_LIBRARY.filter(sfx => sfx.category === "transition");

  if (preferences.transitionStyle === "dramatic") {
    return transitions.find(t => t.id === "whoosh-dramatic" || t.id === "impact-deep") || null;
  }

  return transitions.find(t => t.id === "whoosh-soft" || t.id === "shimmer") || null;
}

/**
 * Generate a complete SFX plan for a video.
 */
export function generateVideoSFXPlan(
  scenes: Scene[],
  videoPurpose: VideoPurpose
): VideoSFXPlan {
  const preferences = PURPOSE_SFX_PREFERENCES[videoPurpose];
  const usedAmbientIds: string[] = [];

  const scenePlans: SceneSFXPlan[] = scenes.map((scene, index) => {
    // Find ambient for this scene
    const ambient = findAmbientForScene(scene, videoPurpose, usedAmbientIds);
    if (ambient) {
      usedAmbientIds.push(ambient.id);
    }

    // Get transition sounds
    const transitionIn = index > 0 ? getTransitionSound(videoPurpose, getEffectiveLegacyTone(scene)) : null;
    const transitionOut = index < scenes.length - 1 ? getTransitionSound(videoPurpose, getEffectiveLegacyTone(scene)) : null;

    return {
      sceneId: scene.id,
      ambientTrack: ambient,
      transitionIn,
      transitionOut,
      accentSounds: [], // Can be expanded for specific accent sounds
    };
  });

  // Find background music if enabled
  let backgroundMusic: AmbientSFX | null = null;
  if (preferences.useBackgroundMusic) {
    const musicalBeds = AMBIENT_LIBRARY.filter(sfx => sfx.category === "musical");
    // Find one that matches the overall mood
    const dominantMood = scenes[0] ? getEffectiveLegacyTone(scenes[0]) : "professional";
    backgroundMusic = musicalBeds.find(m => m.moods.includes(dominantMood)) || musicalBeds[0] || null;
  }

  return {
    scenes: scenePlans,
    backgroundMusic,
    masterVolume: preferences.masterVolume,
  };
}

/**
 * Get SFX suggestions for a scene (for UI display).
 */
export function getSFXSuggestionsForScene(
  scene: Scene,
  videoPurpose: VideoPurpose,
  limit: number = 5
): AmbientSFX[] {
  const preferences = PURPOSE_SFX_PREFERENCES[videoPurpose];
  const sceneKeywords = extractSceneKeywords(scene);

  const candidates = AMBIENT_LIBRARY.filter(sfx =>
    sfx.duration === 0 && sfx.category !== "transition"
  );

  const scored = candidates.map(sfx => ({
    sfx,
    score: scoreSFXMatch(sfx, scene, sceneKeywords, preferences),
  })).sort((a, b) => b.score - a.score);

  return scored.slice(0, limit).map(s => s.sfx);
}

/**
 * Get all available ambient sounds.
 */
export function getAllAmbientSounds(): AmbientSFX[] {
  return AMBIENT_LIBRARY.filter(sfx => sfx.category !== "transition");
}

/**
 * Get all transition sounds.
 */
export function getAllTransitionSounds(): AmbientSFX[] {
  return AMBIENT_LIBRARY.filter(sfx => sfx.category === "transition");
}

/**
 * Get ambient sounds by category.
 */
export function getAmbientByCategory(category: SFXCategory): AmbientSFX[] {
  return AMBIENT_LIBRARY.filter(sfx => sfx.category === category);
}

// --- Export Types ---
export type { SFXCategory as SFXCategoryType };


// --- Freesound Integration ---

import {
  isFreesoundConfigured,
  getAmbientSoundCached,
  getPreviewUrl,
  preloadSounds,
  FreesoundSound
} from "./freesoundService";

/**
 * Fetch real audio URLs from Freesound for an SFX plan.
 * Updates the plan in-place with actual audio URLs.
 */
export async function enrichSFXPlanWithFreesound(plan: VideoSFXPlan): Promise<VideoSFXPlan> {
  if (!isFreesoundConfigured()) {
    console.warn("[SFX] Freesound API not configured, using placeholder SFX");
    return plan;
  }

  console.log("[SFX] Fetching audio from Freesound...");

  // Collect all unique SFX IDs that need audio
  const sfxIds = new Set<string>();

  plan.scenes.forEach(scene => {
    if (scene.ambientTrack) sfxIds.add(scene.ambientTrack.id);
    if (scene.transitionIn) sfxIds.add(scene.transitionIn.id);
    if (scene.transitionOut) sfxIds.add(scene.transitionOut.id);
    scene.accentSounds.forEach(s => sfxIds.add(s.id));
  });

  if (plan.backgroundMusic) {
    sfxIds.add(plan.backgroundMusic.id);
  }

  // Fetch sounds from Freesound
  const soundMap = await preloadSounds(Array.from(sfxIds));

  console.log(`[SFX] Fetched ${soundMap.size}/${sfxIds.size} sounds from Freesound`);

  // Update plan with audio URLs
  const updateSFX = (sfx: AmbientSFX | null): AmbientSFX | null => {
    if (!sfx) return null;

    const freesoundData = soundMap.get(sfx.id);
    if (freesoundData) {
      return {
        ...sfx,
        audioUrl: getPreviewUrl(freesoundData),
        duration: freesoundData.duration,
      };
    }
    return sfx;
  };

  // Create enriched plan
  const enrichedPlan: VideoSFXPlan = {
    ...plan,
    backgroundMusic: updateSFX(plan.backgroundMusic),
    scenes: plan.scenes.map(scene => ({
      ...scene,
      ambientTrack: updateSFX(scene.ambientTrack),
      transitionIn: updateSFX(scene.transitionIn),
      transitionOut: updateSFX(scene.transitionOut),
      accentSounds: scene.accentSounds.map(s => updateSFX(s)!).filter(Boolean),
    })),
  };

  return enrichedPlan;
}

/**
 * Generate SFX plan with real Freesound audio.
 * This is the main function to use for production.
 */
export async function generateVideoSFXPlanWithAudio(
  scenes: Scene[],
  videoPurpose: VideoPurpose
): Promise<VideoSFXPlan> {
  // First generate the plan with local library
  const plan = generateVideoSFXPlan(scenes, videoPurpose);

  // Then enrich with real audio from Freesound
  const enrichedPlan = await enrichSFXPlanWithFreesound(plan);

  return enrichedPlan;
}

/**
 * Search Freesound for a specific ambient sound type.
 * Useful for custom sound selection UI.
 */
export async function searchFreesoundAmbient(
  categoryId: string
): Promise<{ sfx: AmbientSFX; freesoundData: FreesoundSound } | null> {
  if (!isFreesoundConfigured()) {
    return null;
  }

  const freesoundData = await getAmbientSoundCached(categoryId);
  if (!freesoundData) {
    return null;
  }

  // Find the matching SFX from our library
  const sfx = AMBIENT_LIBRARY.find(s => s.id === categoryId);
  if (!sfx) {
    return null;
  }

  return {
    sfx: {
      ...sfx,
      audioUrl: getPreviewUrl(freesoundData),
      duration: freesoundData.duration,
    },
    freesoundData,
  };
}

/**
 * Check if Freesound integration is available.
 */
export function isSFXAudioAvailable(): boolean {
  return isFreesoundConfigured();
}
````

## File: packages/shared/src/services/shared/apiClient.ts
````typescript
import { GoogleGenAI } from "@google/genai";

// --- Environment Detection ---
const isBrowser = typeof window !== "undefined";

// --- Configuration ---
// Vertex AI (server-side only)
export const VERTEX_PROJECT = process.env.GOOGLE_CLOUD_PROJECT || process.env.VITE_GOOGLE_CLOUD_PROJECT || "";
export const VERTEX_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || process.env.VITE_GOOGLE_CLOUD_LOCATION || "global";

// API Key (browser-side, or server fallback)
// Export for LangChain services that need direct API key access
export const GEMINI_API_KEY = process.env.VITE_GEMINI_API_KEY || process.env.GEMINI_API_KEY || "";
// Backward compatibility alias
export const API_KEY = GEMINI_API_KEY;

// Debug: Log configuration
if (isBrowser) {
  console.log(`[API Client] Running in browser mode (using proxy)`);
} else {
  console.log(`[API Client] Running in server mode`);
  console.log(`[API Client] Vertex AI Project: ${VERTEX_PROJECT || "NOT SET"}`);
  console.log(`[API Client] Vertex AI Location: ${VERTEX_LOCATION}`);
}

// Validate configuration based on context
if (!isBrowser && !VERTEX_PROJECT && !GEMINI_API_KEY) {
  console.error(
    "[API Client] Missing authentication. Set either:\n" +
    "- GOOGLE_CLOUD_PROJECT for Vertex AI (recommended for server)\n" +
    "- VITE_GEMINI_API_KEY for API key auth"
  );
}

/**
 * Model availability in Vertex AI (as of January 2026):
 * ✅ Available: gemini-3-flash-preview, gemini-3-pro-preview, veo-3.1-*, imagen-4.0-fast-generate-001
 * ⚠️ Quota needed: veo-*, imagegeneration@006
 * 
 * Note: TTS and Multimodal output supported in Gemini 3.0 and 2.x Flash.
 */
export const MODELS = {
  TEXT: "gemini-3-flash-preview",
  IMAGE: "imagen-4.0-fast-generate-001", // Stable generation model
  VIDEO: "veo-3.1-fast-generate-preview",
  TRANSCRIPTION: "gemini-3-flash-preview",
  TRANSLATION: "gemini-3-flash-preview",
  TTS: "gemini-2.5-flash-preview-tts", // Supports AUDIO output modality

  // Grounded research model (Google Search grounding enabled in researchService)
  TEXT_GROUNDED: "gemini-3-flash-preview",

  // Alternative models
  TEXT_EXP: "gemini-3-pro-preview", // Latest reasoning model
  TEXT_LEGACY: "gemini-3-pro-preview",
  IMAGE_STANDARD: "imagen-3.0-generate-001",
  IMAGE_HD: "gemini-3-pro-preview", // Multimodal image understanding & generation
  VIDEO_STANDARD: "veo-3.1-generate-preview",
  VIDEO_FAST: "veo-3.1-fast-generate-preview",
  VIDEO_LEGACY: "veo-3.1-fast-generate-preview",
};

/**
 * Validates that Vertex AI configuration is properly set up.
 * Throws a descriptive error if configuration is missing.
 */
export function validateVertexConfig(): void {
  if (!VERTEX_PROJECT) {
    throw new Error(
      "Vertex AI not configured. Required setup:\n" +
      "1. Install gcloud CLI: https://cloud.google.com/sdk/docs/install\n" +
      "2. Authenticate: gcloud auth application-default login\n" +
      "3. Set project: export GOOGLE_CLOUD_PROJECT=your-project-id\n" +
      "4. (Optional) Set location: export GOOGLE_CLOUD_LOCATION=global"
    );
  }
}

/**
 * Helper function to handle quota exceeded errors gracefully.
 * Falls back to text model for image/video generation prompts.
 */
export function getModelWithFallback(modelType: keyof typeof MODELS): string {
  const model = MODELS[modelType];

  // For image and video models that might hit quota limits,
  // we can fall back to using the text model for prompt generation
  if (modelType === 'IMAGE' || modelType === 'VIDEO') {
    console.warn(`[API Client] Using ${model} - may require quota increase for actual generation`);
  }

  return model;
}

/**
 * Proxy AI Client for Browser
 * Mimics the GoogleGenAI interface but routes requests through the backend proxy.
 */
class ProxyAIClient {
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  public models: any;

  constructor() {
    this.models = {
      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      generateContent: async (params: any) => {
        return this.callProxy('/api/gemini/proxy/generateContent', params);
      },
      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      generateImages: async (params: any) => {
        return this.callProxy('/api/gemini/proxy/generateImages', params);
      }
    };
  }

  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  private async callProxy(endpoint: string, params: any) {
    try {
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(params)
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const err = new Error(errorData.error || `Proxy call failed: ${response.status}`);
        (err as any).status = response.status;
        throw err;
      }

      return await response.json();
    } catch (error: unknown) {
      console.error(`[ProxyAIClient] Error calling ${endpoint}:`, error);
      throw error;
    }
  }
}

/**
 * Initialize GoogleGenAI with hybrid authentication:
 * - Browser: Uses ProxyAIClient to route through backend (no API key needed in client)
 * - Server: Uses Vertex AI ADC (GOOGLE_CLOUD_PROJECT) or falls back to API key
 */
function createAIClient(): GoogleGenAI | ProxyAIClient {
  if (isBrowser) {
    // Browser: Use Proxy Client
    return new ProxyAIClient() as unknown as GoogleGenAI;
  }

  // Re-read environment variables at creation time (for lazy initialization)
  const vertexProject = process.env.GOOGLE_CLOUD_PROJECT || process.env.VITE_GOOGLE_CLOUD_PROJECT || "";
  const vertexLocation = process.env.GOOGLE_CLOUD_LOCATION || process.env.VITE_GOOGLE_CLOUD_LOCATION || "global";
  const geminiKey = process.env.VITE_GEMINI_API_KEY || process.env.GEMINI_API_KEY || "";

  // Server: Prefer Vertex AI, fall back to API key
  if (vertexProject) {
    console.log(`[API Client] Using Vertex AI with project: ${vertexProject}`);
    return new GoogleGenAI({
      vertexai: true,
      project: vertexProject,
      location: vertexLocation,
    });
  }

  if (geminiKey) {
    console.log("[API Client] Server using API key auth (Vertex AI not configured)");
    return new GoogleGenAI({ apiKey: geminiKey });
  }

  throw new Error(
    "No authentication configured. Set either:\n" +
    "- GOOGLE_CLOUD_PROJECT for Vertex AI\n" +
    "- VITE_GEMINI_API_KEY for API key auth"
  );
}

// Lazy initialization - create client on first access
let _aiClient: GoogleGenAI | null = null;
export const ai = new Proxy({} as GoogleGenAI, {
  get(target, prop) {
    if (!_aiClient) {
      _aiClient = createAIClient() as GoogleGenAI;
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    return (_aiClient as any)[prop];
  }
});

// --- Retry Configuration ---
export interface RetryConfig {
  retries?: number;
  delayMs?: number;
  backoffFactor?: number;
}

// --- Circuit Breaker State ---
// Tracks consecutive failures to prevent hammering a failing API
let consecutiveFailures = 0;
let circuitOpenUntil = 0;
const CIRCUIT_BREAKER_THRESHOLD = 5; // Trip after 5 consecutive failures
const CIRCUIT_COOLDOWN_MS = 30000; // 30 second cooldown when tripped
const MAX_BACKOFF_MS = 30000; // Cap backoff at 30 seconds

/**
 * Check if the circuit breaker is currently open (blocking requests).
 * @returns Time remaining in ms if open, 0 if closed
 */
export function getCircuitBreakerStatus(): number {
  const now = Date.now();
  if (now < circuitOpenUntil) {
    return circuitOpenUntil - now;
  }
  return 0;
}

/**
 * Reset the circuit breaker (for testing or manual recovery).
 */
export function resetCircuitBreaker(): void {
  consecutiveFailures = 0;
  circuitOpenUntil = 0;
}

/**
 * Retry wrapper for AI calls.
 * Handles transient API failures (503, 429) with exponential backoff.
 * Includes circuit breaker pattern to prevent hammering failing APIs.
 */
export async function withRetry<T>(
  fn: () => Promise<T>,
  retries = 3,
  delayMs = 1000,
  backoffFactor = 2,
): Promise<T> {
  // Check if circuit breaker is open
  const circuitRemaining = getCircuitBreakerStatus();
  if (circuitRemaining > 0) {
    const error = new Error(
      `Circuit breaker is open. API calls blocked for ${Math.ceil(circuitRemaining / 1000)} more seconds. ` +
      `This prevents overwhelming the API after ${CIRCUIT_BREAKER_THRESHOLD} consecutive failures.`
    );
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    (error as any).code = "CIRCUIT_BREAKER_OPEN";
    throw error;
  }

  try {
    const result = await fn();
    // Success: reset failure counter
    consecutiveFailures = 0;
    return result;
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
  } catch (error: any) {
    // Check if this is a retryable error
    // Include 500 (Internal Server Error) as these are often transient on Google's side
    const isRetryable =
      error.status === 500 ||
      error.status === 503 ||
      error.status === 429 ||
      error.message?.includes("INTERNAL") ||
      error.message?.includes("fetch failed");

    if (isRetryable) {
      consecutiveFailures++;

      // Check if we should trip the circuit breaker
      if (consecutiveFailures >= CIRCUIT_BREAKER_THRESHOLD) {
        circuitOpenUntil = Date.now() + CIRCUIT_COOLDOWN_MS;
        console.error(
          `[Circuit Breaker] Tripped after ${consecutiveFailures} consecutive failures. ` +
          `Blocking API calls for ${CIRCUIT_COOLDOWN_MS / 1000}s.`
        );
      }

      if (retries > 0) {
        // Cap the delay at MAX_BACKOFF_MS
        const cappedDelay = Math.min(delayMs, MAX_BACKOFF_MS);
        console.warn(
          `API call failed. Retrying in ${cappedDelay}ms... (${retries} attempts left). Error: ${error.message}`,
        );
        await new Promise((resolve) => setTimeout(resolve, cappedDelay));
        return withRetry(
          fn,
          retries - 1,
          Math.min(delayMs * backoffFactor, MAX_BACKOFF_MS), // Cap next delay too
          backoffFactor
        );
      }
    }
    throw error;
  }
}
````

## File: packages/shared/src/services/shared/robustUtils.ts
````typescript
/**
 * Robust Utility Functions for AI Orchestration
 *
 * Provides production-grade utilities for:
 * - Timeout protection for async operations
 * - Retry with exponential backoff
 * - Blob URL lifecycle management (prevents memory leaks)
 * - Safe localStorage operations with size limits
 * - AbortController helpers for cancellation
 * - JSON extraction with multiple fallback strategies
 *
 * @module robustUtils
 */

// ============================================================
// TIMEOUT UTILITIES
// ============================================================

/**
 * Wraps a promise with a timeout.
 * Rejects with a TimeoutError if the promise doesn't resolve within the specified time.
 *
 * @example
 * const result = await withTimeout(
 *   fetchData(),
 *   30000,
 *   'Data fetch timed out'
 * );
 *
 * @param promise - The promise to wrap
 * @param ms - Timeout in milliseconds
 * @param message - Optional custom timeout message
 * @returns The resolved value or throws TimeoutError
 */
export async function withTimeout<T>(
  promise: Promise<T>,
  ms: number,
  message = 'Operation timed out'
): Promise<T> {
  let timeoutId: ReturnType<typeof setTimeout>;

  const timeoutPromise = new Promise<never>((_, reject) => {
    timeoutId = setTimeout(() => {
      const error = new Error(`${message} after ${ms}ms`);
      error.name = 'TimeoutError';
      reject(error);
    }, ms);
  });

  try {
    const result = await Promise.race([promise, timeoutPromise]);
    clearTimeout(timeoutId!);
    return result;
  } catch (error) {
    clearTimeout(timeoutId!);
    throw error;
  }
}

/**
 * Creates an AbortController with automatic timeout.
 * Useful for fetch operations that need cancellation support.
 *
 * @example
 * const { signal, cleanup } = createTimeoutAbortController(30000);
 * try {
 *   await fetch(url, { signal });
 * } finally {
 *   cleanup();
 * }
 *
 * @param ms - Timeout in milliseconds
 * @returns AbortController signal and cleanup function
 */
export function createTimeoutAbortController(ms: number): {
  controller: AbortController;
  signal: AbortSignal;
  cleanup: () => void;
} {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => {
    controller.abort(new Error(`Request timed out after ${ms}ms`));
  }, ms);

  return {
    controller,
    signal: controller.signal,
    cleanup: () => {
      clearTimeout(timeoutId);
    },
  };
}

// ============================================================
// RETRY UTILITIES
// ============================================================

/**
 * Options for retry logic.
 */
export interface RetryOptions {
  /** Maximum number of retry attempts */
  maxRetries?: number;
  /** Base delay between retries in ms */
  baseDelay?: number;
  /** Maximum delay cap in ms */
  maxDelay?: number;
  /** Backoff multiplier (e.g., 2 for exponential) */
  backoffFactor?: number;
  /** Callback on each retry attempt */
  onRetry?: (attempt: number, error: Error, nextDelayMs: number) => void;
  /** Function to determine if error is retryable */
  isRetryable?: (error: Error) => boolean;
  /** AbortSignal for cancellation */
  signal?: AbortSignal;
}

const DEFAULT_RETRY_OPTIONS: Required<Omit<RetryOptions, 'onRetry' | 'isRetryable' | 'signal'>> = {
  maxRetries: 3,
  baseDelay: 1000,
  maxDelay: 30000,
  backoffFactor: 2,
};

/**
 * Default function to determine if an error is retryable.
 * Retries on network errors, rate limits, and server errors.
 */
export function isDefaultRetryable(error: Error): boolean {
  const message = error.message?.toLowerCase() || '';
  const name = error.name?.toLowerCase() || '';

  // Check for specific error types
  if (name === 'timeouterror') return true;
  if (name === 'aborterror') return false; // User-initiated abort, don't retry

  // Network errors
  if (message.includes('network') || message.includes('fetch failed')) return true;

  // Rate limiting
  if (message.includes('429') || message.includes('rate limit')) return true;

  // Server errors (5xx)
  if (message.includes('500') || message.includes('502') ||
      message.includes('503') || message.includes('504')) return true;

  // Generic transient errors
  if (message.includes('internal') || message.includes('unavailable')) return true;

  return false;
}

/**
 * Retries an async function with exponential backoff.
 *
 * @example
 * const result = await withRetry(
 *   async () => fetchFromAPI(),
 *   {
 *     maxRetries: 5,
 *     onRetry: (attempt, error) => console.log(`Retry ${attempt}: ${error.message}`)
 *   }
 * );
 *
 * @param fn - Async function to retry
 * @param options - Retry configuration
 * @returns The resolved value or throws after all retries exhausted
 */
export async function withRetryBackoff<T>(
  fn: () => Promise<T>,
  options: RetryOptions = {}
): Promise<T> {
  const {
    maxRetries = DEFAULT_RETRY_OPTIONS.maxRetries,
    baseDelay = DEFAULT_RETRY_OPTIONS.baseDelay,
    maxDelay = DEFAULT_RETRY_OPTIONS.maxDelay,
    backoffFactor = DEFAULT_RETRY_OPTIONS.backoffFactor,
    onRetry,
    isRetryable = isDefaultRetryable,
    signal,
  } = options;

  let lastError: Error;

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    // Check for abort before each attempt
    if (signal?.aborted) {
      throw new Error('Operation was aborted');
    }

    try {
      return await fn();
    } catch (error) {
      lastError = error instanceof Error ? error : new Error(String(error));

      // Don't retry if not retryable or we've exhausted retries
      if (!isRetryable(lastError) || attempt === maxRetries) {
        throw lastError;
      }

      // Calculate delay with exponential backoff, capped at maxDelay
      const delay = Math.min(
        baseDelay * Math.pow(backoffFactor, attempt),
        maxDelay
      );

      // Add jitter (10-20% random variation) to prevent thundering herd
      const jitter = delay * (0.1 + Math.random() * 0.1);
      const finalDelay = Math.round(delay + jitter);

      // Notify caller of retry
      onRetry?.(attempt + 1, lastError, finalDelay);
      console.warn(
        `[Retry] Attempt ${attempt + 1}/${maxRetries} failed: ${lastError.message}. ` +
        `Retrying in ${finalDelay}ms...`
      );

      // Wait before retry
      await new Promise((resolve, reject) => {
        const timeoutId = setTimeout(resolve, finalDelay);

        // Allow abort during delay
        if (signal) {
          signal.addEventListener('abort', () => {
            clearTimeout(timeoutId);
            reject(new Error('Operation was aborted during retry delay'));
          }, { once: true });
        }
      });
    }
  }

  throw lastError!;
}

// ============================================================
// BLOB URL MANAGEMENT
// ============================================================

/**
 * BlobManager provides centralized Blob URL lifecycle management.
 * Prevents memory leaks by tracking and properly revoking URLs.
 *
 * @example
 * const blobManager = BlobManager.getInstance();
 * const url = blobManager.create(myBlob);
 * // Use the URL...
 * blobManager.revoke(url); // Or call revokeAll() on component unmount
 */
export class BlobManager {
  private urls = new Map<string, { createdAt: number; size: number }>();
  private static instance: BlobManager;

  private constructor() {}

  /**
   * Get the singleton instance.
   */
  static getInstance(): BlobManager {
    if (!BlobManager.instance) {
      BlobManager.instance = new BlobManager();
    }
    return BlobManager.instance;
  }

  /**
   * Create a Blob URL and register it for tracking.
   * @param blob - The Blob to create a URL for
   * @returns The created Object URL
   */
  create(blob: Blob): string {
    const url = URL.createObjectURL(blob);
    this.urls.set(url, {
      createdAt: Date.now(),
      size: blob.size,
    });
    console.log(`[BlobManager] Created URL (total: ${this.urls.size}, size: ${this.formatBytes(blob.size)})`);
    return url;
  }

  /**
   * Revoke a specific Blob URL.
   * Safe to call multiple times with same URL.
   * @param url - The URL to revoke
   * @returns true if the URL was found and revoked
   */
  revoke(url: string): boolean {
    if (this.urls.has(url)) {
      URL.revokeObjectURL(url);
      const info = this.urls.get(url)!;
      this.urls.delete(url);
      console.log(
        `[BlobManager] Revoked URL (remaining: ${this.urls.size}, ` +
        `was alive: ${Math.round((Date.now() - info.createdAt) / 1000)}s)`
      );
      return true;
    }
    return false;
  }

  /**
   * Revoke all tracked Blob URLs.
   * Call this on component unmount or when resetting state.
   */
  revokeAll(): void {
    const count = this.urls.size;
    if (count === 0) return;

    console.log(`[BlobManager] Revoking all ${count} URLs`);
    this.urls.forEach((_, url) => URL.revokeObjectURL(url));
    this.urls.clear();
  }

  /**
   * Get current statistics about tracked URLs.
   */
  getStats(): { count: number; totalSize: number; oldestAge: number } {
    let totalSize = 0;
    let oldestCreatedAt = Date.now();

    this.urls.forEach(info => {
      totalSize += info.size;
      if (info.createdAt < oldestCreatedAt) {
        oldestCreatedAt = info.createdAt;
      }
    });

    return {
      count: this.urls.size,
      totalSize,
      oldestAge: this.urls.size > 0 ? Date.now() - oldestCreatedAt : 0,
    };
  }

  /**
   * Check if a URL is being tracked.
   */
  isTracked(url: string): boolean {
    return this.urls.has(url);
  }

  /**
   * Revoke URLs older than a specified age.
   * Useful for periodic cleanup of leaked URLs.
   * @param maxAgeMs - Maximum age in milliseconds
   * @returns Number of URLs revoked
   */
  revokeOlderThan(maxAgeMs: number): number {
    const now = Date.now();
    let revokedCount = 0;

    this.urls.forEach((info, url) => {
      if (now - info.createdAt > maxAgeMs) {
        URL.revokeObjectURL(url);
        this.urls.delete(url);
        revokedCount++;
      }
    });

    if (revokedCount > 0) {
      console.log(`[BlobManager] Revoked ${revokedCount} stale URLs (older than ${maxAgeMs / 1000}s)`);
    }

    return revokedCount;
  }

  private formatBytes(bytes: number): string {
    if (bytes < 1024) return `${bytes} B`;
    if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;
    return `${(bytes / 1024 / 1024).toFixed(2)} MB`;
  }
}

// ============================================================
// SAFE LOCALSTORAGE UTILITIES
// ============================================================

/**
 * Maximum size for localStorage values (4MB to leave room for other data).
 */
const DEFAULT_MAX_SIZE_KB = 4000;

/**
 * Safe localStorage operations with size limits and error handling.
 * Prevents quota exceeded errors and handles malformed data gracefully.
 */
export const safeLocalStorage = {
  /**
   * Safely set a value in localStorage with size validation.
   * @param key - Storage key
   * @param value - Value to store (will be JSON stringified)
   * @param maxSizeKB - Maximum size in KB (default: 4000)
   * @returns true if successful, false if failed or too large
   */
  set: (key: string, value: unknown, maxSizeKB = DEFAULT_MAX_SIZE_KB): boolean => {
    try {
      const json = JSON.stringify(value);
      const sizeKB = json.length / 1024;

      if (sizeKB > maxSizeKB) {
        console.warn(
          `[Storage] ${key} exceeds ${maxSizeKB}KB (${Math.round(sizeKB)}KB), skipping save`
        );
        return false;
      }

      localStorage.setItem(key, json);
      return true;
    } catch (error) {
      // Handle quota exceeded or other storage errors
      if (error instanceof Error && error.name === 'QuotaExceededError') {
        console.error(`[Storage] Quota exceeded when saving ${key}. Clearing old data...`);
        // Could implement LRU cleanup here
      } else {
        console.error(`[Storage] Failed to save ${key}:`, error);
      }
      return false;
    }
  },

  /**
   * Safely get a value from localStorage with fallback.
   * @param key - Storage key
   * @param fallback - Value to return if key doesn't exist or is invalid
   * @returns Parsed value or fallback
   */
  get: <T>(key: string, fallback: T): T => {
    try {
      const item = localStorage.getItem(key);
      if (item === null) return fallback;
      return JSON.parse(item) as T;
    } catch (error) {
      console.warn(`[Storage] Failed to parse ${key}, using fallback:`, error);
      return fallback;
    }
  },

  /**
   * Remove a key from localStorage.
   * @param key - Storage key to remove
   */
  remove: (key: string): void => {
    try {
      localStorage.removeItem(key);
    } catch (error) {
      console.error(`[Storage] Failed to remove ${key}:`, error);
    }
  },

  /**
   * Get the current size of a stored value in bytes.
   * @param key - Storage key
   * @returns Size in bytes, or 0 if not found
   */
  getSize: (key: string): number => {
    const item = localStorage.getItem(key);
    return item ? new Blob([item]).size : 0;
  },

  /**
   * Get total localStorage usage.
   * @returns Total size in bytes
   */
  getTotalSize: (): number => {
    let total = 0;
    for (let i = 0; i < localStorage.length; i++) {
      const key = localStorage.key(i);
      if (key) {
        const value = localStorage.getItem(key);
        if (value) {
          total += key.length + value.length;
        }
      }
    }
    // UTF-16 encoding uses 2 bytes per character
    return total * 2;
  },
};

// ============================================================
// JSON EXTRACTION UTILITIES
// ============================================================

/**
 * Robust JSON extraction from AI responses with multiple fallback strategies.
 * Handles common LLM output patterns like markdown code blocks, wrapped text, etc.
 *
 * @example
 * const data = extractJSONFromResponse<MyType>(llmResponse);
 * if (data) {
 *   // Use the extracted data
 * }
 *
 * @param response - Raw string response from LLM
 * @returns Parsed JSON object or null if extraction failed
 */
export function extractJSONFromResponse<T = unknown>(response: string): T | null {
  if (!response || typeof response !== 'string') {
    return null;
  }

  const strategies: Array<() => unknown> = [
    // Strategy 1: Direct parse (response is pure JSON)
    () => JSON.parse(response.trim()),

    // Strategy 2: Extract from ```json ... ``` block
    () => {
      const match = response.match(/```json\s*([\s\S]*?)\s*```/i);
      if (!match || !match[1]) throw new Error('No json block found');
      return JSON.parse(match[1].trim());
    },

    // Strategy 3: Extract from ``` ... ``` block (without json tag)
    () => {
      const match = response.match(/```\s*([\s\S]*?)\s*```/);
      if (!match || !match[1]) throw new Error('No code block found');
      return JSON.parse(match[1].trim());
    },

    // Strategy 4: Find first { ... } object
    () => {
      const match = response.match(/\{[\s\S]*\}/);
      if (!match) throw new Error('No object found');
      return JSON.parse(match[0]);
    },

    // Strategy 5: Find first [ ... ] array
    () => {
      const match = response.match(/\[[\s\S]*\]/);
      if (!match) throw new Error('No array found');
      return JSON.parse(match[0]);
    },

    // Strategy 6: Remove common prefixes/suffixes and try again
    () => {
      const cleaned = response
        .replace(/^[\s\S]*?(?=[\[{])/, '') // Remove everything before first [ or {
        .replace(/[\]}][\s\S]*$/, (match) => match[0] ?? "") // Keep only up to last ] or }
        .trim();
      return JSON.parse(cleaned);
    },

    // Strategy 7: Fix common JSON issues and retry
    () => {
      let fixed = response
        .replace(/,\s*([}\]])/g, '$1') // Remove trailing commas
        .replace(/([{,]\s*)(\w+)(\s*:)/g, '$1"$2"$3') // Quote unquoted keys
        .replace(/:\s*'([^']*)'/g, ': "$1"') // Convert single quotes to double
        .replace(/\n/g, '\\n') // Escape newlines in strings
        .trim();

      // Find the JSON part
      const start = fixed.indexOf('{');
      const end = fixed.lastIndexOf('}');
      if (start === -1 || end === -1) throw new Error('No object boundaries');

      return JSON.parse(fixed.substring(start, end + 1));
    },
  ];

  for (let i = 0; i < strategies.length; i++) {
    const strategy = strategies[i];
    if (!strategy) continue;
    try {
      const result = strategy();
      if (result !== null && result !== undefined) {
        console.log(`[JSONExtract] Succeeded with strategy ${i + 1}`);
        return result as T;
      }
    } catch {
      // Strategy failed, try next
      continue;
    }
  }

  console.warn('[JSONExtract] All strategies failed. Response preview:', response.substring(0, 200));
  return null;
}

/**
 * Extract JSON with schema validation using a custom validator function.
 *
 * @param response - Raw string response from LLM
 * @param validate - Validation function (e.g., Zod schema.parse)
 * @returns Validated data or null
 */
export function extractAndValidateJSON<T>(
  response: string,
  validate: (data: unknown) => T
): T | null {
  const extracted = extractJSONFromResponse(response);
  if (extracted === null) return null;

  try {
    return validate(extracted);
  } catch (error) {
    console.warn('[JSONExtract] Validation failed:', error);
    return null;
  }
}

// ============================================================
// ASYNC OPERATION HELPERS
// ============================================================

/**
 * Creates a deferred promise that can be resolved/rejected externally.
 * Useful for complex async workflows.
 */
export function createDeferred<T>(): {
  promise: Promise<T>;
  resolve: (value: T) => void;
  reject: (reason?: unknown) => void;
} {
  let resolve!: (value: T) => void;
  let reject!: (reason?: unknown) => void;

  const promise = new Promise<T>((res, rej) => {
    resolve = res;
    reject = rej;
  });

  return { promise, resolve, reject };
}

/**
 * Runs multiple promises concurrently with a limit on concurrent executions.
 * Prevents overwhelming APIs with too many simultaneous requests.
 *
 * @param items - Array of items to process
 * @param fn - Async function to run for each item
 * @param concurrency - Maximum concurrent executions
 * @returns Array of results (or errors) in same order as input
 */
export async function runWithConcurrency<T, R>(
  items: T[],
  fn: (item: T, index: number) => Promise<R>,
  concurrency: number
): Promise<Array<R | Error>> {
  const results: Array<R | Error> = new Array(items.length);
  let currentIndex = 0;

  async function worker(): Promise<void> {
    while (currentIndex < items.length) {
      const index = currentIndex++;
      const item = items[index];
      if (item === undefined) continue;

      try {
        results[index] = await fn(item, index);
      } catch (error) {
        results[index] = error instanceof Error ? error : new Error(String(error));
      }
    }
  }

  // Start workers
  const workers = Array.from(
    { length: Math.min(concurrency, items.length) },
    () => worker()
  );

  await Promise.all(workers);
  return results;
}

/**
 * Sleep for a specified duration.
 * Supports cancellation via AbortSignal.
 */
export function sleep(ms: number, signal?: AbortSignal): Promise<void> {
  return new Promise((resolve, reject) => {
    if (signal?.aborted) {
      reject(new Error('Sleep was aborted'));
      return;
    }

    const timeoutId = setTimeout(resolve, ms);

    signal?.addEventListener('abort', () => {
      clearTimeout(timeoutId);
      reject(new Error('Sleep was aborted'));
    }, { once: true });
  });
}

// ============================================================
// ERROR HANDLING UTILITIES
// ============================================================

/**
 * Wraps an async function to catch and log errors without throwing.
 * Returns the error instead of throwing.
 */
export async function tryCatch<T>(
  fn: () => Promise<T>,
  context?: string
): Promise<{ data: T; error: null } | { data: null; error: Error }> {
  try {
    const data = await fn();
    return { data, error: null };
  } catch (error) {
    const err = error instanceof Error ? error : new Error(String(error));
    if (context) {
      console.error(`[${context}] ${err.message}`);
    }
    return { data: null, error: err };
  }
}

/**
 * Creates a logger with a service prefix for consistent logging.
 */
export function createServiceLogger(serviceName: string) {
  const prefix = `[${serviceName}]`;

  return {
    info: (message: string, data?: unknown) => {
      console.log(`${prefix} ${message}`, data ?? '');
    },
    warn: (message: string, data?: unknown) => {
      console.warn(`${prefix} ${message}`, data ?? '');
    },
    error: (message: string, data?: unknown) => {
      console.error(`${prefix} ${message}`, data ?? '');
    },
    debug: (message: string, data?: unknown) => {
      if (process.env.NODE_ENV === 'development') {
        console.debug(`${prefix} ${message}`, data ?? '');
      }
    },
  };
}
````

## File: packages/shared/src/services/subtitleService.ts
````typescript
/**
 * Subtitle Service
 * 
 * Handles text segmentation and subtitle timing logic.
 * Extracted from ProductionView to adhere to Single Responsibility Principle.
 */

import { SubtitleItem } from "@/types";

interface SegmentationOptions {
    maxWordsPerSegment?: number;
    delimiters?: RegExp;
}

/**
 * Split long text into shorter segments (max ~8 words per segment)
 * Uses sentence-based splitting first, then word-based chunking
 * This keeps subtitles readable and prevents long text blocks
 */
export function splitTextIntoSegments(
    text: string,
    totalDuration: number,
    options: SegmentationOptions = {}
): { text: string; duration: number }[] {
    const {
        maxWordsPerSegment = 8,
        delimiters = /([.!?،؟])\s*/g
    } = options;

    // First split by sentence delimiters
    const sentences = text.split(delimiters).filter(s => s.trim().length > 0);

    // Recombine sentences with their delimiters
    const sentenceChunks: string[] = [];
    for (let i = 0; i < sentences.length; i += 2) {
        const sentence = sentences[i];
        if (sentence && sentence.trim()) {
            const delimiter = sentences[i + 1] || '';
            sentenceChunks.push((sentence + delimiter).trim());
        }
    }

    // If no sentence delimiters found, treat whole text as one chunk
    if (sentenceChunks.length === 0) {
        sentenceChunks.push(text.trim());
    }

    const finalSegments: string[] = [];

    sentenceChunks.forEach(chunk => {
        const words = chunk.split(/\s+/).filter(w => w.length > 0);
        if (words.length <= maxWordsPerSegment) {
            finalSegments.push(chunk);
        } else {
            // Split long sentences by word count
            for (let i = 0; i < words.length; i += maxWordsPerSegment) {
                const segmentWords = words.slice(i, i + maxWordsPerSegment);
                finalSegments.push(segmentWords.join(' '));
            }
        }
    });

    // Calculate duration per segment based on word count ratio
    const totalWords = text.split(/\s+/).filter(w => w.length > 0).length;
    return finalSegments.map(segment => {
        const segmentWords = segment.split(/\s+/).filter(w => w.length > 0).length;
        const ratio = totalWords > 0 ? segmentWords / totalWords : 1 / finalSegments.length;
        return {
            text: segment,
            duration: totalDuration * ratio,
        };
    });
}
````

## File: packages/shared/src/services/subtitleStyleService.ts
````typescript
/**
 * Subtitle Style Service
 * 
 * Provides theme-based subtitle styling that matches video content type.
 * Supports multiple preset themes with customizable colors, fonts, and effects.
 */

import { VideoPurpose } from "../constants";
import { EmotionalTone } from "../types";

/**
 * Subtitle style configuration for rendering
 */
export interface SubtitleStyle {
    /** Font family */
    fontFamily: string;
    /** Base font size (will be scaled based on video resolution) */
    fontSize: number;
    /** Primary text color (hex) */
    color: string;
    /** Text outline/stroke color (hex, optional) */
    strokeColor?: string;
    /** Stroke width in pixels */
    strokeWidth?: number;
    /** Shadow color (hex, optional) */
    shadowColor?: string;
    /** Shadow blur radius */
    shadowBlur?: number;
    /** Shadow offset X */
    shadowOffsetX?: number;
    /** Shadow offset Y */
    shadowOffsetY?: number;
    /** Background bar color (rgba) */
    backgroundColor?: string;
    /** Text glow effect color (for special effects) */
    glowColor?: string;
    /** Glow blur radius */
    glowRadius?: number;
    /** Letter spacing adjustment */
    letterSpacing?: number;
    /** Line height multiplier */
    lineHeight?: number;
    /** Text alignment */
    textAlign?: 'left' | 'center' | 'right';
    /** Vertical position (0-1, where 0=top, 1=bottom) */
    verticalPosition?: number;
    /** Enable text animation */
    animated?: boolean;
}

/**
 * Subtitle theme names
 */
export type SubtitleTheme =
    | 'documentary'
    | 'cinematic'
    | 'horror'
    | 'kids'
    | 'social'
    | 'romantic'
    | 'sci-fi'
    | 'minimalist'
    | 'retro'
    | 'neon';

/**
 * Predefined subtitle style themes
 */
export const SUBTITLE_THEMES: Record<SubtitleTheme, SubtitleStyle> = {
    documentary: {
        fontFamily: 'Inter, Arial, sans-serif',
        fontSize: 42,
        color: '#FFFFFF',
        strokeColor: '#000000',
        strokeWidth: 2,
        shadowColor: 'rgba(0, 0, 0, 0.7)',
        shadowBlur: 4,
        shadowOffsetX: 2,
        shadowOffsetY: 2,
        backgroundColor: 'rgba(0, 0, 0, 0.6)',
        lineHeight: 1.3,
        textAlign: 'center',
        verticalPosition: 0.85,
    },
    cinematic: {
        fontFamily: 'Georgia, "Times New Roman", serif',
        fontSize: 44,
        color: '#F5F5F5',
        strokeColor: '#1A1A1A',
        strokeWidth: 1,
        shadowColor: 'rgba(0, 0, 0, 0.8)',
        shadowBlur: 8,
        shadowOffsetX: 0,
        shadowOffsetY: 3,
        letterSpacing: 1,
        lineHeight: 1.4,
        textAlign: 'center',
        verticalPosition: 0.88,
    },
    horror: {
        fontFamily: '"Creepster", "Chiller", cursive, serif',
        fontSize: 48,
        color: '#FF3333',
        strokeColor: '#8B0000',
        strokeWidth: 2,
        shadowColor: 'rgba(139, 0, 0, 0.9)',
        shadowBlur: 15,
        shadowOffsetX: 0,
        shadowOffsetY: 5,
        glowColor: '#FF0000',
        glowRadius: 20,
        letterSpacing: 2,
        lineHeight: 1.2,
        textAlign: 'center',
        verticalPosition: 0.80,
    },
    kids: {
        fontFamily: '"Comic Sans MS", "Bubblegum Sans", cursive',
        fontSize: 50,
        color: '#FFFF00',
        strokeColor: '#FF6600',
        strokeWidth: 4,
        shadowColor: 'rgba(255, 102, 0, 0.8)',
        shadowBlur: 0,
        shadowOffsetX: 4,
        shadowOffsetY: 4,
        lineHeight: 1.3,
        textAlign: 'center',
        verticalPosition: 0.82,
        animated: true,
    },
    social: {
        fontFamily: '"Montserrat", "Roboto", sans-serif',
        fontSize: 38,
        color: '#FFFFFF',
        strokeColor: '#000000',
        strokeWidth: 3,
        shadowColor: 'rgba(0, 0, 0, 0.5)',
        shadowBlur: 2,
        shadowOffsetX: 0,
        shadowOffsetY: 0,
        lineHeight: 1.2,
        textAlign: 'center',
        verticalPosition: 0.90,
    },
    romantic: {
        fontFamily: '"Playfair Display", "Didot", serif',
        fontSize: 40,
        color: '#FFE4E1',
        strokeColor: '#8B4557',
        strokeWidth: 1,
        shadowColor: 'rgba(139, 69, 87, 0.6)',
        shadowBlur: 10,
        shadowOffsetX: 0,
        shadowOffsetY: 2,
        glowColor: 'rgba(255, 182, 193, 0.5)',
        glowRadius: 15,
        letterSpacing: 1,
        lineHeight: 1.5,
        textAlign: 'center',
        verticalPosition: 0.85,
    },
    'sci-fi': {
        fontFamily: '"Orbitron", "Share Tech Mono", monospace',
        fontSize: 36,
        color: '#00FFFF',
        strokeColor: '#003366',
        strokeWidth: 2,
        shadowColor: 'rgba(0, 255, 255, 0.5)',
        shadowBlur: 10,
        shadowOffsetX: 0,
        shadowOffsetY: 0,
        glowColor: '#00FFFF',
        glowRadius: 25,
        letterSpacing: 3,
        lineHeight: 1.3,
        textAlign: 'center',
        verticalPosition: 0.88,
    },
    minimalist: {
        fontFamily: '"Helvetica Neue", Helvetica, Arial, sans-serif',
        fontSize: 36,
        color: '#FFFFFF',
        strokeWidth: 0,
        shadowColor: 'rgba(0, 0, 0, 0.3)',
        shadowBlur: 2,
        shadowOffsetX: 0,
        shadowOffsetY: 1,
        lineHeight: 1.4,
        textAlign: 'center',
        verticalPosition: 0.90,
    },
    retro: {
        fontFamily: '"VT323", "Press Start 2P", monospace',
        fontSize: 32,
        color: '#00FF00',
        strokeColor: '#003300',
        strokeWidth: 2,
        backgroundColor: 'rgba(0, 0, 0, 0.9)',
        lineHeight: 1.2,
        textAlign: 'center',
        verticalPosition: 0.85,
    },
    neon: {
        fontFamily: '"Neon", "Broadway", sans-serif',
        fontSize: 44,
        color: '#FF00FF',
        strokeColor: '#660066',
        strokeWidth: 1,
        shadowColor: 'rgba(255, 0, 255, 0.9)',
        shadowBlur: 20,
        shadowOffsetX: 0,
        shadowOffsetY: 0,
        glowColor: '#FF00FF',
        glowRadius: 30,
        letterSpacing: 2,
        lineHeight: 1.3,
        textAlign: 'center',
        verticalPosition: 0.85,
    },
};

/**
 * Map video purpose to recommended subtitle theme
 */
const PURPOSE_THEME_MAP: Partial<Record<VideoPurpose, SubtitleTheme>> = {
    documentary: 'documentary',
    music_video: 'cinematic',
    social_short: 'social',
    commercial: 'minimalist',
    podcast_visual: 'documentary',
    lyric_video: 'neon',
    storytelling: 'cinematic',
    educational: 'minimalist',
    horror_mystery: 'horror',
    travel: 'social',
    motivational: 'social',
    news_report: 'documentary',
};

/**
 * Map emotional tone to theme adjustments
 */
const TONE_THEME_MAP: Partial<Record<EmotionalTone, SubtitleTheme>> = {
    dramatic: 'cinematic',
    calm: 'minimalist',
    urgent: 'social',
    professional: 'documentary',
    friendly: 'social',
};

/**
 * Get the best subtitle style for given context
 * 
 * @param purpose - Video purpose (optional)
 * @param tone - Emotional tone (optional)
 * @param overrideTheme - Explicit theme override (optional)
 * @returns SubtitleStyle configuration
 */
export function getSubtitleStyle(
    purpose?: VideoPurpose,
    tone?: EmotionalTone,
    overrideTheme?: SubtitleTheme
): SubtitleStyle {
    // If explicit theme provided, use it
    if (overrideTheme && SUBTITLE_THEMES[overrideTheme]) {
        return SUBTITLE_THEMES[overrideTheme];
    }

    // Try to match by purpose first
    if (purpose && PURPOSE_THEME_MAP[purpose]) {
        return SUBTITLE_THEMES[PURPOSE_THEME_MAP[purpose]!];
    }

    // Fall back to tone-based selection
    if (tone && TONE_THEME_MAP[tone]) {
        return SUBTITLE_THEMES[TONE_THEME_MAP[tone]!];
    }

    // Default to documentary style
    return SUBTITLE_THEMES.documentary;
}

/**
 * Apply a subtitle style to canvas context for rendering
 */
export function applySubtitleStyle(
    ctx: CanvasRenderingContext2D,
    style: SubtitleStyle,
    scale: number = 1
): void {
    // Font
    ctx.font = `${Math.round(style.fontSize * scale)}px ${style.fontFamily}`;
    ctx.textAlign = style.textAlign || 'center';
    ctx.textBaseline = 'middle';

    // Text color
    ctx.fillStyle = style.color;

    // Shadow
    if (style.shadowColor) {
        ctx.shadowColor = style.shadowColor;
        ctx.shadowBlur = (style.shadowBlur || 0) * scale;
        ctx.shadowOffsetX = (style.shadowOffsetX || 0) * scale;
        ctx.shadowOffsetY = (style.shadowOffsetY || 0) * scale;
    }

    // Stroke
    if (style.strokeColor && style.strokeWidth && style.strokeWidth > 0) {
        ctx.strokeStyle = style.strokeColor;
        ctx.lineWidth = style.strokeWidth * scale;
    }
}

/**
 * Get available theme names
 */
export function getAvailableThemes(): SubtitleTheme[] {
    return Object.keys(SUBTITLE_THEMES) as SubtitleTheme[];
}
````

## File: packages/shared/src/services/sunoService.ts
````typescript
/**
 * Suno API Service
 * 
 * Integrates with Suno API for AI-powered music generation.
 * https://api.sunoapi.org/api/v1
 */

import type { SubtitleItem } from "../types";

// --- Configuration ---
const SUNO_API_BASE = "https://api.sunoapi.org/api/v1";

// --- Rate Limiter ---

/**
 * Simple rate limiter for Suno API.
 * Enforces 20 requests per 10 seconds limit as per API documentation.
 * Implements a sliding window algorithm to track request timestamps.
 */
class SunoRateLimiter {
  private requestTimestamps: number[] = [];
  private readonly maxRequests = 20;
  private readonly windowMs = 10000; // 10 seconds

  /**
   * Wait for an available request slot.
   * If the rate limit is reached, waits until a slot becomes available.
   * 
   * @returns Promise that resolves when a request slot is available
   */
  async waitForSlot(): Promise<void> {
    const now = Date.now();

    // Remove timestamps outside the sliding window
    this.requestTimestamps = this.requestTimestamps.filter(
      ts => now - ts < this.windowMs
    );

    // If at capacity, wait for the oldest request to expire
    if (this.requestTimestamps.length >= this.maxRequests) {
      const oldestTimestamp = this.requestTimestamps[0]!;
      const waitTime = this.windowMs - (now - oldestTimestamp) + 10; // +10ms buffer

      if (waitTime > 0) {
        console.log(`[Suno] Rate limit reached, waiting ${waitTime}ms...`);
        await new Promise(resolve => setTimeout(resolve, waitTime));

        // Clean up again after waiting
        const newNow = Date.now();
        this.requestTimestamps = this.requestTimestamps.filter(
          ts => newNow - ts < this.windowMs
        );
      }
    }

    // Record this request
    this.requestTimestamps.push(Date.now());
  }

  /**
   * Get the current number of requests in the sliding window.
   * Useful for debugging and monitoring.
   */
  getCurrentRequestCount(): number {
    const now = Date.now();
    this.requestTimestamps = this.requestTimestamps.filter(
      ts => now - ts < this.windowMs
    );
    return this.requestTimestamps.length;
  }

  /**
   * Reset the rate limiter state.
   * Useful for testing.
   */
  reset(): void {
    this.requestTimestamps = [];
  }
}

// Singleton instance of the rate limiter
const rateLimiter = new SunoRateLimiter();

// Export for testing
export { SunoRateLimiter, rateLimiter };

// Export status helper functions for testing
export { mapToSunoTaskStatus, isFailedStatus, isIntermediateStatus };

/**
 * Get Suno API key from environment variables.
 * Follows the same pattern as freesoundService.
 */
const getSunoApiKey = (): string => {
  // Try Vite's import.meta.env first (browser)
  if (typeof window !== "undefined") {
    // @ts-ignore - Vite injects this at build time
    const viteEnv = (import.meta as any).env;
    if (viteEnv?.VITE_SUNO_API_KEY) {
      return viteEnv.VITE_SUNO_API_KEY;
    }
  }
  // Fallback to process.env (Node.js/SSR)
  return process.env.VITE_SUNO_API_KEY || "";
};

const SUNO_API_KEY = getSunoApiKey();

// Debug log (without exposing the key)
const isBrowser = typeof window !== "undefined";
if (isBrowser) {
  console.log(`[Suno] API Key configured: ${SUNO_API_KEY ? "YES" : "NO"}`);
}

// --- Helper: Call Backend Proxy ---

// Use backend proxy to bypass CORS
// @ts-ignore - Vite injects this at build time
const SERVER_URL = (import.meta as any).env?.VITE_SERVER_URL || "http://localhost:3001";

async function callSunoProxy(endpoint: string, body?: any, method: string = "POST"): Promise<any> {
  // Wait for rate limiter slot before making request
  await rateLimiter.waitForSlot();

  const fetchOptions: any = {
    method: method,
    headers: {
      "Content-Type": "application/json",
    },
  };

  if (method !== "GET" && method !== "HEAD") {
    fetchOptions.body = JSON.stringify(body);
  }

  const response = await fetch(`${SERVER_URL}/api/suno/proxy/${endpoint}`, fetchOptions);

  const data = await response.json();

  // Enhanced error handling based on error codes
  if (!response.ok || (data.code && data.code !== 200)) {
    const errorCode = data.code || response.status;
    const errorMessage = data.msg || data.error || data.message || `Suno API error: ${endpoint}`;

    // Use the error mapping helper function
    throw mapErrorCodeToError(errorCode, endpoint, errorMessage);
  }

  // For generate endpoint, return taskId; for other endpoints, return full data
  if (endpoint === "generate" && data.data?.taskId) {
    return data.data.taskId;
  }
  return data.data || data; // Return data.data if exists, otherwise full response
}

// --- Types ---

/**
 * Suno AI model versions.
 * V5 is the latest and highest quality.
 */
export type SunoModel = "V4" | "V4_5" | "V4_5PLUS" | "V4_5ALL" | "V5";

/**
 * Status of a Suno generation task.
 * Extended to include all documented statuses from the Suno API.
 */
export type SunoTaskStatus =
  | "PENDING"
  | "PROCESSING"
  | "TEXT_SUCCESS"
  | "FIRST_SUCCESS"
  | "SUCCESS"
  | "CREATE_TASK_FAILED"
  | "GENERATE_AUDIO_FAILED"
  | "CALLBACK_EXCEPTION"
  | "SENSITIVE_WORD_ERROR"
  | "FAILED";

/**
 * Configuration for extending an existing music track.
 * Used with the extend endpoint.
 */
export interface SunoExtendConfig {
  /** Task ID of the original generation */
  taskId: string;
  /** Audio ID of the track to extend */
  audioId: string;
  /** Lyrics or prompt for the extension */
  prompt?: string;
  /** Music style/genre for the extension */
  style?: string;
  /** Title for the extended track */
  title?: string;
  /** Time in seconds to continue from */
  continueAt: number;
  /** AI model version */
  model?: SunoModel;
  /** Optional webhook URL for status updates */
  callBackUrl?: string;
}

/**
 * Configuration for upload-based operations (upload-and-extend, upload-and-cover).
 * Extends SunoGenerationConfig with upload-specific fields.
 */
export interface SunoUploadConfig extends SunoGenerationConfig {
  /** URL of the uploaded audio file */
  uploadUrl: string;
  /** Time in seconds to continue from (for upload-and-extend) */
  continueAt?: number;
  /** Use default parameters flag for custom parameter mode */
  defaultParamFlag?: boolean;
}

/**
 * Configuration for persona generation.
 * Used to create personalized music styles.
 */
export interface SunoPersonaConfig {
  /** Name of the persona */
  name: string;
  /** Description of the persona's characteristics */
  description: string;
  /** Music style associated with the persona */
  style: string;
  /** Optional webhook URL for status updates */
  callBackUrl?: string;
}

/**
 * Result of vocal/instrumental stem separation.
 */
export interface SunoStemSeparationResult {
  /** Task identifier */
  taskId: string;
  /** Current status */
  status: SunoTaskStatus;
  /** URL to the separated vocals track */
  vocalsUrl?: string;
  /** URL to the separated instrumental track */
  instrumentalUrl?: string;
  /** Error message (available when status is FAILED) */
  errorMessage?: string;
}

/**
 * Configuration for music generation request.
 */
export interface SunoGenerationConfig {
  /** Topic or lyrics prompt (required) */
  prompt: string;
  /** Song title (optional) */
  title?: string;
  /** Music style/genre (e.g., "Pop, Upbeat") */
  style?: string;
  /** Generate instrumental only (no vocals) */
  instrumental?: boolean;
  /** AI model version (default: V5) */
  model?: SunoModel;
  /** Vocal gender: 'm' for male, 'f' for female */
  vocalGender?: "m" | "f";
  /** Comma-separated styles to exclude (e.g., "Heavy Metal, Screaming") */
  negativeTags?: string;
  /** Style influence strength (0-1, default: 0.65) */
  styleWeight?: number;
  /** Creative variation (0-1, default: 0.5) */
  weirdnessConstraint?: number;
  /** Audio quality weight (0-1, default: 0.65) */
  audioWeight?: number;
  /** Optional webhook URL for status updates */
  callBackUrl?: string;
  /** Enable Custom Mode (advanced settings) - defaults to true if style/title provided */
  customMode?: boolean;
  /** Persona ID to apply specific style (Custom Mode only) */
  personaId?: string;
}

/**
 * A single generated music track from Suno.
 */
export interface SunoGeneratedTrack {
  /** Suno's audio ID */
  id: string;
  /** Generated or user-provided title */
  title: string;
  /** URL to the MP3 file */
  audio_url: string;
  /** Duration in seconds */
  duration: number;
  /** Style used for generation */
  style?: string;
  /** Lyrics if vocal track */
  lyrics?: string;
}

/**
 * Result of a music generation task.
 */
export interface SunoTaskResult {
  /** Task identifier */
  taskId: string;
  /** Current status */
  status: SunoTaskStatus;
  /** Generated tracks (available when status is SUCCESS) */
  tracks?: SunoGeneratedTrack[];
  /** Error message (available when status is FAILED) */
  errorMessage?: string;
  /** Error code from API (available when status is FAILED) */
  errorCode?: number;
}

/**
 * Full track data from API response.
 */
export interface SunoTrackData {
  /** Suno's audio ID */
  id: string;
  /** URL to the MP3 file */
  audioUrl: string;
  /** URL to the streaming audio */
  streamAudioUrl: string;
  /** URL to the cover image */
  imageUrl: string;
  /** Prompt/lyrics used for generation */
  prompt: string;
  /** Model name used for generation */
  modelName: string;
  /** Generated or user-provided title */
  title: string;
  /** Style tags */
  tags: string;
  /** Creation timestamp */
  createTime: string;
  /** Duration in seconds */
  duration: number;
  /** Track status */
  status: string;
  /** Track type */
  type: string;
  /** Error code if failed */
  errorCode?: number;
  /** Error message if failed */
  errorMessage?: string;
}

/**
 * Detailed task result with full API response fields.
 * Extends SunoTaskResult with additional metadata.
 */
export interface SunoDetailedTaskResult extends SunoTaskResult {
  /** Parent music ID if this is an extension/cover */
  parentMusicId?: string;
  /** Original request parameters */
  param?: Record<string, any>;
  /** Full API response data */
  response?: {
    taskId: string;
    sunoData: SunoTrackData[];
  };
  /** Task type (e.g., "generate", "extend", "cover") */
  type?: string;
  /** Error code from API */
  errorCode?: number;
}

/**
 * Result of a lyrics generation task.
 */
export interface SunoLyricsResult {
  /** Task identifier */
  taskId: string;
  /** Current status */
  status: SunoTaskStatus;
  /** Generated song title */
  title?: string;
  /** Generated lyrics text */
  text?: string;
  /** Error message (available when status is FAILED) */
  errorMessage?: string;
}

/**
 * Suno API credits balance.
 */
export interface SunoCredits {
  /** Remaining credits */
  credits: number;
}

// --- Custom Error Classes ---

/**
 * Base error class for Suno API errors.
 */
export class SunoApiError extends Error {
  constructor(
    message: string,
    public code: number,
    public endpoint: string
  ) {
    super(message);
    this.name = 'SunoApiError';
  }
}

/**
 * Error thrown when API returns error code 429 (insufficient credits).
 */
export class InsufficientCreditsError extends SunoApiError {
  constructor(endpoint: string) {
    super('Insufficient credits. Please top up your account.', 429, endpoint);
    this.name = 'InsufficientCreditsError';
  }
}

/**
 * Error thrown when API returns error code 430 (rate limit exceeded).
 */
export class RateLimitError extends SunoApiError {
  constructor(endpoint: string) {
    super('Rate limit exceeded. Please try again later.', 430, endpoint);
    this.name = 'RateLimitError';
  }
}

/**
 * Error thrown when API returns error code 455 (system maintenance).
 */
export class MaintenanceError extends SunoApiError {
  constructor(endpoint: string) {
    super('System is under maintenance. Please try again later.', 455, endpoint);
    this.name = 'MaintenanceError';
  }
}

/**
 * Maps an error code to the appropriate SunoApiError subclass.
 * This function is exported for testing purposes.
 * 
 * @param errorCode - The error code from the API response
 * @param endpoint - The API endpoint that returned the error
 * @param errorMessage - Optional error message (used for generic errors)
 * @returns The appropriate error instance
 */
export function mapErrorCodeToError(
  errorCode: number,
  endpoint: string,
  errorMessage?: string
): SunoApiError {
  switch (errorCode) {
    case 429:
      return new InsufficientCreditsError(endpoint);
    case 430:
      return new RateLimitError(endpoint);
    case 455:
      return new MaintenanceError(endpoint);
    default:
      return new SunoApiError(
        errorMessage || `Suno API error: ${endpoint}`,
        errorCode,
        endpoint
      );
  }
}

// --- Retry Logic ---

/**
 * Retry utility with exponential backoff for handling transient errors.
 * 
 * - Retries on RateLimitError with exponential backoff
 * - Does NOT retry on InsufficientCreditsError (non-recoverable)
 * - Does NOT retry on other errors (to avoid masking issues)
 * 
 * @param fn - Async function to execute with retry logic
 * @param maxRetries - Maximum number of retry attempts (default: 3)
 * @param baseDelayMs - Base delay in milliseconds for exponential backoff (default: 1000)
 * @returns Result of the function on success
 * @throws The last error encountered after all retries exhausted, or non-retryable errors immediately
 */
export async function withRetry<T>(
  fn: () => Promise<T>,
  maxRetries: number = 3,
  baseDelayMs: number = 1000
): Promise<T> {
  let lastError: Error | undefined;

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error as Error;

      // Don't retry on InsufficientCreditsError - it's not recoverable
      if (error instanceof InsufficientCreditsError) {
        console.log(`[Suno] InsufficientCreditsError - not retrying`);
        throw error;
      }

      // Retry with exponential backoff on RateLimitError
      if (error instanceof RateLimitError) {
        if (attempt < maxRetries) {
          const delay = baseDelayMs * Math.pow(2, attempt);
          console.log(`[Suno] RateLimitError - retry ${attempt + 1}/${maxRetries} after ${delay}ms`);
          await new Promise(resolve => setTimeout(resolve, delay));
          continue;
        }
        // Max retries exhausted for rate limit
        console.log(`[Suno] RateLimitError - max retries (${maxRetries}) exhausted`);
        throw error;
      }

      // Don't retry on other errors (MaintenanceError, SunoApiError, etc.)
      throw error;
    }
  }

  // This should never be reached, but TypeScript needs it
  throw lastError!;
}

// --- API Functions ---

/**
 * Check if Suno API is configured with a valid API key.
 */
export function isSunoConfigured(): boolean {
  // In browser, we assume server has the key (or we'll get an error on request)
  // This allows the UI to enable features even if client doesn't have the key directly
  if (isBrowser) return true;
  return !!SUNO_API_KEY;
}

/**
 * Generate music from a topic/prompt.
 * Endpoint: POST /api/v1/generate
 * Returns the taskId for tracking progress.
 * 
 * @param config - Generation configuration
 * @returns taskId for polling status
 * @throws Error if API key is not configured or request fails
 */
export async function generateMusic(config: SunoGenerationConfig): Promise<string> {
  // Auto-generate title if not provided (required for customMode: true)
  const autoTitle = config.title || config.prompt.slice(0, 50) || "AI Generated Track";

  const requestBody = {
    prompt: config.prompt,
    customMode: true, // Always use custom mode for full control
    style: config.style || "",
    title: autoTitle,
    instrumental: config.instrumental ?? false,
    model: config.model ?? "V5",
    callBackUrl: "playground", // Use "playground" for polling mode
    negativeTags: config.negativeTags || "",
    // Add other fields if needed, like styleWeight etc used in custom mode logic
    // But proxy body logic handles it. However, we should be explicit.
    vocalGender: config.vocalGender,
    styleWeight: config.styleWeight ?? 0.65,
    weirdnessConstraint: config.weirdnessConstraint ?? 0.5,
    audioWeight: config.audioWeight ?? 0.65,
    personaId: config.personaId
  };

  // Clean up undefined
  Object.keys(requestBody).forEach(key => (requestBody as any)[key] === undefined && delete (requestBody as any)[key]);

  // Full path: /api/v1/generate
  return callSunoProxy("generate", requestBody);
}

/**
 * Map raw API status string to SunoTaskStatus type.
 * Handles all documented statuses from the Suno API.
 * 
 * @param rawStatus - Raw status string from API response
 * @returns Normalized SunoTaskStatus value
 */
function mapToSunoTaskStatus(rawStatus: string | undefined): SunoTaskStatus {
  if (!rawStatus) return "PENDING";

  // Normalize to uppercase for comparison
  const normalizedStatus = rawStatus.toUpperCase();

  // Map all documented statuses
  const statusMap: Record<string, SunoTaskStatus> = {
    "PENDING": "PENDING",
    "PROCESSING": "PROCESSING",
    "TEXT_SUCCESS": "TEXT_SUCCESS",
    "FIRST_SUCCESS": "FIRST_SUCCESS",
    "SUCCESS": "SUCCESS",
    "CREATE_TASK_FAILED": "CREATE_TASK_FAILED",
    "GENERATE_AUDIO_FAILED": "GENERATE_AUDIO_FAILED",
    "CALLBACK_EXCEPTION": "CALLBACK_EXCEPTION",
    "SENSITIVE_WORD_ERROR": "SENSITIVE_WORD_ERROR",
    "FAILED": "FAILED",
  };

  return statusMap[normalizedStatus] || "PENDING";
}

/**
 * Check if a status represents a failure state.
 * 
 * @param status - SunoTaskStatus to check
 * @returns true if the status indicates a failure
 */
function isFailedStatus(status: SunoTaskStatus): boolean {
  return [
    "FAILED",
    "CREATE_TASK_FAILED",
    "GENERATE_AUDIO_FAILED",
    "CALLBACK_EXCEPTION",
    "SENSITIVE_WORD_ERROR"
  ].includes(status);
}

/**
 * Check if a status represents an intermediate/in-progress state.
 * These statuses indicate the task is still processing and should continue polling.
 * 
 * @param status - SunoTaskStatus to check
 * @returns true if the status indicates the task is still in progress
 */
function isIntermediateStatus(status: SunoTaskStatus): boolean {
  return [
    "PENDING",
    "PROCESSING",
    "TEXT_SUCCESS",
    "FIRST_SUCCESS"
  ].includes(status);
}

/**
 * Get the status of a generation task.
 * Endpoint: GET /api/v1/generate/record-info?taskId={taskId}
 * 
 * Handles all documented Suno API statuses:
 * - PENDING: Task is queued
 * - PROCESSING: Task is being processed
 * - TEXT_SUCCESS: Lyrics/text generation completed (intermediate)
 * - FIRST_SUCCESS: First audio generated (intermediate)
 * - SUCCESS: All audio generated successfully
 * - CREATE_TASK_FAILED: Task creation failed
 * - GENERATE_AUDIO_FAILED: Audio generation failed
 * - CALLBACK_EXCEPTION: Callback processing failed
 * - SENSITIVE_WORD_ERROR: Content flagged for sensitive words
 * 
 * @param taskId - Task identifier from generateMusic()
 * @returns Task result with status, tracks, and error details if failed
 */
export async function getTaskStatus(taskId: string): Promise<SunoTaskResult> {
  const data = await callSunoProxy(`generate/record-info?taskId=${taskId}`, null, "GET");

  const status = mapToSunoTaskStatus(data.status);

  // Parse tracks if available - API returns sunoData array in response
  let tracks: SunoGeneratedTrack[] | undefined;
  if (status === "SUCCESS" && data.response?.sunoData) {
    tracks = data.response.sunoData.map((track: any) => ({
      id: track.id,
      title: track.title || "Untitled",
      audio_url: track.audioUrl,
      duration: track.duration || 0,
      style: track.tags,
      lyrics: track.prompt,
    }));
  }

  // Build result with error details for failed tasks
  const result: SunoTaskResult = {
    taskId,
    status,
    tracks,
  };

  // Include error details for failed statuses
  if (isFailedStatus(status)) {
    // Extract error code from various possible locations in the response
    result.errorCode = data.errorCode
      || data.code
      || data.response?.sunoData?.[0]?.errorCode;

    // Extract error message from various possible locations in the response
    result.errorMessage = data.errorMessage
      || data.msg
      || data.response?.sunoData?.[0]?.errorMessage
      || getDefaultErrorMessage(status);
  }

  return result;
}

/**
 * Get a default error message based on the failure status.
 * 
 * @param status - The failed status
 * @returns Human-readable error message
 */
function getDefaultErrorMessage(status: SunoTaskStatus): string {
  switch (status) {
    case "CREATE_TASK_FAILED":
      return "Failed to create generation task. Please try again.";
    case "GENERATE_AUDIO_FAILED":
      return "Audio generation failed. Please try again with different parameters.";
    case "CALLBACK_EXCEPTION":
      return "Callback processing failed. The task may have completed but notification failed.";
    case "SENSITIVE_WORD_ERROR":
      return "Content flagged for sensitive words. Please modify your prompt and try again.";
    case "FAILED":
    default:
      return "Generation failed. Please try again.";
  }
}

/**
 * Get detailed status of a generation task with full API response fields.
 * Endpoint: GET /api/v1/generate/record-info?taskId={taskId}
 * 
 * Returns the complete API response including:
 * - parentMusicId: ID of the parent track (for extensions/covers)
 * - param: Original request parameters
 * - response: Full API response with sunoData array
 * - type: Task type (generate, extend, cover, etc.)
 * - errorCode: Numeric error code if failed
 * 
 * Use this function when you need access to all metadata about a task.
 * For simple status checks, use getTaskStatus() instead.
 * 
 * @param taskId - Task identifier from generateMusic() or other generation functions
 * @returns Detailed task result with all API response fields
 */
export async function getDetailedTaskStatus(taskId: string): Promise<SunoDetailedTaskResult> {
  const data = await callSunoProxy(`generate/record-info?taskId=${taskId}`, null, "GET");

  const status = mapToSunoTaskStatus(data.status);

  // Parse tracks if available - API returns sunoData array in response
  let tracks: SunoGeneratedTrack[] | undefined;
  if (status === "SUCCESS" && data.response?.sunoData) {
    tracks = data.response.sunoData.map((track: any) => ({
      id: track.id,
      title: track.title || "Untitled",
      audio_url: track.audioUrl,
      duration: track.duration || 0,
      style: track.tags,
      lyrics: track.prompt,
    }));
  }

  // Build detailed result with all API response fields
  const result: SunoDetailedTaskResult = {
    taskId,
    status,
    tracks,
    // Include parent music ID for extensions/covers
    parentMusicId: data.parentMusicId || data.response?.parentMusicId,
    // Include original request parameters
    param: data.param,
    // Include full response data with sunoData array
    response: data.response ? {
      taskId: data.response.taskId || taskId,
      sunoData: data.response.sunoData || []
    } : undefined,
    // Include task type
    type: data.type || data.taskType,
  };

  // Include error details for failed statuses
  if (isFailedStatus(status)) {
    // Extract error code from various possible locations in the response
    result.errorCode = data.errorCode
      || data.code
      || data.response?.sunoData?.[0]?.errorCode;

    // Extract error message from various possible locations in the response
    result.errorMessage = data.errorMessage
      || data.msg
      || data.response?.sunoData?.[0]?.errorMessage
      || getDefaultErrorMessage(status);
  }

  return result;
}

/**
 * Poll for task completion with timeout.
 * 
 * Handles intermediate statuses properly:
 * - PENDING, PROCESSING: Continue polling (task is queued/processing)
 * - TEXT_SUCCESS: Continue polling (lyrics generated, audio in progress)
 * - FIRST_SUCCESS: Continue polling (first track done, more may be generating)
 * - SUCCESS: Return tracks (all generation complete)
 * - Any failure status: Throw error with details
 * 
 * @param taskId - Task identifier
 * @param maxWaitMs - Maximum wait time in milliseconds (default: 10 minutes)
 * @returns Generated tracks on success
 * @throws Error on failure or timeout
 */
export async function waitForCompletion(
  taskId: string,
  maxWaitMs: number = 10 * 60 * 1000
): Promise<SunoGeneratedTrack[]> {
  const pollIntervalMs = 30 * 1000; // 30 seconds
  const startTime = Date.now();

  while (Date.now() - startTime < maxWaitMs) {
    const result = await getTaskStatus(taskId);

    // Success - return the generated tracks
    if (result.status === "SUCCESS" && result.tracks) {
      console.log("[Suno] Generation completed successfully");
      return result.tracks;
    }

    // Check for any failure status
    if (isFailedStatus(result.status)) {
      const errorDetails = result.errorCode
        ? ` (code: ${result.errorCode})`
        : '';
      throw new Error(
        result.errorMessage || `Music generation failed with status: ${result.status}${errorDetails}`
      );
    }

    // Continue polling for intermediate statuses (PENDING, PROCESSING, TEXT_SUCCESS, FIRST_SUCCESS)
    // The isIntermediateStatus helper identifies these statuses
    if (isIntermediateStatus(result.status)) {
      console.log(`[Suno] Status: ${result.status}, waiting...`);
      await new Promise(resolve => setTimeout(resolve, pollIntervalMs));
      continue;
    }

    // Unknown status - log warning and continue polling
    console.warn(`[Suno] Unknown status: ${result.status}, continuing to poll...`);
    await new Promise(resolve => setTimeout(resolve, pollIntervalMs));
  }

  throw new Error("Music generation timed out. Please try again.");
}

/**
 * Generate lyrics from a topic/prompt.
 * Endpoint: POST /api/v1/generate-lyrics
 * Returns the taskId for tracking progress.
 * 
 * @param prompt - Topic or theme for lyrics (max 200 words)
 * @returns taskId for polling status
 */
export async function generateLyrics(prompt: string): Promise<string> {
  // Full path: /api/v1/generate-lyrics
  return callSunoProxy("generate-lyrics", { prompt, callBackUrl: "playground" });
}

/**
 * Get the status of a lyrics generation task.
 * Endpoint: GET /api/v1/generate-lyrics/record-info?taskId={taskId}
 * 
 * @param taskId - Task identifier from generateLyrics()
 * @returns Lyrics result with status and text
 */
export async function getLyricsStatus(taskId: string): Promise<SunoLyricsResult> {
  const data = await callSunoProxy(`generate-lyrics/record-info?taskId=${taskId}`, null, "GET");

  const status = data.status || "PENDING";

  // API returns data array in response with text and title
  const lyricsData = data.response?.data?.[0];

  return {
    taskId,
    status,
    title: status === "SUCCESS" ? lyricsData?.title : undefined,
    text: status === "SUCCESS" ? lyricsData?.text : undefined,
    errorMessage: status === "FAILED" ? (data.errorMessage || lyricsData?.errorMessage || "Lyrics generation failed") : undefined,
  };
}

/**
 * Get timestamped lyrics for a generated track.
 * Endpoint: GET /api/v1/get-timestamped-lyrics?taskId={taskId}&audioId={audioId}
 * 
 * @param taskId - Original generation task ID
 * @param audioId - Audio ID from the generated track
 * @returns Array of SubtitleItem for timeline display
 */
export async function getTimestampedLyrics(taskId: string, audioId: string): Promise<SubtitleItem[]> {
  try {
    const data = await callSunoProxy(`get-timestamped-lyrics?taskId=${taskId}&audioId=${audioId}`, null, "GET");
    const lyricsData = data.response?.lyrics || [];
    return parseTimestampedLyrics(lyricsData);
  } catch (e) {
    console.warn("[Suno] Failed to get timestamped lyrics", e);
    return [];
  }
}

/**
 * Parse raw timestamped lyrics data into SubtitleItem array.
 * Exported for testing.
 */
export function parseTimestampedLyrics(lyricsData: Array<{ start: number; end: number; text: string }>): SubtitleItem[] {
  return lyricsData.map((item, index) => ({
    id: index + 1,
    startTime: item.start,
    endTime: item.end,
    text: item.text,
  }));
}

/**
 * Get remaining API credits.
 * Endpoint: GET /api/v1/generate/credit
 * 
 * @returns Credits balance
 */
export async function getCredits(): Promise<SunoCredits> {
  const result = await callSunoProxy("generate/credit", null, "GET");
  // API returns credits directly in data field or nested
  const credits = typeof result === "number" ? result : (result?.credits ?? 0);
  console.log("[Suno] Credits:", credits);
  return { credits };
}

/**
 * Test function to verify Suno API is working.
 * Can be called from browser console: testSunoAPI()
 */
export async function testSunoAPI(): Promise<void> {
  console.log("=== Suno API Test ===");
  console.log(`API Key configured: ${isSunoConfigured() ? "YES" : "NO"}`);

  if (!isSunoConfigured()) {
    console.error("❌ Suno API key not found. Add VITE_SUNO_API_KEY to .env.local");
    return;
  }

  try {
    console.log("Testing credits endpoint...");
    const credits = await getCredits();
    console.log(`✅ Credits check successful! Remaining: ${credits.credits}`);
  } catch (error) {
    console.error("❌ Test failed:", error);
  }
}

// ... existing code ...

/**
 * Generate a music video for a track.
 * Endpoint: POST /api/v1/create-music-video
 * 
 * @param taskId - Task ID of the generated music
 * @param audioId - Audio ID of the specific track
 * @param author - Optional artist name to display
 * @param domainName - Optional domain/brand watermark
 * @returns taskId for video generation
 */
export async function createMusicVideo(
  taskId: string,
  audioId: string,
  author?: string,
  domainName?: string
): Promise<string> {
  try {
    const requestBody: any = {
      taskId,
      audioId,
      callBackUrl: "playground"
    };
    if (author) requestBody.author = author;
    if (domainName) requestBody.domainName = domainName;

    // Full path: /api/v1/create-music-video
    return await callSunoProxy("create-music-video", requestBody);
  } catch (e) {
    console.warn("[Suno] create-music-video failed");
    throw e;
  }
}

/**
 * Generate a cover image for a track.
 * Endpoint: POST /api/v1/cover
 */
export async function generateCover(taskId: string): Promise<string> {
  // Full path: /api/v1/cover
  return callSunoProxy("cover", { taskId, callBackUrl: "playground" });
}

/**
 * Boost/Enhance a music style description.
 * Endpoint: POST /api/v1/boost-music-style
 */
export async function boostMusicStyle(style: string): Promise<string> {
  // Full path: /api/v1/boost-music-style
  const result = await callSunoProxy("boost-music-style", { content: style });
  return result?.result || result?.content || result || style;
}

// Expose to window for console testing
if (typeof window !== "undefined") {
  (window as any).testSunoAPI = testSunoAPI;
  // ... existing code ...
  (window as any).sunoVideo = createMusicVideo;
  (window as any).sunoCover = generateCover;
  (window as any).sunoExtend = extendMusic;
  (window as any).sunoUploadExtend = uploadAndExtend;
  (window as any).sunoPersona = generatePersona;
  (window as any).sunoConvertWav = convertToWav;
  (window as any).sunoSeparateVocals = separateVocals;
  (window as any).sunoStemStatus = getStemSeparationStatus;
  (window as any).sunoUploadBase64 = uploadFileBase64;
  (window as any).sunoUploadUrl = uploadFileUrl;
}

/**
 * Add vocals to an instrumental track.
 * Endpoint: POST /api/v1/add-vocals
 */
export async function addVocals(config: SunoGenerationConfig & { uploadUrl: string }): Promise<string> {
  const requestBody = {
    prompt: config.prompt,
    uploadUrl: config.uploadUrl,
    title: config.title || "",
    negativeTags: config.negativeTags || "",
    style: config.style || "",
    vocalGender: config.vocalGender,
    styleWeight: config.styleWeight ?? 0.61,
    weirdnessConstraint: config.weirdnessConstraint ?? 0.72,
    audioWeight: config.audioWeight ?? 0.65,
    model: config.model ?? "V4_5PLUS",
    callBackUrl: config.callBackUrl || "playground"
  };

  // Full path: /api/v1/add-vocals
  return callSunoProxy("add-vocals", requestBody);
}

/**
 * Add instrumental to a vocal track.
 * Endpoint: POST /api/v1/add-instrumental
 */
export async function addInstrumental(config: SunoGenerationConfig & { uploadUrl: string }): Promise<string> {
  const requestBody = {
    uploadUrl: config.uploadUrl,
    title: config.title || "",
    negativeTags: config.negativeTags || "",
    tags: config.style || "Relaxing Piano, Ambient",
    vocalGender: config.vocalGender,
    styleWeight: config.styleWeight ?? 0.61,
    weirdnessConstraint: config.weirdnessConstraint ?? 0.72,
    audioWeight: config.audioWeight ?? 0.65,
    model: config.model ?? "V4_5PLUS",
    callBackUrl: config.callBackUrl || "playground"
  };

  // Full path: /api/v1/add-instrumental
  return callSunoProxy("add-instrumental", requestBody);
}

/**
 * Upload audio and cover it with a new style.
 * Endpoint: POST /api/v1/generate/upload-cover
 */
export async function uploadAndCover(config: SunoGenerationConfig & { uploadUrl: string }): Promise<string> {
  const requestBody = {
    uploadUrl: config.uploadUrl,
    customMode: true,
    instrumental: config.instrumental ?? true,
    model: config.model ?? "V4_5ALL",
    callBackUrl: config.callBackUrl || "playground",
    prompt: config.prompt,
    style: config.style || "",
    title: config.title || "",
    negativeTags: config.negativeTags || "",
    vocalGender: config.vocalGender,
    styleWeight: config.styleWeight ?? 0.65,
    weirdnessConstraint: config.weirdnessConstraint ?? 0.65,
    audioWeight: config.audioWeight ?? 0.65
  };

  // Full path: /api/v1/generate/upload-cover
  return callSunoProxy("generate/upload-cover", requestBody);
}

/**
 * Replace a section of the music track.
 * Endpoint: POST /api/v1/replace-section
 */
export async function replaceSection(
  taskId: string,
  audioId: string,
  startTime: number,
  endTime: number,
  prompt: string,
  style?: string,
  title?: string
): Promise<string> {
  const requestBody = {
    taskId,
    audioId,
    prompt,
    tags: style || "",
    title: title || "",
    infillStartS: startTime,
    infillEndS: endTime,
    callBackUrl: "playground"
  };
  // Full path: /api/v1/replace-section
  return callSunoProxy("replace-section", requestBody);
}

/**
 * Extend an existing music track.
 * Endpoint: POST /api/v1/extend
 * 
 * Allows continuing a generated track from a specific timestamp with new lyrics/style.
 * 
 * @param config - Extension configuration including taskId, audioId, and continueAt time
 * @returns taskId for tracking the extension progress
 */
export async function extendMusic(config: SunoExtendConfig): Promise<string> {
  const requestBody = {
    taskId: config.taskId,
    audioId: config.audioId,
    prompt: config.prompt || "",
    style: config.style || "",
    title: config.title || "",
    continueAt: config.continueAt,
    model: config.model ?? "V5",
    callBackUrl: config.callBackUrl || "playground"
  };

  // Clean up undefined values
  Object.keys(requestBody).forEach(key =>
    (requestBody as any)[key] === undefined && delete (requestBody as any)[key]
  );

  // Full path: /api/v1/extend
  return callSunoProxy("extend", requestBody);
}

/**
 * Upload audio and extend it with new content.
 * Endpoint: POST /api/v1/upload-and-extend
 * 
 * Allows uploading an external audio file and extending it with AI-generated content.
 * 
 * @param config - Upload configuration including uploadUrl, prompt, style, and continueAt time
 * @returns taskId for tracking the extension progress
 */
export async function uploadAndExtend(config: SunoUploadConfig): Promise<string> {
  const requestBody = {
    uploadUrl: config.uploadUrl,
    prompt: config.prompt || "",
    style: config.style || "",
    title: config.title || "",
    continueAt: config.continueAt ?? 0,
    instrumental: config.instrumental ?? false,
    model: config.model ?? "V5",
    defaultParamFlag: config.defaultParamFlag ?? false,
    negativeTags: config.negativeTags || "",
    vocalGender: config.vocalGender,
    styleWeight: config.styleWeight ?? 0.65,
    weirdnessConstraint: config.weirdnessConstraint ?? 0.5,
    audioWeight: config.audioWeight ?? 0.65,
    callBackUrl: config.callBackUrl || "playground"
  };

  // Clean up undefined values
  Object.keys(requestBody).forEach(key =>
    (requestBody as any)[key] === undefined && delete (requestBody as any)[key]
  );

  // Full path: /api/v1/upload-and-extend
  return callSunoProxy("upload-and-extend", requestBody);
}

/**
 * Generate a personalized music style/persona.
 * Endpoint: POST /api/v1/generate-persona
 * 
 * Creates a custom persona that can be used with music generation for consistent style.
 * The returned personaId can be passed to generateMusic() for personalized output.
 * 
 * @param config - Persona configuration including name, description, and style
 * @returns taskId for tracking the persona generation progress
 */
export async function generatePersona(config: SunoPersonaConfig): Promise<string> {
  const requestBody = {
    name: config.name,
    description: config.description,
    style: config.style,
    callBackUrl: config.callBackUrl || "playground"
  };

  // Full path: /api/v1/generate-persona
  return callSunoProxy("generate-persona", requestBody);
}

/**
 * Upload an audio file for use with Cover or Extend features.
 * Uses backend proxy to bypass CORS restrictions.
 * 
 * @param file - The audio file to upload
 * @returns The URL of the uploaded file
 */
export async function uploadAudioFile(file: File): Promise<string> {
  const formData = new FormData();
  formData.append("file", file);

  // Use backend proxy to bypass CORS
  // The server forwards the request to Suno API server-side
  // @ts-ignore - Vite injects this at build time
  const serverUrl = (import.meta as any).env?.VITE_SERVER_URL || "http://localhost:3001";

  const response = await fetch(`${serverUrl}/api/suno/upload`, {
    method: "POST",
    body: formData,
  });

  const data = await response.json();

  if (!response.ok || data.code !== 200) {
    throw new Error(data.error || data.msg || data.message || "File upload failed");
  }

  // Return the accessible URL for the file
  return data.data?.fileUrl || data.data?.url || data.url;
}

/**
 * Convert a generated audio track to WAV format.
 * Endpoint: POST /api/v1/convert-to-wav
 * 
 * Converts an MP3 track to high-quality WAV format for professional use.
 * The conversion is asynchronous - poll getTaskStatus() for completion.
 * 
 * @param taskId - Task ID of the original generation
 * @param audioId - Audio ID of the specific track to convert
 * @returns taskId for tracking the conversion progress
 */
export async function convertToWav(taskId: string, audioId: string): Promise<string> {
  const requestBody = {
    taskId,
    audioId,
    callBackUrl: "playground"
  };

  // Full path: /api/v1/convert-to-wav
  return callSunoProxy("convert-to-wav", requestBody);
}

/**
 * Separate vocals from instrumental in a music track.
 * Endpoint: POST /api/v1/separate-vocals-from-music
 * 
 * Uses AI to split a track into separate vocal and instrumental stems.
 * Useful for remixing, karaoke, or isolating specific elements.
 * 
 * @param taskId - Task ID of the original generation
 * @param audioId - Audio ID of the specific track to separate
 * @returns taskId for tracking the separation progress
 */
export async function separateVocals(taskId: string, audioId: string): Promise<string> {
  const requestBody = {
    taskId,
    audioId,
    callBackUrl: "playground"
  };

  // Full path: /api/v1/separate-vocals-from-music
  return callSunoProxy("separate-vocals-from-music", requestBody);
}

/**
 * Get the status of a stem separation task.
 * Endpoint: GET /api/v1/separate-vocals-from-music/record-info?taskId={taskId}
 * 
 * Returns the separation result with URLs to the separated vocal and instrumental tracks.
 * 
 * @param taskId - Task identifier from separateVocals()
 * @returns Stem separation result with vocalsUrl and instrumentalUrl
 */
export async function getStemSeparationStatus(taskId: string): Promise<SunoStemSeparationResult> {
  const data = await callSunoProxy(`separate-vocals-from-music/record-info?taskId=${taskId}`, null, "GET");

  const status = data.status || "PENDING";

  // Parse stem URLs from response - API returns vocals and instrumental URLs in response
  let vocalsUrl: string | undefined;
  let instrumentalUrl: string | undefined;

  if (status === "SUCCESS" && data.response) {
    // The API typically returns the separated tracks in the response
    vocalsUrl = data.response.vocalsUrl || data.response.vocals_url || data.response.vocalUrl;
    instrumentalUrl = data.response.instrumentalUrl || data.response.instrumental_url || data.response.instrumentUrl;
  }

  return {
    taskId,
    status,
    vocalsUrl,
    instrumentalUrl,
    errorMessage: status === "FAILED" ? (data.errorMessage || "Stem separation failed") : undefined,
  };
}

/**
 * Wait for stem separation to complete and return the result.
 * 
 * Handles intermediate statuses properly:
 * - PENDING, PROCESSING: Continue polling (task is queued/processing)
 * - TEXT_SUCCESS, FIRST_SUCCESS: Continue polling (intermediate progress)
 * - SUCCESS: Return result with stem URLs
 * - Any failure status: Throw error with details
 * 
 * @param taskId - Task identifier from separateVocals()
 * @param maxWaitMs - Maximum wait time in milliseconds (default: 5 minutes)
 * @returns Stem separation result with both URLs
 * @throws Error on failure or timeout
 */
export async function waitForStemSeparation(
  taskId: string,
  maxWaitMs: number = 5 * 60 * 1000
): Promise<SunoStemSeparationResult> {
  const pollIntervalMs = 15 * 1000; // 15 seconds
  const startTime = Date.now();

  while (Date.now() - startTime < maxWaitMs) {
    const result = await getStemSeparationStatus(taskId);

    // Success - return the separation result
    if (result.status === "SUCCESS") {
      console.log("[Suno] Stem separation completed successfully");
      return result;
    }

    // Check for any failure status
    if (isFailedStatus(result.status)) {
      throw new Error(result.errorMessage || `Stem separation failed with status: ${result.status}`);
    }

    // Continue polling for intermediate statuses
    console.log(`[Suno] Stem separation status: ${result.status}, waiting...`);
    await new Promise(resolve => setTimeout(resolve, pollIntervalMs));
  }

  throw new Error("Stem separation timed out. Please try again.");
}

/**
 * Upload a file using Base64 encoded data.
 * Endpoint: POST /api/v1/upload/base64
 * 
 * Uploads a file to Suno's servers using Base64 encoding.
 * The returned URL can be used with upload-and-cover, upload-and-extend, 
 * add-vocals, and add-instrumental endpoints.
 * 
 * @param base64Data - Base64 encoded file data (without data URI prefix)
 * @param fileName - Name of the file including extension (e.g., "audio.mp3")
 * @returns URL of the uploaded file for use with other Suno endpoints
 * @throws SunoApiError if upload fails
 */
export async function uploadFileBase64(base64Data: string, fileName: string): Promise<string> {
  // Strip data URI prefix if present (e.g., "data:audio/mp3;base64,")
  const cleanBase64 = base64Data.includes(',')
    ? base64Data.split(',')[1]
    : base64Data;

  const requestBody = {
    base64Data: cleanBase64,
    fileName: fileName
  };

  // Call the upload endpoint via proxy
  const result = await callSunoProxy("upload/base64", requestBody);

  // Extract the file URL from the response
  const fileUrl = result?.fileUrl || result?.url || result;

  if (!fileUrl || typeof fileUrl !== 'string') {
    throw new SunoApiError('Upload failed: No file URL returned', 500, 'upload/base64');
  }

  console.log(`[Suno] File uploaded via Base64: ${fileName}`);
  return fileUrl;
}

/**
 * Upload a file from a remote URL.
 * Endpoint: POST /api/v1/upload/url
 * 
 * Uploads a file to Suno's servers by providing a source URL.
 * Suno will fetch the file from the URL and store it.
 * The returned URL can be used with upload-and-cover, upload-and-extend,
 * add-vocals, and add-instrumental endpoints.
 * 
 * @param sourceUrl - URL of the audio file to upload (must be publicly accessible)
 * @returns URL of the uploaded file for use with other Suno endpoints
 * @throws SunoApiError if upload fails
 */
export async function uploadFileUrl(sourceUrl: string): Promise<string> {
  const requestBody = {
    url: sourceUrl
  };

  // Call the upload endpoint via proxy
  const result = await callSunoProxy("upload/url", requestBody);

  // Extract the file URL from the response
  const fileUrl = result?.fileUrl || result?.url || result;

  if (!fileUrl || typeof fileUrl !== 'string') {
    throw new SunoApiError('Upload failed: No file URL returned', 500, 'upload/url');
  }

  console.log(`[Suno] File uploaded via URL: ${sourceUrl.substring(0, 50)}...`);
  return fileUrl;
}
````

## File: packages/shared/src/services/textSanitizer.ts
````typescript
/**
 * Text Sanitizer Service
 *
 * Cleans narration text of metadata artifacts before TTS and subtitle rendering.
 * Extracted from useStoryGeneration hook for testability and reuse.
 */

/**
 * Aggressive cleaning for TTS input.
 * Strips all markdown, metadata labels, screenplay directions, and formatting.
 */
export function cleanForTTS(text: string): string {
    return applyCommonPatterns(text);
}

/**
 * Cleaning for subtitle display.
 * Same as TTS cleaning plus length constraints and sentence-level pagination.
 */
export function cleanForSubtitles(
    text: string,
    maxCharsPerChunk = 80,
    minDisplayTimeSec = 1.5,
): { chunks: string[]; minDisplayTime: number } {
    const cleaned = applyCommonPatterns(text);
    if (!cleaned) return { chunks: [], minDisplayTime: minDisplayTimeSec };

    // Split into sentences — support English and Arabic punctuation
    const sentences = cleaned.match(/[^.!?،؛]+[.!?،؛]+/g) || [cleaned];
    const chunks: string[] = [];
    let current = '';

    for (const sentence of sentences) {
        const trimmed = sentence.trim();
        if (!trimmed) continue;

        if (current && (current + ' ' + trimmed).length > maxCharsPerChunk) {
            chunks.push(current);
            current = trimmed;
        } else {
            current = current ? current + ' ' + trimmed : trimmed;
        }
    }
    if (current) chunks.push(current);

    // Enforce 2-line max: if a chunk is very long, split at midpoint
    const finalChunks: string[] = [];
    for (const chunk of chunks) {
        if (chunk.length > maxCharsPerChunk * 1.5) {
            const mid = chunk.lastIndexOf(' ', Math.floor(chunk.length / 2));
            if (mid > 0) {
                finalChunks.push(chunk.substring(0, mid).trim());
                finalChunks.push(chunk.substring(mid + 1).trim());
                continue;
            }
        }
        finalChunks.push(chunk);
    }

    return { chunks: finalChunks, minDisplayTime: minDisplayTimeSec };
}

/**
 * Core regex patterns applied to both TTS and subtitle text.
 */
function applyCommonPatterns(text: string): string {
    return text
        // Markdown patterns
        .replace(/\*\*[^*]*?\*\*:?\s*/g, '')           // **Label:** bold patterns
        .replace(/\*\*/g, '')                            // Remaining ** markers
        .replace(/\*([^*]*?)\*/g, '$1')                  // *italic* → content
        .replace(/#{1,6}\s+/g, '')                       // # headings
        .replace(/`([^`]*?)`/g, '$1')                    // `code` → content

        // Scene/screenplay directions
        .replace(/\[([^\]]*?)\]:?\s*/g, '')              // [Scene Direction:] brackets
        .replace(/\((?:Note|Direction|SFX|Sound|Music|Pause)[^)]*?\)\s*/g, '') // (Note: ...) parentheticals
        .replace(/\bINT\.\s*/g, '')                      // INT. screenplay prefix
        .replace(/\bEXT\.\s*/g, '')                      // EXT. screenplay prefix
        .replace(/Scene\s*\d+\s*[:.]?\s*/gi, '')         // "Scene 3:" headers

        // Arabic-specific metadata (order matters: المشهد before مشهد to prevent partial match)
        .replace(/المشهد\s*[\d٠-٩]*\s*[:.]?\s*/g, '')    // المشهد ١:
        .replace(/مشهد\s*[٠-٩\d]+\s*[:.]?\s*/g, '')      // مشهد ٣:
        .replace(/النقطة السردية[^:]*:\s*/g, '')           // Arabic narrative labels
        .replace(/الخطاف العاطفي[^:]*:\s*/g, '')          // الخطاف العاطفي: (Emotional Hook)
        .replace(/الراوي\s*[:.]?\s*/g, '')               // الراوي: (Narrator:)
        .replace(/وصف\s*المشهد\s*[:.]?\s*/g, '')         // وصف المشهد: (Scene description:)

        // English metadata labels (safety net for labels that survive prompt fixes)
        .replace(/\bEmotional Hook\s*:\s*/gi, '')         // Emotional Hook:
        .replace(/\bNarrative Beat\s*:\s*/gi, '')         // Narrative Beat:
        .replace(/\bKey Beat\s*:\s*/gi, '')               // Key Beat:
        .replace(/\bHook\s*:\s*/gi, '')                   // Hook:
        .replace(/\bBeat\s*:\s*/gi, '')                   // Beat:

        // Structural patterns
        .replace(/^---+\s*$/gm, '')                      // Horizontal rules
        .replace(/^>\s*/gm, '')                           // Blockquotes
        .replace(/^\s*[-–—*+]\s+/gm, '')                 // Bullets/dashes
        .replace(/\s*[0-9\u0660-\u0669\u06F0-\u06F9]+\.\s*\*{0,2}\s*$/, '') // Trailing "٢. **"

        // Whitespace normalization
        .replace(/\n+/g, ' ')                            // Newlines → spaces
        .replace(/\s{2,}/g, ' ')                         // Collapse whitespace
        .trim();
}
````

## File: packages/shared/src/services/tracing/Based on the code you uploaded, I've ana.md
````markdown
Based on the code you uploaded, I've analyzed your Prompt System and found a **Critical Disconnect**.

You have built a beautiful library of style keywords in `styleEnhancements.ts` (e.g., "screentone shading" for Anime), but **the AI never sees them**. The system is currently ignoring your detailed style definitions and falling back to a hardcoded "Cinema" bias, which explains why your non-cinematic styles (like Anime or Pixel Art) might look wrong or generic.

Here is the deep-dive analysis and the fixes:

### 🛑 Critical Issues Found

| Issue | Severity | Location | Description |
| --- | --- | --- | --- |
| **The Style Disconnect** | **CRITICAL** | `promptService.ts` | The `getPromptGenerationInstruction` function accepts a `style` string but **does not** import or inject the rich keywords from `styleEnhancements.ts`. The model is just told "Style: Anime" without the specific "screentone shading" instruction. |
| **The "Arri Alexa" Bias** | **CRITICAL** | `agentDirectorService.ts` | The Director Agent has a hardcoded `PRO_STYLE_WRAPPER` that forces "Arri Alexa 65" and "35mm Film Grain" on **every** video. If you try to generate "Pixel Art" or "Vector", the AI fights itself because it's being told to make it look like a movie camera shot it. |
| **Subject Drift** | **HIGH** | `promptService.ts` | The "Global Subject" is passed as a note, but not forcibly prefixed to the prompts. This allows the AI to "forget" the subject in complex scenes (e.g., generating "a generic robot" instead of "YOUR specific robot"). |

---

### 🛠️ The Fixes

#### 1. Fix the Style Disconnect

You need to actually fetch the keywords and inject them into the system prompt.

**File:** `services/promptService.ts`
**Function:** `getPromptGenerationInstruction`

```typescript
// Import the helper (it is already imported but not used in the function)
import { getStyleEnhancement } from './prompt/styleEnhancements';

export const getPromptGenerationInstruction = (
  style: string,
  mode: "lyrics" | "story",
  content: string,
  globalSubject: string = "",
  purpose: VideoPurpose = "music_video",
) => {
  // 1. GET THE RICH KEYWORDS
  const styleData = getStyleEnhancement(style);
  const richStyleBlock = `
ART STYLE: "${style}"
VISUAL GUIDELINES (MANDATORY):
- ${styleData.keywords.join("\n- ")}
- Aesthetic Goal: ${styleData.mediumDescription}
`;

  // ... rest of the function ...

  return `You are a professional music video director...

${richStyleBlock}  <-- REPLACE 'ART STYLE: "${style}"' WITH THIS
${subjectBlock}
${purposeGuidance}
...
`;
};

```

#### 2. Remove the "Arri Alexa" Bias

The Director Agent is forcing film grain on everything. You need to make the `PRO_STYLE_WRAPPER` dynamic based on the selected style.

**File:** `services/agentDirectorService.ts`
**Function:** `generatePromptsWithAgent`

```typescript
// Import the style helper
import { getStyleEnhancement } from './prompt/styleEnhancements';

export async function generatePromptsWithAgent(...) {
    // ... setup ...

    // 1. FETCH DYNAMIC STYLE DATA
    const styleData = getStyleEnhancement(style);

    // 2. CREATE DYNAMIC WRAPPER INSTEAD OF HARDCODED "ARRI ALEXA"
    const PRO_STYLE_WRAPPER = `GLOBAL VISUAL SIGNATURE (Apply to ALL prompts):
- Aesthetic: ${styleData.mediumDescription}
- Key Elements: ${styleData.keywords.slice(0, 4).join(", ")}
- Aspect Ratio: 16:9 (unless specified otherwise)`;

    // ... continue with taskMessage ...

```

#### 3. Enforce "Subject Anchoring"

The linter checks for subject presence, but we can guarantee it during generation by forcing the AI to use a specific format.

**File:** `services/promptService.ts`
**Function:** `getPromptGenerationInstruction`

Update the `PROMPT WRITING RULES` section inside the prompt string:

```typescript
PROMPT WRITING RULES:
1. FORMAT: "[Subject Description], [Action], [Environment], [Lighting/Style]"
2. If a Global Subject is defined ("${globalSubject}"), every prompt MUST start with exactly that phrase.
   - CORRECT: "${globalSubject} standing in a neon rainstorm..."
   - INCORRECT: "A lonely figure standing..." (Ambiguous)
   - INCORRECT: "Neon rain falls on ${globalSubject}..." (Passive)

```

#### 4. Relax the Linter (Modernization)

Your linter warns if prompts are under 18 words. Modern models (Imagen 3, Flux, Midjourney v6) actually perform *better* with concise prompts.

**File:** `services/promptService.ts`
**Function:** `lintPrompt`

```typescript
// Change the word count threshold
if (words < 10) { // Reduced from 18 to 10
    issues.push({
      code: "too_short",
      message: "Prompt is very short...",
      severity: "warn",
    });
}

// Disable the "generic_conflict" warning for Action/Anime styles
// Sometimes "fighting" is exactly what you want in those genres.
const conflictPatterns = /\b(arguing|slamming|yelling|fighting...)\b/i;
if (conflictPatterns.test(norm) && style.toLowerCase() !== 'anime' && style.toLowerCase() !== 'action') {
    // Only flag conflicts if we aren't in an action genre
    issues.push({ ... });
}

```

### Summary of Impact

1. **Style Injection:** If you select "Anime", the model will now actually receive keywords like *"cel-shaded, speed lines, studio ghibli aesthetic"* instead of just the word "Anime".
2. **No More Grainy Pixel Art:** Removing the hardcoded "Arri Alexa/Film Grain" wrapper ensures digital styles look crisp and clean.
3. **Subject Consistency:** Forcing the prompt to *start* with the Global Subject name prevents the AI from drifting into generic "man/woman/robot" descriptions.
````

## File: packages/shared/src/services/tracing/index.ts
````typescript
/**
 * Tracing module barrel export
 */

export {
    createTraceable,
    traceAsync,
    traceSync,
    traceLLM,
    traceChain,
    traceTool,
    isTracingEnabled,
    getTracingProject,
    startTrace,
} from "./langsmithTracing";
````

## File: packages/shared/src/services/tracing/langsmithTracing.ts
````typescript
/**
 * LangSmith Tracing Service (Browser-Compatible)
 * 
 * Provides tracing utilities for AI pipeline functions.
 * Uses LangChain's callback system for LangSmith integration (browser-safe).
 * 
 * For browser environments, we use manual trace logging via the LangSmith REST API.
 * For server environments (Node.js), the full traceable wrapper is available.
 * 
 * Setup:
 * 1. Set VITE_LANGSMITH_API_KEY in .env.local
 * 2. Set VITE_LANGSMITH_PROJECT (optional, defaults to "lyriclens")
 * 3. Traces appear at: https://smith.langchain.com
 */

// --- Configuration ---

// Access Vite env vars (types are defined in vite-env.d.ts or env.d.ts)
const LANGSMITH_API_KEY = (import.meta as any).env?.VITE_LANGSMITH_API_KEY as string | undefined;
const LANGSMITH_PROJECT = ((import.meta as any).env?.VITE_LANGSMITH_PROJECT as string) || "lyriclens";
const LANGSMITH_ENDPOINT = "https://api.smith.langchain.com";
const TRACING_ENABLED = !!LANGSMITH_API_KEY && (import.meta as any).env?.VITE_LANGSMITH_TRACING !== "false";

if (TRACING_ENABLED && typeof window !== "undefined") {
    console.log(`[Tracing] LangSmith tracing enabled for project: ${LANGSMITH_PROJECT}`);
}

// --- Types ---

interface TraceRun {
    id: string;
    name: string;
    run_type: "llm" | "chain" | "tool" | "retriever";
    start_time: string;
    end_time?: string;
    inputs: Record<string, unknown>;
    outputs?: Record<string, unknown>;
    error?: string;
    extra?: Record<string, unknown>;
    tags?: string[];
    parent_run_id?: string;
}

// --- Utility Functions ---

function generateRunId(): string {
    return crypto.randomUUID();
}

function getCurrentTimestamp(): string {
    return new Date().toISOString();
}

// --- LangSmith REST API Client ---

async function postRun(run: TraceRun): Promise<void> {
    if (!TRACING_ENABLED) return;

    try {
        const response = await fetch(`${LANGSMITH_ENDPOINT}/runs`, {
            method: "POST",
            headers: {
                "Content-Type": "application/json",
                "x-api-key": LANGSMITH_API_KEY!,
            },
            body: JSON.stringify({
                ...run,
                session_name: LANGSMITH_PROJECT,
            }),
        });

        if (!response.ok) {
            console.warn(`[Tracing] Failed to post run: ${response.status}`);
        }
    } catch (error) {
        // Don't let tracing errors break the app
        console.warn("[Tracing] Error posting run:", error);
    }
}

async function patchRun(runId: string, updates: Partial<TraceRun>): Promise<void> {
    if (!TRACING_ENABLED) return;

    try {
        const response = await fetch(`${LANGSMITH_ENDPOINT}/runs/${runId}`, {
            method: "PATCH",
            headers: {
                "Content-Type": "application/json",
                "x-api-key": LANGSMITH_API_KEY!,
            },
            body: JSON.stringify(updates),
        });

        if (!response.ok) {
            console.warn(`[Tracing] Failed to patch run: ${response.status}`);
        }
    } catch (error) {
        console.warn("[Tracing] Error patching run:", error);
    }
}

// --- Traceable Wrapper Factory ---

interface TraceOptions {
    name: string;
    runType?: "llm" | "chain" | "tool" | "retriever";
    metadata?: Record<string, unknown>;
    tags?: string[];
}

/**
 * Wrap an async function with LangSmith tracing.
 * Traces are sent to LangSmith via REST API (browser-compatible).
 * 
 * @param fn - Function to wrap
 * @param name - Name for the trace
 * @param options - Additional tracing options
 * @returns Traced function (or original if tracing disabled)
 */
export function traceAsync<TArgs extends any[], TReturn>(
    fn: (...args: TArgs) => Promise<TReturn>,
    name: string,
    options?: Omit<TraceOptions, "name">
): (...args: TArgs) => Promise<TReturn> {
    if (!TRACING_ENABLED) {
        return fn;
    }

    return async (...args: TArgs): Promise<TReturn> => {
        const runId = generateRunId();
        const startTime = getCurrentTimestamp();

        // Create initial run
        const run: TraceRun = {
            id: runId,
            name,
            run_type: options?.runType || "chain",
            start_time: startTime,
            inputs: { args: args.length === 1 ? args[0] : args },
            extra: options?.metadata,
            tags: options?.tags,
        };

        // Post the run start (fire and forget)
        postRun(run);

        try {
            const result = await fn(...args);

            // Update with success
            patchRun(runId, {
                end_time: getCurrentTimestamp(),
                outputs: { result: result as unknown },
            });

            return result;
        } catch (error) {
            // Update with error
            patchRun(runId, {
                end_time: getCurrentTimestamp(),
                error: error instanceof Error ? error.message : String(error),
            });

            throw error;
        }
    };
}

/**
 * Wrap a sync function with tracing.
 * Note: Tracing is async but won't block the sync function.
 */
export function traceSync<TArgs extends any[], TReturn>(
    fn: (...args: TArgs) => TReturn,
    name: string,
    options?: Omit<TraceOptions, "name">
): (...args: TArgs) => TReturn {
    if (!TRACING_ENABLED) {
        return fn;
    }

    return (...args: TArgs): TReturn => {
        const runId = generateRunId();
        const startTime = getCurrentTimestamp();

        // Create initial run
        const run: TraceRun = {
            id: runId,
            name,
            run_type: options?.runType || "chain",
            start_time: startTime,
            inputs: { args: args.length === 1 ? args[0] : args },
            extra: options?.metadata,
            tags: options?.tags,
        };

        // Post the run start (fire and forget)
        postRun(run);

        try {
            const result = fn(...args);

            // Update with success (fire and forget)
            patchRun(runId, {
                end_time: getCurrentTimestamp(),
                outputs: { result: result as unknown },
            });

            return result;
        } catch (error) {
            // Update with error (fire and forget)
            patchRun(runId, {
                end_time: getCurrentTimestamp(),
                error: error instanceof Error ? error.message : String(error),
            });

            throw error;
        }
    };
}

/**
 * Create a traceable version of a function.
 * Automatically detects async vs sync functions.
 */
export function createTraceable<T extends (...args: any[]) => any>(
    fn: T,
    options: TraceOptions
): T {
    if (!TRACING_ENABLED) {
        return fn;
    }

    // Check if function returns a promise (async)
    const isAsync = fn.constructor.name === "AsyncFunction";

    if (isAsync) {
        return traceAsync(fn as any, options.name, options) as T;
    } else {
        return traceSync(fn as any, options.name, options) as T;
    }
}

// --- Pre-built Traceable Decorators ---

/**
 * Decorator for LLM calls (Gemini, etc.)
 */
export const traceLLM = (name: string, metadata?: Record<string, unknown>) =>
    <T extends (...args: any[]) => any>(fn: T): T =>
        createTraceable(fn, { name, runType: "llm", metadata, tags: ["llm", "gemini"] });

/**
 * Decorator for chain/pipeline operations
 */
export const traceChain = (name: string, metadata?: Record<string, unknown>) =>
    <T extends (...args: any[]) => any>(fn: T): T =>
        createTraceable(fn, { name, runType: "chain", metadata, tags: ["chain"] });

/**
 * Decorator for tool operations (image gen, TTS, etc.)
 */
export const traceTool = (name: string, metadata?: Record<string, unknown>) =>
    <T extends (...args: any[]) => any>(fn: T): T =>
        createTraceable(fn, { name, runType: "tool", metadata, tags: ["tool"] });

// --- Status ---

export function isTracingEnabled(): boolean {
    return TRACING_ENABLED;
}

export function getTracingProject(): string {
    return LANGSMITH_PROJECT;
}

// --- Manual Trace API ---

/**
 * Start a manual trace span. Returns a function to end the span.
 * Useful for tracing code blocks that aren't easily wrapped.
 * 
 * @example
 * const endTrace = startTrace("myOperation", { runType: "tool" });
 * try {
 *     // ... do work ...
 *     endTrace({ result: "success" });
 * } catch (error) {
 *     endTrace(undefined, error);
 * }
 */
export function startTrace(
    name: string,
    options?: Omit<TraceOptions, "name">
): (outputs?: Record<string, unknown>, error?: Error) => void {
    if (!TRACING_ENABLED) {
        return () => {}; // No-op
    }

    const runId = generateRunId();
    const startTime = getCurrentTimestamp();

    const run: TraceRun = {
        id: runId,
        name,
        run_type: options?.runType || "chain",
        start_time: startTime,
        inputs: {},
        extra: options?.metadata,
        tags: options?.tags,
    };

    postRun(run);

    return (outputs?: Record<string, unknown>, error?: Error) => {
        patchRun(runId, {
            end_time: getCurrentTimestamp(),
            outputs,
            error: error?.message,
        });
    };
}
````

## File: packages/shared/src/services/transcriptionService.ts
````typescript
/**
 * Transcription Service
 * Handles audio transcription functionality using Gemini AI.
 */

import { Type } from "@google/genai";
import { SubtitleItem, WordTiming } from "../types";
import { ai, MODELS, withRetry } from "./shared/apiClient";

// --- Interfaces ---

interface TranscriptionLine {
  id: number;
  startTime: number;
  endTime: number;
  text: string;
  words: { word: string; start: number; end: number }[];
}

interface TranscriptionResponse {
  lines: TranscriptionLine[];
}

// --- Helper Functions ---

/**
 * Infer audio MIME type from filename extension when File.type is empty.
 */
export function inferAudioMimeType(filename: string, fileType?: string): string {
  if (fileType) return fileType;
  const ext = filename.split('.').pop()?.toLowerCase();
  const mimeMap: Record<string, string> = {
    mp3: 'audio/mpeg',
    wav: 'audio/wav',
    ogg: 'audio/ogg',
    flac: 'audio/flac',
    m4a: 'audio/mp4',
    aac: 'audio/aac',
    webm: 'audio/webm',
    wma: 'audio/x-ms-wma',
    opus: 'audio/opus',
  };
  return (ext && mimeMap[ext]) || 'audio/mpeg'; // Default to mp3
}

/**
 * Convert a File to base64 string for use with Gemini API.
 */
export const fileToGenerativePart = async (file: File): Promise<string> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => {
      const base64String = reader.result as string;
      if (!base64String) {
        reject(new Error("Failed to read file"));
        return;
      }
      const parts = base64String.split(",");
      const base64Data = parts.length > 1 ? parts[1] : base64String;
      if (!base64Data) {
        reject(new Error("Empty data in file"));
        return;
      }
      resolve(base64Data);
    };
    reader.onerror = reject;
    reader.readAsDataURL(file);
  });
};

// --- Main Services ---

/**
 * Transcribe audio to SRT format.
 */
export const transcribeAudio = async (
  base64Audio: string,
  mimeType: string,
): Promise<string> => {
  return withRetry(async () => {
    const response = await ai.models.generateContent({
      model: MODELS.TRANSCRIPTION,
      contents: {
        parts: [
          { inlineData: { mimeType, data: base64Audio } },
          {
            text: `Transcribe ALL spoken content of this audio to SRT format.

CRITICAL: You MUST transcribe EVERY SINGLE WORD from start to finish. Do NOT skip any sections or repeated phrases.

Rules:
1. Format: ID [newline] HH:MM:SS,mmm --> HH:MM:SS,mmm [newline] Text
2. No markdown blocks. Return ONLY raw SRT text.
3. Transcribe EVERYTHING - all speech, narration, lyrics, dialogue.
4. Cover the ENTIRE audio duration.
5. Do NOT summarize or skip any sections.`,
          },
        ],
      },
    });

    const text = response.text;
    if (!text) throw new Error("No transcription generated");
    return text
      .replace(/^```srt\s*/i, "")
      .replace(/^```\s*/i, "")
      .replace(/```$/, "");
  });
};

/**
 * Transcribe audio with word-level timing.
 * Falls back to line-level SRT transcription if word-level fails.
 */
export const transcribeAudioWithWordTiming = async (
  base64Audio: string,
  mimeType: string,
): Promise<SubtitleItem[]> => {
  return withRetry(async () => {
    try {
      const response = await ai.models.generateContent({
        model: MODELS.TRANSCRIPTION,
        contents: {
          parts: [
            { inlineData: { mimeType, data: base64Audio } },
            {
              text: `Transcribe ALL spoken content of this audio file with precise word-level timing.

CRITICAL: You MUST transcribe EVERY SINGLE WORD from start to finish. Do NOT skip any sections, pauses, or repeated phrases.

Return a JSON object with this structure:
{
  "lines": [
    {
      "id": 1, "startTime": 0.0, "endTime": 3.5, "text": "Example spoken text here",
      "words": [ {"word": "Example", "start": 0.0, "end": 0.8}, ... ]
    }
  ]
}

Rules:
1. Times in SECONDS (e.g. 1.5).
2. Each line is a natural phrase (typically 3-8 words).
3. word.start/end must be precise timestamps.
4. TRANSCRIBE EVERYTHING - all speech, narration, lyrics, dialogue.
5. If there are silent/instrumental sections, the next line should have the correct start time after the break.
6. Do NOT summarize or skip any sections - transcribe them ALL.
7. Cover the ENTIRE audio duration from 0 seconds to the end.`,
            },
          ],
        },
        config: {
          responseMimeType: "application/json",
          responseSchema: {
            type: Type.OBJECT,
            properties: {
              lines: {
                type: Type.ARRAY,
                items: {
                  type: Type.OBJECT,
                  properties: {
                    id: { type: Type.INTEGER },
                    startTime: { type: Type.NUMBER },
                    endTime: { type: Type.NUMBER },
                    text: { type: Type.STRING },
                    words: {
                      type: Type.ARRAY,
                      items: {
                        type: Type.OBJECT,
                        properties: {
                          word: { type: Type.STRING },
                          start: { type: Type.NUMBER },
                          end: { type: Type.NUMBER },
                        },
                        required: ["word", "start", "end"],
                      },
                    },
                  },
                  required: ["id", "startTime", "endTime", "text", "words"],
                },
              },
            },
          },
        },
      });

      const jsonStr = response.text;
      if (!jsonStr) throw new Error("No transcription generated");

      const parsed: TranscriptionResponse = JSON.parse(jsonStr);

      return parsed.lines.map(
        (line: TranscriptionLine): SubtitleItem => ({
          id: line.id || 0,
          startTime: line.startTime || 0,
          endTime: line.endTime || 0,
          text: line.text || "",
          words: (line.words || []).map(
            (w: { word: string; start: number; end: number }): WordTiming => ({
              word: w.word || "",
              startTime: w.start || 0,
              endTime: w.end || 0,
            }),
          ),
        }),
      );
    } catch (error) {
      console.error("Word-level transcription error:", error);
      console.warn("Falling back to line-level SRT transcription...");
      const srt = await transcribeAudio(base64Audio, mimeType);
      const { parseSRT } = await import("../utils/srtParser");
      return parseSRT(srt);
    }
  });
};
````

## File: packages/shared/src/services/translationService.ts
````typescript
/**
 * Translation Service
 * Handles subtitle translation functionality using Gemini AI.
 */

import { Type } from "@google/genai";
import { SubtitleItem } from "../types";
import { ai, MODELS, withRetry } from "./shared/apiClient";

// --- Interfaces ---

interface TranslationItem {
  id: number;
  translation: string;
}

// --- Main Services ---

/**
 * Translate subtitles to a target language.
 * Processes subtitles in batches for efficiency.
 * @param subtitles - Array of subtitle items to translate
 * @param targetLanguage - Target language for translation
 */
export const translateSubtitles = async (
  subtitles: SubtitleItem[],
  targetLanguage: string,
): Promise<{ id: number; translation: string }[]> => {
  const BATCH_SIZE = 50;
  const simplifiedSubs = subtitles.map((s) => ({ id: s.id, text: s.text }));
  const chunks = [];

  for (let i = 0; i < simplifiedSubs.length; i += BATCH_SIZE) {
    chunks.push(simplifiedSubs.slice(i, i + BATCH_SIZE));
  }

  console.log(
    `Translating ${subtitles.length} lines in ${chunks.length} batches...`,
  );

  const processBatch = async (batch: typeof simplifiedSubs) => {
    return withRetry(async () => {
      const response = await ai.models.generateContent({
        model: MODELS.TRANSLATION,
        contents: `Translate these lyrics into ${targetLanguage}.
        Return JSON object with "translations" array [{ id, translation }].
        Keep poetic flow.

        Input:
        ${JSON.stringify(batch)}`,
        config: {
          responseMimeType: "application/json",
          responseSchema: {
            type: Type.OBJECT,
            properties: {
              translations: {
                type: Type.ARRAY,
                items: {
                  type: Type.OBJECT,
                  properties: {
                    id: { type: Type.INTEGER },
                    translation: { type: Type.STRING },
                  },
                  required: ["id", "translation"],
                },
              },
            },
          },
        },
      });

      const jsonStr = response.text;
      if (!jsonStr) throw new Error("No translation generated for batch");

      const parsed = JSON.parse(jsonStr) as { translations: TranslationItem[] };
      return parsed.translations;
    });
  };

  try {
    const results = await Promise.all(
      chunks.map((chunk) => processBatch(chunk)),
    );
    return results.flat().sort((a, b) => a.id - b.id);
  } catch (error) {
    console.error("Translation error:", error);
    throw error;
  }
};
````

## File: packages/shared/src/services/tripletUtils.ts
````typescript
/**
 * Triplet Utilities
 *
 * Bridge between the new InstructionTriplet system and the legacy EmotionalTone.
 * All downstream consumers that need EmotionalTone should use getEffectiveLegacyTone()
 * instead of accessing scene.emotionalTone directly.
 *
 * React-free — safe for Node.js usage.
 */

import type { EmotionalTone, InstructionTriplet, Scene } from "../types";

/**
 * Maps primary emotions (vibe terms) to the nearest legacy EmotionalTone.
 * Used when downstream systems (voice selection, SFX matching) still need
 * the 5-value enum.
 */
export const EMOTION_TO_LEGACY: Record<string, EmotionalTone> = {
  // dramatic bucket
  "visceral-dread": "dramatic",
  "melancholy": "dramatic",
  "seething-rage": "dramatic",
  "bittersweet-longing": "dramatic",
  "euphoric-wonder": "dramatic",
  "tension-drone": "dramatic",
  "heartbeat-pulse": "dramatic",
  "digital-distortion": "dramatic",
  "holographic-decay": "dramatic",

  // calm bucket
  "stoic-resignation": "calm",
  "cold-detachment": "calm",
  "nostalgic-warmth": "calm",
  "desert-silence": "calm",
  "ethereal-echo": "calm",
  "cathedral-reverb": "calm",
  "golden-hour-decay": "calm",
  "twilight-liminal": "calm",
  "whisper-static": "calm",
  "nordic-frost": "calm",

  // urgent bucket
  "quantum-shimmer": "urgent",
  "neural-lace": "urgent",
  "neon-rain": "urgent",
  "tokyo-neon-night": "urgent",

  // friendly bucket
  "liquid-glass": "friendly",
  "middle-eastern-dusk": "friendly",

  // professional bucket
  "server-room-hum": "professional",
  "midnight-blue": "professional",
};

/**
 * Default InstructionTriplets for each legacy EmotionalTone.
 * Used when upgrading old scenes that only have emotionalTone.
 */
export const DEFAULT_TRIPLETS: Record<EmotionalTone, InstructionTriplet> = {
  professional: {
    primaryEmotion: "cold-detachment",
    cinematicDirection: "static",
    environmentalAtmosphere: "server-room-hum",
  },
  dramatic: {
    primaryEmotion: "visceral-dread",
    cinematicDirection: "slow-push-in",
    environmentalAtmosphere: "tension-drone",
  },
  friendly: {
    primaryEmotion: "nostalgic-warmth",
    cinematicDirection: "handheld-float",
    environmentalAtmosphere: "golden-hour-decay",
  },
  urgent: {
    primaryEmotion: "seething-rage",
    cinematicDirection: "tracking-shot",
    environmentalAtmosphere: "neon-rain",
  },
  calm: {
    primaryEmotion: "stoic-resignation",
    cinematicDirection: "static",
    environmentalAtmosphere: "desert-silence",
  },
};

/**
 * Map an InstructionTriplet's primaryEmotion to the nearest EmotionalTone.
 * Falls back to "dramatic" if no mapping exists.
 */
export function tripletToLegacyTone(triplet: InstructionTriplet): EmotionalTone {
  return EMOTION_TO_LEGACY[triplet.primaryEmotion] ?? "dramatic";
}

/**
 * Upgrade a legacy EmotionalTone to a default InstructionTriplet.
 */
export function legacyToneToTriplet(tone: EmotionalTone): InstructionTriplet {
  return DEFAULT_TRIPLETS[tone] ?? DEFAULT_TRIPLETS.dramatic;
}

/**
 * Get the effective InstructionTriplet for a scene.
 * Prefers the new instructionTriplet field; falls back to upgrading emotionalTone.
 */
export function getEffectiveTriplet(scene: Scene): InstructionTriplet {
  if (scene.instructionTriplet) {
    return scene.instructionTriplet;
  }
  return legacyToneToTriplet(scene.emotionalTone ?? "dramatic");
}

/**
 * Get the effective EmotionalTone for a scene.
 * Prefers deriving from instructionTriplet; falls back to legacy field.
 * This is what downstream consumers (voice map, SFX, subtitles) should call
 * instead of accessing scene.emotionalTone directly.
 */
export function getEffectiveLegacyTone(scene: Scene): EmotionalTone {
  if (scene.instructionTriplet) {
    return tripletToLegacyTone(scene.instructionTriplet);
  }
  return scene.emotionalTone ?? "dramatic";
}
````

## File: packages/shared/src/services/tts/deliveryMarkers.ts
````typescript
/**
 * TTS Delivery Markers
 *
 * Converts inline narration markers like [pause: long] and [emphasis]
 * into Gemini-compatible Director's Note instructions while stripping
 * them from the spoken text.
 *
 * Supported markers:
 * - [pause: short|medium|long|beat] — Insert a pause
 * - [emphasis]...[/emphasis]         — Stress/emphasize enclosed text
 * - [low-tone]...[/low-tone]         — Speak in a lower register
 * - [whisper]...[/whisper]            — Whisper enclosed text
 * - [rising-tension]...[/rising-tension] — Gradually increase intensity
 * - [slow]...[/slow]                 — Slow down delivery
 * - [breath]                          — Take a natural breath
 *
 * React-free — safe for Node.js usage.
 */

export interface DeliveryMarkerResult {
  /** Director's Note instructions extracted from markers */
  directorInstructions: string;
  /** Clean text with all markers stripped */
  cleanText: string;
}

interface MarkerMatch {
  instruction: string;
  originalText: string;
  cleanReplacement: string;
}

/**
 * Pause marker: [pause: short|medium|long|beat]
 */
function processPauseMarkers(text: string): MarkerMatch[] {
  const matches: MarkerMatch[] = [];
  const pauseRegex = /\[pause:\s*(short|medium|long|beat)\]/gi;
  let match;

  while ((match = pauseRegex.exec(text)) !== null) {
    const duration = match[1]!.toLowerCase();
    const durationMap: Record<string, string> = {
      short: "Take a brief half-second pause",
      medium: "Pause for one second, letting the moment settle",
      long: "Take a long two-second dramatic pause",
      beat: "Hold for a beat, as if considering what to say next",
    };

    matches.push({
      instruction: durationMap[duration] ?? "Pause briefly",
      originalText: match[0],
      cleanReplacement: " ",
    });
  }

  return matches;
}

/**
 * Emphasis marker: [emphasis]...[/emphasis]
 */
function processEmphasisMarkers(text: string): MarkerMatch[] {
  const matches: MarkerMatch[] = [];
  const emphasisRegex = /\[emphasis\](.*?)\[\/emphasis\]/gi;
  let match;

  while ((match = emphasisRegex.exec(text)) !== null) {
    const content = match[1] ?? "";
    matches.push({
      instruction: `Emphasize and stress the words "${content}" with extra vocal weight`,
      originalText: match[0],
      cleanReplacement: content,
    });
  }

  return matches;
}

/**
 * Low-tone marker: [low-tone]...[/low-tone]
 */
function processLowToneMarkers(text: string): MarkerMatch[] {
  const matches: MarkerMatch[] = [];
  const lowToneRegex = /\[low-tone\](.*?)\[\/low-tone\]/gi;
  let match;

  while ((match = lowToneRegex.exec(text)) !== null) {
    const content = match[1] ?? "";
    matches.push({
      instruction: `Drop to a lower, deeper register for "${content}"`,
      originalText: match[0],
      cleanReplacement: content,
    });
  }

  return matches;
}

/**
 * Whisper marker: [whisper]...[/whisper]
 */
function processWhisperMarkers(text: string): MarkerMatch[] {
  const matches: MarkerMatch[] = [];
  const whisperRegex = /\[whisper\](.*?)\[\/whisper\]/gi;
  let match;

  while ((match = whisperRegex.exec(text)) !== null) {
    const content = match[1] ?? "";
    matches.push({
      instruction: `Whisper the words "${content}" in a hushed, intimate tone`,
      originalText: match[0],
      cleanReplacement: content,
    });
  }

  return matches;
}

/**
 * Rising-tension marker: [rising-tension]...[/rising-tension]
 */
function processRisingTensionMarkers(text: string): MarkerMatch[] {
  const matches: MarkerMatch[] = [];
  const risingRegex = /\[rising-tension\](.*?)\[\/rising-tension\]/gi;
  let match;

  while ((match = risingRegex.exec(text)) !== null) {
    const content = match[1] ?? "";
    matches.push({
      instruction: `Gradually increase intensity and urgency through "${content}"`,
      originalText: match[0],
      cleanReplacement: content,
    });
  }

  return matches;
}

/**
 * Slow marker: [slow]...[/slow]
 */
function processSlowMarkers(text: string): MarkerMatch[] {
  const matches: MarkerMatch[] = [];
  const slowRegex = /\[slow\](.*?)\[\/slow\]/gi;
  let match;

  while ((match = slowRegex.exec(text)) !== null) {
    const content = match[1] ?? "";
    matches.push({
      instruction: `Slow down and deliver "${content}" with deliberate pacing`,
      originalText: match[0],
      cleanReplacement: content,
    });
  }

  return matches;
}

/**
 * Breath marker: [breath]
 */
function processBreathMarkers(text: string): MarkerMatch[] {
  const matches: MarkerMatch[] = [];
  const breathRegex = /\[breath\]/gi;
  let match;

  while ((match = breathRegex.exec(text)) !== null) {
    matches.push({
      instruction: "Take a natural audible breath",
      originalText: match[0],
      cleanReplacement: " ",
    });
  }

  return matches;
}

/**
 * Convert all delivery markers in a script into Director's Note instructions
 * and return the cleaned text.
 */
export function convertMarkersToDirectorNote(script: string): DeliveryMarkerResult {
  if (!script) {
    return { directorInstructions: "", cleanText: "" };
  }

  // Collect all marker matches
  const allMatches: MarkerMatch[] = [
    ...processPauseMarkers(script),
    ...processEmphasisMarkers(script),
    ...processLowToneMarkers(script),
    ...processWhisperMarkers(script),
    ...processRisingTensionMarkers(script),
    ...processSlowMarkers(script),
    ...processBreathMarkers(script),
  ];

  if (allMatches.length === 0) {
    return { directorInstructions: "", cleanText: script };
  }

  // Build director instructions
  const instructions = allMatches.map(m => m.instruction);
  const directorInstructions = instructions.join(". ") + ".";

  // Strip markers from text
  let cleanText = script;
  for (const match of allMatches) {
    cleanText = cleanText.replace(match.originalText, match.cleanReplacement);
  }

  // Clean up extra whitespace
  cleanText = cleanText.replace(/\s{2,}/g, " ").trim();

  return { directorInstructions, cleanText };
}
````

## File: packages/shared/src/services/utils/index.ts
````typescript
/**
 * Service Utilities Index
 * 
 * Centralized exports for shared utility functions.
 */

export * from './textProcessing';
````

## File: packages/shared/src/services/utils/textProcessing.ts
````typescript
/**
 * Text Processing Utilities
 * 
 * Consolidated text processing functions to eliminate duplication across services.
 * Previously duplicated in:
 * - services/researchService.ts
 * - services/promptService.ts
 * - services/documentParser.ts
 * 
 * @module services/utils/textProcessing
 */

/**
 * Normalizes text for similarity comparison.
 * Converts to lowercase, removes punctuation, and normalizes whitespace.
 * 
 * @param s - The input string to normalize
 * @returns Normalized string for comparison
 * 
 * @example
 * normalizeForSimilarity("Hello, World!") // "hello world"
 */
export function normalizeForSimilarity(s: string): string {
    return s
        .toLowerCase()
        .replace(/[^\w\s]/g, "")
        .replace(/\s+/g, " ")
        .trim();
}

/**
 * Counts the number of words in a string.
 * 
 * @param s - The input string
 * @returns Number of words
 * 
 * @example
 * countWords("Hello world") // 2
 * countWords("") // 0
 */
export function countWords(s: string): number {
    const t = s.trim();
    if (!t) return 0;
    return t.split(/\s+/).filter(Boolean).length;
}

/**
 * Tokenizes text into a set of lowercase words.
 * Useful for set-based similarity calculations.
 * 
 * @param text - The input text to tokenize
 * @param minTokenLength - Minimum token length to include (default: 1)
 * @returns Set of unique tokens (lowercase words)
 * 
 * @example
 * tokenize("Hello World Hello") // Set { "hello", "world" }
 * tokenize("I am a cat", 3) // Set { "cat" } - filters out short tokens
 */
export function tokenize(text: string, minTokenLength: number = 1): Set<string> {
    return new Set(
        text
            .toLowerCase()
            .replace(/[^\w\s\u0600-\u06FF]/g, " ") // Preserve Arabic characters
            .split(/\s+/)
            .filter((w) => w.length >= minTokenLength)
    );
}

/**
 * Calculates Jaccard similarity between two sets or strings.
 * 
 * For sets: |A ∩ B| / |A ∪ B|
 * For strings: Converts to token sets first
 * 
 * @param a - First set or string
 * @param b - Second set or string
 * @returns Similarity score between 0 and 1
 * 
 * @example
 * // With sets
 * jaccardSimilarity(new Set(['a', 'b']), new Set(['b', 'c'])) // 0.333...
 * 
 * // With strings
 * jaccardSimilarity("hello world", "hello there") // 0.333...
 */
export function jaccardSimilarity(
    a: Set<string> | string,
    b: Set<string> | string
): number {
    // Convert strings to sets if needed
    const setA = typeof a === "string" ? tokenize(a) : a;
    const setB = typeof b === "string" ? tokenize(b) : b;

    if (setA.size === 0 && setB.size === 0) return 1;
    if (setA.size === 0 || setB.size === 0) return 0;

    const intersection = new Set([...setA].filter((x) => setB.has(x)));
    const union = new Set([...setA, ...setB]);

    return intersection.size / union.size;
}

/**
 * Splits content into chunks of approximately equal size.
 * Respects word boundaries when possible.
 * 
 * @param content - The content to chunk
 * @param chunkSize - Target size for each chunk in characters (default: 500)
 * @returns Array of content chunks
 * 
 * @example
 * chunkContent("Hello world this is a test", 10)
 * // ["Hello world", "this is a", "test"]
 */
export function chunkContent(content: string, chunkSize: number = 500): string[] {
    if (!content.trim()) return [];

    const chunks: string[] = [];
    const words = content.split(/\s+/);
    let currentChunk = "";

    for (const word of words) {
        // If adding this word would exceed chunk size and we have content, start new chunk
        if (currentChunk.length + word.length + 1 > chunkSize && currentChunk.length > 0) {
            chunks.push(currentChunk.trim());
            currentChunk = word;
        } else {
            currentChunk = currentChunk ? `${currentChunk} ${word}` : word;
        }
    }

    // Don't forget the last chunk
    if (currentChunk.trim()) {
        chunks.push(currentChunk.trim());
    }

    return chunks;
}

/**
 * Calculates the edit distance (Levenshtein distance) between two strings.
 * Useful for fuzzy matching and spell checking.
 * 
 * @param a - First string
 * @param b - Second string
 * @returns Number of edits needed to transform a into b
 * 
 * @example
 * editDistance("kitten", "sitting") // 3
 */
export function editDistance(a: string, b: string): number {
    const m = a.length;
    const n = b.length;

    // Use a 1D array for space optimization (O(n) instead of O(m*n))
    const prev: number[] = new Array(n + 1).fill(0) as number[];
    const curr: number[] = new Array(n + 1).fill(0) as number[];

    // Initialize first row
    for (let j = 0; j <= n; j++) prev[j] = j;

    for (let i = 1; i <= m; i++) {
        curr[0] = i;
        for (let j = 1; j <= n; j++) {
            if (a[i - 1] === b[j - 1]) {
                curr[j] = prev[j - 1]!;
            } else {
                curr[j] = 1 + Math.min(prev[j]!, curr[j - 1]!, prev[j - 1]!);
            }
        }
        // Copy current row to previous for next iteration
        for (let j = 0; j <= n; j++) prev[j] = curr[j]!;
    }

    return curr[n]!;
}

/**
 * Calculates a normalized similarity score based on edit distance.
 * Returns a value between 0 (completely different) and 1 (identical).
 * 
 * @param a - First string
 * @param b - Second string
 * @returns Normalized similarity score
 * 
 * @example
 * editSimilarity("hello", "hallo") // 0.8
 */
export function editSimilarity(a: string, b: string): number {
    if (a === b) return 1;
    const maxLen = Math.max(a.length, b.length);
    if (maxLen === 0) return 1;
    return 1 - editDistance(a, b) / maxLen;
}

/**
 * Truncates text to a maximum length, adding ellipsis if truncated.
 * Respects word boundaries when possible.
 * 
 * @param text - Text to truncate
 * @param maxLength - Maximum length including ellipsis
 * @param ellipsis - String to append when truncated (default: "...")
 * @returns Truncated text
 * 
 * @example
 * truncateText("Hello world this is a test", 15) // "Hello world..."
 */
export function truncateText(
    text: string,
    maxLength: number,
    ellipsis: string = "..."
): string {
    if (text.length <= maxLength) return text;

    const targetLength = maxLength - ellipsis.length;
    if (targetLength <= 0) return ellipsis.slice(0, maxLength);

    // Try to break at word boundary
    const breakPoint = text.lastIndexOf(" ", targetLength);
    if (breakPoint > 0 && breakPoint > targetLength - 20) {
        return text.slice(0, breakPoint).trim() + ellipsis;
    }

    return text.slice(0, targetLength).trim() + ellipsis;
}

/**
 * Removes extra whitespace and normalizes line endings.
 * 
 * @param text - Text to clean
 * @returns Cleaned text
 */
export function normalizeWhitespace(text: string): string {
    return text
        .replace(/\r\n/g, "\n")
        .replace(/\r/g, "\n")
        .replace(/[ \t]+/g, " ")
        .replace(/\n{3,}/g, "\n\n")
        .trim();
}

/**
 * Extracts the first N sentences from text.
 * 
 * @param text - Source text
 * @param count - Number of sentences to extract
 * @returns First N sentences
 */
export function extractSentences(text: string, count: number): string[] {
    const sentences = text.match(/[^.!?]+[.!?]+/g) || [];
    return sentences.slice(0, count).map((s) => s.trim());
}
````

## File: packages/shared/src/services/versionHistoryService.ts
````typescript
/**
 * Version History Service
 * 
 * Provides persistent version history with named snapshots, auto-save checkpoints,
 * and the ability to browse/restore previous versions of story projects.
 */

import { openDB, IDBPDatabase } from 'idb';
import type { StoryState } from '@/types';

export interface VersionSnapshot {
  id: string;
  projectId: string;
  name: string;
  description?: string;
  timestamp: number;
  state: StoryState;
  type: 'manual' | 'auto' | 'checkpoint';
  metadata?: {
    step?: string;
    shotCount?: number;
    sceneCount?: number;
    characterCount?: number;
  };
}

export interface VersionHistoryStats {
  totalSnapshots: number;
  manualSnapshots: number;
  autoSnapshots: number;
  oldestSnapshot?: number;
  newestSnapshot?: number;
  totalSizeBytes: number;
}

const DB_NAME = 'aisoul-version-history';
const DB_VERSION = 1;
const STORE_NAME = 'snapshots';
const MAX_AUTO_SNAPSHOTS = 50;
const AUTO_SAVE_INTERVAL_MS = 60000; // 1 minute

let db: IDBPDatabase | null = null;

async function getDB(): Promise<IDBPDatabase> {
  if (db) return db;
  
  db = await openDB(DB_NAME, DB_VERSION, {
    upgrade(database) {
      if (!database.objectStoreNames.contains(STORE_NAME)) {
        const store = database.createObjectStore(STORE_NAME, { keyPath: 'id' });
        store.createIndex('projectId', 'projectId', { unique: false });
        store.createIndex('timestamp', 'timestamp', { unique: false });
        store.createIndex('type', 'type', { unique: false });
        store.createIndex('projectId_timestamp', ['projectId', 'timestamp'], { unique: false });
      }
    },
  });
  
  return db;
}

function generateSnapshotId(): string {
  return `snap_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;
}

function extractMetadata(state: StoryState): VersionSnapshot['metadata'] {
  return {
    step: state.currentStep,
    shotCount: state.shots?.length ?? 0,
    sceneCount: state.breakdown?.length ?? 0,
    characterCount: state.characters?.length ?? 0,
  };
}

export async function createSnapshot(
  projectId: string,
  state: StoryState,
  name: string,
  description?: string,
  type: VersionSnapshot['type'] = 'manual'
): Promise<VersionSnapshot> {
  const database = await getDB();
  
  const snapshot: VersionSnapshot = {
    id: generateSnapshotId(),
    projectId,
    name,
    description,
    timestamp: Date.now(),
    state: JSON.parse(JSON.stringify(state)), // Deep clone
    type,
    metadata: extractMetadata(state),
  };
  
  await database.put(STORE_NAME, snapshot);
  
  // Clean up old auto snapshots if over limit
  if (type === 'auto') {
    await cleanupOldAutoSnapshots(projectId);
  }
  
  return snapshot;
}

export async function createAutoSnapshot(
  projectId: string,
  state: StoryState
): Promise<VersionSnapshot> {
  const stepName = state.currentStep || 'unknown';
  const name = `Auto-save at ${stepName} step`;
  return createSnapshot(projectId, state, name, undefined, 'auto');
}

export async function createCheckpoint(
  projectId: string,
  state: StoryState,
  checkpointName: string
): Promise<VersionSnapshot> {
  const description = `Checkpoint: ${checkpointName}`;
  return createSnapshot(projectId, state, checkpointName, description, 'checkpoint');
}

async function cleanupOldAutoSnapshots(projectId: string): Promise<void> {
  const database = await getDB();
  const tx = database.transaction(STORE_NAME, 'readwrite');
  const store = tx.objectStore(STORE_NAME);
  const index = store.index('projectId_timestamp');
  
  const autoSnapshots: VersionSnapshot[] = [];
  let cursor = await index.openCursor(IDBKeyRange.bound([projectId, 0], [projectId, Infinity]));
  
  while (cursor) {
    if (cursor.value.type === 'auto') {
      autoSnapshots.push(cursor.value);
    }
    cursor = await cursor.continue();
  }
  
  // Sort by timestamp descending
  autoSnapshots.sort((a, b) => b.timestamp - a.timestamp);
  
  // Delete excess auto snapshots
  if (autoSnapshots.length > MAX_AUTO_SNAPSHOTS) {
    const toDelete = autoSnapshots.slice(MAX_AUTO_SNAPSHOTS);
    for (const snap of toDelete) {
      await database.delete(STORE_NAME, snap.id);
    }
  }
  
  await tx.done;
}

export async function getSnapshots(
  projectId: string,
  options?: {
    type?: VersionSnapshot['type'];
    limit?: number;
    offset?: number;
  }
): Promise<VersionSnapshot[]> {
  const database = await getDB();
  const index = database.transaction(STORE_NAME).objectStore(STORE_NAME).index('projectId_timestamp');
  
  const snapshots: VersionSnapshot[] = [];
  let cursor = await index.openCursor(
    IDBKeyRange.bound([projectId, 0], [projectId, Infinity]),
    'prev' // Newest first
  );
  
  let count = 0;
  const offset = options?.offset ?? 0;
  const limit = options?.limit ?? 100;
  
  while (cursor) {
    const snapshot = cursor.value as VersionSnapshot;
    
    // Filter by type if specified
    if (!options?.type || snapshot.type === options.type) {
      if (count >= offset && count < offset + limit) {
        snapshots.push(snapshot);
      }
      count++;
    }
    
    if (snapshots.length >= limit) break;
    cursor = await cursor.continue();
  }
  
  return snapshots;
}

export async function getSnapshot(snapshotId: string): Promise<VersionSnapshot | undefined> {
  const database = await getDB();
  return database.get(STORE_NAME, snapshotId);
}

export async function deleteSnapshot(snapshotId: string): Promise<void> {
  const database = await getDB();
  await database.delete(STORE_NAME, snapshotId);
}

export async function deleteAllSnapshots(projectId: string): Promise<number> {
  const database = await getDB();
  const snapshots = await getSnapshots(projectId, { limit: 1000 });
  
  const tx = database.transaction(STORE_NAME, 'readwrite');
  for (const snap of snapshots) {
    await tx.objectStore(STORE_NAME).delete(snap.id);
  }
  await tx.done;
  
  return snapshots.length;
}

export async function renameSnapshot(
  snapshotId: string,
  newName: string,
  newDescription?: string
): Promise<VersionSnapshot | undefined> {
  const database = await getDB();
  const snapshot = await database.get(STORE_NAME, snapshotId);
  
  if (!snapshot) return undefined;
  
  snapshot.name = newName;
  if (newDescription !== undefined) {
    snapshot.description = newDescription;
  }
  
  await database.put(STORE_NAME, snapshot);
  return snapshot;
}

export async function getHistoryStats(projectId: string): Promise<VersionHistoryStats> {
  const snapshots = await getSnapshots(projectId, { limit: 1000 });
  
  const stats: VersionHistoryStats = {
    totalSnapshots: snapshots.length,
    manualSnapshots: snapshots.filter(s => s.type === 'manual').length,
    autoSnapshots: snapshots.filter(s => s.type === 'auto').length,
    totalSizeBytes: 0,
  };
  
  if (snapshots.length > 0) {
    stats.newestSnapshot = Math.max(...snapshots.map(s => s.timestamp));
    stats.oldestSnapshot = Math.min(...snapshots.map(s => s.timestamp));
    
    // Estimate size
    stats.totalSizeBytes = snapshots.reduce((acc, s) => {
      return acc + JSON.stringify(s.state).length * 2; // Rough byte estimate
    }, 0);
  }
  
  return stats;
}

export async function compareSnapshots(
  snapshotId1: string,
  snapshotId2: string
): Promise<{
  added: string[];
  removed: string[];
  modified: string[];
} | null> {
  const snap1 = await getSnapshot(snapshotId1);
  const snap2 = await getSnapshot(snapshotId2);
  
  if (!snap1 || !snap2) return null;
  
  const changes = {
    added: [] as string[],
    removed: [] as string[],
    modified: [] as string[],
  };
  
  // Compare scenes
  const scenes1 = new Set(snap1.state.breakdown?.map(s => s.sceneNumber) ?? []);
  const scenes2 = new Set(snap2.state.breakdown?.map(s => s.sceneNumber) ?? []);
  
  for (const scene of scenes2) {
    if (!scenes1.has(scene)) changes.added.push(`Scene ${scene}`);
  }
  for (const scene of scenes1) {
    if (!scenes2.has(scene)) changes.removed.push(`Scene ${scene}`);
  }
  
  // Compare characters
  const chars1 = new Set(snap1.state.characters?.map(c => c.name) ?? []);
  const chars2 = new Set(snap2.state.characters?.map(c => c.name) ?? []);
  
  for (const char of chars2) {
    if (!chars1.has(char)) changes.added.push(`Character: ${char}`);
  }
  for (const char of chars1) {
    if (!chars2.has(char)) changes.removed.push(`Character: ${char}`);
  }
  
  // Compare shots
  const shots1 = snap1.state.shots?.length ?? 0;
  const shots2 = snap2.state.shots?.length ?? 0;
  
  if (shots2 > shots1) {
    changes.added.push(`${shots2 - shots1} new shot(s)`);
  } else if (shots1 > shots2) {
    changes.removed.push(`${shots1 - shots2} shot(s)`);
  }
  
  // Compare visual style
  if (snap1.state.visualStyle !== snap2.state.visualStyle) {
    changes.modified.push(`Visual style: ${snap1.state.visualStyle} → ${snap2.state.visualStyle}`);
  }
  
  return changes;
}

export class AutoSaveManager {
  private projectId: string;
  private intervalId: ReturnType<typeof setInterval> | null = null;
  private lastState: string = '';
  private getState: () => StoryState;
  
  constructor(projectId: string, getState: () => StoryState) {
    this.projectId = projectId;
    this.getState = getState;
  }
  
  start(): void {
    if (this.intervalId) return;
    
    this.intervalId = setInterval(async () => {
      const state = this.getState();
      const stateHash = JSON.stringify(state);
      
      // Only save if state changed
      if (stateHash !== this.lastState) {
        await createAutoSnapshot(this.projectId, state);
        this.lastState = stateHash;
      }
    }, AUTO_SAVE_INTERVAL_MS);
  }
  
  stop(): void {
    if (this.intervalId) {
      clearInterval(this.intervalId);
      this.intervalId = null;
    }
  }
  
  async saveNow(): Promise<VersionSnapshot> {
    const state = this.getState();
    this.lastState = JSON.stringify(state);
    return createAutoSnapshot(this.projectId, state);
  }
}

export function formatSnapshotDate(timestamp: number): string {
  const date = new Date(timestamp);
  const now = new Date();
  const diffMs = now.getTime() - timestamp;
  const diffMins = Math.floor(diffMs / 60000);
  const diffHours = Math.floor(diffMs / 3600000);
  const diffDays = Math.floor(diffMs / 86400000);
  
  if (diffMins < 1) return 'Just now';
  if (diffMins < 60) return `${diffMins} minute${diffMins > 1 ? 's' : ''} ago`;
  if (diffHours < 24) return `${diffHours} hour${diffHours > 1 ? 's' : ''} ago`;
  if (diffDays < 7) return `${diffDays} day${diffDays > 1 ? 's' : ''} ago`;
  
  return date.toLocaleDateString(undefined, {
    month: 'short',
    day: 'numeric',
    year: date.getFullYear() !== now.getFullYear() ? 'numeric' : undefined,
    hour: '2-digit',
    minute: '2-digit',
  });
}
````

## File: packages/shared/src/services/videoService.ts
````typescript
/**
 * Video Service
 * Handles video generation functionality using Gemini Veo AI.
 *
 * Features:
 * - Veo 3.1 video generation (Fast and Standard modes)
 * - Professional AI-powered cinematographer prompt generation
 * - Automatic prompt enhancement for cinematic quality
 */

import { GoogleGenAI } from "@google/genai";
import { VIDEO_STYLE_MODIFIERS } from "../constants";
import { generateProfessionalVideoPrompt } from "./promptService";
import { cloudAutosave } from "./cloudStorageService";
import { logAICall } from "./aiLogService";
import { withRetry } from "./shared/apiClient";

// Initialize the AI client
const API_KEY = process.env.VITE_GEMINI_API_KEY || process.env.GEMINI_API_KEY || "";

if (!API_KEY) {
  console.warn("[Video Service] No API key found. Video generation will fail.");
}

const ai = new GoogleGenAI({ apiKey: API_KEY });

// Veo 3.1 Model Names
const MODELS = {
  VIDEO_STANDARD: "veo-3.1-generate-preview",
  VIDEO_FAST: "veo-3.1-fast-generate-preview",
} as const;

/**
 * Poll a Veo video generation operation until complete.
 */
async function pollVideoOperation(
  operation: any,
  maxAttempts: number = 60,
  delayMs: number = 15000
): Promise<any> {
  let currentOp = operation;

  for (let i = 0; i < maxAttempts; i++) {
    // Check if operation is already done
    if (currentOp.done) {
      if (currentOp.error) {
        throw new Error(
          `Video generation failed: ${currentOp.error.message || JSON.stringify(currentOp.error)}`
        );
      }
      return currentOp;
    }

    console.log(
      `[Video Service] Video generation in progress... (attempt ${i + 1}/${maxAttempts})`
    );
    await new Promise((resolve) => setTimeout(resolve, delayMs));

    // FIXED: Use getVideosOperation instead of get
    try {
      currentOp = await ai.operations.getVideosOperation({ operation: currentOp });
    } catch (err: any) {
      // Fallback: try the generic get method if getVideosOperation isn't available
      console.warn("[Video Service] getVideosOperation failed, trying fallback...");
      currentOp = await ai.operations.get(currentOp);
    }
  }

  throw new Error(
    `Video generation timed out after ${(maxAttempts * delayMs) / 1000} seconds`
  );
}

/**
 * Extract a thumbnail from a video URL and save to cloud storage.
 * Uses video element + canvas to capture first frame.
 * Only runs in browser environment.
 */
async function extractVideoThumbnail(
  videoUrl: string,
  sessionId: string,
  sceneIndex: number = 0
): Promise<string | null> {
  // Only run in browser (canvas API not available in Node.js)
  if (typeof window === 'undefined' || typeof document === 'undefined') {
    console.log('[Video Service] Thumbnail extraction skipped (server-side)');
    return null;
  }

  try {
    console.log('[Video Service] Extracting thumbnail from video...');

    return new Promise((resolve, reject) => {
      const video = document.createElement('video');
      video.crossOrigin = 'anonymous';
      video.preload = 'metadata';
      video.muted = true;

      // Timeout after 30 seconds
      const timeout = setTimeout(() => {
        video.remove();
        console.warn('[Video Service] Thumbnail extraction timed out');
        resolve(null);
      }, 30000);

      video.onloadeddata = async () => {
        try {
          // Seek to first frame
          video.currentTime = 0;
        } catch (err) {
          clearTimeout(timeout);
          video.remove();
          resolve(null);
        }
      };

      video.onseeked = async () => {
        try {
          // Create canvas and draw frame
          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth || 1280;
          canvas.height = video.videoHeight || 720;
          const ctx = canvas.getContext('2d');

          if (!ctx) {
            throw new Error('Failed to get canvas context');
          }

          ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

          // Convert to blob
          canvas.toBlob(async (blob) => {
            clearTimeout(timeout);
            video.remove();

            if (!blob) {
              console.warn('[Video Service] Failed to create thumbnail blob');
              resolve(null);
              return;
            }

            // Save to cloud storage
            const filename = `scene_${sceneIndex}.png`;
            await cloudAutosave.saveAsset(sessionId, blob, filename, 'visuals');
            console.log(`[Video Service] ✓ Thumbnail saved: ${filename}`);

            // Return as data URL for immediate use
            const reader = new FileReader();
            reader.onloadend = () => resolve(reader.result as string);
            reader.onerror = () => resolve(null);
            reader.readAsDataURL(blob);
          }, 'image/png', 0.9);
        } catch (err) {
          clearTimeout(timeout);
          video.remove();
          console.warn('[Video Service] Thumbnail extraction failed:', err);
          resolve(null);
        }
      };

      video.onerror = () => {
        clearTimeout(timeout);
        video.remove();
        console.warn('[Video Service] Video load failed for thumbnail');
        resolve(null);
      };

      video.src = videoUrl;
      video.load();
    });
  } catch (err) {
    console.warn('[Video Service] Thumbnail extraction error:', err);
    return null;
  }
}

/**
 * Generate a video from a prompt using Veo 3.1.
 * @param promptText - The prompt describing the video to generate
 * @param style - Art style preset (default: "Cinematic")
 * @param globalSubject - Subject to keep consistent across scenes
 * @param aspectRatio - Video aspect ratio (default: "16:9")
 * @param durationSeconds - Video duration: 4, 6, or 8 seconds (default: 8)
 * @param useFastModel - Use Veo 3.1 Fast (40% faster, lower cost) vs Standard (highest quality)
 * @param outputGcsUri - Optional GCS URI for output
 * @param sessionId - Optional session ID for cloud autosave
 * @param sceneIndex - Optional scene index for cloud autosave filename
 */
export const generateVideoFromPrompt = async (
  promptText: string,
  style: string = "Cinematic",
  globalSubject: string = "",
  aspectRatio: "16:9" | "9:16" = "16:9",
  durationSeconds: 4 | 6 | 8 = 8,
  useFastModel: boolean = true,
  outputGcsUri?: string,
  sessionId?: string,
  sceneIndex?: number
): Promise<string> => {
  if (!API_KEY) {
    throw new Error(
      "Google API key not configured. Video generation requires a valid API key.\n" +
      "Set VITE_GEMINI_API_KEY in your .env file."
    );
  }

  return withRetry(async () => {
    // Get style modifier or use default
    const modifier = VIDEO_STYLE_MODIFIERS?.[style] || "Cinematic film look with professional color grading";

    const subjectBlock = globalSubject
      ? `Global Subject (keep consistent): ${globalSubject}`
      : "";

    const finalPrompt = `
${modifier}. ${promptText}${subjectBlock ? `. ${subjectBlock}` : ""}
Smooth camera motion. Focus on clean visuals without any text elements.
    `.trim();

    // Validate duration (Veo 3.1 supports 4, 6, or 8 seconds)
    const validDurations = [4, 6, 8] as const;
    if (!validDurations.includes(durationSeconds as any)) {
      console.warn(
        `[Video Service] Invalid duration ${durationSeconds}s. Defaulting to 8s. Valid: 4, 6, or 8 seconds.`
      );
      durationSeconds = 8;
    }

    // Select model
    const modelToUse = useFastModel ? MODELS.VIDEO_FAST : MODELS.VIDEO_STANDARD;

    console.log(`[Video Service] Using ${modelToUse} for ${durationSeconds}s video generation`);
    console.log(`[Video Service] Prompt: ${finalPrompt.substring(0, 100)}...`);

    // Build config object
    const config: {
      numberOfVideos?: number;
      aspectRatio?: string;
      outputGcsUri?: string;
    } = {
      numberOfVideos: 1,
      aspectRatio: aspectRatio,
    };

    // Add outputGcsUri if provided
    if (outputGcsUri) {
      config.outputGcsUri = outputGcsUri;
      console.log(`[Video Service] Output will be saved to: ${outputGcsUri}`);
    }

    // Generate video
    const videoGenStart = Date.now();
    let operation;
    try {
      // @ts-ignore - generateVideos types may be incomplete
      operation = await ai.models.generateVideos({
        model: modelToUse,
        prompt: finalPrompt,
        config: config,
      });

      console.log(
        `[Video Service] Video generation started. Operation:`,
        operation.name || "started"
      );
    } catch (err: any) {
      // Provide helpful error messages for common issues
      if (err.status === 404 || err.message?.includes("NOT_FOUND")) {
        throw new Error(
          `Veo 3.1 model not available. Ensure you have:\n` +
          `1. A paid Gemini API plan (Veo requires paid tier)\n` +
          `2. Accepted Veo terms of service in AI Studio\n` +
          `3. Valid model: ${modelToUse}\n\n` +
          `Original error: ${err.message}`
        );
      }
      if (err.status === 403 || err.message?.includes("PERMISSION_DENIED")) {
        throw new Error(
          `Permission denied for Veo 3.1. Ensure your API key has video generation access.\n` +
          `Original error: ${err.message}`
        );
      }
      if (err.status === 429 || err.message?.includes("RATE_LIMIT")) {
        throw new Error(
          `Rate limit exceeded. Please wait before generating another video.\n` +
          `Original error: ${err.message}`
        );
      }
      throw err;
    }

    // Poll until complete
    let completedOp;
    try {
      completedOp = await pollVideoOperation(operation);
    } catch (pollErr) {
      // Log failed video generation
      if (sessionId) {
        logAICall({
          sessionId,
          step: 'animation',
          model: modelToUse,
          input: finalPrompt,
          output: '',
          durationMs: Date.now() - videoGenStart,
          status: 'error',
          error: pollErr instanceof Error ? pollErr.message : String(pollErr),
          metadata: { aspectRatio, durationSeconds },
        });
      }
      throw pollErr;
    }

    // Log successful video generation
    if (sessionId) {
      logAICall({
        sessionId,
        step: 'animation',
        model: modelToUse,
        input: finalPrompt,
        output: `video generated (${aspectRatio}, ${durationSeconds}s)`,
        durationMs: Date.now() - videoGenStart,
        status: 'success',
        metadata: { aspectRatio, durationSeconds },
      });
    }

    console.log(`[Video Service] Video generation complete!`);

    const response = completedOp.response;

    if (!response) {
      throw new Error("No response received from video generation");
    }

    console.log(`[Video Service] Response keys:`, Object.keys(response));

    // Extract video from response - handle multiple formats
    let videoObj: any = null;

    // Format 1: { generatedVideos: [{ video: {...} }] }
    if (response.generatedVideos?.length > 0) {
      console.log(`[Video Service] Found generatedVideos array format`);
      videoObj = response.generatedVideos[0].video || response.generatedVideos[0];
    }
    // Format 2: { video: {...} }
    else if (response.video) {
      console.log(`[Video Service] Found direct video object format`);
      videoObj = response.video;
    }
    // Format 3: REST API format
    else if (response.generateVideoResponse?.generatedSamples?.length > 0) {
      console.log(`[Video Service] Found generateVideoResponse format`);
      videoObj = response.generateVideoResponse.generatedSamples[0].video;
    }

    if (!videoObj) {
      console.error(
        `[Video Service] Unexpected response structure:`,
        JSON.stringify(response, null, 2)
      );
      throw new Error("No video found in response. Check logs for response structure.");
    }

    console.log(`[Video Service] Video object keys:`, Object.keys(videoObj));

    // Helper to trigger cloud autosave and thumbnail extraction
    const triggerAutosave = (videoUrl: string) => {
      if (sessionId && videoUrl) {
        // Save the video clip
        cloudAutosave.saveVideoClip(sessionId, videoUrl, sceneIndex ?? Date.now()).catch(err => {
          console.warn('[Video Service] Cloud autosave failed (non-fatal):', err);
        });

        // Extract and save thumbnail as fallback (fire-and-forget)
        extractVideoThumbnail(videoUrl, sessionId, sceneIndex ?? 0).catch(err => {
          console.warn('[Video Service] Thumbnail extraction failed (non-fatal):', err);
        });
      }
    };

    // Return URI if available
    if (videoObj.uri) {
      console.log(`[Video Service] ✓ Video URI found: ${videoObj.uri}`);
      // Append API key for authenticated download
      const videoUri = videoObj.uri.includes("?")
        ? `${videoObj.uri}&key=${API_KEY}`
        : `${videoObj.uri}?key=${API_KEY}`;

      // Cloud autosave trigger (fire-and-forget, non-blocking)
      triggerAutosave(videoUri);

      return videoUri;
    }

    // Return data URL if inline data
    const videoData =
      videoObj.data || videoObj.videoData || videoObj.bytesBase64Encoded;
    const mimeType = videoObj.mimeType || "video/mp4";

    if (videoData) {
      console.log(`[Video Service] ✓ Video data found inline (${mimeType})`);
      const sizeKB = ((videoData.length * 0.75) / 1024).toFixed(2);
      console.log(`[Video Service] Video size: ~${sizeKB} KB`);
      const dataUrl = `data:${mimeType};base64,${videoData}`;

      // Cloud autosave trigger (fire-and-forget, non-blocking)
      triggerAutosave(dataUrl);

      return dataUrl;
    }

    console.error(
      `[Video Service] Video object has no data:`,
      JSON.stringify(videoObj, null, 2)
    );
    throw new Error("No video URI or inline data found.");
  });
};

/**
 * Generate a professional cinematic video using AI-powered prompt enhancement.
 */
export const generateProfessionalVideo = async (
  sceneDescription: string,
  style: string = "Cinematic",
  mood: string = "dramatic",
  globalSubject: string = "",
  videoPurpose: string = "documentary",
  aspectRatio: "16:9" | "9:16" = "16:9",
  durationSeconds: 4 | 6 | 8 = 6,
  useFastModel: boolean = true,
  outputGcsUri?: string,
  sessionId?: string,
  sceneIndex?: number
): Promise<string> => {
  console.log(`[Video Service] Generating professional video prompt...`);
  console.log(`[Video Service] Style: ${style}, Mood: ${mood}, Duration: ${durationSeconds}s`);

  // Step 1: Generate professional prompt
  const professionalPrompt = await generateProfessionalVideoPrompt(
    sceneDescription,
    style,
    mood,
    globalSubject,
    videoPurpose,
    durationSeconds
  );

  console.log(
    `[Video Service] Professional prompt: "${professionalPrompt.substring(0, 150)}..."`
  );

  // Step 2: Generate video
  return generateVideoFromPrompt(
    professionalPrompt,
    style,
    globalSubject,
    aspectRatio,
    durationSeconds,
    useFastModel,
    outputGcsUri,
    sessionId,
    sceneIndex
  );
};

/**
 * Fetch a video URL and convert to a blob URL for local caching.
 * This prevents expired URL issues during re-export by caching the video data locally.
 * @param videoUrl - The video URL to fetch (can be temporary signed URL)
 * @returns A blob URL that persists in memory until the page is closed
 */
export const fetchAndCacheAsBlob = async (videoUrl: string): Promise<string> => {
  // Skip if already a blob URL or data URL
  if (videoUrl.startsWith('blob:') || videoUrl.startsWith('data:')) {
    console.log('[Video Service] Already cached as blob/data URL');
    return videoUrl;
  }

  try {
    console.log('[Video Service] Caching video as blob URL...');
    const response = await fetch(videoUrl);

    if (!response.ok) {
      throw new Error(`Failed to fetch video: ${response.status} ${response.statusText}`);
    }

    const blob = await response.blob();
    const blobUrl = URL.createObjectURL(blob);

    const sizeMB = (blob.size / (1024 * 1024)).toFixed(2);
    console.log(`[Video Service] ✓ Video cached as blob URL (${sizeMB} MB)`);

    return blobUrl;
  } catch (error) {
    console.warn('[Video Service] Failed to cache video as blob:', error);
    // Return original URL as fallback (may work if not expired yet)
    return videoUrl;
  }
};

/**
 * Generate video with automatic prompt enhancement.
 */
export const generateVideoWithEnhancement = async (
  promptText: string,
  style: string = "Cinematic",
  globalSubject: string = "",
  aspectRatio: "16:9" | "9:16" = "16:9",
  durationSeconds: 4 | 6 | 8 = 6,
  useFastModel: boolean = true,
  enhancePrompt: boolean | "auto" = "auto",
  mood: string = "dramatic",
  videoPurpose: string = "documentary"
): Promise<string> => {
  // Determine if prompt needs enhancement
  const promptWords = promptText.trim().split(/\s+/).length;
  const hasCamera = /camera|dolly|tracking|pan|zoom|steadicam|crane/i.test(promptText);
  const hasLighting = /lighting|lit|glow|backlight|silhouette|golden hour/i.test(promptText);
  const hasTechnical = /35mm|anamorphic|shallow depth|bokeh|lens/i.test(promptText);

  const isAlreadyProfessional =
    promptWords > 80 && (hasCamera || hasLighting || hasTechnical);

  const shouldEnhance =
    enhancePrompt === true || (enhancePrompt === "auto" && !isAlreadyProfessional);

  if (shouldEnhance) {
    console.log(
      `[Video Service] Enhancing prompt (${promptWords} words, professional: ${isAlreadyProfessional})`
    );
    return generateProfessionalVideo(
      promptText,
      style,
      mood,
      globalSubject,
      videoPurpose,
      aspectRatio,
      durationSeconds,
      useFastModel
    );
  }

  console.log(`[Video Service] Using prompt directly (already professional)`);
  return generateVideoFromPrompt(
    promptText,
    style,
    globalSubject,
    aspectRatio,
    durationSeconds,
    useFastModel
  );
};
````

## File: packages/shared/src/services/visualConsistencyService.ts
````typescript
/**
 * Visual Consistency Service
 * 
 * Extracts visual style elements from the first generated scene and applies
 * them to subsequent scene prompts for visual cohesion across the video.
 * 
 * Features:
 * - Color palette extraction using AI vision
 * - Style keyword injection
 * - Consistent lighting and mood descriptors
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HumanMessage } from "@langchain/core/messages";
import { API_KEY, MODELS } from "./shared/apiClient";
import type { CharacterProfile } from "../types";

/**
 * Result of a character consistency check
 */
export interface ConsistencyReport {
    /** Consistency score from 0 to 100 */
    score: number;
    /** Whether the character is considered consistent (score > 75) */
    isConsistent: boolean;
    /** List of specific consistency issues found (e.g., "Hair color changed from blonde to brown") */
    issues: string[];
    /** Suggestions to fix the issues in subsequent prompts */
    suggestions: string[];
    /** Detailed textual analysis from the AI */
    details: string;
}

/**
 * Extracted visual style from a reference image
 */
export interface VisualStyle {
    /** Dominant colors (e.g., "teal", "warm orange", "deep blue") */
    colorPalette: string[];
    /** Lighting description (e.g., "golden hour", "soft diffused", "harsh contrast") */
    lighting: string;
    /** Texture/film style (e.g., "film grain", "clean digital", "vintage fade") */
    texture: string;
    /** Mood keywords (e.g., "mysterious", "serene", "intense") */
    moodKeywords: string[];
    /** Raw style prompt to append to other prompts */
    stylePrompt: string;
}

// Cache for visual style per session
const styleCache = new Map<string, VisualStyle>();

/**
 * Extract visual style from a reference image using Gemini Vision
 * 
 * @param imageUrl - URL or base64 of the reference image
 * @param sessionId - Optional session ID for caching
 * @returns Extracted visual style
 */
export async function extractVisualStyle(
    imageUrl: string,
    sessionId?: string
): Promise<VisualStyle> {
    // Check cache first
    if (sessionId && styleCache.has(sessionId)) {
        console.log(`[VisualConsistency] Using cached style for session ${sessionId}`);
        return styleCache.get(sessionId)!;
    }

    console.log(`[VisualConsistency] Extracting visual style from reference image`);

    if (!API_KEY) {
        console.warn(`[VisualConsistency] No API key, using default style`);
        return getDefaultStyle();
    }

    try {
        const model = new ChatGoogleGenerativeAI({
            apiKey: API_KEY,
            model: MODELS.TEXT,
            temperature: 0.3,
        });

        // Convert URL to base64 if needed (Gemini requires base64 data URLs)
        let base64Image: string;

        if (imageUrl.startsWith('data:image')) {
            // Already base64
            base64Image = imageUrl;
        } else if (imageUrl.startsWith('http')) {
            // Fetch and convert to base64
            try {
                console.log(`[VisualConsistency] Fetching image from URL for analysis...`);
                const response = await fetch(imageUrl);
                if (!response.ok) {
                    throw new Error(`Failed to fetch image: ${response.status}`);
                }
                const blob = await response.blob();
                const arrayBuffer = await blob.arrayBuffer();
                const base64 = btoa(
                    new Uint8Array(arrayBuffer).reduce((data, byte) => data + String.fromCharCode(byte), '')
                );
                const mimeType = blob.type || 'image/png';
                base64Image = `data:${mimeType};base64,${base64}`;
                console.log(`[VisualConsistency] Converted image to base64 (${(base64.length / 1024).toFixed(1)} KB)`);
            } catch (fetchError) {
                console.warn(`[VisualConsistency] Failed to fetch image, using default style:`, fetchError);
                return getDefaultStyle();
            }
        } else {
            // Assume it's already base64 without data prefix
            base64Image = `data:image/png;base64,${imageUrl}`;
        }

        const response = await model.invoke([
            new HumanMessage({
                content: [
                    {
                        type: "image_url" as const,
                        image_url: base64Image
                    },
                    {
                        type: "text",
                        text: `Analyze this image and extract its visual style for consistency in subsequent images.

Return a JSON object with these exact fields:
{
  "colorPalette": ["color1", "color2", "color3"], // 3-5 dominant colors as descriptive names
  "lighting": "description of lighting style",
  "texture": "film grain/clean digital/vintage/etc",
  "moodKeywords": ["mood1", "mood2"], // 2-3 mood descriptors
  "stylePrompt": "A complete style description that can be appended to image prompts"
}

Focus on:
- Specific color names (not "blue" but "deep navy" or "teal")
- Lighting quality (soft, harsh, directional, ambient)
- Overall mood and atmosphere
- Film/photo style (cinematic, documentary, artistic)

Return ONLY the JSON object, no markdown.`
                    }
                ],
            }),
        ]);

        const content = typeof response.content === 'string'
            ? response.content
            : JSON.stringify(response.content);

        // Parse JSON from response
        const jsonMatch = content.match(/\{[\s\S]*\}/);
        if (!jsonMatch) {
            console.warn(`[VisualConsistency] Failed to parse style, using default`);
            return getDefaultStyle();
        }

        const parsed = JSON.parse(jsonMatch[0]) as VisualStyle;

        // Validate and fill missing fields
        const style: VisualStyle = {
            colorPalette: parsed.colorPalette || ["warm amber", "deep shadow", "soft cream"],
            lighting: parsed.lighting || "cinematic lighting with soft shadows",
            texture: parsed.texture || "subtle film grain",
            moodKeywords: parsed.moodKeywords || ["atmospheric", "evocative"],
            stylePrompt: parsed.stylePrompt || generateStylePrompt(parsed),
        };

        // Cache the result
        if (sessionId) {
            styleCache.set(sessionId, style);
        }

        console.log(`[VisualConsistency] Extracted style:`, style.colorPalette.join(", "));
        return style;

    } catch (error) {
        console.error(`[VisualConsistency] Extraction failed:`, error);
        return getDefaultStyle();
    }
}

/**
 * Inject visual style into a scene prompt for consistency
 * 
 * @param prompt - Original visual description prompt
 * @param style - Extracted visual style to apply
 * @returns Enhanced prompt with style elements
 */
export function injectStyleIntoPrompt(
    prompt: string,
    style: VisualStyle
): string {
    // Build style suffix
    const colorDesc = style.colorPalette.slice(0, 3).join(" and ") + " color palette";
    const styleElements = [
        colorDesc,
        style.lighting,
        style.texture,
        ...style.moodKeywords.slice(0, 2),
    ].filter(Boolean).join(", ");

    // Append style to prompt (limit total length to avoid API issues)
    const enhancedPrompt = `${prompt}. Style: ${styleElements}`;

    // Truncate if too long (max 500 chars for most image APIs)
    if (enhancedPrompt.length > 500) {
        return enhancedPrompt.substring(0, 497) + "...";
    }

    return enhancedPrompt;
}

/**
 * Generate a style prompt from extracted elements
 */
function generateStylePrompt(style: Partial<VisualStyle>): string {
    const parts = [];

    if (style.colorPalette?.length) {
        parts.push(`${style.colorPalette.slice(0, 3).join(" and ")} color palette`);
    }
    if (style.lighting) {
        parts.push(style.lighting);
    }
    if (style.texture) {
        parts.push(style.texture);
    }
    if (style.moodKeywords?.length) {
        parts.push(style.moodKeywords.join(", ") + " atmosphere");
    }

    return parts.join(", ");
}

/**
 * Get default cinematic style as fallback
 */
function getDefaultStyle(): VisualStyle {
    return {
        colorPalette: ["teal", "warm orange", "deep shadow"],
        lighting: "cinematic lighting with dramatic shadows",
        texture: "subtle film grain, 35mm look",
        moodKeywords: ["atmospheric", "cinematic"],
        stylePrompt: "teal and orange color palette, cinematic lighting with dramatic shadows, subtle film grain, atmospheric mood",
    };
}

/**
 * Clear cached style for a session
 */
export function clearStyleCache(sessionId: string): void {
    styleCache.delete(sessionId);
}

/**
 * Check if style is cached for a session
 */
export function hasStyleCache(sessionId: string): boolean {
    return styleCache.has(sessionId);
}

/**
 * Verify character consistency across multiple images using Gemini Vision.
 * Compares actors in generated shots against the provided CharacterProfile.
 * 
 * @param imageUrls - Array of URLs or base64 data for the images to verify
 * @param profile - The reference character profile
 * @param language - Optional language for the report (e.g., 'ar')
 * @returns Comparison report with consistency score and issues
 */
export async function verifyCharacterConsistency(
    imageUrls: string[],
    profile: CharacterProfile,
    language: string = 'en'
): Promise<ConsistencyReport> {
    console.log(`[VisualConsistency] Verifying consistency for character: ${profile.name} across ${imageUrls.length} images (Language: ${language})`);

    if (!API_KEY) {
        return {
            score: 100,
            isConsistent: true,
            issues: [],
            suggestions: [],
            details: "API Authentication missing - skipped consistency check."
        };
    }

    if (imageUrls.length === 0) {
        return {
            score: 100,
            isConsistent: true,
            issues: [],
            suggestions: [],
            details: "No images provided for verification."
        };
    }

    try {
        const model = new ChatGoogleGenerativeAI({
            apiKey: API_KEY,
            model: MODELS.TEXT, // Using latest Gemini for vision
            temperature: 0.2,
        });

        // Prepare image parts for Gemini
        const imageParts: any[] = [];

        // Add character profile as text first
        const profileText = `
CHARACTER PROFILE:
Name: ${profile.name}
Role: ${profile.role}
Visual Description: ${profile.visualDescription}
`;

        // Process images (limit to first 5 for performance/quota)
        const activeImages = imageUrls.slice(0, 5);
        for (let i = 0; i < activeImages.length; i++) {
            const url = activeImages[i];
            if (!url) continue;

            let base64Data: string;

            if (url.startsWith('data:image')) {
                base64Data = url;
            } else {
                // Fetch and convert
                const response = await fetch(url);
                const blob = await response.blob();
                const buffer = await blob.arrayBuffer();
                const base64 = btoa(
                    new Uint8Array(buffer).reduce((data, byte) => data + String.fromCharCode(byte), '')
                );
                base64Data = `data:${blob.type || 'image/png'};base64,${base64}`;
            }

            imageParts.push({
                type: "image_url",
                image_url: base64Data
            });
        }

        const prompt = `
You are a Visual Continuity Supervisor. Your task is to verify if the character "${profile.name}" looks consistent across these ${activeImages.length} images.

${profileText}

Compare the actor/character in each image against the profile and against the other images.
Look specifically for consistency in:
1. Facial features and bone structure
2. Hair color and style
3. Eye color
4. Apparent age
5. Body type
6. Clothing (if applicable to the profile)

IMPORTANT: Provide all text fields (issues, suggestions, details) in ${language === 'ar' ? 'Arabic' : 'the requested language'}.

Return a JSON object with this exact structure:
{
  "score": number, // 0-100
  "isConsistent": boolean, // true if score > 75
  "issues": ["list of specific discrepancies"],
  "suggestions": ["how to fix the prompts for better consistency"],
  "details": "detailed analysis of the character appearance across all shots"
}

Return ONLY the JSON object.
`;

        const response = await model.invoke([
            new HumanMessage({
                content: [
                    ...imageParts,
                    { type: "text", text: prompt }
                ]
            })
        ]);

        const content = typeof response.content === 'string'
            ? response.content
            : JSON.stringify(response.content);

        const jsonMatch = content.match(/\{[\s\S]*\}/);
        if (!jsonMatch) {
            throw new Error("Failed to parse consistency report JSON");
        }

        return JSON.parse(jsonMatch[0]) as ConsistencyReport;

    } catch (error) {
        console.error(`[VisualConsistency] Verification failed:`, error);
        return {
            score: 50,
            isConsistent: false,
            issues: ["AI verification failed"],
            suggestions: ["Try manual verification"],
            details: `Error: ${error instanceof Error ? error.message : String(error)}`
        };
    }
}
````

## File: packages/shared/src/stores/appStore.ts
````typescript
/**
 * App Store - Unified Global State
 *
 * Consolidates all Zustand stores into a single source of truth:
 * - Conversation: AI chat history, context, workflow state
 * - Generation: Pipeline progress and stage tracking
 * - Export: Video export settings and progress
 * - UI: Panel/modal visibility, view modes
 * - Production: Scene data and playback state
 * - Navigation: Route-aware state tracking for persistence
 *
 * Requirements: 10.1, 10.2 - Production state persists across navigation
 * Requirements: 10.4 - Chat history persistence
 * Requirements: 10.5 - LocalStorage state recovery on refresh
 *
 * ROBUST PATTERNS IMPLEMENTED:
 * - Safe localStorage operations with size validation
 * - Graceful error handling for storage failures
 * - Size limits to prevent quota exceeded errors
 * - Automatic cleanup of old data when approaching limits
 */

import { create } from 'zustand';
import { persist } from 'zustand/middleware';
import { Scene, SongData, AppState, ContentPlan, NarrationSegment, VideoSFXPlan } from '../types';

// ============================================================
// Types
// ============================================================

// Message types
export type MessageRole = 'user' | 'assistant' | 'system';

export interface QuickAction {
    id: string;
    label: string;
    action: {
        type: string;
        [key: string]: unknown;
    };
    variant?: 'primary' | 'secondary';
}

export interface Message {
    id: string;
    role: MessageRole;
    content: string;
    timestamp: number;
    quickActions?: QuickAction[];
    metadata?: {
        intent?: string;
        confidence?: number;
        workflowTriggered?: string;
        requiresClarification?: boolean;
        error?: string;
    };
}

export interface FeedbackEntry {
    messageId: string;
    userMessage: string;
    agentResponse: string;
    helpful: boolean;
    rating: number;
    comment?: string;
    timestamp: number;
}

export interface ConversationContext {
    userGoals: string[];
    extractedEntities: Record<string, string[]>;
    conversationTurns: number;
    lastIntent?: string;
    clarifiedIntents: string[];
}

export interface WorkflowState {
    isExecuting: boolean;
    currentWorkflow?: string;
    progress: number;
    message?: string;
    result?: unknown;
    error?: string;
}

// Generation types
export type GenerationStage = 'idle' | 'transcribing' | 'planning' | 'generating-images' | 'generating-music' | 'complete' | 'error';

// Export types
export type ExportFormat = 'mp4' | 'webm' | 'gif';
export type ExportQuality = 'draft' | 'standard' | 'high' | 'ultra';

// UI types
export type PanelType = 'music' | 'export' | 'settings' | 'images' | 'scenes' | 'quality' | null;
export type ViewMode = 'simple' | 'advanced' | 'timeline';

// Navigation types (Requirements: 10.1, 10.2)
export interface NavigationState {
    lastRoute: string;
    lastVisitedAt: number;
    hasUnsavedChanges: boolean;
}

// Production persistence types (Requirements: 10.1, 10.2)
export interface PersistedProductionState {
    contentPlan: ContentPlan | null;
    narrationSegments: SerializedNarrationSegment[];
    sfxPlan: VideoSFXPlan | null;
    topic: string;
    visualStyle: string;
    targetDuration: number;
}

// Serialized narration segment (Blob cannot be persisted directly)
export interface SerializedNarrationSegment {
    sceneId: string;
    audioDuration: number;
    transcript: string;
    audioBase64?: string; // Base64 encoded audio for persistence
}

// ============================================================
// Store Interface
// ============================================================

interface AppStore {
    // --- Conversation State ---
    messages: Message[];
    conversationContext: ConversationContext;
    workflow: WorkflowState;
    isTyping: boolean;

    // Conversation Actions
    addMessage: (role: MessageRole, content: string, metadata?: Message['metadata']) => string;
    clearMessages: () => void;
    updateLastMessage: (updates: Partial<Message>) => void;
    updateContext: (updates: Partial<ConversationContext>) => void;
    addEntity: (key: string, value: string) => void;
    startWorkflow: (workflowName: string, message?: string) => void;
    updateWorkflowProgress: (progress: number, message?: string) => void;
    completeWorkflow: (result?: unknown) => void;
    failWorkflow: (error: string) => void;
    setTyping: (typing: boolean) => void;

    // --- Generation State ---
    generationStage: GenerationStage;
    generationProgress: number;
    generationMessage: string;
    generationError: string | null;
    isGeneratingImages: boolean;
    isGeneratingMusic: boolean;
    isTranscribing: boolean;

    // Generation Actions
    setGenerationStage: (stage: GenerationStage) => void;
    setGenerationProgress: (progress: number, message?: string) => void;
    setGenerationError: (error: string | null) => void;
    startImageGeneration: () => void;
    completeImageGeneration: () => void;
    startMusicGeneration: () => void;
    completeMusicGeneration: () => void;
    startTranscription: () => void;
    completeTranscription: () => void;

    // --- Export State ---
    exportFormat: ExportFormat;
    exportQuality: ExportQuality;
    exportAspectRatio: '16:9' | '9:16' | '1:1';
    includeAudio: boolean;
    isExporting: boolean;
    exportProgress: number;
    exportedUrl: string | null;

    // Export Actions
    setExportFormat: (format: ExportFormat) => void;
    setExportQuality: (quality: ExportQuality) => void;
    setExportAspectRatio: (ratio: '16:9' | '9:16' | '1:1') => void;
    setIncludeAudio: (include: boolean) => void;
    startExport: () => void;
    setExportProgress: (progress: number) => void;
    completeExport: (url: string) => void;
    cancelExport: () => void;

    // --- UI State ---
    activePanel: PanelType;
    viewMode: ViewMode;
    isMusicModalOpen: boolean;
    isExportModalOpen: boolean;
    isSettingsModalOpen: boolean;

    // UI Actions
    setActivePanel: (panel: PanelType) => void;
    setViewMode: (mode: ViewMode) => void;
    openPanel: (panel: PanelType) => void;
    closePanel: () => void;
    toggleMusicModal: (open?: boolean) => void;
    toggleExportModal: (open?: boolean) => void;
    toggleSettingsModal: (open?: boolean) => void;

    // --- Production State ---
    songData: SongData | null;
    productionAppState: AppState;
    scenes: Scene[];
    currentSceneIndex: number;

    // Production Actions
    setSongData: (data: SongData | null) => void;
    setProductionAppState: (state: AppState) => void;
    setScenes: (scenes: Scene[]) => void;
    updateScene: (index: number, updates: Partial<Scene>) => void;
    setCurrentSceneIndex: (index: number) => void;

    // --- User Profile State ---
    userProfile: {
        preferences: {
            defaultStyle?: string;
            defaultDuration?: number;
            preferredLanguage?: string;
        };
        history: {
            totalVideosCreated: number;
            totalMusicGenerated: number;
            mostUsedStyles: Record<string, number>;
        };
    };

    // User Profile Actions
    updateUserProfile: (updates: Partial<AppStore['userProfile']>) => void;
    trackVideoCreation: (params: { style: string; duration: number }) => void;
    trackMusicGeneration: () => void;

    // --- Current User State ---
    currentUser: {
        uid: string;
        email: string | null;
        displayName: string | null;
        photoURL: string | null;
        isAuthenticated: boolean;
    } | null;
    currentProjectId: string | null;

    // Current User Actions
    setCurrentUser: (user: AppStore['currentUser']) => void;
    setCurrentProjectId: (projectId: string | null) => void;
    clearCurrentUser: () => void;

    // --- Feedback State ---
    feedbackHistory: FeedbackEntry[];
    recordFeedback: (feedback: FeedbackEntry) => void;

    // --- Navigation State (Requirements: 10.1, 10.2) ---
    navigationState: NavigationState;
    persistedProduction: PersistedProductionState | null;

    // Navigation Actions
    setLastRoute: (route: string) => void;
    setHasUnsavedChanges: (hasChanges: boolean) => void;

    // Production Persistence Actions
    persistProductionState: (state: {
        contentPlan: ContentPlan | null;
        narrationSegments: NarrationSegment[];
        sfxPlan: VideoSFXPlan | null;
        topic: string;
        visualStyle: string;
        targetDuration: number;
    }) => Promise<void>;
    clearPersistedProduction: () => void;
    getPersistedNarrationSegments: () => Promise<NarrationSegment[]>;

    // --- Global Actions ---
    resetAll: () => void;
}

// ============================================================
// Helper
// ============================================================

const generateId = () => `${Date.now()}-${Math.random().toString(36).substring(2, 11)}`;











/**
 * Convert Base64 string back to Blob.
 * Returns null on failure instead of throwing.
 */
function base64ToBlob(base64: string, mimeType: string = 'audio/wav'): Blob | null {
    if (!base64 || base64.length === 0) {
        return null;
    }

    try {
        const byteCharacters = atob(base64);
        const byteNumbers = new Array(byteCharacters.length);
        for (let i = 0; i < byteCharacters.length; i++) {
            byteNumbers[i] = byteCharacters.charCodeAt(i);
        }
        const byteArray = new Uint8Array(byteNumbers);
        return new Blob([byteArray], { type: mimeType });
    } catch (error) {
        console.error('[AppStore] Failed to convert Base64 to Blob:', error);
        return null;
    }
}

// ============================================================
// Store Implementation
// ============================================================

export const useAppStore = create<AppStore>()(
    persist(
        (set, get) => ({
            // ========================================
            // Conversation State
            // ========================================
            messages: [],
            conversationContext: {
                userGoals: [],
                extractedEntities: {},
                conversationTurns: 0,
                clarifiedIntents: [],
            },
            workflow: {
                isExecuting: false,
                progress: 0,
            },
            isTyping: false,

            addMessage: (role, content, metadata) => {
                const id = generateId();
                set((state) => ({
                    messages: [...state.messages, { id, role, content, timestamp: Date.now(), metadata }],
                    conversationContext: {
                        ...state.conversationContext,
                        conversationTurns: state.conversationContext.conversationTurns + 1,
                    },
                }));
                return id;
            },

            clearMessages: () => set({
                messages: [],
                conversationContext: { userGoals: [], extractedEntities: {}, conversationTurns: 0, clarifiedIntents: [] },
            }),

            updateLastMessage: (updates) => set((state) => {
                if (state.messages.length === 0) return state;
                const msgs = [...state.messages];
                const lastMsg = msgs[msgs.length - 1];
                if (!lastMsg) return state;
                msgs[msgs.length - 1] = {
                    ...lastMsg,
                    ...updates,
                    // Ensure required fields are always present
                    id: updates.id ?? lastMsg.id,
                    role: updates.role ?? lastMsg.role,
                    content: updates.content ?? lastMsg.content,
                    timestamp: updates.timestamp ?? lastMsg.timestamp,
                };
                return { messages: msgs };
            }),

            updateContext: (updates) => set((state) => ({
                conversationContext: { ...state.conversationContext, ...updates },
            })),

            addEntity: (key, value) => set((state) => {
                const existing = state.conversationContext.extractedEntities[key] || [];
                if (existing.includes(value)) return state;
                return {
                    conversationContext: {
                        ...state.conversationContext,
                        extractedEntities: { ...state.conversationContext.extractedEntities, [key]: [...existing, value] },
                    },
                };
            }),

            startWorkflow: (workflowName, message) => set({
                workflow: { isExecuting: true, currentWorkflow: workflowName, progress: 0, message },
            }),

            updateWorkflowProgress: (progress, message) => set((state) => ({
                workflow: { ...state.workflow, progress, message },
            })),

            completeWorkflow: (result) => set((state) => ({
                workflow: { ...state.workflow, isExecuting: false, progress: 100, result },
            })),

            failWorkflow: (error) => set((state) => ({
                workflow: { ...state.workflow, isExecuting: false, error },
            })),

            setTyping: (isTyping) => set({ isTyping }),

            // ========================================
            // Generation State
            // ========================================
            generationStage: 'idle',
            generationProgress: 0,
            generationMessage: '',
            generationError: null,
            isGeneratingImages: false,
            isGeneratingMusic: false,
            isTranscribing: false,

            setGenerationStage: (generationStage) => set({ generationStage }),
            setGenerationProgress: (generationProgress, generationMessage) => set({ generationProgress, generationMessage: generationMessage ?? '' }),
            setGenerationError: (generationError) => set({ generationError, generationStage: generationError ? 'error' : 'idle' }),
            startImageGeneration: () => set({ isGeneratingImages: true, generationStage: 'generating-images' }),
            completeImageGeneration: () => set({ isGeneratingImages: false, generationStage: 'idle' }),
            startMusicGeneration: () => set({ isGeneratingMusic: true, generationStage: 'generating-music' }),
            completeMusicGeneration: () => set({ isGeneratingMusic: false, generationStage: 'idle' }),
            startTranscription: () => set({ isTranscribing: true, generationStage: 'transcribing' }),
            completeTranscription: () => set({ isTranscribing: false, generationStage: 'idle' }),

            // ========================================
            // Export State
            // ========================================
            exportFormat: 'mp4',
            exportQuality: 'standard',
            exportAspectRatio: '16:9',
            includeAudio: true,
            isExporting: false,
            exportProgress: 0,
            exportedUrl: null,

            setExportFormat: (exportFormat) => set({ exportFormat }),
            setExportQuality: (exportQuality) => set({ exportQuality }),
            setExportAspectRatio: (exportAspectRatio) => set({ exportAspectRatio }),
            setIncludeAudio: (includeAudio) => set({ includeAudio }),
            startExport: () => set({ isExporting: true, exportProgress: 0, exportedUrl: null }),
            setExportProgress: (exportProgress) => set({ exportProgress }),
            completeExport: (exportedUrl) => set({ isExporting: false, exportProgress: 100, exportedUrl }),
            cancelExport: () => set({ isExporting: false, exportProgress: 0 }),

            // ========================================
            // UI State
            // ========================================
            activePanel: null,
            viewMode: 'simple',
            isMusicModalOpen: false,
            isExportModalOpen: false,
            isSettingsModalOpen: false,

            setActivePanel: (activePanel) => set({ activePanel }),
            setViewMode: (viewMode) => set({ viewMode }),
            openPanel: (panel) => set({ activePanel: panel }),
            closePanel: () => set({ activePanel: null }),
            toggleMusicModal: (open) => set((state) => ({ isMusicModalOpen: open ?? !state.isMusicModalOpen })),
            toggleExportModal: (open) => set((state) => ({ isExportModalOpen: open ?? !state.isExportModalOpen })),
            toggleSettingsModal: (open) => set((state) => ({ isSettingsModalOpen: open ?? !state.isSettingsModalOpen })),

            // ========================================
            // Production State
            // ========================================
            songData: null,
            productionAppState: AppState.IDLE,
            scenes: [],
            currentSceneIndex: 0,

            setSongData: (songData) => set({ songData }),
            setProductionAppState: (productionAppState) => set({ productionAppState }),
            setScenes: (scenes) => set({ scenes }),
            updateScene: (index, updates) => set((state) => ({
                scenes: state.scenes.map((s, i) => i === index ? { ...s, ...updates } : s),
            })),
            setCurrentSceneIndex: (currentSceneIndex) => set({ currentSceneIndex }),

            // ========================================
            // User Profile State
            // ========================================
            userProfile: {
                preferences: {},
                history: {
                    totalVideosCreated: 0,
                    totalMusicGenerated: 0,
                    mostUsedStyles: {},
                },
            },

            updateUserProfile: (updates) => set((state) => ({
                userProfile: {
                    ...state.userProfile,
                    ...updates,
                },
            })),

            trackVideoCreation: (params) => set((state) => {
                const newHistory = {
                    ...state.userProfile.history,
                    totalVideosCreated: state.userProfile.history.totalVideosCreated + 1,
                    mostUsedStyles: {
                        ...state.userProfile.history.mostUsedStyles,
                        [params.style]: (state.userProfile.history.mostUsedStyles[params.style] || 0) + 1,
                    },
                };

                // Auto-set default style after 3 uses
                const newPreferences = { ...state.userProfile.preferences };
                const styleCount = newHistory.mostUsedStyles[params.style];
                if (styleCount && styleCount >= 3 && !newPreferences.defaultStyle) {
                    newPreferences.defaultStyle = params.style;
                    console.log(`[UserProfile] Auto-set default style to ${params.style}`);
                }

                return {
                    userProfile: {
                        preferences: newPreferences,
                        history: newHistory,
                    },
                };
            }),

            trackMusicGeneration: () => set((state) => ({
                userProfile: {
                    ...state.userProfile,
                    history: {
                        ...state.userProfile.history,
                        totalMusicGenerated: state.userProfile.history.totalMusicGenerated + 1,
                    },
                },
            })),

            // ========================================
            // Current User State
            // ========================================
            currentUser: null,
            currentProjectId: null,

            setCurrentUser: (user) => set({ currentUser: user }),
            setCurrentProjectId: (projectId) => set({ currentProjectId: projectId }),
            clearCurrentUser: () => set({ currentUser: null, currentProjectId: null }),

            // ========================================
            // Feedback State
            // ========================================
            feedbackHistory: [],

            recordFeedback: (feedback) => set((state) => ({
                feedbackHistory: [...state.feedbackHistory.slice(-99), feedback], // Keep last 100
            })),

            // ========================================
            // Navigation State (Requirements: 10.1, 10.2)
            // ========================================
            navigationState: {
                lastRoute: '/',
                lastVisitedAt: Date.now(),
                hasUnsavedChanges: false,
            },
            persistedProduction: null,

            setLastRoute: (route) => set((state) => ({
                navigationState: {
                    ...state.navigationState,
                    lastRoute: route,
                    lastVisitedAt: Date.now(),
                },
            })),

            setHasUnsavedChanges: (hasChanges) => set((state) => ({
                navigationState: {
                    ...state.navigationState,
                    hasUnsavedChanges: hasChanges,
                },
            })),

            // Persist production state with metadata only (no audio data)
            // Audio will be regenerated from the content plan when needed
            persistProductionState: async ({ contentPlan, narrationSegments, sfxPlan, topic, visualStyle, targetDuration }) => {
                // Only persist metadata, not audio blobs to avoid LocalStorage 5MB limit
                const serializedSegments: SerializedNarrationSegment[] = narrationSegments.map((segment) => ({
                    sceneId: segment.sceneId,
                    audioDuration: segment.audioDuration,
                    transcript: segment.transcript,
                    // audioBase64 removed - audio will be regenerated from transcript
                }));

                set({
                    persistedProduction: {
                        contentPlan,
                        narrationSegments: serializedSegments,
                        sfxPlan,
                        topic,
                        visualStyle,
                        targetDuration,
                    },
                    navigationState: {
                        lastRoute: '/studio',
                        lastVisitedAt: Date.now(),
                        hasUnsavedChanges: false,
                    },
                });
            },

            clearPersistedProduction: () => set({
                persistedProduction: null,
                navigationState: {
                    lastRoute: '/',
                    lastVisitedAt: Date.now(),
                    hasUnsavedChanges: false,
                },
            }),

            // Restore narration segments from persisted state
            getPersistedNarrationSegments: async () => {
                const state = get();
                if (!state.persistedProduction?.narrationSegments) {
                    return [];
                }

                return state.persistedProduction.narrationSegments.map((segment) => {
                    let audioBlob: Blob | undefined;
                    if (segment.audioBase64) {
                        try {
                            audioBlob = base64ToBlob(segment.audioBase64) ?? undefined;
                        } catch (err) {
                            console.warn('Failed to deserialize audio for scene:', segment.sceneId, err);
                        }
                    }
                    return {
                        sceneId: segment.sceneId,
                        audioDuration: segment.audioDuration,
                        transcript: segment.transcript,
                        audioBlob: audioBlob || new Blob(), // Fallback to empty blob
                    };
                });
            },

            // ========================================
            // Global Reset
            // ========================================
            resetAll: () => set({
                // Conversation
                messages: [],
                conversationContext: { userGoals: [], extractedEntities: {}, conversationTurns: 0, clarifiedIntents: [] },
                workflow: { isExecuting: false, progress: 0 },
                isTyping: false,
                // Generation
                generationStage: 'idle',
                generationProgress: 0,
                generationMessage: '',
                generationError: null,
                isGeneratingImages: false,
                isGeneratingMusic: false,
                isTranscribing: false,
                // Export
                exportFormat: 'mp4',
                exportQuality: 'standard',
                exportAspectRatio: '16:9',
                includeAudio: true,
                isExporting: false,
                exportProgress: 0,
                exportedUrl: null,
                // UI
                activePanel: null,
                viewMode: 'simple',
                isMusicModalOpen: false,
                isExportModalOpen: false,
                isSettingsModalOpen: false,
                // Production
                songData: null,
                productionAppState: AppState.IDLE,
                scenes: [],
                currentSceneIndex: 0,
                // Navigation
                navigationState: {
                    lastRoute: '/',
                    lastVisitedAt: Date.now(),
                    hasUnsavedChanges: false,
                },
                persistedProduction: null,
            }),
        }),
        {
            name: 'lyriclens-app-store',
            partialize: (state) => ({
                // Persist conversation data (Requirements: 10.4)
                messages: state.messages.slice(-50),
                conversationContext: state.conversationContext,
                // Persist export preferences
                exportFormat: state.exportFormat,
                exportQuality: state.exportQuality,
                exportAspectRatio: state.exportAspectRatio,
                // Persist UI preferences
                viewMode: state.viewMode,
                // Persist navigation state (Requirements: 10.1, 10.2)
                navigationState: state.navigationState,
                // Persist production state (Requirements: 10.1, 10.2, 10.5)
                persistedProduction: state.persistedProduction,
                // Persist user profile
                userProfile: state.userProfile,
                // Persist current user and project
                currentUser: state.currentUser,
                currentProjectId: state.currentProjectId,
            }),
        }
    )
);

export default useAppStore;
````

## File: packages/shared/src/stores/index.ts
````typescript
/**
 * Stores Index
 * 
 * Single unified app store - all other stores have been consolidated.
 */

export {
    useAppStore,
    type Message,
    type MessageRole,
    type ConversationContext,
    type WorkflowState,
    type GenerationStage,
    type ExportFormat,
    type ExportQuality,
    type PanelType,
    type ViewMode,
} from './appStore';
````

## File: packages/shared/src/types.ts
````typescript
export interface WordTiming {
  word: string;
  startTime: number; // seconds
  endTime: number; // seconds
}

export interface SubtitleItem {
  id: number;
  startTime: number;
  endTime: number;
  text: string;
  translation?: string;
  words?: WordTiming[]; // Optional for backward-compat with word-level timing
}

/**
 * Asset generation type for each prompt card
 * - image: Generate a still image only
 * - video: Generate a video directly from prompt (Veo)
 * - video_with_image: Generate image first, then animate it (DeAPI style)
 */
export type AssetType = "image" | "video" | "video_with_image";

export interface ImagePrompt {
  id: string;
  text: string;
  mood: string;
  timestamp?: string; // Rough timestamp string (e.g. "00:01:30")
  timestampSeconds?: number; // Parsed seconds for sorting/display
  assetType?: AssetType; // Per-card generation type (defaults to global setting)
}

export interface GeneratedImage {
  promptId: string;
  imageUrl: string;
  type?: "image" | "video";
  /** URL to the generated video file if applicable */
  videoUrl?: string;
  /** Whether this is an animated version of an image */
  isAnimated?: boolean;
  /** Whether the video was generated using Veo 3.1 */
  generatedWithVeo?: boolean;
  /** If video_with_image, stores the base image separately */
  baseImageUrl?: string;
  /** Whether this is a placeholder image due to generation failure */
  isPlaceholder?: boolean;
  /** Cached blob URL for offline/re-export use (prevents expired URL issues) */
  cachedBlobUrl?: string;
}

export enum AppState {
  IDLE = "IDLE",
  CONFIGURING = "CONFIGURING",
  PROCESSING_AUDIO = "PROCESSING_AUDIO",
  TRANSCRIBING = "TRANSCRIBING",
  ANALYZING_LYRICS = "ANALYZING_LYRICS",
  GENERATING_PROMPTS = "GENERATING_PROMPTS",
  READY = "READY",
  ERROR = "ERROR",
  // Multi-agent production pipeline states
  CONTENT_PLANNING = "CONTENT_PLANNING",
  NARRATING = "NARRATING",
  EDITING = "EDITING",
  VALIDATING = "VALIDATING",
}

// --- Multi-Agent Production Types ---

/**
 * Emotional tone for narration voice matching.
 * @deprecated Prefer using InstructionTriplet for richer creative direction.
 * Kept for backward compatibility with existing voice selection logic.
 */
export type EmotionalTone =
  | "professional"
  | "dramatic"
  | "friendly"
  | "urgent"
  | "calm";

/**
 * Instruction Triplet: 3-axis creative direction system.
 * Replaces the flat EmotionalTone with richer vibe-based control.
 *
 * - primaryEmotion: Core emotional state (e.g., "visceral-dread", "bittersweet-longing")
 * - cinematicDirection: Visual/camera style (e.g., "dutch-angle", "slow-push-in")
 * - environmentalAtmosphere: Ambient texture (e.g., "foggy-ruins", "neon-rain")
 */
export interface InstructionTriplet {
  primaryEmotion: string;
  cinematicDirection: string;
  environmentalAtmosphere: string;
}

/**
 * Camera shot type for cinematography
 */
export type ShotType =
  | "extreme-close-up"
  | "close-up"
  | "medium"
  | "full"
  | "wide"
  | "extreme-wide";

/**
 * Camera movement type for animations
 */
export type CameraMovement =
  | "static"
  | "zoom-in"
  | "zoom-out"
  | "pan"
  | "tracking"
  | "pull-back";

/**
 * Character definition for visual consistency across scenes
 */
export interface CharacterDefinition {
  name: string;
  appearance: string; // Detailed physical description
  clothing: string; // Specific garments and colors
  distinguishingFeatures?: string; // Scars, tattoos, jewelry, etc.
  /** Compact 5-word visual anchor for image prompts, e.g. "10yo wiry boy, messy black hair" */
  consistencyKey?: string;
}

/**
 * Scene structure produced by ContentPlanner
 */
export interface Scene {
  id: string;
  name: string;
  duration: number; // seconds
  visualDescription: string;
  narrationScript: string;
  /**
   * @deprecated Prefer instructionTriplet for new scenes.
   * Still used by voice selection (TONE_VOICE_MAP) and SFX matching.
   * Use getEffectiveLegacyTone() from tripletUtils for safe access.
   */
  emotionalTone?: EmotionalTone;
  /** 3-axis creative direction (new system) */
  instructionTriplet?: InstructionTriplet;
  transitionTo?: TransitionType;
  /** AI-suggested ambient sound effect ID */
  ambientSfx?: string;
  /** Cinematography - shot type */
  shotType?: ShotType;
  /** Cinematography - camera movement */
  cameraMovement?: CameraMovement;
  /** Cinematography - lighting description */
  lighting?: string;
}

/**
 * Full content plan from ContentPlanner agent
 */
export interface ContentPlan {
  title: string;
  totalDuration: number; // seconds
  targetAudience: string;
  scenes: Scene[];
  overallTone: string;
  /** Character definitions for visual consistency */
  characters?: CharacterDefinition[];
}

/**
 * Narration output from Narrator agent
 */
export interface NarrationSegment {
  sceneId: string;
  audioBlob: Blob;
  audioDuration: number; // seconds
  transcript: string;
}

/**
 * Validation result from Editor/Critic agent
 */
export interface ValidationResult {
  approved: boolean;
  score: number; // 0-100
  issues: Array<{ scene: string; type: string; message: string }>;
  suggestions: string[];
}

export interface SongData {
  fileName: string;
  audioUrl: string; // Blob URL for playback
  srtContent: string;
  parsedSubtitles: SubtitleItem[];
  prompts: ImagePrompt[];
  generatedImages: GeneratedImage[];
}

/**
 * Transition effects between scenes during video export
 */
export type TransitionType =
  | "none"      // Hard cut
  | "fade"      // Fade through black
  | "dissolve"  // Cross-dissolve (blend)
  | "zoom"      // Zoom into next scene
  | "slide";    // Slide left/right

/**
 * Text reveal direction for wipe animations
 */
export type TextRevealDirection = "ltr" | "rtl" | "center-out" | "center-in";

/**
 * Layout zone definition for zone-based rendering
 * Uses normalized coordinates (0-1) for responsive scaling
 */
export interface LayoutZone {
  name: string;
  x: number; // normalized 0-1
  y: number; // normalized 0-1
  width: number; // normalized 0-1
  height: number; // normalized 0-1
  zIndex: number;
}

/**
 * Layout configuration with zone definitions
 */
export interface LayoutConfig {
  orientation: "landscape" | "portrait";
  zones: {
    background: LayoutZone;
    visualizer: LayoutZone;
    text: LayoutZone;
    translation: LayoutZone;
  };
}

/**
 * Text animation configuration for wipe effects
 */
export interface TextAnimationConfig {
  revealDirection: TextRevealDirection;
  revealDuration: number; // seconds
  wordReveal: boolean; // word-by-word or line-by-line
}

/**
 * Visualizer configuration options
 */
export interface VisualizerConfig {
  enabled: boolean;
  opacity: number; // 0.0-1.0
  maxHeightRatio: number; // 0.0-1.0
  zIndex: number;
  barWidth: number; // pixels
  barGap: number; // pixels
  colorScheme: "cyan-purple" | "rainbow" | "monochrome";
}

// --- SFX Types ---

export type SFXCategory =
  | "ambient"      // Background atmosphere
  | "nature"       // Natural sounds
  | "urban"        // City/industrial sounds
  | "weather"      // Weather effects
  | "transition"   // Scene transition sounds
  | "musical"      // Musical stingers/beds
  | "supernatural" // Eerie/mystical sounds
  | "action";      // Impact/movement sounds

export interface AmbientSFX {
  id: string;
  name: string;
  description: string;
  category: SFXCategory;
  moods: EmotionalTone[];
  keywords: string[];
  /** Duration in seconds (0 = loopable) */
  duration: number;
  /** Volume level 0-1 (relative to narration) */
  suggestedVolume: number;
  /** URL to audio file (if available) */
  audioUrl?: string;
  /** Base64 audio data (if generated) */
  audioData?: string;
}

export interface SceneSFXPlan {
  sceneId: string;
  ambientTrack: AmbientSFX | null;
  transitionIn: AmbientSFX | null;
  transitionOut: AmbientSFX | null;
  accentSounds: AmbientSFX[];
}

export interface VideoSFXPlan {
  scenes: SceneSFXPlan[];
  backgroundMusic: AmbientSFX | null;
  masterVolume: number;
  /** AI-generated music track from Suno API */
  generatedMusic?: {
    trackId: string;
    audioUrl: string;
    duration: number;
    title: string;
  };
}

// --- AI Assistant Types ---

export type ConversationRole = "user" | "assistant" | "system";

export interface ConversationMessage {
  id: string;
  role: ConversationRole;
  content: string;
  timestamp: number;
  /** Optional: Intent parsed from user message */
  intent?: ParsedIntent;
  /** Optional: Workflow triggered by this message */
  workflowId?: string;
  /** Optional: Whether this is a clarification request */
  isClarification?: boolean;
  /** Optional: Confidence score of intent parsing (0-1) */
  confidence?: number;
}

export interface ConversationContext {
  /** Thread of messages for context */
  messages: ConversationMessage[];
  /** Current user goal if known */
  currentGoal?: string;
  /** Any extracted entities from conversation */
  entities: ExtractedEntity[];
  /** Number of exchanges in current conversation */
  exchangeCount: number;
  /** Whether context has been established */
  contextEstablished: boolean;
}

export interface ExtractedEntity {
  type: "video_type" | "duration" | "mood" | "topic" | "style" | "language" | "custom";
  value: string;
  confidence: number;
  sourceMessageId: string;
}

export interface ParsedIntent {
  /** Unique intent type identifier */
  intentType: IntentType;
  /** Human-readable description of what user wants */
  description: string;
  /** Extracted parameters for workflow */
  parameters: Record<string, unknown>;
  /** Confidence score (0-1) */
  confidence: number;
  /** Whether clarification is needed */
  needsClarification: boolean;
  /** Questions to ask if clarification needed */
  clarificationQuestions?: string[];
  /** Suggested follow-up intents */
  suggestedIntents?: IntentType[];
}

export type IntentType =
  | "create_video"
  | "edit_video"
  | "generate_music"
  | "transcribe_audio"
  | "translate_content"
  | "export_project"
  | "import_project"
  | "analyze_lyrics"
  | "generate_images"
  | "add_sfx"
  | "set_timeline"
  | "get_help"
  | "show_features"
  | "pricing_info"
  | "technical_support"
  | "general_chat"
  | "clarification_request"
  | "escalate_to_human"
  | "unknown";

export interface WorkflowDefinition<TParams = Record<string, unknown>, TResult = unknown> {
  id: string;
  name: string;
  description: string;
  intentTypes: IntentType[];
  requiredParams: string[];
  optionalParams: string[];
  execute: (params: TParams) => Promise<WorkflowResult<TResult>>;
  estimatedDuration?: string;
  complexity?: "simple" | "moderate" | "complex";
}

export interface WorkflowResult<TData = unknown> {
  success: boolean;
  data?: TData;
  error?: string;
  message?: string;
  nextSteps?: string[];
}

export interface AIAssistantState {
  isOpen: boolean;
  isTyping: boolean;
  conversationContext: ConversationContext;
  currentIntent?: ParsedIntent;
  suggestedWorkflows: WorkflowDefinition[];
  errorState?: AIErrorState;
  showEscalationOption: boolean;
}

export interface AIErrorState {
  hasError: boolean;
  errorType: "parsing_error" | "workflow_error" | "context_error" | "network_error";
  message: string;
  canRetry: boolean;
  fallbackMessage: string;
}

export interface SuggestionChip {
  id: string;
  text: string;
  icon?: string;
  intentType: IntentType;
  params?: Record<string, unknown>;
}

export interface QuickAction {
  id: string;
  label: string;
  description: string;
  icon: string;
  intentType: IntentType;
  shortcut?: string;
}

// --- Story Mode Types ---

/**
 * Character profile for visual consistency across story scenes
 */
export interface CharacterProfile {
  id: string;
  name: string;
  role: string;
  visualDescription: string; // The "Golden Prompt" for consistency
  facialTags?: string; // 5-keyword compact face/clothing tags for prompt anchoring
  referenceImageUrl?: string; // Generated "Sheet" for the character
  /** 30-50 word structured visual identity anchor for prompt injection.
   * Format: "[name]: [first 2 sentences of visualDescription]. Face: [facialTags]. Rendered in [style] art style."
   * Built by enrichCharactersWithCoreAnchors() after character extraction.
   * Injected as "CHARACTERS IN FRAME:" prefix in image prompts for consistency.
   */
  coreAnchors?: string;
}

/**
 * Screenplay scene in standard format
 */
export interface ScreenplayScene {
  id: string;
  sceneNumber: number;
  heading: string; // INT. BEDROOM - DAY
  action: string;
  dialogue: Array<{ speaker: string; text: string }>;
  charactersPresent: string[];
}

/**
 * Shotlist entry for storyboard generation
 */
export interface ShotlistEntry {
  id: string;
  sceneId: string;
  shotNumber: number;
  description: string;
  cameraAngle: string; // "Wide", "Close-up"
  movement: string; // "Pan", "Static"
  lighting?: string; // "Cinematic", "Natural", etc.
  dialogue?: string; // Associated dialogue for the shot
  scriptSegment?: string; // 1-3 sentences of narration mapped to this shot
  imageUrl?: string; // The final generated image
  durationEst?: number; // Estimated duration in seconds
  // Extended cinematography metadata (Shot Editor)
  shotType?: string; // "Wide", "Medium", "Close-up", etc.
  equipment?: string; // Camera equipment, e.g. "Tripod", "Steadicam"
  focalLength?: string; // e.g. "35mm", "85mm"
  aspectRatio?: string; // e.g. "16:9", "9:16", "1:1"
  notes?: string; // Production notes
}

/**
 * Story Mode workflow step
 */
export type StoryStep = 'idea' | 'breakdown' | 'script' | 'characters' | 'shots' | 'style' | 'storyboard' | 'narration' | 'animation' | 'export';

/**
 * Shot type for shot-level breakdown (Storyboarder.ai style)
 */
export type StoryShotType = 'Wide' | 'Medium' | 'Close-up' | 'Extreme Close-up' | 'POV' | 'Over-the-shoulder';
export type StoryCameraAngle = 'Eye-level' | 'High' | 'Low' | 'Dutch' | "Bird's-eye" | "Worm's-eye";
export type StoryCameraMovement = 'Static' | 'Pan' | 'Tilt' | 'Zoom' | 'Dolly' | 'Tracking' | 'Handheld';

/**
 * Individual shot within a scene for storyboard workflow
 */
export interface StoryShot {
  id: string;
  sceneId: string;
  shotNumber: number;
  shotType: StoryShotType;
  cameraAngle: StoryCameraAngle;
  movement: StoryCameraMovement;
  duration: number;
  description: string;
  emotion: string;
  lighting: string;
  scriptSegment?: string; // 1-3 sentences of narration mapped to this shot
  imageUrl?: string; // Generated image for this shot
  // Extended cinematography metadata (matches ShotlistEntry for shot-table display)
  equipment?: string; // Camera equipment, e.g. "Tripod", "Steadicam"
  focalLength?: string; // e.g. "35mm", "85mm"
  aspectRatio?: string; // e.g. "16:9", "9:16", "1:1"
  notes?: string; // Production notes
}

/**
 * Result of a character consistency check
 */
export interface ConsistencyReport {
  score: number;
  isConsistent: boolean;
  issues: string[];
  suggestions: string[];
  details: string;
}

/**
 * Story Mode complete state
 */
export interface StoryState {
  currentStep: StoryStep;
  breakdown: ScreenplayScene[];
  script: { title: string; scenes: ScreenplayScene[] } | null;
  characters: CharacterProfile[];
  shotlist: ShotlistEntry[];
  consistencyReports?: Record<string, ConsistencyReport>; // characterId -> report

  // Storyboarder.ai-style workflow fields
  isLocked?: boolean;
  lockedAt?: string;
  version?: 'draft' | 'locked_v1';
  shots?: StoryShot[];
  visualStyle?: string;
  aspectRatio?: string;
  genre?: string;
  imageProvider?: 'gemini' | 'deapi';  // Image generation provider for storyboard visuals

  // Per-scene generation progress tracking
  scenesWithShots?: string[]; // scene IDs that have shots generated
  scenesWithVisuals?: string[]; // scene IDs that have storyboard visuals generated

  // Narration (TTS) state
  narrationSegments?: Array<{
    sceneId: string;
    audioUrl: string;
    duration: number;
    text: string;
  }>;
  scenesWithNarration?: string[]; // scene IDs that have narration generated

  // Animation (Veo/DeAPI) state
  animatedShots?: Array<{
    shotId: string;
    videoUrl: string;
    thumbnailUrl?: string;
    duration: number;
  }>;
  shotsWithAnimation?: string[]; // shot IDs that have animation generated

  // Visual style DNA extracted from first generated shot (Issue 3)
  masterStyle?: {
    colorPalette: string[];
    lighting: string;
    texture: string;
    moodKeywords: string[];
  };

  // Final export state
  finalVideoUrl?: string;
  exportProgress?: number;

  // Error tracking for specific stages
  stageErrors?: Record<StoryStep, string | null>;

  /**
   * Per-shot image generation status for error recovery and resume.
   * Key is shot.id, value is 'pending' | 'success' | 'failed'.
   * Populated/updated during generateVisuals().
   */
  storyboardStatus?: Record<string, 'pending' | 'success' | 'failed'>;

  /**
   * Per-shot narration generation status for error recovery and resume.
   * Key is shot.id, value is 'pending' | 'success' | 'failed'.
   * Populated/updated during generateNarration().
   */
  narrationStatus?: Record<string, 'pending' | 'success' | 'failed'>;

  /**
   * Per-shot narration audio segments.
   * Enables exact timing alignment: each shot's audio maps directly to its visual.
   * Coexists with narrationSegments (scene-level) for backward compatibility.
   */
  shotNarrationSegments?: Array<{
    shotId: string;
    sceneId: string;
    audioUrl: string;
    duration: number;
    text: string;
  }>;
}

// --- Multi-Format Pipeline Types ---

/**
 * Video format types for the multi-format pipeline
 */
export type VideoFormat =
  | 'youtube-narrator'
  | 'advertisement'
  | 'movie-animation'
  | 'educational'
  | 'shorts'
  | 'documentary'
  | 'music-video'
  | 'news-politics';

/**
 * Format metadata for pipeline configuration
 */
export interface FormatMetadata {
  id: VideoFormat;
  name: string;
  description: string;
  icon: string;
  durationRange: { min: number; max: number }; // seconds
  aspectRatio: '16:9' | '9:16' | '1:1';
  applicableGenres: string[];
  checkpointCount: number;
  concurrencyLimit: number;
  requiresResearch: boolean;
  supportedLanguages: ('ar' | 'en')[];
  deprecated?: boolean;
  deprecationMessage?: string;
}

/**
 * Pipeline phase configuration
 */
export interface PipelinePhase {
  id: string;
  name: string;
  order: number;
  tasks: PhaseTask[];
  parallel: boolean;
  required: boolean;
}

/**
 * Individual task within a pipeline phase
 */
export interface PhaseTask {
  id: string;
  type: 'research' | 'script' | 'visual' | 'audio' | 'assembly';
  service: string;
  parameters: Record<string, any>;
  dependencies: string[]; // task IDs that must complete first
  retryable: boolean;
  timeout: number; // milliseconds
}

/**
 * Format-specific pipeline configuration
 */
export interface FormatPipelineConfig {
  formatId: VideoFormat;
  phases: PipelinePhase[];
  checkpoints: CheckpointConfig[];
  concurrencyLimit: number;
  defaultDuration: number; // seconds
}

/**
 * Checkpoint configuration for user approval
 */
export interface CheckpointConfig {
  id: string;
  phase: string;
  title: string;
  description: string;
  timeout: number; // milliseconds
  required: boolean;
}

/**
 * Checkpoint state during execution
 */
export interface CheckpointState {
  checkpointId: string;
  phase: string;
  status: 'pending' | 'approved' | 'rejected';
  approvedAt?: Date;
  /** Arbitrary preview data attached by the pipeline at checkpoint creation */
  data?: Record<string, unknown>;
}

// --- Music / Beat Metadata Types (Task 9.3) ---

/**
 * A single beat event within a music track.
 * Used for beat-synchronized visual transitions.
 *
 * Requirements: 8.4, 8.6, 15.3
 */
export interface BeatEvent {
  /** Beat timestamp in seconds */
  timestamp: number;
  /** Beat intensity 0–1 (0 = weak, 1 = strong downbeat) */
  intensity: number;
}

/**
 * Beat metadata extracted from or generated alongside a music track.
 */
export interface BeatMetadata {
  /** Beats per minute */
  bpm: number;
  /** Total track duration in seconds */
  durationSeconds: number;
  /** Ordered list of beat events */
  beats: BeatEvent[];
}

/**
 * Configuration for AI music generation (Music Video format).
 *
 * Requirements: 8.3, 14.2
 */
export interface MusicGenerationConfig {
  genre: string;
  mood: string;
  tempo?: number;
  durationSeconds: number;
  instrumental?: boolean;
}

// --- Assembly Types (Task 10) ---

/**
 * A chapter marker for Documentary format video assembly.
 *
 * Requirements: 5.6, 15.4
 */
export interface ChapterMarker {
  id: string;
  title: string;
  startTime: number; // seconds
  endTime: number;   // seconds
}

/**
 * CTA (Call-to-Action) marker for Advertisement format.
 *
 * Requirements: 4.6, 15.2
 */
export interface CTAMarker {
  text: string;
  /** CTA start position in seconds (should be in final 5 seconds) */
  startTime: number;
  /** CTA duration in seconds */
  duration: number;
}

/**
 * Timeline clip for assembly.
 */
export interface TimelineClip {
  id: string;
  type: 'visual' | 'audio' | 'text' | 'transition';
  startTime: number;
  endTime: number;
  assetUrl?: string;
  metadata?: Record<string, unknown>;
}

/**
 * Format-specific assembly rules applied during video export.
 *
 * Requirements: 15.1
 */
export interface FormatAssemblyRules {
  formatId: VideoFormat;
  /** Aspect ratio override (from format registry) */
  aspectRatio: '16:9' | '9:16' | '1:1';
  /** Default transition between scenes */
  defaultTransition: TransitionType;
  /** Transition duration in seconds */
  transitionDuration: number;
  /** CTA configuration for Advertisement format */
  ctaMarker?: CTAMarker;
  /** Chapter markers for Documentary format */
  chapters?: ChapterMarker[];
  /** Beat metadata for Music Video format */
  beatMetadata?: BeatMetadata;
  /** Whether to organize content by chapters */
  useChapterStructure?: boolean;
  /** Whether to sync visuals to beat timestamps */
  useBeatSync?: boolean;
}
````

## File: packages/shared/src/types/arabic-reshaper.d.ts
````typescript
declare module "arabic-reshaper" {
  const ArabicReshaper: {
    convertArabic(text: string): string;
    convertArabicBack(text: string): string;
  };
  export default ArabicReshaper;
}
````

## File: packages/shared/src/types/audio-editor.ts
````typescript
/**
 * Audio Editor Types
 *
 * Type definitions for the AudioTimelineEditor component and its sub-components.
 * These types define the internal data model used by the timeline editor.
 *
 * @see .kiro/specs/timeline-editor-replacement/design.md for architecture details
 */

/**
 * Represents a track in the timeline editor.
 * Tracks are horizontal lanes that contain clips of a specific media type.
 *
 * @requirements 9.1, 9.2, 9.3
 */
export interface Track {
  /** Unique identifier for the track */
  id: string;
  /** Type of media this track contains */
  type: "narrator" | "sfx" | "subtitle" | "video" | "image";
  /** Display name for the track */
  name: string;
  /** Text content associated with the track (e.g., narration script) */
  text: string;
  /** Whether the track's content has been generated */
  isGenerated: boolean;
}

/**
 * Represents an audio clip on a track (narrator or SFX).
 * Audio clips display waveform visualization.
 *
 * @requirements 9.2
 */
export interface AudioClip {
  /** Unique identifier for the clip */
  id: string;
  /** ID of the track this clip belongs to */
  trackId: string;
  /** Start time in seconds */
  startTime: number;
  /** Duration in seconds */
  duration: number;
  /** Waveform data for visualization (normalized values 0-1) */
  waveformData: number[];
}

/**
 * Represents a video clip on the video track.
 * Video clips display thumbnail previews.
 *
 * @requirements 9.1
 */
export interface VideoClip {
  /** Unique identifier for the clip */
  id: string;
  /** ID of the track this clip belongs to */
  trackId: string;
  /** Start time in seconds */
  startTime: number;
  /** Duration in seconds */
  duration: number;
  /** URL of the thumbnail image */
  thumbnailUrl: string;
  /** Display name for the clip */
  name: string;
}

/**
 * Represents an image clip on the image track.
 * Image clips display the image as a thumbnail.
 *
 * @requirements 9.1
 */
export interface ImageClip {
  /** Unique identifier for the clip */
  id: string;
  /** ID of the track this clip belongs to */
  trackId: string;
  /** Start time in seconds */
  startTime: number;
  /** Duration in seconds */
  duration: number;
  /** URL of the image */
  imageUrl: string;
  /** Display name for the clip */
  name: string;
}

/**
 * Represents a subtitle cue on the subtitle track.
 * Subtitle cues have start and end times for display duration.
 *
 * @requirements 9.3
 */
export interface SubtitleCue {
  /** Unique identifier for the cue */
  id: string;
  /** ID of the track this cue belongs to */
  trackId: string;
  /** Start time in seconds */
  startTime: number;
  /** End time in seconds */
  endTime: number;
  /** Subtitle text content */
  text: string;
}

/**
 * Represents a media file that can be imported into the timeline.
 * Used by the import modal and media management.
 *
 * @requirements 9.1, 9.2, 9.3
 */
export interface MediaFile {
  /** Unique identifier for the file */
  id: string;
  /** Type of media file */
  type: "video" | "image" | "subtitle";
  /** Display name for the file */
  name: string;
  /** URL or data URI of the file */
  url: string;
  /** Duration in seconds (for video files) */
  duration?: number;
  /** URL of the thumbnail image (for video files) */
  thumbnailUrl?: string;
}

/**
 * Represents the current state of the timeline.
 * Used for managing playback and zoom state.
 */
export interface TimelineState {
  /** Current playback time in seconds */
  currentTime: number;
  /** Whether playback is active */
  isPlaying: boolean;
  /** Zoom level (10-100) */
  zoom: number;
  /** Total duration in seconds */
  duration: number;
}

/**
 * Represents the media files associated with a project.
 * Used for managing imported media.
 */
export interface ProjectMedia {
  /** Main video file */
  video: MediaFile | null;
  /** Imported image files */
  images: MediaFile[];
  /** Parsed subtitle cues */
  subtitles: SubtitleCue[];
}
````

## File: packages/shared/src/utils/audioAnalysis.ts
````typescript
export const extractFrequencyData = async (
  audioBuffer: AudioBuffer,
  fps: number,
  fftSize: number = 256
): Promise<Uint8Array[]> => {
  const duration = audioBuffer.duration;
  const totalFrames = Math.ceil(duration * fps);

  // Guard: If we are in an environment without proper Web Audio API support
  // (e.g. some Node.js polyfills missing suspend()), safely return empty data.
  // We try to instantiate the context first.
  let offlineCtx: OfflineAudioContext;
  try {
    offlineCtx = new OfflineAudioContext(
      audioBuffer.numberOfChannels,
      audioBuffer.length,
      audioBuffer.sampleRate
    );
  } catch (e) {
    console.warn('Failed to create OfflineAudioContext, skipping audio analysis:', e);
    return Array(totalFrames).fill(new Uint8Array(fftSize / 2).fill(0));
  }

  // Check if suspend is supported (Node.js web-audio-api might lack this)
  if (typeof offlineCtx.suspend !== 'function') {
    console.warn('OfflineAudioContext.suspend() is not supported in this environment. Skipping precise audio analysis.');
    return Array(totalFrames).fill(new Uint8Array(fftSize / 2).fill(0));
  }

  const source = offlineCtx.createBufferSource();
  source.buffer = audioBuffer;

  const analyser = offlineCtx.createAnalyser();
  analyser.fftSize = fftSize;

  source.connect(analyser);
  analyser.connect(offlineCtx.destination);

  // Use a Map to store frames by index to guarantee correct ordering
  const frameDataMap = new Map<number, Uint8Array>();
  const bufferLength = analyser.frequencyBinCount;

  // Schedule suspends for each frame (skip frame 0, handle it separately)
  for (let i = 1; i < totalFrames; i++) {
    const frameIndex = i;
    const time = i / fps;

    // We validated suspend exists above
    offlineCtx.suspend(time).then(() => {
      const dataArray = new Uint8Array(bufferLength);
      analyser.getByteFrequencyData(dataArray);
      frameDataMap.set(frameIndex, new Uint8Array(dataArray));
      offlineCtx.resume();
    }).catch(err => {
      console.error(`Error processing audio frame ${i}:`, err);
    });
  }

  source.start(0);
  try {
    await offlineCtx.startRendering();
  } catch (e) {
    console.error('Audio rendering failed:', e);
    return Array(totalFrames).fill(new Uint8Array(bufferLength).fill(0));
  }

  // Frame 0: Use empty data since audio hasn't played yet at t=0
  frameDataMap.set(0, new Uint8Array(bufferLength).fill(0));

  // Convert Map to ordered array
  const frequencyDataArray: Uint8Array[] = [];
  for (let i = 0; i < totalFrames; i++) {
    frequencyDataArray.push(frameDataMap.get(i) || new Uint8Array(bufferLength).fill(0));
  }

  return frequencyDataArray;
};
````

## File: packages/shared/src/utils/costEstimator.ts
````typescript
/**
 * Cost Estimator Utility
 *
 * Estimates generation costs for the story workflow.
 * Used to display cost warnings before locking the screenplay.
 */

import type { StoryState } from '@/types';

/**
 * Detailed cost breakdown for transparency
 */
export interface CostBreakdown {
    scenes: number;
    shots: number;
    llmCost: number;
    imageCost: number;
    videoCost: number;
    total: number;
}

/**
 * Cost rates (estimated)
 */
const COST_RATES = {
    // LLM costs per scene
    sceneBreakdown: 0.10,       // Scene analysis and breakdown
    shotBreakdown: 0.05,        // Shot generation per scene

    // Image generation costs per shot
    imageGeneration: 0.08,      // Per image (Imagen 4 / similar)

    // Video generation costs (optional)
    videoGeneration: 0.25,      // Per video clip (Veo / similar)

    // Narration costs
    narrationPerMinute: 0.15,   // TTS per minute
};

/**
 * Estimate total project cost based on story state
 */
export function estimateProjectCost(story: StoryState): number {
    const sceneCount = story.breakdown.length;
    const shotCount = story.shots?.length || sceneCount * 5; // Estimate 5 shots per scene

    const llmCost = sceneCount * COST_RATES.sceneBreakdown;
    const shotLLMCost = sceneCount * COST_RATES.shotBreakdown;
    const imageCost = shotCount * COST_RATES.imageGeneration;

    return llmCost + shotLLMCost + imageCost;
}

/**
 * Get detailed cost breakdown
 */
export function getDetailedCostBreakdown(story: StoryState): CostBreakdown {
    const sceneCount = story.breakdown.length;
    const shotCount = story.shots?.length || sceneCount * 5;

    const llmCost = sceneCount * COST_RATES.sceneBreakdown + sceneCount * COST_RATES.shotBreakdown;
    const imageCost = shotCount * COST_RATES.imageGeneration;

    return {
        scenes: sceneCount,
        shots: shotCount,
        llmCost,
        imageCost,
        videoCost: 0, // Optional video generation
        total: llmCost + imageCost,
    };
}

/**
 * Estimate cost for shot breakdown only
 */
export function estimateShotBreakdownCost(sceneCount: number): number {
    return sceneCount * COST_RATES.shotBreakdown;
}

/**
 * Estimate cost for image generation
 */
export function estimateImageGenerationCost(shotCount: number): number {
    return shotCount * COST_RATES.imageGeneration;
}

/**
 * Estimate cost with video generation
 */
export function estimateWithVideoCost(story: StoryState, videoCount: number): number {
    const baseCost = estimateProjectCost(story);
    const videoCost = videoCount * COST_RATES.videoGeneration;
    return baseCost + videoCost;
}

/**
 * Format cost as currency string
 */
export function formatCost(cost: number): string {
    return `$${cost.toFixed(2)}`;
}

/**
 * Get cost tier label
 */
export function getCostTier(cost: number): 'low' | 'medium' | 'high' {
    if (cost < 1) return 'low';
    if (cost < 5) return 'medium';
    return 'high';
}

/**
 * Get cost tier color class
 */
export function getCostTierColor(cost: number): string {
    const tier = getCostTier(cost);
    switch (tier) {
        case 'low':
            return 'text-emerald-400';
        case 'medium':
            return 'text-amber-400';
        case 'high':
            return 'text-red-400';
    }
}
````

## File: packages/shared/src/utils/platformUtils.ts
````typescript
import { Capacitor } from '@capacitor/core';

/**
 * Platform detection utilities for Capacitor mobile apps
 */

/**
 * Check if running in a native mobile environment (Capacitor)
 */
export const isNative = (): boolean => {
    return Capacitor.isNativePlatform();
};

/**
 * Check if running on Android
 */
export const isAndroid = (): boolean => {
    return Capacitor.getPlatform() === 'android';
};

/**
 * Check if running on iOS
 */
export const isIOS = (): boolean => {
    return Capacitor.getPlatform() === 'ios';
};

/**
 * Check if running in a web browser (not native)
 */
export const isWeb = (): boolean => {
    return Capacitor.getPlatform() === 'web';
};

/**
 * Get the current platform name
 */
export const getPlatformName = (): 'android' | 'ios' | 'web' => {
    return Capacitor.getPlatform() as 'android' | 'ios' | 'web';
};

/**
 * Check if FFmpeg WASM is supported on current platform
 * FFmpeg WASM requires SharedArrayBuffer which is not available in mobile WebViews
 */
export const isFFmpegWasmSupported = (): boolean => {
    // WASM FFmpeg requires SharedArrayBuffer and specific headers
    // These are typically not available in mobile WebViews
    if (isNative()) {
        return false;
    }

    // Check for SharedArrayBuffer support (required for WASM threading)
    try {
        return typeof SharedArrayBuffer !== 'undefined';
    } catch {
        return false;
    }
};

/**
 * Get the recommended export engine based on platform capabilities
 */
export const getRecommendedExportEngine = (): 'cloud' | 'browser' => {
    if (isNative() || !isFFmpegWasmSupported()) {
        return 'cloud';
    }
    return 'browser';
};

/**
 * Check if the device has touch capability
 */
export const isTouchDevice = (): boolean => {
    return isNative() || 'ontouchstart' in window || navigator.maxTouchPoints > 0;
};
````

## File: packages/shared/src/utils/srtParser.ts
````typescript
import { SubtitleItem } from "../types";

export const parseSRT = (srt: string): SubtitleItem[] => {
  const subtitles: SubtitleItem[] = [];
  const lines = srt.replace(/\r\n/g, '\n').split('\n');

  let currentSub: Partial<SubtitleItem> = {};
  let collectingText = false;

  for (let i = 0; i < lines.length; i++) {
    const line = (lines[i] ?? '').trim();

    // Skip empty lines unless they signify end of a block
    if (!line) {
      if (collectingText && currentSub.startTime !== undefined && currentSub.endTime !== undefined && currentSub.text) {
        subtitles.push(currentSub as SubtitleItem);
        currentSub = {};
        collectingText = false;
      }
      continue;
    }

    // Check if line is a timestamp line (contains --> or ->)
    if (line.includes('-->') || line.includes('->')) {
      // If we were already collecting text, push the previous subtitle
      if (collectingText && currentSub.startTime !== undefined && currentSub.endTime !== undefined && currentSub.text) {
        subtitles.push(currentSub as SubtitleItem);
        currentSub = {};
      }

      const parts = line.split(/-->|->/).map(s => s.trim());
      if (parts.length >= 2) {
        const startTime = parseSRTTimestamp(parts[0] ?? '');
        const endTime = parseSRTTimestamp(parts[1] ?? '');

        if (startTime !== null && endTime !== null) {
          currentSub.startTime = startTime;
          currentSub.endTime = endTime;

          // Guess ID if not set by looking at previous line
          if (!currentSub.id) {
            const prevLine = i > 0 ? (lines[i - 1] ?? '').trim() : '';
            if (/^\d+$/.test(prevLine)) {
              currentSub.id = parseInt(prevLine);
            } else {
              currentSub.id = subtitles.length + 1;
            }
          }
          collectingText = true;
          currentSub.text = ''; // Ready to collect text
          continue;
        }
      }
    }

    // Check if line is a numeric ID (and we are not currently reading text for a sub)
    if (/^\d+$/.test(line) && !collectingText) {
      currentSub.id = parseInt(line);
      continue;
    }

    // Collect Text
    if (collectingText) {
      currentSub.text = currentSub.text ? `${currentSub.text}\n${line}` : line;
    }
  }

  // Push final subtitle if loop ends
  if (collectingText && currentSub.startTime !== undefined && currentSub.endTime !== undefined && currentSub.text) {
    subtitles.push(currentSub as SubtitleItem);
  }

  return subtitles;
};

// Helper for various timestamp formats
// Supports: 
// HH:MM:SS,mmm (Standard)
// HH:MM:SS.mmm
// MM:SS:mmm (Common model output for short clips)
// MM:SS,mmm
export const parseSRTTimestamp = (timeStr: string): number | null => {
  if (!timeStr) return null;

  // Handle clean up
  const cleanStr = timeStr.trim();

  // Replace all commas with dots for consistency
  const normalized = cleanStr.replace(',', '.');

  // Split by colon
  const parts = normalized.split(':');

  let seconds = 0;

  if (parts.length === 4) {
    // HH:MM:SS:mmm (colon used for ms separator)
    seconds += parseFloat(parts[0] || '0') * 3600;
    seconds += parseFloat(parts[1] || '0') * 60;
    seconds += parseFloat(parts[2] || '0');
    seconds += parseFloat(parts[3] || '0') / 1000;
  } else if (parts.length === 3) {
    // Ambiguous: HH:MM:SS or MM:SS:mmm
    const p1 = parseFloat(parts[0] || '0');
    const p2 = parseFloat(parts[1] || '0');
    const p3Str = parts[2] || '0';
    const p3 = parseFloat(p3Str);

    // Check if the 3rd part looks like milliseconds (3 digits, no decimal)
    // or if its value is >= 60 (which is invalid for seconds).
    // Example: "00:26:477" -> p3=477. p3 >= 60. Treat as ms.
    // Example: "00:00:20.500" -> p3=20.5. p3 < 60. Treat as seconds.

    const isMilliseconds = !p3Str.includes('.') && (p3 >= 60 || p3Str.length === 3);

    if (isMilliseconds) {
      // MM:SS:mmm
      seconds += p1 * 60;
      seconds += p2;
      // If it's 3 digits like 477, it's 477ms. If it's 020, it's 20ms.
      // But usually MM:SS:mmm means milliseconds part is integer 0-999.
      seconds += p3 / 1000;
    } else {
      // HH:MM:SS
      seconds += p1 * 3600;
      seconds += p2 * 60;
      seconds += p3;
    }
  } else if (parts.length === 2) {
    // MM:SS
    seconds += parseFloat(parts[0] || '0') * 60;
    seconds += parseFloat(parts[1] || '0');
  } else {
    return null;
  }

  return isNaN(seconds) ? null : seconds;
};

/**
 * Converts seconds to SRT timestamp format (HH:MM:SS,mmm)
 */
const formatSRTTimestamp = (seconds: number): string => {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  const ms = Math.round((seconds % 1) * 1000);

  return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')},${ms.toString().padStart(3, '0')}`;
};

/**
 * Converts structured SubtitleItem[] back to SRT string format
 */
export const subtitlesToSRT = (subtitles: SubtitleItem[]): string => {
  return subtitles.map((sub, index) => {
    const id = sub.id || index + 1;
    const start = formatSRTTimestamp(sub.startTime);
    const end = formatSRTTimestamp(sub.endTime);
    return `${id}\n${start} --> ${end}\n${sub.text}`;
  }).join('\n\n');
};
````

## File: packages/shared/src/utils/testData.ts
````typescript
import { SongData, ImagePrompt, GeneratedImage } from '../types';
import { parseSRT } from './srtParser';

export const createTestSongData = (): SongData => {
  console.log('🎵 Creating test song data from public assets...');

  const srtContent = `1
00:00:51,539 --> 00:00:57,289
Oh, you who trick the soul with hopes that fade,

2
00:00:58,709 --> 00:01:04,019
this world is but a shadow and a shade.

3
00:01:05,489 --> 00:01:11,549
You weep for life, yet deep inside you know,

4
00:01:12,659 --> 00:01:16,879
the only peace is found in letting go.

5
00:01:19,439 --> 00:01:24,209
The days are travelers that will not stay.

6
00:01:26,389 --> 00:01:31,459
And happiness is walking a different way.

7
00:01:32,499 --> 00:01:37,709
There is no home awaiting after death,

8
00:01:39,369 --> 00:01:44,489
except the one you built with living breath.

9
00:01:45,959 --> 00:01:51,289
If built with good, a palace you shall find.

10
00:01:52,659 --> 00:01:57,809
If built with sin, you leave all hope behind.

11
00:02:03,819 --> 00:02:10,319
So work for the home where angels guard the gate.

12
00:02:10,819 --> 00:02:16,619
Where the prophet is the neighbor and the rewards are great.

13
00:02:17,769 --> 00:02:23,909
The walls are made of gold, the mortar smells of musk.

14
00:02:24,679 --> 00:02:28,849
A life that never fades into the dusk.

15
00:02:49,439 --> 00:02:54,779
Where are the kings, where is their royal throne?

16
00:02:56,089 --> 00:03:00,819
Where is the glory that the past has known?

17
00:03:03,349 --> 00:03:08,899
How many cities rose beneath the skies,

18
00:03:09,929 --> 00:03:15,009
only to crumble, wither and die?

19
00:03:16,459 --> 00:03:21,069
The cupbearer of death came to their halt,

20
00:03:22,469 --> 00:03:28,689
and poured the wine that ends it for us all.

21
00:03:28,689 --> 00:03:34,519
And now the sands have covered every storm.

22
00:03:36,049 --> 00:03:40,799
The high and low are buried and alone.

23
00:03:42,509 --> 00:03:48,399
Do not trust time, the seconds cut like knives.

24
00:03:50,479 --> 00:03:54,929
A single blink can swallow up our lives.

25
00:04:04,509 --> 00:04:08,589
True nobility is not in gold or birth,

26
00:04:11,109 --> 00:04:15,199
but in the faith you carry on this earth.

27
00:04:17,869 --> 00:04:23,439
Generosity comes first, patience is the key.

28
00:04:24,549 --> 00:04:28,759
A single prayer can set the spirit free.

29
00:04:58,899 --> 00:05:04,999
So work for the home where angels guard the gate.

30
00:05:05,429 --> 00:05:11,179
Where the prophet is the neighbor and the rewards are great.

31
00:05:12,479 --> 00:05:18,509
Its rivers flow with honey and with wine,

32
00:05:19,269 --> 00:05:23,809
for those who seek the mercy of the divine.

33
00:05:26,989 --> 00:05:32,239
This is the garden, eternal and green.

34
00:05:33,489 --> 00:05:37,739
The greatest sight the eyes have ever seen.

35
00:05:40,789 --> 00:05:45,999
And yet my soul will struggle to the end.

36
00:06:04,919 --> 00:06:10,879
I only find the path when I refuse to be my passion's friend.`;

  const parsedSubtitles = parseSRT(srtContent);

  // Map of image filenames to their metadata
  const imageMetadata = [
    { file: 'lyric-art-Intro - Melancholic.png', section: 'Intro', mood: 'Melancholic', time: 0 },
    { file: 'lyric-art-Verse 1 - Somber.png', section: 'Verse 1', mood: 'Somber', time: 51 },
    { file: 'lyric-art-Verse 2 - Contemplative.png', section: 'Verse 2', mood: 'Contemplative', time: 119 },
    { file: 'lyric-art-Bridge 1 - Urgent Caution.png', section: 'Bridge 1', mood: 'Urgent Caution', time: 183 },
    { file: 'lyric-art-Chorus_Outro 1 - Serene Wonder.png', section: 'Chorus_Outro 1', mood: 'Serene Wonder', time: 203 },
    { file: 'lyric-art-Interlude - Ethereal Lament.png', section: 'Interlude', mood: 'Ethereal Lament', time: 229 },
    { file: 'lyric-art-Verse 3 - Grand Melancholy.png', section: 'Verse 3', mood: 'Grand Melancholy', time: 289 },
    { file: 'lyric-art-Bridge 2 - Hopeful Instruction.png', section: 'Bridge 2', mood: 'Hopeful Instruction', time: 298 },
    { file: 'lyric-art-Verse 4 - Solitary Desolation.png', section: 'Verse 4', mood: 'Solitary Desolation', time: 326 },
    { file: 'lyric-art-Outro 2 - Blissful Climax.png', section: 'Outro 2', mood: 'Blissful Climax', time: 340 },
    { file: 'lyric-art-Outro 3 - Resolute Struggle.png', section: 'Outro 3', mood: 'Resolute Struggle', time: 364 }
  ];

  const prompts: ImagePrompt[] = imageMetadata.map((meta, index) => ({
    id: `prompt-test-${index}`,
    text: `A ${meta.mood.toLowerCase()} scene representing the ${meta.section.toLowerCase()} of the song`,
    mood: `${meta.section} - ${meta.mood}`,
    timestamp: formatTimestamp(meta.time),
    timestampSeconds: meta.time
  }));

  const generatedImages: GeneratedImage[] = imageMetadata.map((meta, index) => ({
    promptId: `prompt-test-${index}`,
    imageUrl: `/test_data/${meta.file}`
  }));

  console.log(`✨ Loaded ${prompts.length} test prompts with actual images`);

  return {
    fileName: 'the true Saba.mp3',
    audioUrl: '/test_data/the true Saba.mp3',
    srtContent,
    parsedSubtitles,
    prompts,
    generatedImages
  };
};

const formatTimestamp = (seconds: number): string => {
  const mins = Math.floor(seconds / 60);
  const secs = Math.floor(seconds % 60);
  return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
};
````

## File: packages/shared/src/utils/timeFormatting.ts
````typescript
/**
 * Time formatting utilities
 * Addresses Design Review Issue #28 - Relative time display
 */

/**
 * Format a timestamp as relative time (e.g., "2 hours ago")
 * Falls back to absolute time for old dates
 */
export function formatRelativeTime(timestamp: number | Date): string {
  const date = timestamp instanceof Date ? timestamp : new Date(timestamp);
  const now = new Date();
  const diffMs = now.getTime() - date.getTime();
  const diffSeconds = Math.floor(diffMs / 1000);
  const diffMinutes = Math.floor(diffSeconds / 60);
  const diffHours = Math.floor(diffMinutes / 60);
  const diffDays = Math.floor(diffHours / 24);
  const diffWeeks = Math.floor(diffDays / 7);
  const diffMonths = Math.floor(diffDays / 30);
  const diffYears = Math.floor(diffDays / 365);

  // Future dates
  if (diffMs < 0) {
    return 'in the future';
  }

  // Less than a minute
  if (diffSeconds < 60) {
    return 'just now';
  }

  // Less than an hour
  if (diffMinutes < 60) {
    return `${diffMinutes} ${diffMinutes === 1 ? 'minute' : 'minutes'} ago`;
  }

  // Less than a day
  if (diffHours < 24) {
    return `${diffHours} ${diffHours === 1 ? 'hour' : 'hours'} ago`;
  }

  // Less than a week
  if (diffDays < 7) {
    return `${diffDays} ${diffDays === 1 ? 'day' : 'days'} ago`;
  }

  // Less than a month
  if (diffWeeks < 4) {
    return `${diffWeeks} ${diffWeeks === 1 ? 'week' : 'weeks'} ago`;
  }

  // Less than a year
  if (diffMonths < 12) {
    return `${diffMonths} ${diffMonths === 1 ? 'month' : 'months'} ago`;
  }

  // More than a year - show absolute date
  return date.toLocaleDateString(undefined, {
    year: 'numeric',
    month: 'short',
    day: 'numeric',
  });
}

/**
 * Format absolute time for tooltips or full display
 */
export function formatAbsoluteTime(timestamp: number | Date): string {
  const date = timestamp instanceof Date ? timestamp : new Date(timestamp);
  return date.toLocaleString(undefined, {
    year: 'numeric',
    month: 'long',
    day: 'numeric',
    hour: '2-digit',
    minute: '2-digit',
  });
}

/**
 * Format duration in seconds to human-readable format
 */
export function formatDuration(seconds: number): string {
  if (seconds < 60) {
    return `${seconds}s`;
  }

  const minutes = Math.floor(seconds / 60);
  const remainingSeconds = seconds % 60;

  if (minutes < 60) {
    return remainingSeconds > 0 ? `${minutes}m ${remainingSeconds}s` : `${minutes}m`;
  }

  const hours = Math.floor(minutes / 60);
  const remainingMinutes = minutes % 60;

  return remainingMinutes > 0 ? `${hours}h ${remainingMinutes}m` : `${hours}h`;
}
````

## File: scripts/auto-generate-sitemap.ts
````typescript
/**
 * Automatic Sitemap Generator
 * Reads React Router configuration and generates sitemap automatically
 * Run with: npm run generate:sitemap:auto
 */

import { writeFileSync } from 'fs';
import { join } from 'path';
import { routes } from '../router/routes';

// Configuration
const DOMAIN = process.env.VITE_APP_URL || 'https://yourdomain.com';

interface SitemapConfig {
  changefreq: 'always' | 'hourly' | 'daily' | 'weekly' | 'monthly' | 'yearly' | 'never';
  priority: number;
}

// Default configuration for different route types
const defaultConfigs: Record<string, SitemapConfig> = {
  '/': { changefreq: 'weekly', priority: 1.0 },
  '/projects': { changefreq: 'weekly', priority: 0.8 },
  '/studio': { changefreq: 'monthly', priority: 0.9 },
  '/visualizer': { changefreq: 'monthly', priority: 0.9 },
  '/settings': { changefreq: 'monthly', priority: 0.5 },
  '/signin': { changefreq: 'yearly', priority: 0.6 },
};

// Routes to exclude from sitemap
const excludedRoutes = [
  '/404',
  '*', // Catch-all route
];

function generateSitemap(): string {
  const today = new Date().toISOString().split('T')[0];
  
  // Filter and map routes
  const validRoutes = routes
    .filter(route => !excludedRoutes.includes(route.path))
    .filter(route => !route.meta?.requiresAuth) // Optionally exclude auth-required routes
    .map(route => {
      const config = defaultConfigs[route.path] || {
        changefreq: 'monthly',
        priority: 0.5,
      };
      
      return `  <url>
    <loc>${DOMAIN}${route.path}</loc>
    <lastmod>${today}</lastmod>
    <changefreq>${config.changefreq}</changefreq>
    <priority>${config.priority}</priority>
  </url>`;
    })
    .join('\n\n');

  return `<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:xhtml="http://www.w3.org/1999/xhtml">
  
${validRoutes}

</urlset>
`;
}

function generateRobotsTxt(): string {
  return `# robots.txt for LyricLens

User-agent: *
Allow: /
Disallow: /api/

# Sitemap location
Sitemap: ${DOMAIN}/sitemap.xml
`;
}

// Generate files
try {
  const sitemap = generateSitemap();
  const robotsTxt = generateRobotsTxt();
  
  const publicDir = join(process.cwd(), 'public');
  
  writeFileSync(join(publicDir, 'sitemap.xml'), sitemap, 'utf-8');
  writeFileSync(join(publicDir, 'robots.txt'), robotsTxt, 'utf-8');
  
  console.log('✅ Sitemap generated successfully!');
  console.log(`📝 Total URLs: ${routes.filter(r => !excludedRoutes.includes(r.path)).length}`);
  console.log(`🌐 Domain: ${DOMAIN}`);
  console.log(`📍 Files created:`);
  console.log(`   - ${join(publicDir, 'sitemap.xml')}`);
  console.log(`   - ${join(publicDir, 'robots.txt')}`);
} catch (error) {
  console.error('❌ Error generating sitemap:', error);
  process.exit(1);
}
````

## File: scripts/download_mp3.sh
````bash
#!/bin/bash

# Check if a URL is provided
if [ -z "$1" ]; then
    echo "Usage: $0 <youtube-url>"
    exit 1
fi

URL=$1

# Download and convert to mp3
# -x: extract audio
# --audio-format mp3: convert to mp3
# --audio-quality 0: best quality (usually VBR 0, approx 250kbps)
# -o: output template
yt-dlp -x --audio-format mp3 --audio-quality 0 -o "%(title)s.%(ext)s" "$URL"

echo "Download and conversion complete!"
````

## File: scripts/generate-sitemap.ts
````typescript
/**
 * Generate sitemap.xml dynamically from route configuration
 * Run with: npm run generate:sitemap
 */

import { writeFileSync } from 'fs';
import { join } from 'path';

// Your domain - update this!
const DOMAIN = 'https://yourdomain.com';

// Route configuration
interface SitemapRoute {
  path: string;
  changefreq: 'always' | 'hourly' | 'daily' | 'weekly' | 'monthly' | 'yearly' | 'never';
  priority: number;
  exclude?: boolean; // Exclude from sitemap
}

const routes: SitemapRoute[] = [
  {
    path: '/',
    changefreq: 'weekly',
    priority: 1.0,
  },
  {
    path: '/projects',
    changefreq: 'weekly',
    priority: 0.8,
  },
  {
    path: '/studio',
    changefreq: 'monthly',
    priority: 0.9,
  },
  {
    path: '/visualizer',
    changefreq: 'monthly',
    priority: 0.9,
  },
  {
    path: '/settings',
    changefreq: 'monthly',
    priority: 0.5,
  },
  {
    path: '/signin',
    changefreq: 'yearly',
    priority: 0.6,
  },
];

function generateSitemap(): string {
  const today = new Date().toISOString().split('T')[0];
  
  const urls = routes
    .filter(route => !route.exclude)
    .map(route => `  <url>
    <loc>${DOMAIN}${route.path}</loc>
    <lastmod>${today}</lastmod>
    <changefreq>${route.changefreq}</changefreq>
    <priority>${route.priority}</priority>
  </url>`)
    .join('\n\n');

  return `<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:xhtml="http://www.w3.org/1999/xhtml">
  
${urls}

</urlset>
`;
}

// Generate and write sitemap
const sitemap = generateSitemap();
const outputPath = join(process.cwd(), 'public', 'sitemap.xml');

writeFileSync(outputPath, sitemap, 'utf-8');
console.log(`✅ Sitemap generated successfully at: ${outputPath}`);
console.log(`📝 Total URLs: ${routes.filter(r => !r.exclude).length}`);
````

## File: scripts/test-story-pipeline.ts
````typescript
/**
 * Test script: Run story pipeline with the FlamingThrow story idea.
 * Covers: breakdown → screenplay → characters → shot breakdown (1 scene)
 * Also verifies Feature 1 (persona negatives) and Feature 2 (split motion prompt).
 *
 * Run with: npx tsx --env-file=.env scripts/test-story-pipeline.ts
 */

import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { z } from "zod";
import { readFileSync, writeFileSync } from "fs";
import { resolve } from "path";

// --- Output collector: mirrors console.log and writes to file ---
const OUTPUT_FILE = resolve(import.meta.dirname, "../story-pipeline-output.md");
const lines: string[] = [];

const log = (...args: unknown[]) => {
    const text = args.map(a => String(a)).join(" ");
    console.log(text);
    lines.push(text);
};

const flush = () => {
    writeFileSync(OUTPUT_FILE, lines.join("\n") + "\n", "utf-8");
};

// --- Full story context from StoryIdea.txt ---
const STORY_TOPIC = readFileSync(
    resolve(import.meta.dirname, "../StoryIdea.txt"),
    "utf-8"
).trim();

const GENRE = "Action"; // maps to story_action persona

const API_KEY = process.env.VITE_GEMINI_API_KEY || process.env.GEMINI_API_KEY || "";
const MODEL_NAME = "gemini-3-flash-preview";

if (!API_KEY) {
    console.error("ERROR: VITE_GEMINI_API_KEY not set");
    process.exit(1);
}

// ---------- Schemas ----------

const BreakdownSchema = z.object({
    acts: z.array(z.object({
        title: z.string(),
        emotionalHook: z.string(),
        narrativeBeat: z.string(),
    })).min(3).max(5),
});

const ScreenplaySchema = z.object({
    scenes: z.array(z.object({
        heading: z.string(),
        action: z.string(),
        dialogue: z.array(z.object({
            speaker: z.string().max(30),
            text: z.string().min(1),
        })),
    })).min(3).max(8),
});

const CharacterSchema = z.object({
    characters: z.array(z.object({
        name: z.string(),
        role: z.string(),
        visualDescription: z.string(),
        facialTags: z.string().optional(),
    })),
});

const ShotSchema = z.object({
    shots: z.array(z.object({
        shotNumber: z.number(),
        shotType: z.string(),
        cameraAngle: z.string(),
        movement: z.string(),
        lighting: z.string(),
        emotion: z.string(),
        description: z.string(),
    })).min(2).max(5),
});

const MotionSchema = z.object({
    camera_motion: z.string(),
    subject_physics: z.string(),
});

const VoiceoverTestSchema = z.object({
    voiceovers: z.array(z.object({
        sceneId: z.string(),
        script: z.string(),
    })),
});

// ---------- Helper: print section ----------

function section(title: string) {
    const line = "─".repeat(60);
    log(`\n${line}`);
    log(`  ${title}`);
    log(line);
}

// ---------- Main ----------

async function main() {
    log("# Story Pipeline Test — الرمية الملتهبة");
    log(`Model: ${MODEL_NAME}  |  API key: ${API_KEY.slice(0, 8)}...  |  Genre: ${GENRE}`);

    const llm = new ChatGoogleGenerativeAI({
        model: MODEL_NAME,
        apiKey: API_KEY,
        temperature: 0.7,
    });

    // ── STEP 1: Breakdown ──────────────────────────────────────────
    section("STEP 1 — Story Breakdown");
    log("Input topic: " + STORY_TOPIC.slice(0, 120) + "...\n");

    const breakdownModel = llm.withStructuredOutput(BreakdownSchema);
    const breakdown = await breakdownModel.invoke(
        `You are a story development expert. The following is the COMPLETE story context for an ${GENRE} anime series.\n` +
        `Use ALL the details provided — characters, special moves, training methods, rivalries, and the climax.\n\n` +
        `FULL STORY CONTEXT:\n${STORY_TOPIC}\n\n` +
        `Create a narrative breakdown for a short ${GENRE} video story. Divide into 3-5 acts. For each act provide:\n` +
        `1. Title - A compelling act title that references specific story events\n` +
        `2. Emotional Hook - The emotional core drawn directly from the story (grief, rivalry, perseverance)\n` +
        `3. Narrative Beat - Key story event referencing named characters (Sami/سامي, Safwan/صفوان, Rajih/راجح, Wael/وائل, Essam/عصام) and named techniques (Blazing Throw, Thunderbolt, Flying Star, Claw Block)\n` +
        `Keep each field concise (1-2 sentences max). Write in English.`
    );

    breakdown.acts.forEach((act, i) => {
        log(`\nAct ${i + 1}: ${act.title}`);
        log(`  Hook : ${act.emotionalHook}`);
        log(`  Beat : ${act.narrativeBeat}`);
    });

    // ── STEP 2: Screenplay ─────────────────────────────────────────
    section("STEP 2 — Screenplay");

    const breakdownText = breakdown.acts
        .map((a, i) => `Act ${i + 1}: ${a.title}\n- Hook: ${a.emotionalHook}\n- Beat: ${a.narrativeBeat}`)
        .join("\n\n");

    // Character roster extracted from the full story context for screenplay fidelity
    const characterRoster = `
Known characters from the story:
- Sami (سامي): protagonist, energetic boy, son of the Blazing Throw legend
- Safwan (صفوان): Sami's best friend, smart team playmaker with precise passing vision
- Rajih (راجح): strict team captain who teaches Sami discipline and teamwork
- Wael (وائل): aristocratic rival from "Al-Shula" team, master of the Thunderbolt Throw (nearly invisible speed)
- Essam (عصام): rival with the Flying Star Throw (curves back unexpectedly)

Named techniques:
- Blazing Throw (الرمية الملتهبة): legendary move requiring finger flame-grip, spiritual focus, full-body leap — ball appears as real fire
- Thunderbolt Throw (رمية الصاعقة): ultra-fast, nearly invisible
- Flying Star Throw (رمية النجم الطائر): deceptive curved trajectory
- Claw Block (صد المخلب): defensive two-handed catch to absorb and stop the ball

Training methods referenced: waterfall climbing, blocking heavy balls on rubber ropes, father's cryptic journal with symbols`;

    const screenplayModel = llm.withStructuredOutput(ScreenplaySchema);
    const screenplay = await screenplayModel.invoke(
        `Write a short screenplay based on this outline:\n\n${breakdownText}\n\n` +
        `${characterRoster}\n\n` +
        `Create 3-8 scenes that USE THE NAMED CHARACTERS AND TECHNIQUES above. For each scene:\n` +
        `1. Heading - Location/time (e.g., "EXT. SCHOOL YARD - DAY")\n` +
        `2. Action - Vivid visual description referencing the specific characters and techniques\n` +
        `3. Dialogue - Character lines ("speaker" must be the character's name only, ≤4 words)\n` +
        `Write in English. Make the special moves visually dramatic and cinematically descriptive.`
    );

    screenplay.scenes.forEach((scene, i) => {
        log(`\nScene ${i + 1}: ${scene.heading}`);
        log(`  Action: ${scene.action.slice(0, 200)}${scene.action.length > 200 ? "..." : ""}`);
        if (scene.dialogue.length > 0) {
            scene.dialogue.slice(0, 3).forEach(d => {
                log(`  ${d.speaker}: "${d.text.slice(0, 100)}"`);
            });
        }
    });

    // ── STEP 3: Characters ─────────────────────────────────────────
    section("STEP 3 — Character Extraction");

    const scenesSummary = screenplay.scenes
        .map(s => `${s.heading}: ${s.action.slice(0, 180)}`)
        .join("\n");
    const speakers = new Set<string>();
    screenplay.scenes.forEach(s => s.dialogue.forEach(d => speakers.add(d.speaker)));

    const charModel = llm.withStructuredOutput(CharacterSchema);
    const charResult = await charModel.invoke(
        `Extract main characters from this screenplay:\n\n${scenesSummary}\n\n` +
        `Characters mentioned in dialogue: ${Array.from(speakers).join(", ")}\n\n` +
        `IMPORTANT: Also include Safwan and Rajih if present in the story — they are key characters:\n` +
        `- Safwan: Sami's best friend, smart tactical playmaker, sharp eyes, athletic build\n` +
        `- Rajih: strict team captain, strong authoritative presence, older teen or young adult\n\n` +
        `For each character provide:\n` +
        `1. Name (use the English name)\n` +
        `2. Role (protagonist/antagonist/supporting)\n` +
        `3. Visual Description - Detailed appearance for image generation (age, ethnicity, hair, clothing, athletic gear, anime-style features)\n` +
        `4. Facial Tags - Exactly 5 comma-separated visual keywords (face shape, hair, expression, clothing item, distinguishing feature)\n` +
        `Write in English. These will be used to generate anime-style character illustrations.`
    );

    charResult.characters.forEach(c => {
        log(`\n${c.name} [${c.role}]`);
        log(`  Visual: ${c.visualDescription}`);
        if (c.facialTags) log(`  Tags  : ${c.facialTags}`);
    });

    // ── STEP 4: Shot Breakdown (first scene only) ──────────────────
    section(`STEP 4 — Shot Breakdown for Scene 1: "${screenplay.scenes[0]?.heading}"`);

    const firstScene = screenplay.scenes[0]!;
    const shotModel = llm.withStructuredOutput(ShotSchema);
    const shotResult = await shotModel.invoke(
        `Break this screenplay scene into 2-5 camera shots for a high-energy ${GENRE} anime-style story.\n` +
        `Style reference: Dodge Danpei / sports anime — dynamic angles, speed lines, dramatic close-ups during special moves.\n\n` +
        `Scene: ${firstScene.heading}\n${firstScene.action}\n\n` +
        `For each shot specify:\n` +
        `- shotNumber\n` +
        `- shotType (Wide/Medium/Close-up/Extreme Close-up/POV)\n` +
        `- cameraAngle (Eye-level/High/Low/Dutch/Bird's-eye)\n` +
        `- movement (static/dolly/pan/tilt/handheld/whip-pan)\n` +
        `- lighting (quality + source, e.g. "harsh overhead gymnasium fluorescents", "dramatic rim light from gym windows")\n` +
        `- emotion (single mood word)\n` +
        `- description: vivid visual of exactly what is in frame — reference character names and technique names explicitly (1-2 sentences).`
    );

    shotResult.shots.forEach(shot => {
        log(`\nShot ${shot.shotNumber} — ${shot.shotType} | ${shot.cameraAngle} | ${shot.movement}`);
        log(`  Lighting : ${shot.lighting}`);
        log(`  Emotion  : ${shot.emotion}`);
        log(`  Desc     : ${shot.description}`);
    });

    // ── STEP 5: Motion Prompt (Feature 2 test) ────────────────────
    section("STEP 5 — Split Motion Prompt (Feature 2)");

    const firstShotDesc = shotResult.shots[0]?.description ?? firstScene.action;
    const motionLLM = new ChatGoogleGenerativeAI({
        model: MODEL_NAME,
        apiKey: API_KEY,
        temperature: 0.4,
    }).withStructuredOutput(MotionSchema);

    const motionResult = await motionLLM.invoke(
        `You are a video director creating motion instructions for animating a still image.\n\n` +
        `IMAGE: ${firstShotDesc}\n\nMOOD: intense, action\n\n` +
        `Generate TWO separate motion descriptions (≤25 words each):\n` +
        `1. camera_motion — Camera ONLY: movement type, direction, speed\n` +
        `2. subject_physics — Environment/subject ONLY: dust, motion blur, cloth, crowd particles\n` +
        `Use present continuous tense.`
    );

    log(`\nCamera motion : ${motionResult.camera_motion}`);
    log(`Subject physics: ${motionResult.subject_physics}`);
    log(`Combined       : ${motionResult.camera_motion}. ${motionResult.subject_physics}`);

    // ── STEP 6: Voiceover Script Generation ────────────────────────
    section("STEP 6 — Voiceover Script Generation");

    const voiceoverLLM = new ChatGoogleGenerativeAI({
        model: MODEL_NAME,
        apiKey: API_KEY,
        temperature: 0.6,
    }).withStructuredOutput(VoiceoverTestSchema);

    const sceneDescriptions = screenplay.scenes.map((s, i) => {
        const dialogueText = s.dialogue.length > 0
            ? `\nDialogue: ${s.dialogue.map((d: { speaker: string; text: string }) => `${d.speaker}: "${d.text}"`).join(' | ')}`
            : '';
        return `Scene ${i + 1} [id: scene_${i}]:\nLocation: ${s.heading}\nAction: ${s.action}${dialogueText}`;
    }).join('\n\n');

    const voiceoverResult = await voiceoverLLM.invoke(
        `You are a voiceover scriptwriter. Rewrite these screenplay action descriptions into narration scripts optimized for spoken delivery.\n\n` +
        `SCREENPLAY SCENES:\n${sceneDescriptions}\n\n` +
        `RULES:\n` +
        `1. Convert visual/camera directions into evocative spoken narration\n` +
        `2. Use sensory language: sounds, textures, temperature, movement\n` +
        `3. Keep roughly the same length as the original action text (±20%)\n` +
        `4. Do NOT include character dialogue — only the narrator's voiceover\n` +
        `5. Do NOT include scene headings, metadata labels, or markdown formatting\n\n` +
        `DELIVERY MARKERS — Insert these where appropriate:\n` +
        `- [pause: beat] — After a dramatic reveal or scene transition\n` +
        `- [pause: long] — Before a climactic moment\n` +
        `- [emphasis]key phrase[/emphasis] — On emotionally charged words\n` +
        `- [rising-tension]text[/rising-tension] — When intensity builds\n` +
        `- [slow]text[/slow] — For solemn or awe-inspiring moments\n` +
        `- [whisper]text[/whisper] — For secrets or danger\n` +
        `- [breath] — Before a long emotional passage\n\n` +
        `Return one voiceover script per scene, preserving the scene IDs exactly.`
    );

    voiceoverResult.voiceovers.forEach((vo, i) => {
        log(`\nScene ${i + 1} [${vo.sceneId}]:`);
        log(`  Original : ${screenplay.scenes[i]?.action.slice(0, 120)}...`);
        log(`  Voiceover: ${vo.script}`);
        // Count delivery markers
        const markers = vo.script.match(/\[(pause|emphasis|whisper|rising-tension|slow|breath)[^\]]*\]/g) || [];
        log(`  Markers  : ${markers.length} (${markers.map(m => m.match(/\[([^\s:]+)/)?.[1]).join(', ')})`);
    });

    // ── STEP 7: Persona Negatives (Feature 1 test) ────────────────
    section("STEP 7 — Persona Negative Constraints (Feature 1)");

    // Inline the persona data to avoid @/ imports
    const STORY_ACTION_NEGATIVES = [
        "static symmetrical portrait compositions",
        "slow meditative camera movement",
        "muted desaturated low-energy color palettes",
        "ambiguous spatial geography hiding action",
        "emotionally quiet contemplative scenes without kinetic energy",
    ];

    log(`\nGenre: ${GENRE}  →  Persona: story_action`);
    log("Negative constraints injected into image style guide avoid list:");
    STORY_ACTION_NEGATIVES.forEach((n, i) => log(`  ${i + 1}. ${n}`));

    // Build a sample avoid list like buildImageStyleGuide() would
    const DEFAULT_NEGATIVES = [
        "text", "watermark", "blurry", "low quality", "distorted", "noisy",
        "overexposed", "underexposed", "duplicate subjects", "cloned faces",
    ];
    const resolvedAvoid = [...new Set([...DEFAULT_NEGATIVES, ...STORY_ACTION_NEGATIVES])];
    log(`\nFull resolved avoid list (${resolvedAvoid.length} items):`);
    resolvedAvoid.forEach((a, i) => log(`  ${i + 1}. ${a}`));

    // ── SUMMARY ───────────────────────────────────────────────────
    section("PIPELINE SUMMARY");
    log(`✓ Breakdown      : ${breakdown.acts.length} acts`);
    log(`✓ Screenplay     : ${screenplay.scenes.length} scenes`);
    log(`✓ Characters     : ${charResult.characters.length} characters`);
    log(`✓ Shots (scene 1): ${shotResult.shots.length} shots`);
    log(`✓ Motion prompt  : camera="${motionResult.camera_motion.slice(0, 50)}..."`);
    log(`✓ Voiceover      : ${voiceoverResult.voiceovers.length} scripts with delivery markers`);
    log(`✓ Persona negatives (story_action): ${STORY_ACTION_NEGATIVES.length} constraints`);
    log("\nAll steps completed successfully.");

    flush();
    console.log(`\n📄 Output saved to: story-pipeline-output.md`);
}

main().catch(err => {
    log(`\nPipeline failed: ${err}`);
    flush();
    console.error("\nPipeline failed:", err);
    process.exit(1);
});
````

## File: vitest.config.ts
````typescript
import { defineConfig } from 'vitest/config';
import path from 'path';

export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    include: ['__tests__/**/*.{test,spec}.{ts,tsx}'],
  },
  resolve: {
    alias: {
      '@studio/shared/src': path.resolve(__dirname, 'packages/shared/src'),
      '@shared': path.resolve(__dirname, 'packages/shared/src'),
    },
  },
});
````

## File: scripts/test-ffmpeg-export.ts
````typescript
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import { execSync } from 'child_process';

/**
 * FFmpeg Export Integration Test (Full-Length with Video Clips)
 *
 * Tests the full server-side FFmpeg pipeline with production content:
 * 1. Extracts JPEG frames from the 5 Veo video clips (~8s each, 24fps)
 * 2. Concatenates all 5 narration audio clips (~70s total)
 * 3. Uploads extracted frames → finalizes → validates output MP4
 *
 * This mirrors the real production pipeline where video clips (not
 * static images) are the primary visual source.
 */

const __dirname = path.dirname(fileURLToPath(import.meta.url));
const ROOT = path.join(__dirname, '..');
const TEST_DATA_DIR = path.join(ROOT, 'public/production_prod_1769364025193_ch60ee8c1');
const SERVER_URL = 'http://localhost:3001';
const FPS = 24;
const BATCH_SIZE = 96;

// ── Helpers ──────────────────────────────────────────────────────

/** Extract all frames from a video clip as JPEG buffers */
function extractFramesFromVideo(videoPath: string, fps: number, tmpDir: string): Buffer[] {
    const prefix = path.join(tmpDir, 'vframe_');
    execSync(
        `ffmpeg -hide_banner -loglevel error -i "${videoPath}" -vf fps=${fps} -q:v 2 "${prefix}%06d.jpg" -y`,
        { maxBuffer: 50 * 1024 * 1024 }
    );

    const frameFiles = fs.readdirSync(tmpDir)
        .filter(f => f.startsWith('vframe_') && f.endsWith('.jpg'))
        .sort();

    const buffers = frameFiles.map(f => fs.readFileSync(path.join(tmpDir, f)));

    // Clean up extracted files
    frameFiles.forEach(f => fs.unlinkSync(path.join(tmpDir, f)));

    return buffers;
}

/** Concatenate WAV files into a single file */
function concatAudioFiles(wavPaths: string[], outPath: string): void {
    const inputs = wavPaths.map(p => `-i "${p}"`).join(' ');
    const filterParts = wavPaths.map((_, i) => `[${i}:a]`).join('');
    const filter = `${filterParts}concat=n=${wavPaths.length}:v=0:a=1[out]`;

    execSync(
        `ffmpeg -hide_banner -loglevel error ${inputs} -filter_complex "${filter}" -map "[out]" -y "${outPath}"`,
        { maxBuffer: 50 * 1024 * 1024 }
    );
}

/** Get audio duration in seconds */
function getAudioDuration(filePath: string): number {
    const out = execSync(
        `ffprobe -v quiet -show_entries format=duration -of csv=p=0 "${filePath}"`,
        { encoding: 'utf-8' }
    );
    return parseFloat(out.trim());
}

/** Check server health */
async function waitForServer(maxRetries = 5): Promise<void> {
    for (let i = 0; i < maxRetries; i++) {
        try {
            const res = await fetch(`${SERVER_URL}/api/health`);
            if (res.ok) return;
        } catch { /* retry */ }
        console.log(`   Waiting for server... (${i + 1}/${maxRetries})`);
        await new Promise(r => setTimeout(r, 2000));
    }
    throw new Error('Server is not reachable at ' + SERVER_URL);
}

function fmtTime(s: number): string {
    const m = Math.floor(s / 60);
    const sec = Math.floor(s % 60);
    return `${m}:${sec.toString().padStart(2, '0')}`;
}

// ── Main Test ────────────────────────────────────────────────────

async function testExport() {
    const t0 = Date.now();
    console.log('🚀 FFmpeg Export Integration Test (Full-Length + Video Clips)');
    console.log(`📂 Test data: ${TEST_DATA_DIR}\n`);

    await waitForServer();
    console.log('✅ Server is healthy\n');

    // Temp workspace
    const tmpDir = path.join(ROOT, 'temp', '_test_export');
    if (fs.existsSync(tmpDir)) fs.rmSync(tmpDir, { recursive: true });
    fs.mkdirSync(tmpDir, { recursive: true });

    // 1. Concatenate all narration audio
    console.log('🔊 Preparing audio...');
    const audioFiles = fs.readdirSync(path.join(TEST_DATA_DIR, 'audio'))
        .filter(f => f.startsWith('narration_scene-') && f.endsWith('.wav'))
        .sort()
        .map(f => path.join(TEST_DATA_DIR, 'audio', f));

    const mergedAudioPath = path.join(tmpDir, 'merged_narration.wav');
    concatAudioFiles(audioFiles, mergedAudioPath);

    const audioDuration = getAudioDuration(mergedAudioPath);
    const totalFrames = Math.ceil(audioDuration * FPS);
    console.log(`   ${audioFiles.length} clips → ${fmtTime(audioDuration)} (${audioDuration.toFixed(1)}s)`);
    console.log(`   Total frames needed: ${totalFrames} @ ${FPS} FPS\n`);

    // 2. Extract frames from each Veo video clip
    console.log('🎥 Extracting frames from video clips...');
    const videoFiles = fs.readdirSync(path.join(TEST_DATA_DIR, 'video_clips'))
        .filter(f => f.endsWith('.mp4'))
        .sort();

    const sceneFrames: Buffer[][] = [];
    let totalExtracted = 0;

    for (const vid of videoFiles) {
        const vidPath = path.join(TEST_DATA_DIR, 'video_clips', vid);
        const frames = extractFramesFromVideo(vidPath, FPS, tmpDir);
        sceneFrames.push(frames);
        totalExtracted += frames.length;
        console.log(`   ✓ ${vid} → ${frames.length} frames (${(frames.reduce((s, b) => s + b.length, 0) / 1024 / 1024).toFixed(1)} MB)`);
    }
    console.log(`   ${totalExtracted} total frames extracted from ${videoFiles.length} clips\n`);

    // 3. Build the full frame list by distributing scenes evenly across the timeline
    //    Each scene gets an equal share of the total duration.
    //    If a scene's video is shorter, loop its frames; if longer, truncate.
    const framesPerScene = Math.ceil(totalFrames / sceneFrames.length);
    const allFrames: { buffer: Buffer; name: string }[] = [];

    for (let i = 0; i < totalFrames; i++) {
        const sceneIdx = Math.min(Math.floor(i / framesPerScene), sceneFrames.length - 1);
        const sceneData = sceneFrames[sceneIdx]!;
        const localFrame = (i - sceneIdx * framesPerScene) % sceneData.length;
        allFrames.push({
            buffer: sceneData[localFrame]!,
            name: `frame${i.toString().padStart(6, '0')}.jpg`,
        });
    }
    console.log(`📦 ${allFrames.length} frames assembled (${sceneFrames.length} scenes × ~${framesPerScene} frames each)\n`);

    // 4. Init session
    console.log('📡 Initializing export session...');
    const audioBlob = new Blob([fs.readFileSync(mergedAudioPath)], { type: 'audio/wav' });
    const initForm = new FormData();
    initForm.append('audio', audioBlob, 'audio.mp3');
    initForm.append('fps', String(FPS));
    initForm.append('totalFrames', String(totalFrames));

    const initRes = await fetch(`${SERVER_URL}/api/export/init`, {
        method: 'POST',
        body: initForm,
    });
    if (!initRes.ok) throw new Error(`Init failed: ${initRes.status} - ${await initRes.text()}`);

    const { sessionId, jobId } = await initRes.json();
    console.log(`   Session: ${sessionId}  Job: ${jobId}\n`);

    // 5. Upload frames in batches
    const totalBatches = Math.ceil(totalFrames / BATCH_SIZE);
    console.log(`📤 Uploading ${totalFrames} frames in ${totalBatches} batches...`);
    const uploadStart = Date.now();

    for (let b = 0; b < totalBatches; b++) {
        const chunk = new FormData();
        const start = b * BATCH_SIZE;
        const end = Math.min(start + BATCH_SIZE, totalFrames);

        for (let i = start; i < end; i++) {
            const f = allFrames[i]!;
            chunk.append('frames', new Blob([new Uint8Array(f.buffer)], { type: 'image/jpeg' }), f.name);
        }

        const res = await fetch(`${SERVER_URL}/api/export/chunk?sessionId=${sessionId}`, {
            method: 'POST',
            body: chunk,
        });
        if (!res.ok) throw new Error(`Chunk upload batch ${b + 1}/${totalBatches} failed`);

        const pct = Math.round(((b + 1) / totalBatches) * 100);
        const bar = '█'.repeat(Math.floor(pct / 5)) + '░'.repeat(20 - Math.floor(pct / 5));
        process.stdout.write(`\r   [${bar}] ${pct}% (batch ${b + 1}/${totalBatches})`);
    }

    const uploadTime = ((Date.now() - uploadStart) / 1000).toFixed(1);
    console.log(`\n   Upload complete in ${uploadTime}s\n`);

    // 6. Finalize (sync encode)
    console.log('🎬 Finalizing (sync encode)...');
    const encodeStart = Date.now();

    const finalRes = await fetch(`${SERVER_URL}/api/export/finalize`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ sessionId, fps: FPS, totalFrames, sync: true }),
    });

    if (!finalRes.ok) {
        const errBody = await finalRes.text();
        throw new Error(`Finalize failed (${finalRes.status}): ${errBody}`);
    }

    const videoBlob = await finalRes.arrayBuffer();
    const encodeTime = ((Date.now() - encodeStart) / 1000).toFixed(1);
    const sizeMB = (videoBlob.byteLength / 1024 / 1024).toFixed(2);

    console.log(`   Encode completed in ${encodeTime}s`);
    console.log(`   Output size: ${sizeMB} MB`);

    // 7. Validate
    const errors: string[] = [];

    if (videoBlob.byteLength < 10 * 1024) {
        errors.push(`Output too small (${videoBlob.byteLength} bytes)`);
    }

    const header = new Uint8Array(videoBlob.slice(0, 12));
    const ftyp = String.fromCharCode(header[4]!, header[5]!, header[6]!, header[7]!);
    if (ftyp !== 'ftyp') {
        errors.push(`Invalid MP4 header: expected 'ftyp', got '${ftyp}'`);
    }

    // Save for inspection
    const outputPath = path.join(tmpDir, 'test_output.mp4');
    fs.writeFileSync(outputPath, Buffer.from(videoBlob));
    console.log(`   Saved to: ${outputPath}`);

    // ffprobe validation
    try {
        const probeOut = execSync(
            `ffprobe -v quiet -show_entries format=duration,size,bit_rate -show_entries stream=codec_name,width,height,r_frame_rate -of json "${outputPath}"`,
            { encoding: 'utf-8' }
        );
        const probe = JSON.parse(probeOut);
        const fmt = probe.format;
        const vStream = probe.streams?.find((s: any) => s.codec_name === 'h264');
        const aStream = probe.streams?.find((s: any) => s.codec_name === 'aac');

        console.log(`\n   📋 ffprobe validation:`);
        console.log(`      Duration:   ${parseFloat(fmt.duration).toFixed(1)}s (expected ~${audioDuration.toFixed(1)}s)`);
        console.log(`      Bitrate:    ${(parseInt(fmt.bit_rate) / 1000).toFixed(0)} kbps`);
        console.log(`      Video:      ${vStream?.codec_name} ${vStream?.width}x${vStream?.height} @ ${vStream?.r_frame_rate}`);
        console.log(`      Audio:      ${aStream?.codec_name}`);

        const outDuration = parseFloat(fmt.duration);
        if (Math.abs(outDuration - audioDuration) > 2) {
            errors.push(`Duration mismatch: expected ~${audioDuration.toFixed(1)}s, got ${outDuration.toFixed(1)}s`);
        }
        if (!vStream) errors.push('No H.264 video stream found');
        if (!aStream) errors.push('No AAC audio stream found');
    } catch (e) {
        errors.push(`ffprobe validation failed: ${e}`);
    }

    // 8. Stats
    const statsRes = await fetch(`${SERVER_URL}/api/export/stats`);
    const stats = await statsRes.json();

    // 9. Report
    const totalTime = ((Date.now() - t0) / 1000).toFixed(1);
    console.log('\n' + '═'.repeat(60));
    if (errors.length === 0) {
        console.log('✅ ALL CHECKS PASSED');
        console.log(`   • Source: ${videoFiles.length} Veo clips + ${audioFiles.length} narration clips`);
        console.log(`   • Pipeline: init → ${totalBatches} chunk batches → sync finalize`);
        console.log(`   • Encoder: ${stats.encoder.selected} (hardware: ${stats.encoder.isHardware})`);
        console.log(`   • Frames: ${totalFrames} @ ${FPS} FPS → ${fmtTime(audioDuration)} video`);
        console.log(`   • Output: ${sizeMB} MB valid MP4`);
        console.log(`   • Timing: extract ${((Date.now() - t0) / 1000 - parseFloat(uploadTime) - parseFloat(encodeTime)).toFixed(1)}s | upload ${uploadTime}s | encode ${encodeTime}s | total ${totalTime}s`);
    } else {
        console.error('❌ TEST FAILED:');
        errors.forEach(e => console.error(`   • ${e}`));
        process.exit(1);
    }
    console.log('═'.repeat(60));
}

testExport().catch(err => {
    console.error('\n❌ Test failed:', err.message || err);
    process.exit(1);
});
````
